{
  "status": "success",
  "functions_generated": 2,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 16052,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: civil_rights_rhetoric_comparison\nDescription: Statistical analysis experiment\nGenerated: 2025-09-02T16:07:12.873967+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived metrics for the Cohesive Flourishing Framework v10.0.\n\n    This function implements the formulas specified in the CFF v10.0 framework document.\n    It computes tension indices, the strategic contradiction index, and the three \n    salience-weighted cohesion indices. This is a necessary preprocessing step before\n    conducting comparative analysis.\n\n    Methodology:\n    - The function takes a pandas DataFrame containing the raw and salience scores for the\n      ten primary dimensions.\n    - It calculates the five tension indices based on the formula:\n      Tension = min(Score_A, Score_B) * |Salience_A - Salience_B|\n    - The Strategic Contradiction Index is the mean of the five tension indices.\n    - The three Cohesion Indices (Descriptive, Motivational, Full) are calculated using\n      salience-weighted scores, normalized by the sum of the relevant salience scores\n      to keep the index within a -1.0 to +1.0 range. An epsilon of 0.001 is added\n      to the denominator to prevent division by zero.\n    - All calculations use the exact column names specified in the data structure.\n\n    Args:\n        data (pd.DataFrame): A DataFrame with columns for raw and salience scores for\n                             each of the 10 CFF dimensions.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pd.DataFrame: The input DataFrame with added columns for each derived metric,\n                      or None if the input data is invalid or missing required columns.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # Ensure all required columns are present\n        required_cols = [\n            'tribal_dominance_raw', 'tribal_dominance_salience',\n            'individual_dignity_raw', 'individual_dignity_salience',\n            'fear_raw', 'fear_salience', 'hope_raw', 'hope_salience',\n            'envy_raw', 'envy_salience', 'compersion_raw', 'compersion_salience',\n            'enmity_raw', 'enmity_salience', 'amity_raw', 'amity_salience',\n            'fragmentative_goals_raw', 'fragmentative_goals_salience',\n            'cohesive_goals_raw', 'cohesive_goals_salience'\n        ]\n        if not all(col in data.columns for col in required_cols):\n            # As a conservative practitioner, I cannot proceed without the correct data.\n            return None\n\n        df = data.copy()\n\n        # --- Tension Indices ---\n        df['identity_tension'] = np.minimum(df['tribal_dominance_raw'], df['individual_dignity_raw']) * \\\n                                 np.abs(df['tribal_dominance_salience'] - df['individual_dignity_salience'])\n        \n        df['emotional_tension'] = np.minimum(df['fear_raw'], df['hope_raw']) * \\\n                                  np.abs(df['fear_salience'] - df['hope_salience'])\n\n        df['success_tension'] = np.minimum(df['envy_raw'], df['compersion_raw']) * \\\n                                np.abs(df['envy_salience'] - df['compersion_salience'])\n\n        df['relational_tension'] = np.minimum(df['enmity_raw'], df['amity_raw']) * \\\n                                   np.abs(df['enmity_salience'] - df['amity_salience'])\n\n        df['goal_tension'] = np.minimum(df['fragmentative_goals_raw'], df['cohesive_goals_raw']) * \\\n                             np.abs(df['fragmentative_goals_salience'] - df['cohesive_goals_salience'])\n\n        # --- Strategic Contradiction Index ---\n        tension_cols = ['identity_tension', 'emotional_tension', 'success_tension', 'relational_tension', 'goal_tension']\n        df['strategic_contradiction_index'] = df[tension_cols].mean(axis=1)\n\n        # --- Intermediate Cohesion Components ---\n        df['identity_cohesion_component'] = (df['individual_dignity_raw'] * df['individual_dignity_salience']) - \\\n                                             (df['tribal_dominance_raw'] * df['tribal_dominance_salience'])\n        \n        df['emotional_cohesion_component'] = (df['hope_raw'] * df['hope_salience']) - \\\n                                              (df['fear_raw'] * df['fear_salience'])\n\n        df['success_cohesion_component'] = (df['compersion_raw'] * df['compersion_salience']) - \\\n                                            (df['envy_raw'] * df['envy_salience'])\n\n        df['relational_cohesion_component'] = (df['amity_raw'] * df['amity_salience']) - \\\n                                               (df['enmity_raw'] * df['enmity_salience'])\n\n        df['goal_cohesion_component'] = (df['cohesive_goals_raw'] * df['cohesive_goals_salience']) - \\\n                                         (df['fragmentative_goals_raw'] * df['fragmentative_goals_salience'])\n\n        # --- Salience Totals for Normalization ---\n        df['descriptive_salience_total'] = df['hope_salience'] + df['fear_salience'] + \\\n                                           df['compersion_salience'] + df['envy_salience'] + \\\n                                           df['amity_salience'] + df['enmity_salience']\n\n        df['motivational_salience_total'] = df['descriptive_salience_total'] + \\\n                                            df['cohesive_goals_salience'] + df['fragmentative_goals_salience']\n\n        df['full_salience_total'] = df['motivational_salience_total'] + \\\n                                    df['individual_dignity_salience'] + df['tribal_dominance_salience']\n\n        # --- Final Salience-Weighted Cohesion Indices ---\n        epsilon = 0.001\n        \n        # Descriptive Cohesion Index\n        numerator = df['emotional_cohesion_component'] + df['success_cohesion_component'] + df['relational_cohesion_component']\n        denominator = df['descriptive_salience_total'] + epsilon\n        df['descriptive_cohesion_index'] = numerator / denominator\n\n        # Motivational Cohesion Index\n        numerator = df['emotional_cohesion_component'] + df['success_cohesion_component'] + \\\n                    df['relational_cohesion_component'] + df['goal_cohesion_component']\n        denominator = df['motivational_salience_total'] + epsilon\n        df['motivational_cohesion_index'] = numerator / denominator\n\n        # Full Cohesion Index\n        numerator = df['identity_cohesion_component'] + df['emotional_cohesion_component'] + \\\n                    df['success_cohesion_component'] + df['relational_cohesion_component'] + \\\n                    df['goal_cohesion_component']\n        denominator = df['full_salience_total'] + epsilon\n        df['full_cohesion_index'] = numerator / denominator\n        \n        # Clamp results to the theoretical [-1, 1] range to handle potential floating point inaccuracies\n        cohesion_indices = ['descriptive_cohesion_index', 'motivational_cohesion_index', 'full_cohesion_index']\n        for col in cohesion_indices:\n            df[col] = df[col].clip(-1.0, 1.0)\n\n        return df\n\n    except Exception:\n        # In case of any unexpected error, return None to signify failure.\n        return None\n\ndef generate_speaker_comparison_descriptives(data, **kwargs):\n    \"\"\"\n    Generates a descriptive statistical comparison between speakers.\n\n    This function directly addresses the experiment's research questions by providing a\n    side-by-side comparison of the rhetorical strategies of Malcolm X and Martin\n    Luther King Jr.\n\n    Statistical Methodology:\n    - STATISTICAL CONSERVATISM NOTE: The sample size for this experiment (N=2, n=1 per\n      group) is critically insufficient for any form of inferential statistical\n      testing (e.g., t-tests). Attempting such tests would violate fundamental\n      statistical principles and produce meaningless results.\n    - Therefore, this function adheres to a strictly descriptive approach, which is the\n      only methodologically sound option.\n    - It calculates all derived metrics from the CFF v10.0 framework.\n    - It identifies speakers based on document filenames, as no formal corpus manifest\n      was provided.\n    - It presents the scores for each speaker across all raw, salience, and derived\n      metrics. This allows for a direct, evidence-based comparison that aligns with\n      the hypotheses without making unwarranted statistical inferences.\n\n    Args:\n        data (pd.DataFrame): A DataFrame containing the raw analysis data, including\n                             the 'document_name' column and all dimensional scores.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the descriptive comparison, with metrics\n                      as rows and speakers as columns. Returns None if data is\n                      insufficient or speakers cannot be identified.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty or 'document_name' not in data.columns:\n            return None\n\n        # Step 1: Calculate all derived metrics using the dedicated function.\n        # This ensures all necessary variables for comparison are present.\n        full_data = calculate_derived_metrics(data)\n        if full_data is None:\n            # Calculation failed, cannot proceed.\n            return None\n\n        # Step 2: Identify speakers based on document names.\n        # This mapping is created directly as per the system specification.\n        full_data['speaker'] = np.nan\n        full_data.loc[full_data['document_name'].str.contains('malcom_x', case=False, na=False), 'speaker'] = 'Malcolm X'\n        full_data.loc[full_data['document_name'].str.contains('mlk', case=False, na=False), 'speaker'] = 'Martin Luther King Jr.'\n\n        # Filter out any documents that do not match the speakers of interest.\n        analysis_df = full_data.dropna(subset=['speaker'])\n        \n        if analysis_df.empty or analysis_df['speaker'].nunique() < 2:\n            # Cannot perform a comparison without at least two distinct speakers.\n            return None\n\n        # Step 3: Generate descriptive statistics.\n        # With n=1 per group, mean() simply returns the values for each speaker.\n        # This is the most rigorous approach for this sample size.\n        descriptive_stats = analysis_df.groupby('speaker').mean()\n\n        # Step 4: Format the output for clear comparison.\n        # Transposing makes it easier to compare speakers column-wise for each metric.\n        comparison_table = descriptive_stats.transpose()\n        \n        # Reorder columns for logical presentation if both speakers are present\n        if 'Malcolm X' in comparison_table.columns and 'Martin Luther King Jr.' in comparison_table.columns:\n            comparison_table = comparison_table[['Malcolm X', 'Martin Luther King Jr.']]\n\n        return comparison_table\n\n    except Exception:\n        # A catch-all to ensure the function is robust and returns None on failure.\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}