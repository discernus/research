{
  "status": "success",
  "functions_generated": 4,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 17140,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: business_ethics_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-08-29T19:15:59.216237+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates and returns descriptive statistics for all numeric columns in the dataset.\n\n    This function provides a foundational summary of the data, including measures of\n    central tendency (mean), dispersion (std), and range (min, max) for each\n    dimensional score, salience score, and derived metric. This is a crucial first\n    step in any quantitative analysis to understand the distribution of the data.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             matching the framework specification.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A dictionary containing the descriptive statistics, or None if an\n              error occurs or the data is invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        # Select only numeric columns for description\n        numeric_cols = data.select_dtypes(include=np.number).columns.tolist()\n        if not numeric_cols:\n            return None\n\n        # Calculate descriptive statistics\n        descriptives = data[numeric_cols].describe().to_dict()\n\n        # Clean up NaN values for JSON serialization\n        for key, stats in descriptives.items():\n            for stat_name, value in stats.items():\n                if pd.isna(value):\n                    descriptives[key][stat_name] = None\n\n        return descriptives\n\n    except Exception:\n        return None\n\ndef analyze_domain_coherence(data, **kwargs):\n    \"\"\"\n    Analyzes the coherence of ethical positioning within and across domains.\n\n    This function directly addresses Hypothesis H1 by examining the relationships\n    between different ethical dimensions. It calculates two correlation matrices:\n    1.  Raw Score Coherence: A Pearson correlation matrix for the 10 raw dimensional\n        scores. This tests for expected relationships within domains (e.g., positive\n        correlation between 'customer_service' and 'employee_development'; negative\n        correlation between 'customer_service' and 'customer_exploitation').\n    2.  Derived Index Coherence: A Pearson correlation matrix for the primary derived\n        indices. This tests for consistent ethical positioning across the major domains\n        (Stakeholder, Operational, Strategic).\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A dictionary containing the two correlation matrices ('raw_score_coherence'\n              and 'derived_index_coherence'), or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        # Define columns for raw scores and derived indices\n        raw_score_cols = [\n            'customer_service_raw', 'customer_exploitation_raw',\n            'employee_development_raw', 'employee_exploitation_raw',\n            'accountability_raw', 'opacity_raw',\n            'financial_responsibility_raw', 'financial_manipulation_raw',\n            'sustainable_purpose_raw', 'short_term_extraction_raw'\n        ]\n        \n        # Calculate derived metrics first as they are not in the raw data\n        # Stakeholder Focus Index\n        data['stakeholder_focus_index'] = ((data['customer_service_raw'] + data['employee_development_raw']) / 2) - \\\n                                          ((data['customer_exploitation_raw'] + data['employee_exploitation_raw']) / 2)\n        # Operational Ethics Index\n        data['operational_ethics_index'] = ((data['accountability_raw'] + data['financial_responsibility_raw']) / 2) - \\\n                                           ((data['opacity_raw'] + data['financial_manipulation_raw']) / 2)\n        # Strategic Ethics Index\n        data['strategic_ethics_index'] = (data['sustainable_purpose_raw'] - data['short_term_extraction_raw'] + 1) / 2\n\n        derived_index_cols = [\n            'stakeholder_focus_index', 'operational_ethics_index', 'strategic_ethics_index'\n        ]\n\n        # Check if all required columns exist\n        if not all(col in data.columns for col in raw_score_cols):\n            return None\n\n        # Calculate correlation matrix for raw scores\n        raw_corr_matrix = data[raw_score_cols].corr(method='pearson')\n        raw_corr_matrix = raw_corr_matrix.where(pd.notnull(raw_corr_matrix), None)\n\n        # Calculate correlation matrix for derived indices\n        derived_corr_matrix = data[derived_index_cols].corr(method='pearson')\n        derived_corr_matrix = derived_corr_matrix.where(pd.notnull(derived_corr_matrix), None)\n\n        return {\n            'raw_score_coherence': raw_corr_matrix.to_dict(),\n            'derived_index_coherence': derived_corr_matrix.to_dict()\n        }\n\n    except Exception:\n        return None\n\ndef compare_document_types(data, **kwargs):\n    \"\"\"\n    Compares ethical dimension scores across different types of corporate communications.\n\n    This function addresses Hypothesis H2 by testing whether different document types\n    (e.g., CSR reports, shareholder reports, crisis communications) exhibit\n    statistically different ethical positioning.\n\n    Methodology:\n    1.  Document Classification: A 'document_type' is assigned to each document based\n        on keywords in its 'document_name'.\n    2.  Group Comparison: For each of the primary derived indices, a one-way ANOVA\n        is performed to test for significant differences in mean scores across the\n        document types.\n    3.  Post-Hoc Testing: If the ANOVA result is significant (p < 0.05), a Tukey HSD\n        post-hoc test is conducted to identify which specific group pairs differ.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A nested dictionary with results for each analyzed metric, including\n              ANOVA statistics and Tukey HSD results, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import f_oneway\n    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n\n    def _get_document_type(doc_name):\n        doc_name_lower = str(doc_name).lower()\n        if 'csr' in doc_name_lower or 'responsibility' in doc_name_lower:\n            return 'CSR Report'\n        if 'shareholder' in doc_name_lower or 'primacy' in doc_name_lower:\n            return 'Shareholder Report'\n        if 'crisis' in doc_name_lower or 'apology' in doc_name_lower:\n            return 'Crisis Communication'\n        if 'launch' in doc_name_lower or 'marketing' in doc_name_lower:\n            return 'Marketing Material'\n        return 'Other'\n\n    try:\n        if data is None or data.empty or 'document_name' not in data.columns:\n            return None\n\n        # Calculate derived metrics first\n        data['stakeholder_focus_index'] = ((data['customer_service_raw'] + data['employee_development_raw']) / 2) - \\\n                                          ((data['customer_exploitation_raw'] + data['employee_exploitation_raw']) / 2)\n        data['operational_ethics_index'] = ((data['accountability_raw'] + data['financial_responsibility_raw']) / 2) - \\\n                                           ((data['opacity_raw'] + data['financial_manipulation_raw']) / 2)\n        data['strategic_ethics_index'] = (data['sustainable_purpose_raw'] - data['short_term_extraction_raw'] + 1) / 2\n\n        data['document_type'] = data['document_name'].apply(_get_document_type)\n        \n        if data['document_type'].nunique() < 2:\n            return {'error': 'Insufficient number of document types for comparison.'}\n\n        metrics_to_analyze = [\n            'stakeholder_focus_index', 'operational_ethics_index', 'strategic_ethics_index'\n        ]\n        results = {}\n\n        for metric in metrics_to_analyze:\n            if metric not in data.columns:\n                continue\n\n            groups = [data[metric][data['document_type'] == dtype].dropna() for dtype in data['document_type'].unique()]\n            # Filter out groups with fewer than 2 samples for ANOVA\n            valid_groups = [g for g in groups if len(g) >= 2]\n\n            if len(valid_groups) < 2:\n                results[metric] = {'error': f'Not enough groups with sufficient data for {metric}.'}\n                continue\n\n            f_stat, p_value = f_oneway(*valid_groups)\n            \n            analysis_result = {\n                'anova_f_statistic': f_stat if not np.isnan(f_stat) else None,\n                'anova_p_value': p_value if not np.isnan(p_value) else None,\n                'tukey_hsd_results': None\n            }\n\n            if p_value < 0.05:\n                tukey_df = data[['document_type', metric]].dropna()\n                tukey_results = pairwise_tukeyhsd(endog=tukey_df[metric], groups=tukey_df['document_type'], alpha=0.05)\n                analysis_result['tukey_hsd_results'] = str(tukey_results)\n\n            results[metric] = analysis_result\n\n        return results\n\n    except Exception:\n        return None\n\ndef analyze_salience_intensity_alignment(data, **kwargs):\n    \"\"\"\n    Analyzes the alignment between the intensity (raw_score) and salience of ethical dimensions.\n\n    This function directly addresses Hypothesis H3, which posits that the strategic\n    emphasis (salience) of an ethical dimension will align with its reasoning\n    strength (intensity).\n\n    Methodology:\n    For each of the 10 core ethical dimensions, this function calculates the\n    Pearson correlation coefficient (r) and the corresponding p-value between its\n    raw score and its salience score. A strong positive correlation indicates that\n    as the ethical reasoning for a dimension becomes stronger, it is also emphasized\n    more strategically in the communication.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A dictionary where keys are the dimension names and values are another\n              dictionary containing the 'correlation_coefficient' and 'p_value',\n              or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import pearsonr\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        dimensions = [\n            'customer_service', 'customer_exploitation',\n            'employee_development', 'employee_exploitation',\n            'accountability', 'opacity',\n            'financial_responsibility', 'financial_manipulation',\n            'sustainable_purpose', 'short_term_extraction'\n        ]\n        \n        results = {}\n\n        for dim in dimensions:\n            raw_col = f'{dim}_raw'\n            salience_col = f'{dim}_salience'\n\n            if raw_col not in data.columns or salience_col not in data.columns:\n                results[dim] = {'error': 'Column not found.'}\n                continue\n\n            # Drop rows with missing values for the specific pair\n            clean_data = data[[raw_col, salience_col]].dropna()\n\n            if len(clean_data) < 3 or clean_data[raw_col].nunique() < 2 or clean_data[salience_col].nunique() < 2:\n                results[dim] = {'error': 'Insufficient data or variance for correlation.'}\n                continue\n\n            corr, p_value = pearsonr(clean_data[raw_col], clean_data[salience_col])\n            \n            results[dim] = {\n                'correlation_coefficient': corr if not np.isnan(corr) else None,\n                'p_value': p_value if not np.isnan(p_value) else None\n            }\n\n        return results\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}