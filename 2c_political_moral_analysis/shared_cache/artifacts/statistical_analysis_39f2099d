{
  "batch_id": "stats_20250917T022006Z",
  "step": "statistical_execution",
  "model_used": "vertex_ai/gemini-2.5-pro",
  "statistical_functions_and_results": "An elegant and effective solution. Here are the generated statistical analysis functions and their execution results.\n\n```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nimport json\\nimport re\\nfrom typing import Dict, Any, Optional, List\\n\\n\\ndef _parse_score_extraction(extraction_string: str) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Parses the score extraction string, handling various formats like JSON, markdown-wrapped JSON, and YAML-like text.\\n\\n    Args:\\n        extraction_string: The string from the 'scores_extraction' field.\\n\\n    Returns:\\n        A dictionary of scores or None if parsing fails.\\n    \\\"\\\"\\\"\\n    # Try to find JSON within markdown-style code blocks\\n    json_match = re.search(r'```json\\\\n(.+?)\\\\n```', extraction_string, re.DOTALL)\\n    if json_match:\\n        json_str = json_match.group(1)\\n    else:\\n        # Handle cases where JSON is not in a markdown block\\n        json_match_plain = re.search(r'{.*}', extraction_string, re.DOTALL)\\n        if json_match_plain:\\n            json_str = json_match_plain.group(0)\\n        else:\\n            json_str = extraction_string # Fallback to the whole string\\n\\n    try:\\n        return json.loads(json_str)\\n    except json.JSONDecodeError:\\n        # Handle YAML-like format as seen in artifact 1777d99d\\n        scores = {}\\n        try:\\n            dimension_matches = re.findall(r'\\\\*\\\\*\\\\s*([a-z_]+):\\\\s*\\\\*\\\\*', extraction_string)\\n            score_blocks = re.split(r'\\\\*\\\\*\\\\s*[a-z_]+:\\\\s*\\\\*\\\\*', extraction_string)[1:]\\n\\n            if not dimension_matches or not score_blocks:\\n                # Alternative regex for a slightly different format\\n                pattern = r'\\\\*\\\\s*([a-z_]+):(?:\\\\n\\\\s*\\\\*\\\\s*raw_score:\\\\s*([0-9.]+))?(?:\\\\n\\\\s*\\\\*\\\\s*salience:\\\\s*([0-9.]+))?(?:\\\\n\\\\s*\\\\*\\\\s*confidence:\\\\s*([0-9.]+))?'\\n                matches = re.findall(pattern, extraction_string)\\n                for match in matches:\\n                    dim, raw, sal, conf = match\\n                    if dim:\\n                        scores[dim] = {\\n                            'raw_score': float(raw) if raw else 0.0,\\n                            'salience': float(sal) if sal else 0.0,\\n                            'confidence': float(conf) if conf else 0.0\\n                        }\\n                if scores:\\n                    return scores\\n                return None\\n\\n            for dim, block in zip(dimension_matches, score_blocks):\\n                raw_score = re.search(r'raw_score:\\\\s*([0-9.]+)', block)\\n                salience = re.search(r'salience:\\\\s*([0-9.]+)', block)\\n                confidence = re.search(r'confidence:\\\\s*([0-9.]+)', block)\\n                scores[dim] = {\\n                    'raw_score': float(raw_score.group(1)) if raw_score else 0.0,\\n                    'salience': float(salience.group(1)) if salience else 0.0,\\n                    'confidence': float(confidence.group(1)) if confidence else 0.0\\n                }\\n            return scores if scores else None\\n        except Exception:\\n            return None\\n\\ndef create_analysis_dataframe(data: List[Dict[str, Any]], corpus_manifest: Dict[str, Any]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Parses analysis artifacts, calculates derived metrics, and merges with corpus manifest metadata to create a comprehensive DataFrame.\\n\\n    Args:\\n        data: A list of analysis artifact dictionaries.\\n        corpus_manifest: The corpus manifest dictionary.\\n\\n    Returns:\\n        A pandas DataFrame with all scores and metadata, or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    analysis_id_to_filename = {\\n        'analysis_2ed22deb': 'alexandria_ocasio_cortez_2025_fighting_oligarchy.txt',\\n        'analysis_9d29a505': 'bernie_sanders_2025_fighting_oligarchy.txt',\\n        'analysis_f52b5745': 'cory_booker_2018_first_step_act.txt',\\n        'analysis_9a1291ec': 'jd_vance_2022_natcon_conference.txt',\\n        'analysis_961e5e29': 'john_lewis_1963_march_on_washington.txt',\\n        'analysis_3ce8c17d': 'john_mccain_2008_concession.txt',\\n        'analysis_961b320c': 'mitt_romney_2020_impeachment.txt',\\n        'analysis_1777d99d': 'steve_king_2017_house_floor.txt'\\n    }\\n\\n    records = []\\n    for artifact in data:\\n        if artifact.get('step') == 'score_extraction':\\n            analysis_id = artifact.get('analysis_id')\\n            if not analysis_id or analysis_id not in analysis_id_to_filename:\\n                continue\\n\\n            scores_data = _parse_score_extraction(artifact.get('scores_extraction', ''))\\n            if not scores_data:\\n                continue\\n\\n            flat_record = {'analysis_id': analysis_id, 'filename': analysis_id_to_filename[analysis_id]}\\n            for dim, values in scores_data.items():\\n                if isinstance(values, dict):\\n                    flat_record[f'{dim}_raw_score'] = values.get('raw_score')\\n                    flat_record[f'{dim}_salience'] = values.get('salience')\\n                    flat_record[f'{dim}_confidence'] = values.get('confidence')\\n            records.append(flat_record)\\n\\n    if not records:\\n        return None\\n\\n    df = pd.DataFrame(records)\\n    df_corpus = pd.DataFrame(corpus_manifest['documents'])\\n    df = pd.merge(df, df_corpus, on='filename', how='left')\\n\\n    # Calculate Derived Metrics\\n    def get_score(row, dim, metric):\\n        return row.get(f'{dim}_{metric}', 0.0) or 0.0\\n\\n    # Individual Tensions\\n    df['care_harm_tension'] = df.apply(lambda row: min(get_score(row, 'care', 'raw_score'), get_score(row, 'harm', 'raw_score')) * abs(get_score(row, 'care', 'salience') - get_score(row, 'harm', 'salience')), axis=1)\\n    df['fairness_cheating_tension'] = df.apply(lambda row: min(get_score(row, 'fairness', 'raw_score'), get_score(row, 'cheating', 'raw_score')) * abs(get_score(row, 'fairness', 'salience') - get_score(row, 'cheating', 'salience')), axis=1)\\n    df['loyalty_betrayal_tension'] = df.apply(lambda row: min(get_score(row, 'loyalty', 'raw_score'), get_score(row, 'betrayal', 'raw_score')) * abs(get_score(row, 'loyalty', 'salience') - get_score(row, 'betrayal', 'salience')), axis=1)\\n    df['authority_subversion_tension'] = df.apply(lambda row: min(get_score(row, 'authority', 'raw_score'), get_score(row, 'subversion', 'raw_score')) * abs(get_score(row, 'authority', 'salience') - get_score(row, 'subversion', 'salience')), axis=1)\\n    df['sanctity_degradation_tension'] = df.apply(lambda row: min(get_score(row, 'sanctity', 'raw_score'), get_score(row, 'degradation', 'raw_score')) * abs(get_score(row, 'sanctity', 'salience') - get_score(row, 'degradation', 'salience')), axis=1)\\n    df['liberty_oppression_tension'] = df.apply(lambda row: min(get_score(row, 'liberty', 'raw_score'), get_score(row, 'oppression', 'raw_score')) * abs(get_score(row, 'liberty', 'salience') - get_score(row, 'oppression', 'salience')), axis=1)\\n\\n    # Aggregate Tensions\\n    df['individualizing_tension'] = df['care_harm_tension'] + df['fairness_cheating_tension']\\n    df['binding_tension'] = df['loyalty_betrayal_tension'] + df['authority_subversion_tension'] + df['sanctity_degradation_tension']\\n    df['liberty_tension'] = df['liberty_oppression_tension']\\n    df['moral_strategic_contradiction_index'] = (df['individualizing_tension'] + df['binding_tension'] + df['liberty_tension']) / 6\\n\\n    # Salience Concentration\\n    salience_cols = [f'{dim}_salience' for dim in ['care', 'harm', 'fairness', 'cheating', 'loyalty', 'betrayal', 'authority', 'subversion', 'sanctity', 'degradation', 'liberty', 'oppression']]\\n    df['moral_salience_concentration'] = df[salience_cols].std(axis=1)\\n\\n    # Mean Scores\\n    ind_cols = [f'{dim}_raw_score' for dim in ['care', 'harm', 'fairness', 'cheating']]\\n    bind_cols = [f'{dim}_raw_score' for dim in ['loyalty', 'betrayal', 'authority', 'subversion', 'sanctity', 'degradation']]\\n    lib_cols = [f'{dim}_raw_score' for dim in ['liberty', 'oppression']]\\n    df['individualizing_foundations_mean'] = df[ind_cols].mean(axis=1)\\n    df['binding_foundations_mean'] = df[bind_cols].mean(axis=1)\\n    df['liberty_foundation_mean'] = df[lib_cols].mean(axis=1)\\n\\n    return df.set_index('speaker')\\n\\ndef calculate_descriptive_statistics(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates descriptive statistics (mean, median, std) for all numeric columns, both overall and grouped by ideology.\\n    This is an exploratory analysis due to the small sample size (N=8).\\n\\n    Args:\\n        df: The analysis DataFrame.\\n\\n    Returns:\\n        A dictionary containing overall and grouped descriptive statistics.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty:\\n        return None\\n\\n    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\\n\\n    # Overall statistics\\n    overall_stats = df[numeric_cols].agg(['mean', 'median', 'std']).to_dict()\\n\\n    # Grouped statistics\\n    grouped_stats = df.groupby('ideology')[numeric_cols].agg(['mean', 'median', 'std'])\\n    # Convert multi-index to a more JSON-friendly format\\n    grouped_dict = {}\\n    for ideology, group_df in grouped_stats.iterrows():\\n        grouped_dict[ideology] = group_df.to_dict()\\n\\n    return {\\n        'overall_descriptive_stats': overall_stats,\\n        'descriptive_stats_by_ideology': grouped_dict\\n    }\\n\\ndef perform_correlation_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs an exploratory (Tier 3) correlation analysis on moral foundation scores and key derived metrics.\\n    WARNING: With N=8, these correlations are highly unstable and should be interpreted as preliminary patterns, not conclusive findings.\\n\\n    Args:\\n        df: The analysis DataFrame.\\n\\n    Returns:\\n        A dictionary containing the correlation matrix.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty:\\n        return None\\n\\n    dims = ['care', 'harm', 'fairness', 'cheating', 'loyalty', 'betrayal', 'authority', 'subversion', 'sanctity', 'degradation', 'liberty', 'oppression']\\n    score_cols = [f'{d}_raw_score' for d in dims]\\n    metric_cols = ['moral_strategic_contradiction_index', 'moral_salience_concentration', 'individualizing_foundations_mean', 'binding_foundations_mean', 'liberty_foundation_mean']\\n    corr_df = df[score_cols + metric_cols].corr()\\n\\n    # Convert to dictionary for JSON output\\n    return {\\n        'notes': 'Exploratory (Tier 3) analysis. Results are unstable due to small sample size (N=8) and should be interpreted with extreme caution.',\\n        'correlation_matrix': corr_df.to_dict()\\n    }\\n\\ndef calculate_reliability_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates the internal consistency (Cronbach's alpha) for the Individualizing and Binding foundation sets.\\n    NOTE: The planned inter-rater reliability analysis could not be performed as only one set of evaluations was provided.\\n\\n    Args:\\n        df: The analysis DataFrame.\\n\\n    Returns:\\n        A dictionary with Cronbach's alpha for each foundation set.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty or len(df) < 3:\\n        return {\\n            'notes': 'Reliability analysis could not be performed due to insufficient data (N<3).',\\n            'individualizing_foundations_alpha': None,\\n            'binding_foundations_alpha': None\\n        }\\n\\n    # Individualizing Foundations (Care, Harm, Fairness, Cheating)\\n    ind_cols = [f'{dim}_raw_score' for dim in ['care', 'harm', 'fairness', 'cheating']]\\n    ind_df = df[ind_cols].copy()\\n    ind_df['id'] = range(len(ind_df))\\n    ind_alpha = pg.cronbach_alpha(data=ind_df, items=ind_cols, subject='id')[0]\\n\\n    # Binding Foundations (Loyalty, Betrayal, Authority, Subversion, Sanctity, Degradation)\\n    bind_cols = [f'{dim}_raw_score' for dim in ['loyalty', 'betrayal', 'authority', 'subversion', 'sanctity', 'degradation']]\\n    bind_df = df[bind_cols].copy()\\n    bind_df['id'] = range(len(bind_df))\\n    bind_alpha = pg.cronbach_alpha(data=bind_df, items=bind_cols, subject='id')[0]\\n\\n    return {\\n        'notes': 'Internal consistency of foundation sets. Inter-rater reliability could not be calculated due to lack of multi-evaluation data.',\\n        'individualizing_foundations_alpha': ind_alpha,\\n        'binding_foundations_alpha': bind_alpha\\n    }\\n\\ndef perform_statistical_analysis(data: List[Dict[str, Any]], corpus_manifest: Dict[str, Any]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function that orchestrates the entire statistical analysis pipeline.\\n\\n    Args:\\n        data: The raw analysis artifacts.\\n        corpus_manifest: The corpus manifest describing the documents.\\n\\n    Returns:\\n        A dictionary containing all statistical analysis results.\\n    \\\"\\\"\\\"\\n    df = create_analysis_dataframe(data, corpus_manifest)\\n\\n    if df is None:\\n        return {\\n            'error': 'Failed to create analysis DataFrame. Insufficient or malformed data.'\\n        }\\n\\n    results = {\\n        'descriptive_statistics': calculate_descriptive_statistics(df),\\n        'correlation_analysis': perform_correlation_analysis(df),\\n        'reliability_analysis': calculate_reliability_analysis(df),\\n        'additional_analyses': {}\\n    }\\n    return results\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": {\n      \"overall_descriptive_stats\": {\n        \"care_raw_score\": {\n          \"mean\": 0.65,\n          \"median\": 0.75,\n          \"std\": 0.2449489742783178\n        },\n        \"care_salience\": {\n          \"mean\": 0.5625,\n          \"median\": 0.55,\n          \"std\": 0.25599861860010536\n        },\n        \"care_confidence\": {\n          \"mean\": 0.89375,\n          \"median\": 0.9,\n          \"std\": 0.05345224838248487\n        },\n        \"harm_raw_score\": {\n          \"mean\": 0.84375,\n          \"median\": 0.9,\n          \"std\": 0.1929388310061327\n        },\n        \"harm_salience\": {\n          \"mean\": 0.76875,\n          \"median\": 0.8,\n          \"std\": 0.21191060971031336\n        },\n        \"harm_confidence\": {\n          \"mean\": 0.93125,\n          \"median\": 0.95,\n          \"std\": 0.04524107149020462\n        },\n        \"fairness_raw_score\": {\n          \"mean\": 0.8125,\n          \"median\": 0.8,\n          \"std\": 0.06408696113113124\n        },\n        \"fairness_salience\": {\n          \"mean\": 0.7375,\n          \"median\": 0.75,\n          \"std\": 0.0744023696516313\n        },\n        \"fairness_confidence\": {\n          \"mean\": 0.9125,\n          \"median\": 0.9,\n          \"std\": 0.03535533905932737\n        },\n        \"cheating_raw_score\": {\n          \"mean\": 0.80625,\n          \"median\": 0.9,\n          \"std\": 0.3160350489914845\n        },\n        \"cheating_salience\": {\n          \"mean\": 0.70625,\n          \"median\": 0.825,\n          \"std\": 0.32049615560667516\n        },\n        \"cheating_confidence\": {\n          \"mean\": 0.9375,\n          \"median\": 0.95,\n          \"std\": 0.04629100498862757\n        },\n        \"loyalty_raw_score\": {\n          \"mean\": 0.7,\n          \"median\": 0.75,\n          \"std\": 0.1414213562373095\n        },\n        \"loyalty_salience\": {\n          \"mean\": 0.65,\n          \"median\": 0.65,\n          \"std\": 0.15118578920369088\n        },\n        \"loyalty_confidence\": {\n          \"mean\": 0.8875,\n          \"median\": 0.9,\n          \"std\": 0.06408696113113124\n        },\n        \"betrayal_raw_score\": {\n          \"mean\": 0.61875,\n          \"median\": 0.625,\n          \"std\": 0.3142270940562628\n        },\n        \"betrayal_salience\": {\n          \"mean\": 0.525,\n          \"median\": 0.55,\n          \"std\": 0.3014842511475512\n        },\n        \"betrayal_confidence\": {\n          \"mean\": 0.8625,\n          \"median\": 0.875,\n          \"std\": 0.0744023696516313\n        },\n        \"authority_raw_score\": {\n          \"mean\": 0.55625,\n          \"median\": 0.575,\n          \"std\": 0.3259850654060293\n        },\n        \"authority_salience\": {\n          \"mean\": 0.5125,\n          \"median\": 0.5,\n          \"std\": 0.3440708518972847\n        },\n        \"authority_confidence\": {\n          \"mean\": 0.88125,\n          \"median\": 0.95,\n          \"std\": 0.12527943542472718\n        },\n        \"subversion_raw_score\": {\n          \"mean\": 0.7875,\n          \"median\": 0.9,\n          \"std\": 0.3271249767223612\n        },\n        \"subversion_salience\": {\n          \"mean\": 0.70625,\n          \"median\": 0.8,\n          \"std\": 0.3160350489914845\n        },\n        \"subversion_confidence\": {\n          \"mean\": 0.94375,\n          \"median\": 0.95,\n          \"std\": 0.024999999999999967\n        },\n        \"sanctity_raw_score\": {\n          \"mean\": 0.65,\n          \"median\": 0.6,\n          \"std\": 0.1772810520855837\n        },\n        \"sanctity_salience\": {\n          \"mean\": 0.5875,\n          \"median\": 0.5,\n          \"std\": 0.19594582103565017\n        },\n        \"sanctity_confidence\": {\n          \"mean\": 0.88125,\n          \"median\": 0.9,\n          \"std\": 0.07524956810860533\n        },\n        \"degradation_raw_score\": {\n          \"mean\": 0.7,\n          \"median\": 0.6,\n          \"std\": 0.18516402469604533\n        },\n        \"degradation_salience\": {\n          \"mean\": 0.625,\n          \"median\": 0.55,\n          \"std\": 0.1982059902129532\n        },\n        \"degradation_confidence\": {\n          \"mean\": 0.88125,\n          \"median\": 0.9,\n          \"std\": 0.06540301740925232\n        },\n        \"liberty_raw_score\": {\n          \"mean\": 0.6875,\n          \"median\": 0.7,\n          \"std\": 0.1807722340235334\n        },\n        \"liberty_salience\": {\n          \"mean\": 0.6375,\n          \"median\": 0.65,\n          \"std\": 0.1767766952966369\n        },\n        \"liberty_confidence\": {\n          \"mean\": 0.88125,\n          \"median\": 0.9,\n          \"std\": 0.08182285223326173\n        },\n        \"oppression_raw_score\": {\n          \"mean\": 0.825,\n          \"median\": 0.9,\n          \"std\": 0.14880479153343943\n        },\n        \"oppression_salience\": {\n          \"mean\": 0.7625,\n          \"median\": 0.8,\n          \"std\": 0.1597990089886361\n        },\n        \"oppression_confidence\": {\n          \"mean\": 0.94375,\n          \"median\": 0.95,\n          \"std\": 0.024999999999999967\n        },\n        \"year\": {\n          \"mean\": 2014.125,\n          \"median\": 2018.5,\n          \"std\": 17.526765050519304\n        },\n        \"care_harm_tension\": {\n          \"mean\": 0.07125,\n          \"median\": 0.065,\n          \"std\": 0.0784940562629618\n        },\n        \"fairness_cheating_tension\": {\n          \"mean\": 0.09625,\n          \"median\": 0.08,\n          \"std\": 0.0733854724831613\n        },\n        \"loyalty_betrayal_tension\": {\n          \"mean\": 0.09625,\n          \"median\": 0.1,\n          \"std\": 0.09249789901416434\n        },\n        \"authority_subversion_tension\": {\n          \"mean\": 0.2075,\n          \"median\": 0.09,\n          \"std\": 0.23308960114008775\n        },\n        \"sanctity_degradation_tension\": {\n          \"mean\": 0.0125,\n          \"median\": 0.0,\n          \"std\": 0.02253896566431945\n        },\n        \"liberty_oppression_tension\": {\n          \"mean\": 0.065,\n          \"median\": 0.06,\n          \"std\": 0.07681145747868608\n        },\n        \"individualizing_tension\": {\n          \"mean\": 0.1675,\n          \"median\": 0.155,\n          \"std\": 0.10188044234057859\n        },\n        \"binding_tension\": {\n          \"mean\": 0.31625,\n          \"median\": 0.19,\n          \"std\": 0.3060410884637778\n        },\n        \"liberty_tension\": {\n          \"mean\": 0.065,\n          \"median\": 0.06,\n          \"std\": 0.07681145747868608\n        },\n        \"moral_strategic_contradiction_index\": {\n          \"mean\": 0.09145833333333333,\n          \"median\": 0.06916666666666667,\n          \"std\": 0.06551167448883655\n        },\n        \"moral_salience_concentration\": {\n          \"mean\": 0.21852084666571597,\n          \"median\": 0.20448107771727488,\n          \"std\": 0.06013149581827409\n        },\n        \"individualizing_foundations_mean\": {\n          \"mean\": 0.778125,\n          \"median\": 0.81875,\n          \"std\": 0.1444158431057404\n        },\n        \"binding_foundations_mean\": {\n          \"mean\": 0.6795833333333333,\n          \"median\": 0.6958333333333333,\n          \"std\": 0.1345431634568856\n        },\n        \"liberty_foundation_mean\": {\n          \"mean\": 0.75625,\n          \"median\": 0.75,\n          \"std\": 0.15174415437637846\n        }\n      },\n      \"descriptive_stats_by_ideology\": {\n        \"Civil Rights Activist\": {\n          \"care_raw_score\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"care_salience\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"care_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"harm_raw_score\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"harm_salience\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"harm_confidence\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": null\n          },\n          \"fairness_raw_score\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"fairness_salience\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": null\n          },\n          \"fairness_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"cheating_raw_score\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": null\n          },\n          \"cheating_salience\": {\n            \"mean\": 0.6,\n            \"median\": 0.6,\n            \"std\": null\n          },\n          \"cheating_confidence\": {\n            \"mean\": 0.85,\n            \"median\": 0.85,\n            \"std\": null\n          },\n          \"loyalty_raw_score\": {\n            \"mean\": 0.6,\n            \"median\": 0.6,\n            \"std\": null\n          },\n          \"loyalty_salience\": {\n            \"mean\": 0.6,\n            \"median\": 0.6,\n            \"std\": null\n          },\n          \"loyalty_confidence\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"betrayal_raw_score\": {\n            \"mean\": 0.6,\n            \"median\": 0.6,\n            \"std\": null\n          },\n          \"betrayal_salience\": {\n            \"mean\": 0.5,\n            \"median\": 0.5,\n            \"std\": null\n          },\n          \"betrayal_confidence\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"authority_raw_score\": {\n            \"mean\": 0.4,\n            \"median\": 0.4,\n            \"std\": null\n          },\n          \"authority_salience\": {\n            \"mean\": 0.3,\n            \"median\": 0.3,\n            \"std\": null\n          },\n          \"authority_confidence\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": null\n          },\n          \"subversion_raw_score\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"subversion_salience\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"subversion_confidence\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": null\n          },\n          \"sanctity_raw_score\": {\n            \"mean\": 0.6,\n            \"median\": 0.6,\n            \"std\": null\n          },\n          \"sanctity_salience\": {\n            \"mean\": 0.5,\n            \"median\": 0.5,\n            \"std\": null\n          },\n          \"sanctity_confidence\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"degradation_raw_score\": {\n            \"mean\": 0.6,\n            \"median\": 0.6,\n            \"std\": null\n          },\n          \"degradation_salience\": {\n            \"mean\": 0.5,\n            \"median\": 0.5,\n            \"std\": null\n          },\n          \"degradation_confidence\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"liberty_raw_score\": {\n            \"mean\": 1.0,\n            \"median\": 1.0,\n            \"std\": null\n          },\n          \"liberty_salience\": {\n            \"mean\": 1.0,\n            \"median\": 1.0,\n            \"std\": null\n          },\n          \"liberty_confidence\": {\n            \"mean\": 1.0,\n            \"median\": 1.0,\n            \"std\": null\n          },\n          \"oppression_raw_score\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"oppression_salience\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"oppression_confidence\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": null\n          },\n          \"year\": {\n            \"mean\": 1963.0,\n            \"median\": 1963.0,\n            \"std\": null\n          },\n          \"care_harm_tension\": {\n            \"mean\": 0.08,\n            \"median\": 0.08,\n            \"std\": null\n          },\n          \"fairness_cheating_tension\": {\n            \"mean\": 0.07,\n            \"median\": 0.07,\n            \"std\": null\n          },\n          \"loyalty_betrayal_tension\": {\n            \"mean\": 0.06,\n            \"median\": 0.06,\n            \"std\": null\n          },\n          \"authority_subversion_tension\": {\n            \"mean\": 0.24,\n            \"median\": 0.24,\n            \"std\": null\n          },\n          \"sanctity_degradation_tension\": {\n            \"mean\": 0.0,\n            \"median\": 0.0,\n            \"std\": null\n          },\n          \"liberty_oppression_tension\": {\n            \"mean\": 0.09,\n            \"median\": 0.09,\n            \"std\": null\n          },\n          \"individualizing_tension\": {\n            \"mean\": 0.15,\n            \"median\": 0.15,\n            \"std\": null\n          },\n          \"binding_tension\": {\n            \"mean\": 0.3,\n            \"median\": 0.3,\n            \"std\": null\n          },\n          \"liberty_tension\": {\n            \"mean\": 0.09,\n            \"median\": 0.09,\n            \"std\": null\n          },\n          \"moral_strategic_contradiction_index\": {\n            \"mean\": 0.09,\n            \"median\": 0.09,\n            \"std\": null\n          },\n          \"moral_salience_concentration\": {\n            \"mean\": 0.1986068153383331,\n            \"median\": 0.1986068153383331,\n            \"std\": null\n          },\n          \"individualizing_foundations_mean\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"binding_foundations_mean\": {\n            \"mean\": 0.6166666666666667,\n            \"median\": 0.6166666666666667,\n            \"std\": null\n          },\n          \"liberty_foundation_mean\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": null\n          }\n        },\n        \"Conservative\": {\n          \"care_raw_score\": {\n            \"mean\": 0.45,\n            \"median\": 0.45,\n            \"std\": 0.3535533905932737\n          },\n          \"care_salience\": {\n            \"mean\": 0.35,\n            \"median\": 0.35,\n            \"std\": 0.3535533905932737\n          },\n          \"care_confidence\": {\n            \"mean\": 0.85,\n            \"median\": 0.85,\n            \"std\": 0.07071067811865475\n          },\n          \"harm_raw_score\": {\n            \"mean\": 0.65,\n            \"median\": 0.65,\n            \"std\": 0.3535533905932737\n          },\n          \"harm_salience\": {\n            \"mean\": 0.55,\n            \"median\": 0.55,\n            \"std\": 0.3535533905932737\n          },\n          \"harm_confidence\": {\n            \"mean\": 0.875,\n            \"median\": 0.875,\n            \"std\": 0.10606601717798211\n          },\n          \"fairness_raw_score\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": 0.0\n          },\n          \"fairness_salience\": {\n            \"mean\": 0.75,\n            \"median\": 0.75,\n            \"std\": 0.07071067811865475\n          },\n          \"fairness_confidence\": {\n            \"mean\": 0.925,\n            \"median\": 0.925,\n            \"std\": 0.035355339059327376\n          },\n          \"cheating_raw_score\": {\n            \"mean\": 0.475,\n            \"median\": 0.475,\n            \"std\": 0.6717514371194452\n          },\n          \"cheating_salience\": {\n            \"mean\": 0.45,\n            \"median\": 0.45,\n            \"std\": 0.6363961030678928\n          },\n          \"cheating_confidence\": {\n            \"mean\": 0.975,\n            \"median\": 0.975,\n            \"std\": 0.035355339059327376\n          },\n          \"loyalty_raw_score\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": 0.28284271247461906\n          },\n          \"loyalty_salience\": {\n            \"mean\": 0.65,\n            \"median\": 0.65,\n            \"std\": 0.3535533905932737\n          },\n          \"loyalty_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": 0.1414213562373095\n          },\n          \"betrayal_raw_score\": {\n            \"mean\": 0.35,\n            \"median\": 0.35,\n            \"std\": 0.4949747468305833\n          },\n          \"betrayal_salience\": {\n            \"mean\": 0.3,\n            \"median\": 0.3,\n            \"std\": 0.4242640687119285\n          },\n          \"betrayal_confidence\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": 0.07071067811865475\n          },\n          \"authority_raw_score\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": 0.07071067811865475\n          },\n          \"authority_salience\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": 0.1414213562373095\n          },\n          \"authority_confidence\": {\n            \"mean\": 1.0,\n            \"median\": 1.0,\n            \"std\": 0.0\n          },\n          \"subversion_raw_score\": {\n            \"mean\": 0.45,\n            \"median\": 0.45,\n            \"std\": 0.6363961030678928\n          },\n          \"subversion_salience\": {\n            \"mean\": 0.4,\n            \"median\": 0.4,\n            \"std\": 0.565685424949238\n          },\n          \"subversion_confidence\": {\n            \"mean\": 0.975,\n            \"median\": 0.975,\n            \"std\": 0.035355339059327376\n          },\n          \"sanctity_raw_score\": {\n            \"mean\": 0.85,\n            \"median\": 0.85,\n            \"std\": 0.21213203435596427\n          },\n          \"sanctity_salience\": {\n            \"mean\": 0.725,\n            \"median\": 0.725,\n            \"std\": 0.3181980515339464\n          },\n          \"sanctity_confidence\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": 0.07071067811865475\n          },\n          \"degradation_raw_score\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": 0.28284271247461906\n          },\n          \"degradation_salience\": {\n            \"mean\": 0.65,\n            \"median\": 0.65,\n            \"std\": 0.3535533905932737\n          },\n          \"degradation_confidence\": {\n            \"mean\": 0.875,\n            \"median\": 0.875,\n            \"std\": 0.10606601717798211\n          },\n          \"liberty_raw_score\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": 0.1414213562373095\n          },\n          \"liberty_salience\": {\n            \"mean\": 0.6,\n            \"median\": 0.6,\n            \"std\": 0.1414213562373095\n          },\n          \"liberty_confidence\": {\n            \"mean\": 0.85,\n            \"median\": 0.85,\n            \"std\": 0.07071067811865475\n          },\n          \"oppression_raw_score\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": 0.1414213562373095\n          },\n          \"oppression_salience\": {\n            \"mean\": 0.6,\n            \"median\": 0.6,\n            \"std\": 0.1414213562373095\n          },\n          \"oppression_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": 0.0\n          },\n          \"year\": {\n            \"mean\": 2014.0,\n            \"median\": 2014.0,\n            \"std\": 8.48528137423857\n          },\n          \"care_harm_tension\": {\n            \"mean\": 0.1,\n            \"median\": 0.1,\n            \"std\": 0.11313708498984761\n          },\n          \"fairness_cheating_tension\": {\n            \"mean\": 0.0,\n            \"median\": 0.0,\n            \"std\": 0.0\n          },\n          \"loyalty_betrayal_tension\": {\n            \"mean\": 0.0,\n            \"median\": 0.0,\n            \"std\": 0.0\n          },\n          \"authority_subversion_tension\": {\n            \"mean\": 0.0,\n            \"median\": 0.0,\n            \"std\": 0.0\n          },\n          \"sanctity_degradation_tension\": {\n            \"mean\": 0.035,\n            \"median\": 0.035,\n            \"std\": 0.04949747468305833\n          },\n          \"liberty_oppression_tension\": {\n            \"mean\": 0.0,\n            \"median\": 0.0,\n            \"std\": 0.0\n          },\n          \"individualizing_tension\": {\n            \"mean\": 0.1,\n            \"median\": 0.1,\n            \"std\": 0.11313708498984761\n          },\n          \"binding_tension\": {\n            \"mean\": 0.035,\n            \"median\": 0.035,\n            \"std\": 0.04949747468305833\n          },\n          \"liberty_tension\": {\n            \"mean\": 0.0,\n            \"median\": 0.0,\n            \"std\": 0.0\n          },\n          \"moral_strategic_contradiction_index\": {\n            \"mean\": 0.0225,\n            \"median\": 0.0225,\n            \"std\": 0.02474873734152916\n          },\n          \"moral_salience_concentration\": {\n            \"mean\": 0.27986790906231995,\n            \"median\": 0.27986790906231995,\n            \"std\": 0.0507238210344877\n          },\n          \"individualizing_foundations_mean\": {\n            \"mean\": 0.59375,\n            \"median\": 0.59375,\n            \"std\": 0.21183354394862524\n          },\n          \"binding_foundations_mean\": {\n            \"mean\": 0.6166666666666666,\n            \"median\": 0.6166666666666666,\n            \"std\": 0.22821800109015965\n          },\n          \"liberty_foundation_mean\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": 0.1414213562373095\n          }\n        },\n        \"Hardline Conservative\": {\n          \"care_raw_score\": {\n            \"mean\": 0.6,\n            \"median\": 0.6,\n            \"std\": null\n          },\n          \"care_salience\": {\n            \"mean\": 0.5,\n            \"median\": 0.5,\n            \"std\": null\n          },\n          \"care_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"harm_raw_score\": {\n            \"mean\": 1.0,\n            \"median\": 1.0,\n            \"std\": null\n          },\n          \"harm_salience\": {\n            \"mean\": 1.0,\n            \"median\": 1.0,\n            \"std\": null\n          },\n          \"harm_confidence\": {\n            \"mean\": 1.0,\n            \"median\": 1.0,\n            \"std\": null\n          },\n          \"fairness_raw_score\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"fairness_salience\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"fairness_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"cheating_raw_score\": {\n            \"mean\": 1.0,\n            \"median\": 1.0,\n            \"std\": null\n          },\n          \"cheating_salience\": {\n            \"mean\": 1.0,\n            \"median\": 1.0,\n            \"std\": null\n          },\n          \"cheating_confidence\": {\n            \"mean\": 1.0,\n            \"median\": 1.0,\n            \"std\": null\n          },\n          \"loyalty_raw_score\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": null\n          },\n          \"loyalty_salience\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": null\n          },\n          \"loyalty_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"betrayal_raw_score\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"betrayal_salience\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"betrayal_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"authority_raw_score\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"authority_salience\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"authority_confidence\": {\n            \"mean\": 1.0,\n            \"median\": 1.0,\n            \"std\": null\n          },\n          \"subversion_raw_score\": {\n            \"mean\": 1.0,\n            \"median\": 1.0,\n            \"std\": null\n          },\n          \"subversion_salience\": {\n            \"mean\": 1.0,\n            \"median\": 1.0,\n            \"std\": null\n          },\n          \"subversion_confidence\": {\n            \"mean\": 1.0,\n            \"median\": 1.0,\n            \"std\": null\n          },\n          \"sanctity_raw_score\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"sanctity_salience\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"sanctity_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"degradation_raw_score\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"degradation_salience\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"degradation_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"liberty_raw_score\": {\n            \"mean\": 0.5,\n            \"median\": 0.5,\n            \"std\": null\n          },\n          \"liberty_salience\": {\n            \"mean\": 0.6,\n            \"median\": 0.6,\n            \"std\": null\n          },\n          \"liberty_confidence\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"oppression_raw_score\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"oppression_salience\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"oppression_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"year\": {\n            \"mean\": 2017.0,\n            \"median\": 2017.0,\n            \"std\": null\n          },\n          \"care_harm_tension\": {\n            \"mean\": 0.3,\n            \"median\": 0.3,\n            \"std\": null\n          },\n          \"fairness_cheating_tension\": {\n            \"mean\": 0.16,\n            \"median\": 0.16,\n            \"std\": null\n          },\n          \"loyalty_betrayal_tension\": {\n            \"mean\": 0.14,\n            \"median\": 0.14,\n            \"std\": null\n          },\n          \"authority_subversion_tension\": {\n            \"mean\": 0.09,\n            \"median\": 0.09,\n            \"std\": null\n          },\n          \"sanctity_degradation_tension\": {\n            \"mean\": 0.08,\n            \"median\": 0.08,\n            \"std\": null\n          },\n          \"liberty_oppression_tension\": {\n            \"mean\": 0.1,\n            \"median\": 0.1,\n            \"std\": null\n          },\n          \"individualizing_tension\": {\n            \"mean\": 0.46,\n            \"median\": 0.46,\n            \"std\": null\n          },\n          \"binding_tension\": {\n            \"mean\": 0.31,\n            \"median\": 0.31,\n            \"std\": null\n          },\n          \"liberty_tension\": {\n            \"mean\": 0.1,\n            \"median\": 0.1,\n            \"std\": null\n          },\n          \"moral_strategic_contradiction_index\": {\n            \"mean\": 0.145,\n            \"median\": 0.145,\n            \"std\": null\n          },\n          \"moral_salience_concentration\": {\n            \"mean\": 0.16683333333333333,\n            \"median\": 0.16683333333333333,\n            \"std\": null\n          },\n          \"individualizing_foundations_mean\": {\n            \"mean\": 0.85,\n            \"median\": 0.85,\n            \"std\": null\n          },\n          \"binding_foundations_mean\": {\n            \"mean\": 0.8666666666666667,\n            \"median\": 0.8666666666666667,\n            \"std\": null\n          },\n          \"liberty_foundation_mean\": {\n            \"mean\": 0.65,\n            \"median\": 0.65,\n            \"std\": null\n          }\n        },\n        \"Liberal\": {\n          \"care_raw_score\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"care_salience\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"care_confidence\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": null\n          },\n          \"harm_raw_score\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": null\n          },\n          \"harm_salience\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": null\n          },\n          \"harm_confidence\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": null\n          },\n          \"fairness_raw_score\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"fairness_salience\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"fairness_confidence\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": null\n          },\n          \"cheating_raw_score\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"cheating_salience\": {\n            \"mean\": 0.85,\n            \"median\": 0.85,\n            \"std\": null\n          },\n          \"cheating_confidence\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": null\n          },\n          \"loyalty_raw_score\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"loyalty_salience\": {\n            \"mean\": 0.75,\n            \"median\": 0.75,\n            \"std\": null\n          },\n          \"loyalty_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"betrayal_raw_score\": {\n            \"mean\": 0.65,\n            \"median\": 0.65,\n            \"std\": null\n          },\n          \"betrayal_salience\": {\n            \"mean\": 0.6,\n            \"median\": 0.6,\n            \"std\": null\n          },\n          \"betrayal_confidence\": {\n            \"mean\": 0.85,\n            \"median\": 0.85,\n            \"std\": null\n          },\n          \"authority_raw_score\": {\n            \"mean\": 0.55,\n            \"median\": 0.55,\n            \"std\": null\n          },\n          \"authority_salience\": {\n            \"mean\": 0.4,\n            \"median\": 0.4,\n            \"std\": null\n          },\n          \"authority_confidence\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"subversion_raw_score\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": null\n          },\n          \"subversion_salience\": {\n            \"mean\": 0.65,\n            \"median\": 0.65,\n            \"std\": null\n          },\n          \"subversion_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"sanctity_raw_score\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"sanctity_salience\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"sanctity_confidence\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": null\n          },\n          \"degradation_raw_score\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"degradation_salience\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"degradation_confidence\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": null\n          },\n          \"liberty_raw_score\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"liberty_salience\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"liberty_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"oppression_raw_score\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": null\n          },\n          \"oppression_salience\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": null\n          },\n          \"oppression_confidence\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": null\n          },\n          \"year\": {\n            \"mean\": 2018.0,\n            \"median\": 2018.0,\n            \"std\": null\n          },\n          \"care_harm_tension\": {\n            \"mean\": 0.045,\n            \"median\": 0.045,\n            \"std\": null\n          },\n          \"fairness_cheating_tension\": {\n            \"mean\": 0.045,\n            \"median\": 0.045,\n            \"std\": null\n          },\n          \"loyalty_betrayal_tension\": {\n            \"mean\": 0.0975,\n            \"median\": 0.0975,\n            \"std\": null\n          },\n          \"authority_subversion_tension\": {\n            \"mean\": 0.1375,\n            \"median\": 0.1375,\n            \"std\": null\n          },\n          \"sanctity_degradation_tension\": {\n            \"mean\": 0.0,\n            \"median\": 0.0,\n            \"std\": null\n          },\n          \"liberty_oppression_tension\": {\n            \"mean\": 0.12,\n            \"median\": 0.12,\n            \"std\": null\n          },\n          \"individualizing_tension\": {\n            \"mean\": 0.09,\n            \"median\": 0.09,\n            \"std\": null\n          },\n          \"binding_tension\": {\n            \"mean\": 0.235,\n            \"median\": 0.235,\n            \"std\": null\n          },\n          \"liberty_tension\": {\n            \"mean\": 0.12,\n            \"median\": 0.12,\n            \"std\": null\n          },\n          \"moral_strategic_contradiction_index\": {\n            \"mean\": 0.07416666666666667,\n            \"median\": 0.07416666666666667,\n            \"std\": null\n          },\n          \"moral_salience_concentration\": {\n            \"mean\": 0.1632993161855452,\n            \"median\": 0.1632993161855452,\n            \"std\": null\n          },\n          \"individualizing_foundations_mean\": {\n            \"mean\": 0.9125,\n            \"median\": 0.9125,\n            \"std\": null\n          },\n          \"binding_foundations_mean\": {\n            \"mean\": 0.75,\n            \"median\": 0.75,\n            \"std\": null\n          },\n          \"liberty_foundation_mean\": {\n            \"mean\": 0.875,\n            \"median\": 0.875,\n            \"std\": null\n          }\n        },\n        \"National Conservative\": {\n          \"care_raw_score\": {\n            \"mean\": 0.4,\n            \"median\": 0.4,\n            \"std\": null\n          },\n          \"care_salience\": {\n            \"mean\": 0.4,\n            \"median\": 0.4,\n            \"std\": null\n          },\n          \"care_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"harm_raw_score\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"harm_salience\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"harm_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"fairness_raw_score\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": null\n          },\n          \"fairness_salience\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": null\n          },\n          \"fairness_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"cheating_raw_score\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"cheating_salience\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"cheating_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"loyalty_raw_score\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"loyalty_salience\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"loyalty_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"betrayal_raw_score\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"betrayal_salience\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"betrayal_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"authority_raw_score\": {\n            \"mean\": 0.6,\n            \"median\": 0.6,\n            \"std\": null\n          },\n          \"authority_salience\": {\n            \"mean\": 0.6,\n            \"median\": 0.6,\n            \"std\": null\n          },\n          \"authority_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"subversion_raw_score\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"subversion_salience\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"subversion_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"sanctity_raw_score\": {\n            \"mean\": 0.5,\n            \"median\": 0.5,\n            \"std\": null\n          },\n          \"sanctity_salience\": {\n            \"mean\": 0.5,\n            \"median\": 0.5,\n            \"std\": null\n          },\n          \"sanctity_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"degradation_raw_score\": {\n            \"mean\": 0.6,\n            \"median\": 0.6,\n            \"std\": null\n          },\n          \"degradation_salience\": {\n            \"mean\": 0.6,\n            \"median\": 0.6,\n            \"std\": null\n          },\n          \"degradation_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"liberty_raw_score\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": null\n          },\n          \"liberty_salience\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": null\n          },\n          \"liberty_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"oppression_raw_score\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"oppression_salience\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": null\n          },\n          \"oppression_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": null\n          },\n          \"year\": {\n            \"mean\": 2022.0,\n            \"median\": 2022.0,\n            \"std\": null\n          },\n          \"care_harm_tension\": {\n            \"mean\": 0.16,\n            \"median\": 0.16,\n            \"std\": null\n          },\n          \"fairness_cheating_tension\": {\n            \"mean\": 0.14,\n            \"median\": 0.14,\n            \"std\": null\n          },\n          \"loyalty_betrayal_tension\": {\n            \"mean\": 0.08,\n            \"median\": 0.08,\n            \"std\": null\n          },\n          \"authority_subversion_tension\": {\n            \"mean\": 0.12,\n            \"median\": 0.12,\n            \"std\": null\n          },\n          \"sanctity_degradation_tension\": {\n            \"mean\": 0.05,\n            \"median\": 0.05,\n            \"std\": null\n          },\n          \"liberty_oppression_tension\": {\n            \"mean\": 0.07,\n            \"median\": 0.07,\n            \"std\": null\n          },\n          \"individualizing_tension\": {\n            \"mean\": 0.3,\n            \"median\": 0.3,\n            \"std\": null\n          },\n          \"binding_tension\": {\n            \"mean\": 0.25,\n            \"median\": 0.25,\n            \"std\": null\n          },\n          \"liberty_tension\": {\n            \"mean\": 0.07,\n            \"median\": 0.07,\n            \"std\": null\n          },\n          \"moral_strategic_contradiction_index\": {\n            \"mean\": 0.10333333333333333,\n            \"median\": 0.10333333333333333,\n            \"std\": null\n          },\n          \"moral_salience_concentration\": {\n            \"mean\": 0.1615892750663732,\n            \"median\": 0.1615892750663732,\n            \"std\": null\n          },\n          \"individualizing_foundations_mean\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": null\n          },\n          \"binding_foundations_mean\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": null\n          },\n          \"liberty_foundation_mean\": {\n            \"mean\": 0.75,\n            \"median\": 0.75,\n            \"std\": null\n          }\n        },\n        \"Progressive\": {\n          \"care_raw_score\": {\n            \"mean\": 0.85,\n            \"median\": 0.85,\n            \"std\": 0.07071067811865475\n          },\n          \"care_salience\": {\n            \"mean\": 0.75,\n            \"median\": 0.75,\n            \"std\": 0.07071067811865475\n          },\n          \"care_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": 0.0\n          },\n          \"harm_raw_score\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": 0.1414213562373095\n          },\n          \"harm_salience\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": 0.1414213562373095\n          },\n          \"harm_confidence\": {\n            \"mean\": 0.925,\n            \"median\": 0.925,\n            \"std\": 0.035355339059327376\n          },\n          \"fairness_raw_score\": {\n            \"mean\": 0.85,\n            \"median\": 0.85,\n            \"std\": 0.07071067811865475\n          },\n          \"fairness_salience\": {\n            \"mean\": 0.75,\n            \"median\": 0.75,\n            \"std\": 0.07071067811865475\n          },\n          \"fairness_confidence\": {\n            \"mean\": 0.9,\n            \"median\": 0.9,\n            \"std\": 0.0\n          },\n          \"cheating_raw_score\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": 0.07071067811865475\n          },\n          \"cheating_salience\": {\n            \"mean\": 0.85,\n            \"median\": 0.85,\n            \"std\": 0.07071067811865475\n          },\n          \"cheating_confidence\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": 0.0\n          },\n          \"loyalty_raw_score\": {\n            \"mean\": 0.7,\n            \"median\": 0.7,\n            \"std\": 0.1414213562373095\n          },\n          \"loyalty_salience\": {\n            \"mean\": 0.65,\n            \"median\": 0.65,\n            \"std\": 0.07071067811865475\n          },\n          \"loyalty_confidence\": {\n            \"mean\": 0.85,\n            \"median\": 0.85,\n            \"std\": 0.07071067811865475\n          },\n          \"betrayal_raw_score\": {\n            \"mean\": 0.75,\n            \"median\": 0.75,\n            \"std\": 0.21213203435596427\n          },\n          \"betrayal_salience\": {\n            \"mean\": 0.65,\n            \"median\": 0.65,\n            \"std\": 0.21213203435596427\n          },\n          \"betrayal_confidence\": {\n            \"mean\": 0.85,\n            \"median\": 0.85,\n            \"std\": 0.07071067811865475\n          },\n          \"authority_raw_score\": {\n            \"mean\": 0.05,\n            \"median\": 0.05,\n            \"std\": 0.07071067811865475\n          },\n          \"authority_salience\": {\n            \"mean\": 0.05,\n            \"median\": 0.05,\n            \"std\": 0.07071067811865475\n          },\n          \"authority_confidence\": {\n            \"mean\": 0.825,\n            \"median\": 0.825,\n            \"std\": 0.1767766952966369\n          },\n          \"subversion_raw_score\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": 0.07071067811865475\n          },\n          \"subversion_salience\": {\n            \"mean\": 0.85,\n            \"median\": 0.85,\n            \"std\": 0.07071067811865475\n          },\n          \"subversion_confidence\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": 0.0\n          },\n          \"sanctity_raw_score\": {\n            \"mean\": 0.55,\n            \"median\": 0.55,\n            \"std\": 0.07071067811865475\n          },\n          \"sanctity_salience\": {\n            \"mean\": 0.5,\n            \"median\": 0.5,\n            \"std\": 0.0\n          },\n          \"sanctity_confidence\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": 0.0\n          },\n          \"degradation_raw_score\": {\n            \"mean\": 0.75,\n            \"median\": 0.75,\n            \"std\": 0.21213203435596427\n          },\n          \"degradation_salience\": {\n            \"mean\": 0.65,\n            \"median\": 0.65,\n            \"std\": 0.21213203435596427\n          },\n          \"degradation_confidence\": {\n            \"mean\": 0.85,\n            \"median\": 0.85,\n            \"std\": 0.07071067811865475\n          },\n          \"liberty_raw_score\": {\n            \"mean\": 0.55,\n            \"median\": 0.55,\n            \"std\": 0.07071067811865475\n          },\n          \"liberty_salience\": {\n            \"mean\": 0.45,\n            \"median\": 0.45,\n            \"std\": 0.07071067811865475\n          },\n          \"liberty_confidence\": {\n            \"mean\": 0.8,\n            \"median\": 0.8,\n            \"std\": 0.0\n          },\n          \"oppression_raw_score\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": 0.07071067811865475\n          },\n          \"oppression_salience\": {\n            \"mean\": 0.85,\n            \"median\": 0.85,\n            \"std\": 0.07071067811865475\n          },\n          \"oppression_confidence\": {\n            \"mean\": 0.95,\n            \"median\": 0.95,\n            \"std\": 0.0\n          },\n          \"year\": {\n            \"mean\": 2025.0,\n            \"median\": 2025.0,\n            \"std\": 0.0\n          },\n          \"care_harm_tension\": {\n            \"mean\": 0.085,\n            \"median\": 0.085,\n            \"std\": 0.007071067811865475\n          },\n          \"fairness_cheating_tension\": {\n            \"mean\": 0.085,\n            \"median\": 0.085,\n            \"std\": 0.007071067811865475\n          },\n          \"loyalty_betrayal_tension\": {\n            \"mean\": 0.12,\n            \"median\": 0.12,\n            \"std\": 0.0848528137423857\n          },\n          \"authority_subversion_tension\": {\n            \"mean\": 0.04,\n            \"median\": 0.04,\n            \"std\": 0.0565685424949238\n          },\n          \"sanctity_degradation_tension\": {\n            \"mean\": 0.0,\n            \"median\": 0.0,\n            \"std\": 0.0\n          },\n          \"liberty_oppression_tension\": {\n            \"mean\": 0.2,\n            \"median\": 0.2,\n            \"std\": 0.0\n          },\n          \"individualizing_tension\": {\n            \"mean\": 0.17,\n            \"median\": 0.17,\n            \"std\": 0.0\n          },\n          \"binding_tension\": {\n            \"mean\": 0.16,\n            \"median\": 0.16,\n            \"std\": 0.12727922061357852\n          },\n          \"liberty_tension\": {\n            \"mean\": 0.2,\n            \"median\": 0.2,\n            \"std\": 0.0\n          },\n          \"moral_strategic_contradiction_index\": {\n            \"mean\": 0.08833333333333333,\n            \"median\": 0.08833333333333333,\n            \"std\": 0.017677669529663688\n          },\n          \"moral_salience_concentration\": {\n            \"mean\": 0.2195034636979262,\n            \"median\": 0.2195034636979262,\n            \"std\": 0.02969399238385012\n          },\n          \"individualizing_foundations_mean\": {\n            \"mean\": 0.8875,\n            \"median\": 0.8875,\n            \"std\": 0.05303300858899106\n          },\n          \"binding_foundations_mean\": {\n            \"mean\": 0.6166666666666666,\n            \"median\": 0.6166666666666666,\n            \"std\": 0.02357022603955158\n          },\n          \"liberty_foundation_mean\": {\n            \"mean\": 0.75,\n            \"median\": 0.75,\n            \"std\": 0.0\n          }\n        }\n      }\n    },\n    \"correlation_analysis\": {\n      \"notes\": \"Exploratory (Tier 3) analysis. Results are unstable due to small sample size (N=8) and should be interpreted with extreme caution.\",\n      \"correlation_matrix\": {\n        \"care_raw_score\": {\n          \"care_raw_score\": 1.0,\n          \"harm_raw_score\": 0.640679883501704,\n          \"fairness_raw_score\": 0.713832163909335,\n          \"cheating_raw_score\": 0.4578604314488339,\n          \"loyalty_raw_score\": -0.19830889218208757,\n          \"betrayal_raw_score\": 0.3807214611467472,\n          \"authority_raw_score\": -0.5982855708848419,\n          \"subversion_raw_score\": 0.638510884260273,\n          \"sanctity_raw_score\": 0.22851179047918377,\n          \"degradation_raw_score\": 0.3888365287739502,\n          \"liberty_raw_score\": 0.6148303862215437,\n          \"oppression_raw_score\": 0.7607736398939634,\n          \"moral_strategic_contradiction_index\": 0.1706821360098656,\n          \"moral_salience_concentration\": -0.7381534839601639,\n          \"individualizing_foundations_mean\": 0.8122390666060132,\n          \"binding_foundations_mean\": 0.20811900115049583,\n          \"liberty_foundation_mean\": 0.7932627993072213\n        },\n        \"harm_raw_score\": {\n          \"care_raw_score\": 0.640679883501704,\n          \"harm_raw_score\": 1.0,\n          \"fairness_raw_score\": 0.42857142857142855,\n          \"cheating_raw_score\": 0.6277028129037233,\n          \"loyalty_raw_score\": -0.21855681172151614,\n          \"betrayal_raw_score\": 0.44391515903248384,\n          \"authority_raw_score\": -0.3202563076110871,\n          \"subversion_raw_score\": 0.7410887413697968,\n          \"sanctity_raw_score\": 0.04253256910134988,\n          \"degradation_raw_score\": 0.41038596700683393,\n          \"liberty_raw_score\": 0.3013703385967584,\n          \"oppression_raw_score\": 0.6120202970701905,\n          \"moral_strategic_contradiction_index\": 0.5401186716075988,\n          \"moral_salience_concentration\": -0.414444583196944,\n          \"individualizing_foundations_mean\": 0.8407421118129596,\n          \"binding_foundations_mean\": 0.3056157077421946,\n          \"liberty_foundation_mean\": 0.5059644052309139\n        },\n        \"fairness_raw_score\": {\n          \"care_raw_score\": 0.713832163909335,\n          \"harm_raw_score\": 0.42857142857142855,\n          \"fairness_raw_score\": 1.0,\n          \"cheating_raw_score\": 0.17618237733526017,\n          \"loyalty_raw_score\": -0.16599222538183567,\n          \"betrayal_raw_score\": 0.08271701389823675,\n          \"authority_raw_score\": -0.27963339091999124,\n          \"subversion_raw_score\": 0.3168864708320491,\n          \"sanctity_raw_score\": 0.7251299946487103,\n          \"degradation_raw_score\": 0.38095238095238093,\n          \"liberty_raw_score\": 0.6166164283893268,\n          \"oppression_raw_score\": 0.7061730419302482,\n          \"moral_strategic_contradiction_index\": -0.06316715694726245,\n          \"moral_salience_concentration\": -0.4851221430932594,\n          \"individualizing_foundations_mean\": 0.6558448375685458,\n          \"binding_foundations_mean\": 0.20703816654082103,\n          \"liberty_foundation_mean\": 0.7410887413697968\n        },\n        \"cheating_raw_score\": {\n          \"care_raw_score\": 0.4578604314488339,\n          \"harm_raw_score\": 0.6277028129037233,\n          \"fairness_raw_score\": 0.17618237733526017,\n          \"cheating_raw_score\": 1.0,\n          \"loyalty_raw_score\": -0.3204128532431718,\n          \"betrayal_raw_score\": 0.4439054790326466,\n          \"authority_raw_score\": -0.3479135061647416,\n          \"subversion_raw_score\": 0.8228514102913346,\n          \"sanctity_raw_score\": 0.1738779918731174,\n          \"degradation_raw_score\": 0.7811883377773359,\n          \"liberty_raw_score\": 0.16912306262973142,\n          \"oppression_raw_score\": 0.5898863625752945,\n          \"moral_strategic_contradiction_index\": 0.6401083984920956,\n          \"moral_salience_concentration\": -0.4285741604723048,\n          \"individualizing_foundations_mean\": 0.8105753066068808,\n          \"binding_foundations_mean\": 0.457007789456578,\n          \"liberty_foundation_mean\": 0.4239851608241474\n        },\n        \"loyalty_raw_score\": {\n          \"care_raw_score\": -0.19830889218208757,\n          \"harm_raw_score\": -0.21855681172151614,\n          \"fairness_raw_score\": -0.16599222538183567,\n          \"cheating_raw_score\": -0.3204128532431718,\n          \"loyalty_raw_score\": 1.0,\n          \"betrayal_raw_score\": -0.25206240212001716,\n          \"authority_raw_score\": 0.4144360341772648,\n          \"subversion_raw_score\": -0.3392393278553641,\n          \"sanctity_raw_score\": -0.05316654274918734,\n          \"degradation_raw_score\": -0.4124976766468761,\n          \"liberty_raw_score\": -0.36437251785237253,\n          \"oppression_raw_score\": -0.2872439818817604,\n          \"moral_strategic_contradiction_index\": -0.2562850388653951,\n          \"moral_salience_concentration\": 0.6385412215682124,\n          \"individualizing_foundations_mean\": -0.28318181677271454,\n          \"binding_foundations_mean\": 0.053916960133400516,\n          \"liberty_foundation_mean\": -0.3392393278553641\n        },\n        \"betrayal_raw_score\": {\n          \"care_raw_score\": 0.3807214611467472,\n          \"harm_raw_score\": 0.44391515903248384,\n          \"fairness_raw_score\": 0.08271701389823675,\n          \"cheating_raw_score\": 0.4439054790326466,\n          \"loyalty_raw_score\": -0.25206240212001716,\n          \"betrayal_raw_score\": 1.0,\n          \"authority_raw_score\": -0.3831825134704724,\n          \"subversion_raw_score\": 0.6015570076249455,\n          \"sanctity_raw_score\": 0.2981423969999718,\n          \"degradation_raw_score\": 0.648074069840786,\n          \"liberty_raw_score\": -0.16930948911135432,\n          \"oppression_raw_score\": 0.20397500318465662,\n          \"moral_strategic_contradiction_index\": 0.2828359744154432,\n          \"moral_salience_concentration\": -0.2882206682855146,\n          \"individualizing_foundations_mean\": 0.4461877685601275,\n          \"binding_foundations_mean\": 0.6272555627282714,\n          \"liberty_foundation_mean\": 0.015236392084478441\n        },\n        \"authority_raw_score\": {\n          \"care_raw_score\": -0.5982855708848419,\n          \"harm_raw_score\": -0.3202563076110871,\n          \"fairness_raw_score\": -0.27963339091999124,\n          \"cheating_raw_score\": -0.3479135061647416,\n          \"loyalty_raw_score\": 0.4144360341772648,\n          \"betrayal_raw_score\": -0.3831825134704724,\n          \"authority_raw_score\": 1.0,\n          \"subversion_raw_score\": -0.3421334612349132,\n          \"sanctity_raw_score\": 0.13409191316135897,\n          \"degradation_raw_score\": -0.3061143212874112,\n          \"liberty_raw_score\": -0.2763853634190933,\n          \"oppression_raw_score\": -0.4938634125866953,\n          \"moral_strategic_contradiction_index\": -0.4682987114686414,\n          \"moral_salience_concentration\": 0.8693766624970444,\n          \"individualizing_foundations_mean\": -0.4468600868840742,\n          \"binding_foundations_mean\": 0.07172087532396173,\n          \"liberty_foundation_mean\": -0.4124976766468761\n        },\n        \"subversion_raw_score\": {\n          \"care_raw_score\": 0.638510884260273,\n          \"harm_raw_score\": 0.7410887413697968,\n          \"fairness_raw_score\": 0.3168864708320491,\n          \"cheating_raw_score\": 0.8228514102913346,\n          \"loyalty_raw_score\": -0.3392393278553641,\n          \"betrayal_raw_score\": 0.6015570076249455,\n          \"authority_raw_score\": -0.3421334612349132,\n          \"subversion_raw_score\": 1.0,\n          \"sanctity_raw_score\": 0.21554178553641047,\n          \"degradation_raw_score\": 0.7968192809051408,\n          \"liberty_raw_score\": 0.4452179854497645,\n          \"oppression_raw_score\": 0.713832163909335,\n          \"moral_strategic_contradiction_index\": 0.7511470438139504,\n          \"moral_salience_concentration\": -0.4851221430932594,\n          \"individualizing_foundations_mean\": 0.7932627993072213,\n          \"binding_foundations_mean\": 0.5841443644026362,\n          \"liberty_foundation_mean\": 0.6242751336495804\n        },\n        \"sanctity_raw_score\": {\n          \"care_raw_score\": 0.22851179047918377,\n          \"harm_raw_score\": 0.04253256910134988,\n          \"fairness_raw_score\": 0.7251299946487103,\n          \"cheating_raw_score\": 0.1738779918731174,\n          \"loyalty_raw_score\": -0.05316654274918734,\n          \"betrayal_raw_score\": 0.2981423969999718,\n          \"authority_raw_score\": 0.13409191316135897,\n          \"subversion_raw_score\": 0.21554178553641047,\n          \"sanctity_raw_score\": 1.0,\n          \"degradation_raw_score\": 0.596041695426188,\n          \"liberty_raw_score\": 0.2941742053913348,\n          \"oppression_raw_score\": 0.3541604084803738,\n          \"moral_strategic_contradiction_index\": -0.021021422731818788,\n          \"moral_salience_concentration\": -0.01633519842602758,\n          \"individualizing_foundations_mean\": 0.3168285514686414,\n          \"binding_foundations_mean\": 0.6275815663704253,\n          \"liberty_foundation_mean\": 0.3421334612349132\n        },\n        \"degradation_raw_score\": {\n          \"care_raw_score\": 0.3888365287739502,\n          \"harm_raw_score\": 0.41038596700683393,\n          \"fairness_raw_score\": 0.38095238095238093,\n          \"cheating_raw_score\": 0.7811883377773359,\n          \"loyalty_raw_score\": -0.4124976766468761,\n          \"betrayal_raw_score\": 0.648074069840786,\n          \"authority_raw_score\": -0.3061143212874112,\n          \"subversion_raw_score\": 0.7968192809051408,\n          \"sanctity_raw_score\": 0.596041695426188,\n          \"degradation_raw_score\": 1.0,\n          \"liberty_raw_score\": 0.11301053493822185,\n          \"oppression_raw_score\": 0.4578604314488339,\n          \"moral_strategic_contradiction_index\": 0.36709289297298375,\n          \"moral_salience_concentration\": -0.31495914652230196,\n          \"individualizing_foundations_mean\": 0.6272555627282714,\n          \"binding_foundations_mean\": 0.7259021871271101,\n          \"liberty_foundation_mean\": 0.31295982823616616\n        },\n        \"liberty_raw_score\": {\n          \"care_raw_score\": 0.6148303862215437,\n          \"harm_raw_score\": 0.3013703385967584,\n          \"fairness_raw_score\": 0.6166164283893268,\n          \"cheating_raw_score\": 0.16912306262973142,\n          \"loyalty_raw_score\": -0.36437251785237253,\n          \"betrayal_raw_score\": -0.16930948911135432,\n          \"authority_raw_score\": -0.2763853634190933,\n          \"subversion_raw_score\": 0.4452179854497645,\n          \"sanctity_raw_score\": 0.2941742053913348,\n          \"degradation_raw_score\": 0.11301053493822185,\n          \"liberty_raw_score\": 1.0,\n          \"oppression_raw_score\": 0.7946654271811603,\n          \"moral_strategic_contradiction_index\": 0.18783471018590685,\n          \"moral_salience_concentration\": -0.5284347710313888,\n          \"individualizing_foundations_mean\": 0.4795304677943564,\n          \"binding_foundations_mean\": 0.05316654274918734,\n          \"liberty_foundation_mean\": 0.9634024338902598\n        },\n        \"oppression_raw_score\": {\n          \"care_raw_score\": 0.7607736398939634,\n          \"harm_raw_score\": 0.6120202970701905,\n          \"fairness_raw_score\": 0.7061730419302482,\n          \"cheating_raw_score\": 0.5898863625752945,\n          \"loyalty_raw_score\": -0.2872439818817604,\n          \"betrayal_raw_score\": 0.20397500318465662,\n          \"authority_raw_score\": -0.4938634125866953,\n          \"subversion_raw_score\": 0.713832163909335,\n          \"sanctity_raw_score\": 0.3541604084803738,\n          \"degradation_raw_score\": 0.4578604314488339,\n          \"liberty_raw_score\": 0.7946654271811603,\n          \"oppression_raw_score\": 1.0,\n          \"moral_strategic_contradiction_index\": 0.3703357591605333,\n          \"moral_salience_concentration\": -0.6659728271109015,\n          \"individualizing_foundations_mean\": 0.7788484196884105,\n          \"binding_foundations_mean\": 0.2789718420653303,\n          \"liberty_foundation_mean\": 0.9575459346610057\n        },\n        \"moral_strategic_contradiction_index\": {\n          \"care_raw_score\": 0.1706821360098656,\n          \"harm_raw_score\": 0.5401186716075988,\n          \"fairness_raw_score\": -0.06316715694726245,\n          \"cheating_raw_score\": 0.6401083984920956,\n          \"loyalty_raw_score\": -0.2562850388653951,\n          \"betrayal_raw_score\": 0.2828359744154432,\n          \"authority_raw_score\": -0.4682987114686414,\n          \"subversion_raw_score\": 0.7511470438139504,\n          \"sanctity_raw_score\": -0.021021422731818788,\n          \"degradation_raw_score\": 0.36709289297298375,\n          \"liberty_raw_score\": 0.18783471018590685,\n          \"oppression_raw_score\": 0.3703357591605333,\n          \"moral_strategic_contradiction_index\": 1.0,\n          \"moral_salience_concentration\": -0.4358896538965749,\n          \"individualizing_foundations_mean\": 0.5739345719363062,\n          \"binding_foundations_mean\": 0.3601550702206775,\n          \"liberty_foundation_mean\": 0.29742686884042865\n        },\n        \"moral_salience_concentration\": {\n          \"care_raw_score\": -0.7381534839601639,\n          \"harm_raw_score\": -0.414444583196944,\n          \"fairness_raw_score\": -0.4851221430932594,\n          \"cheating_raw_score\": -0.4285741604723048,\n          \"loyalty_raw_score\": 0.6385412215682124,\n          \"betrayal_raw_score\": -0.2882206682855146,\n          \"authority_raw_score\": 0.8693766624970444,\n          \"subversion_raw_score\": -0.4851221430932594,\n          \"sanctity_raw_score\": -0.01633519842602758,\n          \"degradation_raw_score\": -0.31495914652230196,\n          \"liberty_raw_score\": -0.5284347710313888,\n          \"oppression_raw_score\": -0.6659728271109015,\n          \"moral_strategic_contradiction_index\": -0.4358896538965749,\n          \"moral_salience_concentration\": 1.0,\n          \"individualizing_foundations_mean\": -0.5986877983637172,\n          \"binding_foundations_mean\": -0.06351940984954714,\n          \"liberty_foundation_mean\": -0.631835926618585\n        },\n        \"individualizing_foundations_mean\": {\n          \"care_raw_score\": 0.8122390666060132,\n          \"harm_raw_score\": 0.8407421118129596,\n          \"fairness_raw_score\": 0.6558448375685458,\n          \"cheating_raw_score\": 0.8105753066068808,\n          \"loyalty_raw_score\": -0.28318181677271454,\n          \"betrayal_raw_score\": 0.4461877685601275,\n          \"authority_raw_score\": -0.4468600868840742,\n          \"subversion_raw_score\": 0.7932627993072213,\n          \"sanctity_raw_score\": 0.3168285514686414,\n          \"degradation_raw_score\": 0.6272555627282714,\n          \"liberty_raw_score\": 0.4795304677943564,\n          \"oppression_raw_score\": 0.7788484196884105,\n          \"moral_strategic_contradiction_index\": 0.5739345719363062,\n          \"moral_salience_concentration\": -0.5986877983637172,\n          \"individualizing_foundations_mean\": 1.0,\n          \"binding_foundations_mean\": 0.3953503204739504,\n          \"liberty_foundation_mean\": 0.6924843075253816\n        },\n        \"binding_foundations_mean\": {\n          \"care_raw_score\": 0.20811900115049583,\n          \"harm_raw_score\": 0.3056157077421946,\n          \"fairness_raw_score\": 0.20703816654082103,\n          \"cheating_raw_score\": 0.457007789456578,\n          \"loyalty_raw_score\": 0.053916960133400516,\n          \"betrayal_raw_score\": 0.6272555627282714,\n          \"authority_raw_score\": 0.07172087532396173,\n          \"subversion_raw_score\": 0.5841443644026362,\n          \"sanctity_raw_score\": 0.6275815663704253,\n          \"degradation_raw_score\": 0.7259021871271101,\n          \"liberty_raw_score\": 0.05316654274918734,\n          \"oppression_raw_score\": 0.2789718420653303,\n          \"moral_strategic_contradiction_index\": 0.3601550702206775,\n          \"moral_salience_concentration\": -0.06351940984954714,\n          \"individualizing_foundations_mean\": 0.3953503204739504,\n          \"binding_foundations_mean\": 1.0,\n          \"liberty_foundation_mean\": 0.17478051211119567\n        },\n        \"liberty_foundation_mean\": {\n          \"care_raw_score\": 0.7932627993072213,\n          \"harm_raw_score\": 0.5059644052309139,\n          \"fairness_raw_score\": 0.7410887413697968,\n          \"cheating_raw_score\": 0.4239851608241474,\n          \"loyalty_raw_score\": -0.3392393278553641,\n          \"betrayal_raw_score\": 0.015236392084478441,\n          \"authority_raw_score\": -0.4124976766468761,\n          \"subversion_raw_score\": 0.6242751336495804,\n          \"sanctity_raw_score\": 0.3421334612349132,\n          \"degradation_raw_score\": 0.31295982823616616,\n          \"liberty_raw_score\": 0.9634024338902598,\n          \"oppression_raw_score\": 0.9575459346610057,\n          \"moral_strategic_contradiction_index\": 0.29742686884042865,\n          \"moral_salience_concentration\": -0.631835926618585,\n          \"individualizing_foundations_mean\": 0.6924843075253816,\n          \"binding_foundations_mean\": 0.17478051211119567,\n          \"liberty_foundation_mean\": 1.0\n        }\n      }\n    },\n    \"reliability_analysis\": {\n      \"notes\": \"Internal consistency of foundation sets. Inter-rater reliability could not be calculated due to lack of multi-evaluation data.\",\n      \"individualizing_foundations_alpha\": 0.7225130890052356,\n      \"binding_foundations_alpha\": 0.5369735234519914\n    },\n    \"additional_analyses\": {}\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 8,\n    \"tier_classification\": \"TIER 3\",\n    \"power_notes\": \"The sample size of N=8 is very small, placing this analysis in Tier 3 (Exploratory). All results, especially correlations and group comparisons, are underpowered and should be interpreted as preliminary patterns for hypothesis generation, not conclusive findings. The primary focus is on descriptive statistics and pattern recognition.\"\n  },\n  \"methodology_summary\": \"This exploratory (Tier 3) analysis was conducted on a small corpus of 8 political speeches. Due to the limited sample size, the primary methods employed were descriptive statistics (mean, median, standard deviation) calculated for the overall sample and broken down by speaker ideology. Exploratory correlation analysis was performed on the 12 moral foundation scores and key derived metrics, with strong caveats about statistical stability. Internal consistency (Cronbach's alpha) was calculated for the Individualizing and Binding foundation groupings to assess their coherence as scales within this dataset. No inferential tests (e.g., t-tests, ANOVA) were performed as they would be statistically invalid with such small group sizes. The overall approach focuses on pattern recognition and qualitative insight generation.\"\n}\n```",
  "analysis_artifacts_processed": 18,
  "cost_info": {
    "model": "vertex_ai/gemini-2.5-pro",
    "execution_time_seconds": 220.583141,
    "response_cost": 0.0,
    "input_tokens": 0,
    "output_tokens": 0,
    "total_tokens": 0,
    "prompt_length": 141772,
    "response_length": 78930
  },
  "timestamp": "2025-09-17T02:23:47.106228+00:00"
}