{
  "status": "success",
  "functions_generated": 8,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 26170,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: political_moral_analysis\nDescription: Statistical analysis experiment\nGenerated: 2025-08-28T03:00:49.897436+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived metrics as specified in the Moral Foundations Theory Framework v10.0\n    and appends them as new columns to the DataFrame. This includes tension scores,\n    the Moral Strategic Contradiction Index (MSCI), Moral Salience Concentration (MSC),\n    and mean scores for foundation groups.\n\n    This function is a foundational step for many other analyses.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw and salience scores for each moral foundation.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pd.DataFrame: The original DataFrame with added columns for each derived metric, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = data.copy()\n\n        # Define foundation pairs for tension calculation\n        tension_pairs = {\n            'care_harm': ('care', 'harm'),\n            'fairness_cheating': ('fairness', 'cheating'),\n            'loyalty_betrayal': ('loyalty', 'betrayal'),\n            'authority_subversion': ('authority', 'subversion'),\n            'sanctity_degradation': ('sanctity', 'degradation'),\n            'liberty_oppression': ('liberty', 'oppression')\n        }\n\n        # Calculate individual tension scores for each pair\n        for tension_name, (f1, f2) in tension_pairs.items():\n            score1_col, salience1_col = f'{f1}_raw', f'{f1}_salience'\n            score2_col, salience2_col = f'{f2}_raw', f'{f2}_salience'\n            \n            # Ensure required columns exist\n            required_cols = [score1_col, salience1_col, score2_col, salience2_col]\n            if not all(col in df.columns for col in required_cols):\n                # Silently skip if columns are missing, or handle as needed\n                continue\n\n            min_score = df[[score1_col, score2_col]].min(axis=1)\n            salience_diff = abs(df[salience1_col] - df[salience2_col])\n            df[f'{tension_name}_tension'] = min_score * salience_diff\n\n        # Calculate Aggregate Tension Metrics\n        df['individualizing_tension'] = df.get('care_harm_tension', 0) + df.get('fairness_cheating_tension', 0)\n        df['binding_tension'] = df.get('loyalty_betrayal_tension', 0) + df.get('authority_subversion_tension', 0) + df.get('sanctity_degradation_tension', 0)\n        df['liberty_tension'] = df.get('liberty_oppression_tension', 0)\n\n        # Calculate Moral Strategic Contradiction Index (MSCI)\n        all_tension_cols = [f'{name}_tension' for name in tension_pairs.keys() if f'{name}_tension' in df.columns]\n        if all_tension_cols:\n            df['moral_strategic_contradiction_index'] = df[all_tension_cols].sum(axis=1) / len(all_tension_cols)\n        else:\n            df['moral_strategic_contradiction_index'] = np.nan\n\n        # Calculate Moral Salience Concentration (MSC)\n        salience_cols = [f'{f}_salience' for f in ['care', 'harm', 'fairness', 'cheating', 'loyalty', 'betrayal', 'authority', 'subversion', 'sanctity', 'degradation', 'liberty', 'oppression']]\n        salience_cols_exist = [col for col in salience_cols if col in df.columns]\n        if salience_cols_exist:\n            df['moral_salience_concentration'] = df[salience_cols_exist].std(axis=1)\n        else:\n            df['moral_salience_concentration'] = np.nan\n\n        # Calculate Mean Foundation Scores\n        ind_foundations = ['care_raw', 'harm_raw', 'fairness_raw', 'cheating_raw']\n        bind_foundations = ['loyalty_raw', 'betrayal_raw', 'authority_raw', 'subversion_raw', 'sanctity_raw', 'degradation_raw']\n        lib_foundations = ['liberty_raw', 'oppression_raw']\n        \n        df['individualizing_foundations_mean'] = df[[c for c in ind_foundations if c in df.columns]].mean(axis=1)\n        df['binding_foundations_mean'] = df[[c for c in bind_foundations if c in df.columns]].mean(axis=1)\n        df['liberty_foundation_mean'] = df[[c for c in lib_foundations if c in df.columns]].mean(axis=1)\n\n        return df\n\n    except Exception:\n        return None\n\ndef descriptive_stats_summary(data, **kwargs):\n    \"\"\"\n    Calculates and returns descriptive statistics (mean, std, min, max) for all\n    raw scores, salience scores, and derived metrics in the dataset. This provides\n    a foundational overview of the distribution of moral foundation usage.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing pandas DataFrames for descriptive statistics\n              of raw scores, salience scores, and derived metrics, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        # First, calculate derived metrics to include them in the summary\n        data_with_metrics = calculate_derived_metrics(data)\n        if data_with_metrics is None:\n            return None # Propagate failure\n\n        raw_score_cols = [col for col in data_with_metrics.columns if col.endswith('_raw')]\n        salience_cols = [col for col in data_with_metrics.columns if col.endswith('_salience')]\n        derived_cols = [\n            'individualizing_tension', 'binding_tension', 'liberty_tension',\n            'moral_strategic_contradiction_index', 'moral_salience_concentration',\n            'individualizing_foundations_mean', 'binding_foundations_mean', 'liberty_foundation_mean'\n        ]\n        # Filter for derived columns that actually exist in the dataframe\n        derived_cols = [col for col in derived_cols if col in data_with_metrics.columns]\n\n        results = {}\n        if raw_score_cols:\n            results['raw_scores_stats'] = data_with_metrics[raw_score_cols].describe().to_dict()\n        if salience_cols:\n            results['salience_scores_stats'] = data_with_metrics[salience_cols].describe().to_dict()\n        if derived_cols:\n            results['derived_metrics_stats'] = data_with_metrics[derived_cols].describe().to_dict()\n\n        return results if results else None\n\n    except Exception:\n        return None\n\ndef foundation_correlation_matrix(data, **kwargs):\n    \"\"\"\n    Computes the Pearson correlation matrix for all raw moral foundation scores.\n    This analysis helps to identify which moral foundations are frequently invoked\n    together in the analyzed discourse, revealing common patterns of moral argumentation.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters. Set 'method' to 'spearman' or 'kendall' for\n                  different correlation types. Default is 'pearson'.\n\n    Returns:\n        dict: A dictionary representing the correlation matrix, or None if insufficient data.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        method = kwargs.get('method', 'pearson')\n        raw_score_cols = [col for col in data.columns if col.endswith('_raw')]\n\n        if len(raw_score_cols) < 2:\n            return None\n\n        correlation_matrix = data[raw_score_cols].corr(method=method)\n        \n        # Clean column names for better readability in the output\n        correlation_matrix.columns = [c.replace('_raw', '') for c in correlation_matrix.columns]\n        correlation_matrix.index = [i.replace('_raw', '') for i in correlation_matrix.index]\n\n        # Replace NaN with None for JSON compatibility\n        return correlation_matrix.where(pd.notnull(correlation_matrix), None).to_dict()\n\n    except Exception:\n        return None\n\ndef analyze_moral_tension_and_contradiction(data, **kwargs):\n    \"\"\"\n    Quantifies and classifies moral contradiction within the dataset. It calculates\n    aggregate tension scores and the Moral Strategic Contradiction Index (MSCI) for each document.\n    It then classifies each document's rhetorical coherence based on the MSCI value.\n\n    Methodology:\n    - Calculates derived metrics using the framework's formulas.\n    - Classifies MSCI based on predefined thresholds:\n        - Moral Coherence (MSCI < 0.3)\n        - Moral Complexity (MSCI 0.3-0.5)\n        - Moral Ambivalence (MSCI 0.5-0.7)\n        - Moral Contradiction (MSCI > 0.7)\n    - Returns per-document classifications and an overall summary.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary with per-document tension analysis and an aggregate summary, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        data_with_metrics = calculate_derived_metrics(data)\n        if data_with_metrics is None:\n            return None\n\n        def classify_msci(msci):\n            if pd.isna(msci):\n                return \"Not Available\"\n            if msci < 0.3:\n                return \"Moral Coherence\"\n            elif msci < 0.5:\n                return \"Moral Complexity\"\n            elif msci < 0.7:\n                return \"Moral Ambivalence\"\n            else:\n                return \"Moral Contradiction\"\n\n        msci_col = 'moral_strategic_contradiction_index'\n        if msci_col not in data_with_metrics.columns:\n            return None\n            \n        data_with_metrics['msci_classification'] = data_with_metrics[msci_col].apply(classify_msci)\n\n        document_results = data_with_metrics[[\n            'document_name',\n            'individualizing_tension',\n            'binding_tension',\n            'liberty_tension',\n            msci_col,\n            'msci_classification'\n        ]].to_dict(orient='records')\n\n        summary_stats = data_with_metrics['msci_classification'].value_counts(normalize=True).to_dict()\n\n        return {\n            \"document_tension_analysis\": document_results,\n            \"aggregate_coherence_summary\": summary_stats\n        }\n\n    except Exception:\n        return None\n\ndef analyze_moral_identity_and_ideology(data, **kwargs):\n    \"\"\"\n    Analyzes and classifies the moral identity and ideological profile for each document\n    based on the Moral Foundations Theory Framework v10.0 specifications.\n\n    Methodology:\n    - Moral Identity Focus: Uses Moral Salience Concentration (MSC) to classify identity as\n      'Focused' (MSC > 0.3) or 'Distributed' (MSC < 0.2).\n    - Ideological Profile: Classifies profiles based on mean scores of foundation groups:\n        - Liberal: High Individualizing (>0.6), Low Binding (<0.4)\n        - Conservative: High Binding (>0.6), Low Individualizing (<0.4)\n        - Libertarian: High Liberty (>0.6)\n        - Balanced: None of the above conditions are met strongly.\n    - Tension-Salience Interaction: Classifies the interaction between moral focus (MSC) and\n      coherence (MSCI) to assess authenticity vs. confusion.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary with per-document profile classifications and an aggregate summary, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = calculate_derived_metrics(data)\n        if df is None:\n            return None\n\n        # 1. Moral Identity Focus (MSC)\n        def classify_msc(msc):\n            if pd.isna(msc): return \"Not Available\"\n            if msc > 0.3: return \"Focused Moral Identity\"\n            if msc < 0.2: return \"Distributed Moral Identity\"\n            return \"Moderate Moral Focus\"\n        \n        msc_col = 'moral_salience_concentration'\n        if msc_col in df.columns:\n            df['moral_identity_focus'] = df[msc_col].apply(classify_msc)\n        else:\n            df['moral_identity_focus'] = \"Not Available\"\n\n        # 2. Ideological Profile\n        def classify_ideology(row):\n            ind_mean = row.get('individualizing_foundations_mean', 0)\n            bind_mean = row.get('binding_foundations_mean', 0)\n            lib_mean = row.get('liberty_foundation_mean', 0)\n            \n            is_liberal = ind_mean > 0.6 and bind_mean < 0.4\n            is_conservative = bind_mean > 0.6 and ind_mean < 0.4\n            is_libertarian = lib_mean > 0.6\n\n            profiles = []\n            if is_liberal: profiles.append(\"Liberal Profile\")\n            if is_conservative: profiles.append(\"Conservative Profile\")\n            if is_libertarian: profiles.append(\"Libertarian Profile\")\n            \n            if not profiles:\n                return \"Balanced Profile\"\n            return \" / \".join(profiles) # Handle cases that meet multiple criteria\n\n        df['ideological_profile'] = df.apply(classify_ideology, axis=1)\n\n        # 3. Tension-Salience Interaction\n        def classify_interaction(row):\n            focus = row.get('moral_identity_focus')\n            msci = row.get('moral_strategic_contradiction_index')\n            if focus == \"Not Available\" or pd.isna(msci): return \"Not Available\"\n\n            high_focus = focus == \"Focused Moral Identity\"\n            low_focus = focus == \"Distributed Moral Identity\"\n            high_msci = msci > 0.5 # Using 0.5 as the threshold for high tension\n            \n            if high_focus and not high_msci: return \"Authentic moral identity\"\n            if high_focus and high_msci: return \"Moral identity confusion\"\n            if low_focus and not high_msci: return \"Sophisticated moral complexity\"\n            if low_focus and high_msci: return \"Inconsistent moral reasoning\"\n            return \"Moderate Profile\"\n\n        df['tension_salience_interaction'] = df.apply(classify_interaction, axis=1)\n\n        # Prepare results\n        result_cols = [\n            'document_name', 'moral_identity_focus', 'ideological_profile',\n            'tension_salience_interaction', 'moral_salience_concentration',\n            'moral_strategic_contradiction_index'\n        ]\n        document_results = df[[col for col in result_cols if col in df.columns]].to_dict(orient='records')\n\n        summary = {\n            'identity_focus_distribution': df['moral_identity_focus'].value_counts(normalize=True).to_dict(),\n            'ideological_profile_distribution': df['ideological_profile'].value_counts(normalize=True).to_dict(),\n            'interaction_effect_distribution': df['tension_salience_interaction'].value_counts(normalize=True).to_dict()\n        }\n\n        return {\n            \"document_profile_analysis\": document_results,\n            \"aggregate_profile_summary\": summary\n        }\n\n    except Exception:\n        return None\n\ndef _get_speaker_metadata_map(data, **kwargs):\n    \"\"\"\n    Internal helper to create a speaker-to-document mapping.\n    It attempts to load a corpus manifest file. If not found, it gracefully falls back\n    to parsing speaker names from the 'document_name' column as a best-effort approach.\n    This handles the framework requirement for manifest-based identification while being\n    robust to its absence.\n\n    Args:\n        data (pd.DataFrame): The main data with a 'document_name' column.\n        **kwargs: Can include 'manifest_path' to specify a corpus manifest file.\n\n    Returns:\n        pd.Series: A pandas Series mapping document_name to speaker_id.\n    \"\"\"\n    import pandas as pd\n    import json\n    from pathlib import Path\n\n    manifest_path = kwargs.get('manifest_path')\n\n    # Attempt to load from manifest if path is provided\n    if manifest_path and Path(manifest_path).exists():\n        # This part is flexible to handle various manifest formats (e.g., JSON, CSV)\n        # For this implementation, we assume a simple JSON or CSV structure.\n        try:\n            if manifest_path.endswith('.json'):\n                with open(manifest_path, 'r') as f:\n                    manifest = json.load(f)\n                manifest_df = pd.DataFrame(manifest)\n            elif manifest_path.endswith('.csv'):\n                manifest_df = pd.read_csv(manifest_path)\n            \n            # Assuming manifest has 'filename' and 'speaker' columns\n            if 'filename' in manifest_df.columns and 'speaker' in manifest_df.columns:\n                mapping = pd.Series(manifest_df.speaker.values, index=manifest_df.filename).to_dict()\n                return data['document_name'].map(mapping).fillna('Unknown Speaker')\n        except Exception:\n            # Fallback if manifest loading fails\n            pass\n\n    # Graceful fallback: Parse speaker from filename (e.g., \"cory_booker_2018...\" -> \"cory_booker\")\n    # This is a fallback as per the \"handle missing metadata gracefully\" requirement.\n    def parse_speaker(filename):\n        try:\n            # Simple parsing: take the first part of the filename before the first underscore\n            # This is a heuristic and may need adjustment for different naming schemes.\n            parts = Path(filename).stem.split('_')\n            if len(parts) > 1:\n                # e.g., 'cory_booker' or 'jd_vance'\n                return f\"{parts[0]}_{parts[1]}\" if not parts[1].isdigit() else parts[0]\n            return parts[0]\n        except:\n            return 'Unknown Speaker'\n\n    return data['document_name'].apply(parse_speaker)\n\ndef aggregate_by_speaker(data, **kwargs):\n    \"\"\"\n    Aggregates moral foundation scores and derived metrics by speaker.\n    It uses a helper function to identify speakers, prioritizing a corpus manifest\n    if available, and falling back to filename parsing otherwise. This allows for\n    comparative analysis of moral rhetoric between different political actors.\n\n    Methodology:\n    - Identifies speakers using the _get_speaker_metadata_map helper.\n    - Calculates derived metrics for each document.\n    - Groups the data by speaker and calculates the mean for all numeric scores\n      (raw, salience, and derived metrics).\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Can be passed to the metadata helper (e.g., 'manifest_path').\n\n    Returns:\n        dict: A dictionary of speaker profiles with their average scores, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from pathlib import Path\n    import json\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = calculate_derived_metrics(data)\n        if df is None:\n            return None\n\n        # Get speaker mapping\n        df['speaker_id'] = _get_speaker_metadata_map(df, **kwargs)\n\n        # Select only numeric columns for aggregation\n        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n        \n        # Group by speaker and calculate the mean\n        speaker_profiles = df.groupby('speaker_id')[numeric_cols].mean()\n        \n        # Add document count for context\n        speaker_profiles['document_count'] = df.groupby('speaker_id')['document_name'].count()\n\n        return speaker_profiles.to_dict(orient='index')\n\n    except Exception:\n        return None\n\ndef summarize_analyst_confidence(data, **kwargs):\n    \"\"\"\n    Analyzes the 'confidence' scores provided by the analysis agent for each dimension.\n    This serves as a proxy for inter-rater reliability and data quality assessment.\n    It calculates the mean confidence across all dimensions for each document and\n    provides an overall summary for the entire dataset.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data, including confidence scores.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing per-document average confidence and an overall\n              dataset confidence summary, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        confidence_cols = [col for col in data.columns if col.endswith('_confidence')]\n        if not confidence_cols:\n            return {\"error\": \"No confidence score columns found.\"}\n\n        df = data.copy()\n        df['mean_confidence'] = df[confidence_cols].mean(axis=1)\n\n        document_confidence = df[['document_name', 'mean_confidence']].to_dict(orient='records')\n\n        overall_summary = {\n            'overall_mean_confidence': df['mean_confidence'].mean(),\n            'overall_std_dev_confidence': df['mean_confidence'].std(),\n            'min_mean_confidence': df['mean_confidence'].min(),\n            'max_mean_confidence': df['mean_confidence'].max(),\n            'confidence_per_dimension': df[confidence_cols].mean().to_dict()\n        }\n\n        return {\n            \"document_confidence_summary\": document_confidence,\n            \"dataset_confidence_summary\": overall_summary\n        }\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}