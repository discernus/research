{
  "status": "success",
  "functions_generated": 6,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 22459,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: political_moral_analysis\nDescription: Statistical analysis experiment\nGenerated: 2025-08-29T12:29:45.249894+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived metrics as specified in the Moral Foundations Theory v10.0 framework.\n\n    This function computes moral tension scores, the Moral Strategic Contradiction Index (MSCI),\n    Moral Salience Concentration (MSC), and mean scores for foundation groups. It uses the\n    formulas provided in the framework's machine-readable appendix.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw and salience scores for each of the 12 moral foundations.\n                             Must use the exact column names from the specification (e.g., 'care_raw', 'care_salience').\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pd.DataFrame: The input DataFrame with added columns for each derived metric, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        df = data.copy()\n\n        # Moral Foundation Tension Scoring\n        df['care_harm_tension'] = df.apply(lambda row: min(row['care_raw'], row['harm_raw']) * abs(row['care_salience'] - row['harm_salience']), axis=1)\n        df['fairness_cheating_tension'] = df.apply(lambda row: min(row['fairness_raw'], row['cheating_raw']) * abs(row['fairness_salience'] - row['cheating_salience']), axis=1)\n        df['loyalty_betrayal_tension'] = df.apply(lambda row: min(row['loyalty_raw'], row['betrayal_raw']) * abs(row['loyalty_salience'] - row['betrayal_salience']), axis=1)\n        df['authority_subversion_tension'] = df.apply(lambda row: min(row['authority_raw'], row['subversion_raw']) * abs(row['authority_salience'] - row['subversion_salience']), axis=1)\n        df['sanctity_degradation_tension'] = df.apply(lambda row: min(row['sanctity_raw'], row['degradation_raw']) * abs(row['sanctity_salience'] - row['degradation_salience']), axis=1)\n        df['liberty_oppression_tension'] = df.apply(lambda row: min(row['liberty_raw'], row['oppression_raw']) * abs(row['liberty_salience'] - row['oppression_salience']), axis=1)\n\n        # Aggregate Tension Metrics\n        df['individualizing_tension'] = df['care_harm_tension'] + df['fairness_cheating_tension']\n        df['binding_tension'] = df['loyalty_betrayal_tension'] + df['authority_subversion_tension'] + df['sanctity_degradation_tension']\n        df['liberty_tension'] = df['liberty_oppression_tension']\n\n        # Moral Strategic Contradiction Index (MSCI)\n        tension_cols = [\n            'care_harm_tension', 'fairness_cheating_tension', 'loyalty_betrayal_tension',\n            'authority_subversion_tension', 'sanctity_degradation_tension', 'liberty_oppression_tension'\n        ]\n        df['moral_strategic_contradiction_index'] = df[tension_cols].sum(axis=1) / len(tension_cols)\n\n        # Moral Salience Concentration (MSC)\n        salience_cols = [f'{f}_salience' for f in ['care', 'harm', 'fairness', 'cheating', 'loyalty', 'betrayal', 'authority', 'subversion', 'sanctity', 'degradation', 'liberty', 'oppression']]\n        df['moral_salience_concentration'] = df[salience_cols].std(axis=1)\n\n        # Foundation Group Means (Raw Scores)\n        individualizing_raw_cols = ['care_raw', 'harm_raw', 'fairness_raw', 'cheating_raw']\n        binding_raw_cols = ['loyalty_raw', 'betrayal_raw', 'authority_raw', 'subversion_raw', 'sanctity_raw', 'degradation_raw']\n        liberty_raw_cols = ['liberty_raw', 'oppression_raw']\n        df['individualizing_foundations_mean'] = df[individualizing_raw_cols].mean(axis=1)\n        df['binding_foundations_mean'] = df[binding_raw_cols].mean(axis=1)\n        df['liberty_foundation_mean'] = df[liberty_raw_cols].mean(axis=1)\n\n        # Foundation Group Means (Salience Scores)\n        individualizing_salience_cols = ['care_salience', 'harm_salience', 'fairness_salience', 'cheating_salience']\n        binding_salience_cols = ['loyalty_salience', 'betrayal_salience', 'authority_salience', 'subversion_salience', 'sanctity_salience', 'degradation_salience']\n        liberty_salience_cols = ['liberty_salience', 'oppression_salience']\n        df['individualizing_salience_mean'] = df[individualizing_salience_cols].mean(axis=1)\n        df['binding_salience_mean'] = df[binding_salience_cols].mean(axis=1)\n        df['liberty_salience_mean'] = df[liberty_salience_cols].mean(axis=1)\n\n        return df\n\n    except Exception:\n        return None\n\ndef add_speaker_metadata(data, **kwargs):\n    \"\"\"\n    Adds speaker metadata (name, ideology, party) to the DataFrame based on document names.\n\n    This function uses a predefined mapping to associate document filenames with known political\n    speakers and their attributes. This is essential for subsequent group-based analyses, such as\n    comparing moral foundation usage across different ideologies. It handles missing or unknown\n    documents gracefully by assigning 'Unknown' values.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing analysis data with a 'document_name' column.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pd.DataFrame: The input DataFrame with 'speaker', 'ideology', and 'party' columns added,\n                      or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        df = data.copy()\n        if 'document_name' not in df.columns:\n            return df # Cannot proceed without document_name\n\n        speaker_map = {\n            'cory_booker': {'speaker': 'Cory Booker', 'ideology': 'Progressive', 'party': 'Democrat'},\n            'jd_vance': {'speaker': 'JD Vance', 'ideology': 'Conservative', 'party': 'Republican'},\n            'mitt_romney': {'speaker': 'Mitt Romney', 'ideology': 'Conservative', 'party': 'Republican'},\n            'steve_king': {'speaker': 'Steve King', 'ideology': 'Conservative', 'party': 'Republican'},\n            'john_lewis': {'speaker': 'John Lewis', 'ideology': 'Progressive', 'party': 'Democrat'},\n            'bernie_sanders': {'speaker': 'Bernie Sanders', 'ideology': 'Progressive', 'party': 'Independent'},\n            'alexandria_ocasio-cortez': {'speaker': 'Alexandria Ocasio-Cortez', 'ideology': 'Progressive', 'party': 'Democrat'},\n            'aoc': {'speaker': 'Alexandria Ocasio-Cortez', 'ideology': 'Progressive', 'party': 'Democrat'},\n            'john_mccain': {'speaker': 'John McCain', 'ideology': 'Conservative', 'party': 'Republican'},\n        }\n\n        def get_metadata(doc_name):\n            for key, meta in speaker_map.items():\n                if key in doc_name:\n                    return pd.Series(meta)\n            return pd.Series({'speaker': 'Unknown', 'ideology': 'Unknown', 'party': 'Unknown'})\n\n        meta_df = df['document_name'].apply(get_metadata)\n        df = pd.concat([df, meta_df], axis=1)\n\n        return df\n\n    except Exception:\n        return None\n\ndef get_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Computes descriptive statistics for all numerical columns in the dataset.\n\n    This function provides a summary of the central tendency, dispersion, and shape of the\n    dataset's distribution for each moral foundation score, salience, confidence, and any\n    derived metrics. It uses pandas' .describe() method for robust and standard calculations.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing the descriptive statistics, or None if an error occurs.\n              The dictionary is formatted for easy JSON serialization.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # Ensure derived metrics are calculated if not present\n        if 'moral_strategic_contradiction_index' not in data.columns:\n            data = calculate_derived_metrics(data)\n            if data is None:\n                raise ValueError(\"Failed to calculate derived metrics.\")\n\n        numeric_cols = data.select_dtypes(include=np.number).columns.tolist()\n        if not numeric_cols:\n            return {}\n\n        descriptives = data[numeric_cols].describe().round(4)\n        return descriptives.to_dict()\n\n    except Exception:\n        return None\n\ndef get_correlation_matrix(data, **kwargs):\n    \"\"\"\n    Calculates the Pearson correlation matrix for moral foundation scores and derived metrics.\n\n    This analysis helps to identify relationships and patterns between different moral foundations.\n    For example, it can reveal whether appeals to 'Care' are positively or negatively correlated\n    with appeals to 'Loyalty'. The function focuses on raw scores and key derived metrics.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary representing the correlation matrix, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # Ensure derived metrics are calculated if not present\n        if 'moral_strategic_contradiction_index' not in data.columns:\n            data = calculate_derived_metrics(data)\n            if data is None:\n                raise ValueError(\"Failed to calculate derived metrics.\")\n\n        foundation_cols = [f'{f}_raw' for f in ['care', 'harm', 'fairness', 'cheating', 'loyalty', 'betrayal', 'authority', 'subversion', 'sanctity', 'degradation', 'liberty', 'oppression']]\n        derived_cols = ['moral_strategic_contradiction_index', 'moral_salience_concentration', 'individualizing_foundations_mean', 'binding_foundations_mean', 'liberty_foundation_mean']\n        \n        cols_to_correlate = [col for col in foundation_cols + derived_cols if col in data.columns]\n        \n        if len(cols_to_correlate) < 2:\n            return None\n\n        # Average scores for documents with multiple evaluations before correlation\n        if 'document_name' in data.columns:\n            agg_data = data.groupby('document_name')[cols_to_correlate].mean().reset_index()\n        else:\n            agg_data = data\n\n        correlation_matrix = agg_data[cols_to_correlate].corr(method='pearson').round(4)\n        return correlation_matrix.to_dict()\n\n    except Exception:\n        return None\n\ndef get_inter_rater_reliability(data, **kwargs):\n    \"\"\"\n    Calculates Inter-rater Reliability (IRR) using the Intraclass Correlation Coefficient (ICC).\n\n    This function assesses the consistency of scores between different evaluations for the same document,\n    a key requirement of the experiment's multi-evaluation design. It calculates ICC for each raw score\n    and salience score dimension. An ICC > 0.70 is targeted, as per the experiment spec.\n\n    Methodology:\n    1. Assigns a 'rater_id' to each evaluation within a document group.\n    2. Pivots the data to a wide format where each row is a document and columns represent rater scores.\n    3. Calculates ICC (specifically ICC2, 'two-way random-effects, absolute agreement, single rater/measurement')\n       for each dimension.\n\n    Args:\n        data (pd.DataFrame): DataFrame in long format, with one row per evaluation and a 'document_name' column.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary where keys are dimension names (e.g., 'care_raw') and values are their\n              ICC results, or None if data is not suitable for IRR analysis.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    try:\n        import pingouin as pg\n    except ImportError:\n        # This is a reasonable fallback if the library isn't in the environment.\n        return {\"error\": \"The 'pingouin' library is required for IRR analysis. Please install it.\"}\n\n    try:\n        if 'document_name' not in data.columns:\n            return None\n\n        # Filter for documents with at least 2 ratings\n        doc_counts = data['document_name'].value_counts()\n        docs_with_multiple_ratings = doc_counts[doc_counts >= 2].index\n        df_irr = data[data['document_name'].isin(docs_with_multiple_ratings)].copy()\n\n        if df_irr.empty:\n            return {\"message\": \"No documents with multiple ratings found for IRR analysis.\"}\n\n        # Assign rater ID\n        df_irr['rater_id'] = df_irr.groupby('document_name').cumcount() + 1\n\n        score_cols = [col for col in data.columns if '_raw' in col or '_salience' in col]\n        irr_results = {}\n\n        for col in score_cols:\n            # Prepare data for this specific column\n            irr_data = df_irr[['document_name', 'rater_id', col]].copy()\n            \n            # Check for sufficient data\n            if irr_data[col].isnull().all() or irr_data.groupby('document_name')['rater_id'].nunique().min() < 2:\n                irr_results[col] = {\"error\": \"Insufficient data for ICC calculation.\"}\n                continue\n\n            # Calculate ICC\n            icc = pg.intraclass_corr(data=irr_data, targets='document_name', raters='rater_id', ratings=col).set_index('Type')\n            \n            # We are interested in ICC2: Two-way random, absolute agreement, single rater\n            icc2_result = icc.loc['ICC2']\n            \n            irr_results[col] = {\n                'icc': icc2_result['ICC'],\n                'f_stat': icc2_result['F'],\n                'df1': icc2_result['df1'],\n                'df2': icc2_result['df2'],\n                'p_value': icc2_result['pval'],\n                'ci95': icc2_result['CI95%']\n            }\n\n        return irr_results\n\n    except Exception:\n        return None\n\ndef compare_ideological_groups(data, **kwargs):\n    \"\"\"\n    Compares moral foundation scores across different ideological groups using t-tests or ANOVA.\n\n    This function directly tests the \"Ideological Differentiation\" hypothesis by examining whether\n    moral foundation usage varies significantly between political ideologies (e.g., Progressive vs. Conservative).\n\n    Methodology:\n    1. Adds speaker metadata to identify ideological groups.\n    2. Averages scores for each document to create a single data point per speaker/speech.\n    3. If two ideological groups are present, performs an independent t-test for each score.\n    4. If more than two groups are present, performs a one-way ANOVA.\n    5. Returns the statistical results (test statistic, p-value) for each comparison.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A nested dictionary containing the results of the statistical tests for each\n              moral dimension, or None if an error occurs or there are not enough groups to compare.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import ttest_ind, f_oneway\n\n    try:\n        # Add metadata and derived metrics\n        df = add_speaker_metadata(data)\n        if df is None: raise ValueError(\"Failed to add speaker metadata.\")\n        df = calculate_derived_metrics(df)\n        if df is None: raise ValueError(\"Failed to calculate derived metrics.\")\n\n        # Aggregate scores per document\n        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n        agg_cols = ['document_name', 'ideology'] + numeric_cols\n        df_agg = df[agg_cols].groupby(['document_name', 'ideology']).mean().reset_index()\n\n        # Filter out unknown ideologies and groups with fewer than 2 members\n        df_agg = df_agg[df_agg['ideology'] != 'Unknown']\n        ideology_counts = df_agg['ideology'].value_counts()\n        valid_ideologies = ideology_counts[ideology_counts >= 2].index\n        df_agg = df_agg[df_agg['ideology'].isin(valid_ideologies)]\n\n        groups = df_agg['ideology'].unique()\n        if len(groups) < 2:\n            return {\"message\": \"Fewer than two valid ideological groups with sufficient data for comparison.\"}\n\n        results = {}\n        metrics_to_compare = [col for col in numeric_cols if col not in ['rater_id']] # Exclude rater_id if present\n\n        if len(groups) == 2:\n            group1_name, group2_name = groups[0], groups[1]\n            group1_data = df_agg[df_agg['ideology'] == group1_name]\n            group2_data = df_agg[df_agg['ideology'] == group2_name]\n            results['test_type'] = 'independent_t-test'\n            results['groups'] = [group1_name, group2_name]\n            \n            for metric in metrics_to_compare:\n                stat, p_val = ttest_ind(group1_data[metric], group2_data[metric], nan_policy='omit')\n                results[metric] = {'statistic': stat, 'p_value': p_val}\n\n        else: # More than 2 groups -> ANOVA\n            results['test_type'] = 'one-way_anova'\n            results['groups'] = groups.tolist()\n            \n            for metric in metrics_to_compare:\n                group_data = [df_agg[df_agg['ideology'] == g][metric].dropna() for g in groups]\n                # Ensure all groups have data for this metric\n                if any(len(d) == 0 for d in group_data):\n                    results[metric] = {'error': 'Metric not present in all groups.'}\n                    continue\n                \n                f_stat, p_val = f_oneway(*group_data)\n                results[metric] = {'f_statistic': f_stat, 'p_value': p_val}\n        \n        return results\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}