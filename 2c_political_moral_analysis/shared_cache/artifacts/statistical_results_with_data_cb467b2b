{
  "generation_metadata": {
    "status": "success",
    "functions_generated": 8,
    "output_file": "automatedstatisticalanalysisagent_functions.py",
    "module_size": 26170
  },
  "statistical_data": {
    "aggregate_by_speaker": {
      "alexandria_ocasio": {
        "care_raw": 0.3,
        "care_salience": 0.2,
        "care_confidence": 0.7,
        "harm_raw": 0.9,
        "harm_salience": 0.9,
        "harm_confidence": 0.95,
        "fairness_raw": 0.7,
        "fairness_salience": 0.7,
        "fairness_confidence": 0.9,
        "cheating_raw": 0.9,
        "cheating_salience": 0.9,
        "cheating_confidence": 0.95,
        "loyalty_raw": 0.8,
        "loyalty_salience": 0.8,
        "loyalty_confidence": 0.9,
        "betrayal_raw": 0.4,
        "betrayal_salience": 0.4,
        "betrayal_confidence": 0.75,
        "authority_raw": 0.1,
        "authority_salience": 0.1,
        "authority_confidence": 0.8,
        "subversion_raw": 0.9,
        "subversion_salience": 0.9,
        "subversion_confidence": 0.95,
        "sanctity_raw": 0.2,
        "sanctity_salience": 0.1,
        "sanctity_confidence": 0.6,
        "degradation_raw": 0.3,
        "degradation_salience": 0.2,
        "degradation_confidence": 0.7,
        "liberty_raw": 0.8,
        "liberty_salience": 0.8,
        "liberty_confidence": 0.9,
        "oppression_raw": 0.9,
        "oppression_salience": 0.9,
        "oppression_confidence": 0.95,
        "care_harm_tension": 0.21,
        "fairness_cheating_tension": 0.14000000000000004,
        "loyalty_betrayal_tension": 0.16000000000000003,
        "authority_subversion_tension": 0.08000000000000002,
        "sanctity_degradation_tension": 0.020000000000000004,
        "liberty_oppression_tension": 0.07999999999999999,
        "individualizing_tension": 0.35000000000000003,
        "binding_tension": 0.26000000000000006,
        "liberty_tension": 0.07999999999999999,
        "moral_strategic_contradiction_index": 0.115,
        "moral_salience_concentration": 0.34410622038709343,
        "individualizing_foundations_mean": 0.7,
        "binding_foundations_mean": 0.45,
        "liberty_foundation_mean": 0.8500000000000001,
        "document_count": 1
      },
      "bernie_sanders": {
        "care_raw": 0.3,
        "care_salience": 0.3,
        "care_confidence": 0.8,
        "harm_raw": 0.9,
        "harm_salience": 0.9,
        "harm_confidence": 0.95,
        "fairness_raw": 0.9,
        "fairness_salience": 0.95,
        "fairness_confidence": 0.95,
        "cheating_raw": 0.95,
        "cheating_salience": 0.95,
        "cheating_confidence": 0.95,
        "loyalty_raw": 0.6,
        "loyalty_salience": 0.4,
        "loyalty_confidence": 0.8,
        "betrayal_raw": 0.7,
        "betrayal_salience": 0.6,
        "betrayal_confidence": 0.85,
        "authority_raw": 0.3,
        "authority_salience": 0.2,
        "authority_confidence": 0.7,
        "subversion_raw": 0.9,
        "subversion_salience": 0.85,
        "subversion_confidence": 0.9,
        "sanctity_raw": 0.2,
        "sanctity_salience": 0.1,
        "sanctity_confidence": 0.6,
        "degradation_raw": 0.7,
        "degradation_salience": 0.6,
        "degradation_confidence": 0.8,
        "liberty_raw": 0.5,
        "liberty_salience": 0.4,
        "liberty_confidence": 0.75,
        "oppression_raw": 0.9,
        "oppression_salience": 0.9,
        "oppression_confidence": 0.95,
        "care_harm_tension": 0.18000000000000002,
        "fairness_cheating_tension": 0.0,
        "loyalty_betrayal_tension": 0.11999999999999997,
        "authority_subversion_tension": 0.19499999999999998,
        "sanctity_degradation_tension": 0.1,
        "liberty_oppression_tension": 0.25,
        "individualizing_tension": 0.18000000000000002,
        "binding_tension": 0.4149999999999999,
        "liberty_tension": 0.25,
        "moral_strategic_contradiction_index": 0.14083333333333334,
        "moral_salience_concentration": 0.3114907946497218,
        "individualizing_foundations_mean": 0.7625,
        "binding_foundations_mean": 0.5666666666666668,
        "liberty_foundation_mean": 0.7,
        "document_count": 1
      },
      "cory_booker": {
        "care_raw": 0.8,
        "care_salience": 0.7,
        "care_confidence": 0.9,
        "harm_raw": 0.9,
        "harm_salience": 0.8,
        "harm_confidence": 0.9,
        "fairness_raw": 0.9,
        "fairness_salience": 0.8,
        "fairness_confidence": 0.9,
        "cheating_raw": 0.8,
        "cheating_salience": 0.7,
        "cheating_confidence": 0.9,
        "loyalty_raw": 0.7,
        "loyalty_salience": 0.6,
        "loyalty_confidence": 0.8,
        "betrayal_raw": 0.1,
        "betrayal_salience": 0.1,
        "betrayal_confidence": 0.7,
        "authority_raw": 0.5,
        "authority_salience": 0.4,
        "authority_confidence": 0.8,
        "subversion_raw": 0.7,
        "subversion_salience": 0.6,
        "subversion_confidence": 0.8,
        "sanctity_raw": 0.6,
        "sanctity_salience": 0.5,
        "sanctity_confidence": 0.8,
        "degradation_raw": 0.7,
        "degradation_salience": 0.6,
        "degradation_confidence": 0.9,
        "liberty_raw": 0.8,
        "liberty_salience": 0.7,
        "liberty_confidence": 0.9,
        "oppression_raw": 0.9,
        "oppression_salience": 0.8,
        "oppression_confidence": 0.9,
        "care_harm_tension": 0.08000000000000007,
        "fairness_cheating_tension": 0.08000000000000007,
        "loyalty_betrayal_tension": 0.05,
        "authority_subversion_tension": 0.09999999999999998,
        "sanctity_degradation_tension": 0.059999999999999984,
        "liberty_oppression_tension": 0.08000000000000007,
        "individualizing_tension": 0.16000000000000014,
        "binding_tension": 0.20999999999999996,
        "liberty_tension": 0.08000000000000007,
        "moral_strategic_contradiction_index": 0.07500000000000002,
        "moral_salience_concentration": 0.20207259421636903,
        "individualizing_foundations_mean": 0.8500000000000001,
        "binding_foundations_mean": 0.5499999999999999,
        "liberty_foundation_mean": 0.8500000000000001,
        "document_count": 1
      },
      "jd_vance": {
        "care_raw": 0.6,
        "care_salience": 0.7,
        "care_confidence": 0.9,
        "harm_raw": 0.8,
        "harm_salience": 0.8,
        "harm_confidence": 0.9,
        "fairness_raw": 0.7,
        "fairness_salience": 0.7,
        "fairness_confidence": 0.8,
        "cheating_raw": 0.8,
        "cheating_salience": 0.8,
        "cheating_confidence": 0.9,
        "loyalty_raw": 0.9,
        "loyalty_salience": 0.9,
        "loyalty_confidence": 0.9,
        "betrayal_raw": 0.8,
        "betrayal_salience": 0.8,
        "betrayal_confidence": 0.9,
        "authority_raw": 0.3,
        "authority_salience": 0.4,
        "authority_confidence": 0.7,
        "subversion_raw": 0.7,
        "subversion_salience": 0.7,
        "subversion_confidence": 0.9,
        "sanctity_raw": 0.6,
        "sanctity_salience": 0.6,
        "sanctity_confidence": 0.8,
        "degradation_raw": 0.7,
        "degradation_salience": 0.7,
        "degradation_confidence": 0.8,
        "liberty_raw": 0.5,
        "liberty_salience": 0.5,
        "liberty_confidence": 0.7,
        "oppression_raw": 0.7,
        "oppression_salience": 0.7,
        "oppression_confidence": 0.8,
        "care_harm_tension": 0.06000000000000005,
        "fairness_cheating_tension": 0.07000000000000006,
        "loyalty_betrayal_tension": 0.07999999999999999,
        "authority_subversion_tension": 0.08999999999999998,
        "sanctity_degradation_tension": 0.059999999999999984,
        "liberty_oppression_tension": 0.09999999999999998,
        "individualizing_tension": 0.13000000000000012,
        "binding_tension": 0.22999999999999998,
        "liberty_tension": 0.09999999999999998,
        "moral_strategic_contradiction_index": 0.07666666666666667,
        "moral_salience_concentration": 0.13789543689024492,
        "individualizing_foundations_mean": 0.7249999999999999,
        "binding_foundations_mean": 0.6666666666666666,
        "liberty_foundation_mean": 0.6,
        "document_count": 1
      },
      "john_lewis": {
        "care_raw": 0.85,
        "care_salience": 0.8,
        "care_confidence": 0.9,
        "harm_raw": 0.9,
        "harm_salience": 0.9,
        "harm_confidence": 0.95,
        "fairness_raw": 0.95,
        "fairness_salience": 0.95,
        "fairness_confidence": 0.95,
        "cheating_raw": 0.8,
        "cheating_salience": 0.75,
        "cheating_confidence": 0.85,
        "loyalty_raw": 0.6,
        "loyalty_salience": 0.6,
        "loyalty_confidence": 0.8,
        "betrayal_raw": 0.6,
        "betrayal_salience": 0.65,
        "betrayal_confidence": 0.8,
        "authority_raw": 0.2,
        "authority_salience": 0.2,
        "authority_confidence": 0.7,
        "subversion_raw": 0.95,
        "subversion_salience": 0.9,
        "subversion_confidence": 0.95,
        "sanctity_raw": 0.5,
        "sanctity_salience": 0.4,
        "sanctity_confidence": 0.75,
        "degradation_raw": 0.5,
        "degradation_salience": 0.4,
        "degradation_confidence": 0.75,
        "liberty_raw": 1.0,
        "liberty_salience": 1.0,
        "liberty_confidence": 1.0,
        "oppression_raw": 0.9,
        "oppression_salience": 0.9,
        "oppression_confidence": 0.95,
        "care_harm_tension": 0.08499999999999998,
        "fairness_cheating_tension": 0.15999999999999998,
        "loyalty_betrayal_tension": 0.030000000000000027,
        "authority_subversion_tension": 0.13999999999999999,
        "sanctity_degradation_tension": 0.0,
        "liberty_oppression_tension": 0.08999999999999998,
        "individualizing_tension": 0.24499999999999994,
        "binding_tension": 0.17,
        "liberty_tension": 0.08999999999999998,
        "moral_strategic_contradiction_index": 0.08416666666666665,
        "moral_salience_concentration": 0.25713308663496465,
        "individualizing_foundations_mean": 0.875,
        "binding_foundations_mean": 0.5583333333333332,
        "liberty_foundation_mean": 0.95,
        "document_count": 1
      },
      "john_mccain": {
        "care_raw": 0.2,
        "care_salience": 0.1,
        "care_confidence": 0.7,
        "harm_raw": 0.4,
        "harm_salience": 0.3,
        "harm_confidence": 0.8,
        "fairness_raw": 0.7,
        "fairness_salience": 0.7,
        "fairness_confidence": 0.9,
        "cheating_raw": 0.1,
        "cheating_salience": 0.1,
        "cheating_confidence": 0.6,
        "loyalty_raw": 0.8,
        "loyalty_salience": 0.8,
        "loyalty_confidence": 0.9,
        "betrayal_raw": 0.1,
        "betrayal_salience": 0.1,
        "betrayal_confidence": 0.6,
        "authority_raw": 0.6,
        "authority_salience": 0.5,
        "authority_confidence": 0.8,
        "subversion_raw": 0.1,
        "subversion_salience": 0.1,
        "subversion_confidence": 0.6,
        "sanctity_raw": 0.0,
        "sanctity_salience": 0.0,
        "sanctity_confidence": 0.9,
        "degradation_raw": 0.0,
        "degradation_salience": 0.0,
        "degradation_confidence": 0.9,
        "liberty_raw": 0.7,
        "liberty_salience": 0.6,
        "liberty_confidence": 0.9,
        "oppression_raw": 0.4,
        "oppression_salience": 0.3,
        "oppression_confidence": 0.8,
        "care_harm_tension": 0.04,
        "fairness_cheating_tension": 0.06,
        "loyalty_betrayal_tension": 0.07,
        "authority_subversion_tension": 0.04000000000000001,
        "sanctity_degradation_tension": 0.0,
        "liberty_oppression_tension": 0.12,
        "individualizing_tension": 0.1,
        "binding_tension": 0.11000000000000001,
        "liberty_tension": 0.12,
        "moral_strategic_contradiction_index": 0.055,
        "moral_salience_concentration": 0.28284271247461895,
        "individualizing_foundations_mean": 0.35000000000000003,
        "binding_foundations_mean": 0.26666666666666666,
        "liberty_foundation_mean": 0.55,
        "document_count": 1
      },
      "mitt_romney": {
        "care_raw": 0.1,
        "care_salience": 0.05,
        "care_confidence": 0.8,
        "harm_raw": 0.8,
        "harm_salience": 0.8,
        "harm_confidence": 0.9,
        "fairness_raw": 0.9,
        "fairness_salience": 0.9,
        "fairness_confidence": 0.95,
        "cheating_raw": 0.9,
        "cheating_salience": 0.9,
        "cheating_confidence": 0.95,
        "loyalty_raw": 0.6,
        "loyalty_salience": 0.7,
        "loyalty_confidence": 0.85,
        "betrayal_raw": 0.7,
        "betrayal_salience": 0.7,
        "betrayal_confidence": 0.9,
        "authority_raw": 0.9,
        "authority_salience": 0.9,
        "authority_confidence": 0.95,
        "subversion_raw": 0.8,
        "subversion_salience": 0.8,
        "subversion_confidence": 0.9,
        "sanctity_raw": 0.9,
        "sanctity_salience": 0.9,
        "sanctity_confidence": 0.95,
        "degradation_raw": 0.9,
        "degradation_salience": 0.9,
        "degradation_confidence": 0.95,
        "liberty_raw": 0.8,
        "liberty_salience": 0.8,
        "liberty_confidence": 0.9,
        "oppression_raw": 0.8,
        "oppression_salience": 0.8,
        "oppression_confidence": 0.9,
        "care_harm_tension": 0.07500000000000001,
        "fairness_cheating_tension": 0.0,
        "loyalty_betrayal_tension": 0.0,
        "authority_subversion_tension": 0.07999999999999999,
        "sanctity_degradation_tension": 0.0,
        "liberty_oppression_tension": 0.0,
        "individualizing_tension": 0.07500000000000001,
        "binding_tension": 0.07999999999999999,
        "liberty_tension": 0.0,
        "moral_strategic_contradiction_index": 0.025833333333333333,
        "moral_salience_concentration": 0.23657115016609498,
        "individualizing_foundations_mean": 0.675,
        "binding_foundations_mean": 0.7999999999999999,
        "liberty_foundation_mean": 0.8,
        "document_count": 1
      },
      "steve_king": {
        "care_raw": 0.8,
        "care_salience": 0.8,
        "care_confidence": 0.9,
        "harm_raw": 0.9,
        "harm_salience": 0.9,
        "harm_confidence": 0.9,
        "fairness_raw": 0.7,
        "fairness_salience": 0.7,
        "fairness_confidence": 0.8,
        "cheating_raw": 0.8,
        "cheating_salience": 0.8,
        "cheating_confidence": 0.9,
        "loyalty_raw": 0.6,
        "loyalty_salience": 0.6,
        "loyalty_confidence": 0.8,
        "betrayal_raw": 0.9,
        "betrayal_salience": 0.9,
        "betrayal_confidence": 0.9,
        "authority_raw": 0.9,
        "authority_salience": 0.9,
        "authority_confidence": 0.9,
        "subversion_raw": 0.9,
        "subversion_salience": 0.9,
        "subversion_confidence": 0.9,
        "sanctity_raw": 0.7,
        "sanctity_salience": 0.6,
        "sanctity_confidence": 0.8,
        "degradation_raw": 0.8,
        "degradation_salience": 0.8,
        "degradation_confidence": 0.9,
        "liberty_raw": 0.3,
        "liberty_salience": 0.2,
        "liberty_confidence": 0.7,
        "oppression_raw": 0.8,
        "oppression_salience": 0.8,
        "oppression_confidence": 0.9,
        "care_harm_tension": 0.07999999999999999,
        "fairness_cheating_tension": 0.07000000000000006,
        "loyalty_betrayal_tension": 0.18000000000000002,
        "authority_subversion_tension": 0.0,
        "sanctity_degradation_tension": 0.14000000000000004,
        "liberty_oppression_tension": 0.18000000000000002,
        "individualizing_tension": 0.15000000000000005,
        "binding_tension": 0.32000000000000006,
        "liberty_tension": 0.18000000000000002,
        "moral_strategic_contradiction_index": 0.10833333333333335,
        "moral_salience_concentration": 0.20207259421636903,
        "individualizing_foundations_mean": 0.8,
        "binding_foundations_mean": 0.7999999999999999,
        "liberty_foundation_mean": 0.55,
        "document_count": 1
      }
    },
    "analyze_moral_identity_and_ideology": {
      "document_profile_analysis": [
        {
          "document_name": "alexandria_ocasio_cortez_2025_fighting_oligarchy.txt",
          "moral_identity_focus": "Focused Moral Identity",
          "ideological_profile": "Libertarian Profile",
          "tension_salience_interaction": "Authentic moral identity",
          "moral_salience_concentration": 0.34410622038709343,
          "moral_strategic_contradiction_index": 0.115
        },
        {
          "document_name": "bernie_sanders_2025_fighting_oligarchy.txt",
          "moral_identity_focus": "Focused Moral Identity",
          "ideological_profile": "Libertarian Profile",
          "tension_salience_interaction": "Authentic moral identity",
          "moral_salience_concentration": 0.3114907946497218,
          "moral_strategic_contradiction_index": 0.14083333333333334
        },
        {
          "document_name": "cory_booker_2018_first_step_act.txt",
          "moral_identity_focus": "Moderate Moral Focus",
          "ideological_profile": "Libertarian Profile",
          "tension_salience_interaction": "Moderate Profile",
          "moral_salience_concentration": 0.20207259421636903,
          "moral_strategic_contradiction_index": 0.07500000000000002
        },
        {
          "document_name": "jd_vance_2022_natcon_conference.txt",
          "moral_identity_focus": "Distributed Moral Identity",
          "ideological_profile": "Balanced Profile",
          "tension_salience_interaction": "Sophisticated moral complexity",
          "moral_salience_concentration": 0.13789543689024492,
          "moral_strategic_contradiction_index": 0.07666666666666667
        },
        {
          "document_name": "john_lewis_1963_march_on_washington.txt",
          "moral_identity_focus": "Moderate Moral Focus",
          "ideological_profile": "Libertarian Profile",
          "tension_salience_interaction": "Moderate Profile",
          "moral_salience_concentration": 0.25713308663496465,
          "moral_strategic_contradiction_index": 0.08416666666666665
        },
        {
          "document_name": "john_mccain_2008_concession.txt",
          "moral_identity_focus": "Moderate Moral Focus",
          "ideological_profile": "Balanced Profile",
          "tension_salience_interaction": "Moderate Profile",
          "moral_salience_concentration": 0.28284271247461895,
          "moral_strategic_contradiction_index": 0.055
        },
        {
          "document_name": "mitt_romney_2020_impeachment.txt",
          "moral_identity_focus": "Moderate Moral Focus",
          "ideological_profile": "Libertarian Profile",
          "tension_salience_interaction": "Moderate Profile",
          "moral_salience_concentration": 0.23657115016609498,
          "moral_strategic_contradiction_index": 0.025833333333333333
        },
        {
          "document_name": "steve_king_2017_house_floor.txt",
          "moral_identity_focus": "Moderate Moral Focus",
          "ideological_profile": "Balanced Profile",
          "tension_salience_interaction": "Moderate Profile",
          "moral_salience_concentration": 0.20207259421636903,
          "moral_strategic_contradiction_index": 0.10833333333333335
        }
      ],
      "aggregate_profile_summary": {
        "identity_focus_distribution": {
          "Moderate Moral Focus": 0.625,
          "Focused Moral Identity": 0.25,
          "Distributed Moral Identity": 0.125
        },
        "ideological_profile_distribution": {
          "Libertarian Profile": 0.625,
          "Balanced Profile": 0.375
        },
        "interaction_effect_distribution": {
          "Moderate Profile": 0.625,
          "Authentic moral identity": 0.25,
          "Sophisticated moral complexity": 0.125
        }
      }
    },
    "analyze_moral_tension_and_contradiction": {
      "document_tension_analysis": [
        {
          "document_name": "alexandria_ocasio_cortez_2025_fighting_oligarchy.txt",
          "individualizing_tension": 0.35000000000000003,
          "binding_tension": 0.26000000000000006,
          "liberty_tension": 0.07999999999999999,
          "moral_strategic_contradiction_index": 0.115,
          "msci_classification": "Moral Coherence"
        },
        {
          "document_name": "bernie_sanders_2025_fighting_oligarchy.txt",
          "individualizing_tension": 0.18000000000000002,
          "binding_tension": 0.4149999999999999,
          "liberty_tension": 0.25,
          "moral_strategic_contradiction_index": 0.14083333333333334,
          "msci_classification": "Moral Coherence"
        },
        {
          "document_name": "cory_booker_2018_first_step_act.txt",
          "individualizing_tension": 0.16000000000000014,
          "binding_tension": 0.20999999999999996,
          "liberty_tension": 0.08000000000000007,
          "moral_strategic_contradiction_index": 0.07500000000000002,
          "msci_classification": "Moral Coherence"
        },
        {
          "document_name": "jd_vance_2022_natcon_conference.txt",
          "individualizing_tension": 0.13000000000000012,
          "binding_tension": 0.22999999999999998,
          "liberty_tension": 0.09999999999999998,
          "moral_strategic_contradiction_index": 0.07666666666666667,
          "msci_classification": "Moral Coherence"
        },
        {
          "document_name": "john_lewis_1963_march_on_washington.txt",
          "individualizing_tension": 0.24499999999999994,
          "binding_tension": 0.17,
          "liberty_tension": 0.08999999999999998,
          "moral_strategic_contradiction_index": 0.08416666666666665,
          "msci_classification": "Moral Coherence"
        },
        {
          "document_name": "john_mccain_2008_concession.txt",
          "individualizing_tension": 0.1,
          "binding_tension": 0.11000000000000001,
          "liberty_tension": 0.12,
          "moral_strategic_contradiction_index": 0.055,
          "msci_classification": "Moral Coherence"
        },
        {
          "document_name": "mitt_romney_2020_impeachment.txt",
          "individualizing_tension": 0.07500000000000001,
          "binding_tension": 0.07999999999999999,
          "liberty_tension": 0.0,
          "moral_strategic_contradiction_index": 0.025833333333333333,
          "msci_classification": "Moral Coherence"
        },
        {
          "document_name": "steve_king_2017_house_floor.txt",
          "individualizing_tension": 0.15000000000000005,
          "binding_tension": 0.32000000000000006,
          "liberty_tension": 0.18000000000000002,
          "moral_strategic_contradiction_index": 0.10833333333333335,
          "msci_classification": "Moral Coherence"
        }
      ],
      "aggregate_coherence_summary": {
        "Moral Coherence": 1.0
      }
    },
    "calculate_derived_metrics": {
      "type": "dataframe",
      "data": [
        {
          "document_name": "alexandria_ocasio_cortez_2025_fighting_oligarchy.txt",
          "care_raw": 0.3,
          "care_salience": 0.2,
          "care_confidence": 0.7,
          "harm_raw": 0.9,
          "harm_salience": 0.9,
          "harm_confidence": 0.95,
          "fairness_raw": 0.7,
          "fairness_salience": 0.7,
          "fairness_confidence": 0.9,
          "cheating_raw": 0.9,
          "cheating_salience": 0.9,
          "cheating_confidence": 0.95,
          "loyalty_raw": 0.8,
          "loyalty_salience": 0.8,
          "loyalty_confidence": 0.9,
          "betrayal_raw": 0.4,
          "betrayal_salience": 0.4,
          "betrayal_confidence": 0.75,
          "authority_raw": 0.1,
          "authority_salience": 0.1,
          "authority_confidence": 0.8,
          "subversion_raw": 0.9,
          "subversion_salience": 0.9,
          "subversion_confidence": 0.95,
          "sanctity_raw": 0.2,
          "sanctity_salience": 0.1,
          "sanctity_confidence": 0.6,
          "degradation_raw": 0.3,
          "degradation_salience": 0.2,
          "degradation_confidence": 0.7,
          "liberty_raw": 0.8,
          "liberty_salience": 0.8,
          "liberty_confidence": 0.9,
          "oppression_raw": 0.9,
          "oppression_salience": 0.9,
          "oppression_confidence": 0.95,
          "care_harm_tension": 0.21,
          "fairness_cheating_tension": 0.14000000000000004,
          "loyalty_betrayal_tension": 0.16000000000000003,
          "authority_subversion_tension": 0.08000000000000002,
          "sanctity_degradation_tension": 0.020000000000000004,
          "liberty_oppression_tension": 0.07999999999999999,
          "individualizing_tension": 0.35000000000000003,
          "binding_tension": 0.26000000000000006,
          "liberty_tension": 0.07999999999999999,
          "moral_strategic_contradiction_index": 0.115,
          "moral_salience_concentration": 0.34410622038709343,
          "individualizing_foundations_mean": 0.7,
          "binding_foundations_mean": 0.45,
          "liberty_foundation_mean": 0.8500000000000001
        },
        {
          "document_name": "bernie_sanders_2025_fighting_oligarchy.txt",
          "care_raw": 0.3,
          "care_salience": 0.3,
          "care_confidence": 0.8,
          "harm_raw": 0.9,
          "harm_salience": 0.9,
          "harm_confidence": 0.95,
          "fairness_raw": 0.9,
          "fairness_salience": 0.95,
          "fairness_confidence": 0.95,
          "cheating_raw": 0.95,
          "cheating_salience": 0.95,
          "cheating_confidence": 0.95,
          "loyalty_raw": 0.6,
          "loyalty_salience": 0.4,
          "loyalty_confidence": 0.8,
          "betrayal_raw": 0.7,
          "betrayal_salience": 0.6,
          "betrayal_confidence": 0.85,
          "authority_raw": 0.3,
          "authority_salience": 0.2,
          "authority_confidence": 0.7,
          "subversion_raw": 0.9,
          "subversion_salience": 0.85,
          "subversion_confidence": 0.9,
          "sanctity_raw": 0.2,
          "sanctity_salience": 0.1,
          "sanctity_confidence": 0.6,
          "degradation_raw": 0.7,
          "degradation_salience": 0.6,
          "degradation_confidence": 0.8,
          "liberty_raw": 0.5,
          "liberty_salience": 0.4,
          "liberty_confidence": 0.75,
          "oppression_raw": 0.9,
          "oppression_salience": 0.9,
          "oppression_confidence": 0.95,
          "care_harm_tension": 0.18000000000000002,
          "fairness_cheating_tension": 0.0,
          "loyalty_betrayal_tension": 0.11999999999999997,
          "authority_subversion_tension": 0.19499999999999998,
          "sanctity_degradation_tension": 0.1,
          "liberty_oppression_tension": 0.25,
          "individualizing_tension": 0.18000000000000002,
          "binding_tension": 0.4149999999999999,
          "liberty_tension": 0.25,
          "moral_strategic_contradiction_index": 0.14083333333333334,
          "moral_salience_concentration": 0.3114907946497218,
          "individualizing_foundations_mean": 0.7625,
          "binding_foundations_mean": 0.5666666666666668,
          "liberty_foundation_mean": 0.7
        },
        {
          "document_name": "cory_booker_2018_first_step_act.txt",
          "care_raw": 0.8,
          "care_salience": 0.7,
          "care_confidence": 0.9,
          "harm_raw": 0.9,
          "harm_salience": 0.8,
          "harm_confidence": 0.9,
          "fairness_raw": 0.9,
          "fairness_salience": 0.8,
          "fairness_confidence": 0.9,
          "cheating_raw": 0.8,
          "cheating_salience": 0.7,
          "cheating_confidence": 0.9,
          "loyalty_raw": 0.7,
          "loyalty_salience": 0.6,
          "loyalty_confidence": 0.8,
          "betrayal_raw": 0.1,
          "betrayal_salience": 0.1,
          "betrayal_confidence": 0.7,
          "authority_raw": 0.5,
          "authority_salience": 0.4,
          "authority_confidence": 0.8,
          "subversion_raw": 0.7,
          "subversion_salience": 0.6,
          "subversion_confidence": 0.8,
          "sanctity_raw": 0.6,
          "sanctity_salience": 0.5,
          "sanctity_confidence": 0.8,
          "degradation_raw": 0.7,
          "degradation_salience": 0.6,
          "degradation_confidence": 0.9,
          "liberty_raw": 0.8,
          "liberty_salience": 0.7,
          "liberty_confidence": 0.9,
          "oppression_raw": 0.9,
          "oppression_salience": 0.8,
          "oppression_confidence": 0.9,
          "care_harm_tension": 0.08000000000000007,
          "fairness_cheating_tension": 0.08000000000000007,
          "loyalty_betrayal_tension": 0.05,
          "authority_subversion_tension": 0.09999999999999998,
          "sanctity_degradation_tension": 0.059999999999999984,
          "liberty_oppression_tension": 0.08000000000000007,
          "individualizing_tension": 0.16000000000000014,
          "binding_tension": 0.20999999999999996,
          "liberty_tension": 0.08000000000000007,
          "moral_strategic_contradiction_index": 0.07500000000000002,
          "moral_salience_concentration": 0.20207259421636903,
          "individualizing_foundations_mean": 0.8500000000000001,
          "binding_foundations_mean": 0.5499999999999999,
          "liberty_foundation_mean": 0.8500000000000001
        },
        {
          "document_name": "jd_vance_2022_natcon_conference.txt",
          "care_raw": 0.6,
          "care_salience": 0.7,
          "care_confidence": 0.9,
          "harm_raw": 0.8,
          "harm_salience": 0.8,
          "harm_confidence": 0.9,
          "fairness_raw": 0.7,
          "fairness_salience": 0.7,
          "fairness_confidence": 0.8,
          "cheating_raw": 0.8,
          "cheating_salience": 0.8,
          "cheating_confidence": 0.9,
          "loyalty_raw": 0.9,
          "loyalty_salience": 0.9,
          "loyalty_confidence": 0.9,
          "betrayal_raw": 0.8,
          "betrayal_salience": 0.8,
          "betrayal_confidence": 0.9,
          "authority_raw": 0.3,
          "authority_salience": 0.4,
          "authority_confidence": 0.7,
          "subversion_raw": 0.7,
          "subversion_salience": 0.7,
          "subversion_confidence": 0.9,
          "sanctity_raw": 0.6,
          "sanctity_salience": 0.6,
          "sanctity_confidence": 0.8,
          "degradation_raw": 0.7,
          "degradation_salience": 0.7,
          "degradation_confidence": 0.8,
          "liberty_raw": 0.5,
          "liberty_salience": 0.5,
          "liberty_confidence": 0.7,
          "oppression_raw": 0.7,
          "oppression_salience": 0.7,
          "oppression_confidence": 0.8,
          "care_harm_tension": 0.06000000000000005,
          "fairness_cheating_tension": 0.07000000000000006,
          "loyalty_betrayal_tension": 0.07999999999999999,
          "authority_subversion_tension": 0.08999999999999998,
          "sanctity_degradation_tension": 0.059999999999999984,
          "liberty_oppression_tension": 0.09999999999999998,
          "individualizing_tension": 0.13000000000000012,
          "binding_tension": 0.22999999999999998,
          "liberty_tension": 0.09999999999999998,
          "moral_strategic_contradiction_index": 0.07666666666666667,
          "moral_salience_concentration": 0.13789543689024492,
          "individualizing_foundations_mean": 0.7249999999999999,
          "binding_foundations_mean": 0.6666666666666666,
          "liberty_foundation_mean": 0.6
        },
        {
          "document_name": "john_lewis_1963_march_on_washington.txt",
          "care_raw": 0.85,
          "care_salience": 0.8,
          "care_confidence": 0.9,
          "harm_raw": 0.9,
          "harm_salience": 0.9,
          "harm_confidence": 0.95,
          "fairness_raw": 0.95,
          "fairness_salience": 0.95,
          "fairness_confidence": 0.95,
          "cheating_raw": 0.8,
          "cheating_salience": 0.75,
          "cheating_confidence": 0.85,
          "loyalty_raw": 0.6,
          "loyalty_salience": 0.6,
          "loyalty_confidence": 0.8,
          "betrayal_raw": 0.6,
          "betrayal_salience": 0.65,
          "betrayal_confidence": 0.8,
          "authority_raw": 0.2,
          "authority_salience": 0.2,
          "authority_confidence": 0.7,
          "subversion_raw": 0.95,
          "subversion_salience": 0.9,
          "subversion_confidence": 0.95,
          "sanctity_raw": 0.5,
          "sanctity_salience": 0.4,
          "sanctity_confidence": 0.75,
          "degradation_raw": 0.5,
          "degradation_salience": 0.4,
          "degradation_confidence": 0.75,
          "liberty_raw": 1.0,
          "liberty_salience": 1.0,
          "liberty_confidence": 1.0,
          "oppression_raw": 0.9,
          "oppression_salience": 0.9,
          "oppression_confidence": 0.95,
          "care_harm_tension": 0.08499999999999998,
          "fairness_cheating_tension": 0.15999999999999998,
          "loyalty_betrayal_tension": 0.030000000000000027,
          "authority_subversion_tension": 0.13999999999999999,
          "sanctity_degradation_tension": 0.0,
          "liberty_oppression_tension": 0.08999999999999998,
          "individualizing_tension": 0.24499999999999994,
          "binding_tension": 0.17,
          "liberty_tension": 0.08999999999999998,
          "moral_strategic_contradiction_index": 0.08416666666666665,
          "moral_salience_concentration": 0.25713308663496465,
          "individualizing_foundations_mean": 0.875,
          "binding_foundations_mean": 0.5583333333333332,
          "liberty_foundation_mean": 0.95
        },
        {
          "document_name": "john_mccain_2008_concession.txt",
          "care_raw": 0.2,
          "care_salience": 0.1,
          "care_confidence": 0.7,
          "harm_raw": 0.4,
          "harm_salience": 0.3,
          "harm_confidence": 0.8,
          "fairness_raw": 0.7,
          "fairness_salience": 0.7,
          "fairness_confidence": 0.9,
          "cheating_raw": 0.1,
          "cheating_salience": 0.1,
          "cheating_confidence": 0.6,
          "loyalty_raw": 0.8,
          "loyalty_salience": 0.8,
          "loyalty_confidence": 0.9,
          "betrayal_raw": 0.1,
          "betrayal_salience": 0.1,
          "betrayal_confidence": 0.6,
          "authority_raw": 0.6,
          "authority_salience": 0.5,
          "authority_confidence": 0.8,
          "subversion_raw": 0.1,
          "subversion_salience": 0.1,
          "subversion_confidence": 0.6,
          "sanctity_raw": 0.0,
          "sanctity_salience": 0.0,
          "sanctity_confidence": 0.9,
          "degradation_raw": 0.0,
          "degradation_salience": 0.0,
          "degradation_confidence": 0.9,
          "liberty_raw": 0.7,
          "liberty_salience": 0.6,
          "liberty_confidence": 0.9,
          "oppression_raw": 0.4,
          "oppression_salience": 0.3,
          "oppression_confidence": 0.8,
          "care_harm_tension": 0.04,
          "fairness_cheating_tension": 0.06,
          "loyalty_betrayal_tension": 0.07,
          "authority_subversion_tension": 0.04000000000000001,
          "sanctity_degradation_tension": 0.0,
          "liberty_oppression_tension": 0.12,
          "individualizing_tension": 0.1,
          "binding_tension": 0.11000000000000001,
          "liberty_tension": 0.12,
          "moral_strategic_contradiction_index": 0.055,
          "moral_salience_concentration": 0.28284271247461895,
          "individualizing_foundations_mean": 0.35000000000000003,
          "binding_foundations_mean": 0.26666666666666666,
          "liberty_foundation_mean": 0.55
        },
        {
          "document_name": "mitt_romney_2020_impeachment.txt",
          "care_raw": 0.1,
          "care_salience": 0.05,
          "care_confidence": 0.8,
          "harm_raw": 0.8,
          "harm_salience": 0.8,
          "harm_confidence": 0.9,
          "fairness_raw": 0.9,
          "fairness_salience": 0.9,
          "fairness_confidence": 0.95,
          "cheating_raw": 0.9,
          "cheating_salience": 0.9,
          "cheating_confidence": 0.95,
          "loyalty_raw": 0.6,
          "loyalty_salience": 0.7,
          "loyalty_confidence": 0.85,
          "betrayal_raw": 0.7,
          "betrayal_salience": 0.7,
          "betrayal_confidence": 0.9,
          "authority_raw": 0.9,
          "authority_salience": 0.9,
          "authority_confidence": 0.95,
          "subversion_raw": 0.8,
          "subversion_salience": 0.8,
          "subversion_confidence": 0.9,
          "sanctity_raw": 0.9,
          "sanctity_salience": 0.9,
          "sanctity_confidence": 0.95,
          "degradation_raw": 0.9,
          "degradation_salience": 0.9,
          "degradation_confidence": 0.95,
          "liberty_raw": 0.8,
          "liberty_salience": 0.8,
          "liberty_confidence": 0.9,
          "oppression_raw": 0.8,
          "oppression_salience": 0.8,
          "oppression_confidence": 0.9,
          "care_harm_tension": 0.07500000000000001,
          "fairness_cheating_tension": 0.0,
          "loyalty_betrayal_tension": 0.0,
          "authority_subversion_tension": 0.07999999999999999,
          "sanctity_degradation_tension": 0.0,
          "liberty_oppression_tension": 0.0,
          "individualizing_tension": 0.07500000000000001,
          "binding_tension": 0.07999999999999999,
          "liberty_tension": 0.0,
          "moral_strategic_contradiction_index": 0.025833333333333333,
          "moral_salience_concentration": 0.23657115016609498,
          "individualizing_foundations_mean": 0.675,
          "binding_foundations_mean": 0.7999999999999999,
          "liberty_foundation_mean": 0.8
        },
        {
          "document_name": "steve_king_2017_house_floor.txt",
          "care_raw": 0.8,
          "care_salience": 0.8,
          "care_confidence": 0.9,
          "harm_raw": 0.9,
          "harm_salience": 0.9,
          "harm_confidence": 0.9,
          "fairness_raw": 0.7,
          "fairness_salience": 0.7,
          "fairness_confidence": 0.8,
          "cheating_raw": 0.8,
          "cheating_salience": 0.8,
          "cheating_confidence": 0.9,
          "loyalty_raw": 0.6,
          "loyalty_salience": 0.6,
          "loyalty_confidence": 0.8,
          "betrayal_raw": 0.9,
          "betrayal_salience": 0.9,
          "betrayal_confidence": 0.9,
          "authority_raw": 0.9,
          "authority_salience": 0.9,
          "authority_confidence": 0.9,
          "subversion_raw": 0.9,
          "subversion_salience": 0.9,
          "subversion_confidence": 0.9,
          "sanctity_raw": 0.7,
          "sanctity_salience": 0.6,
          "sanctity_confidence": 0.8,
          "degradation_raw": 0.8,
          "degradation_salience": 0.8,
          "degradation_confidence": 0.9,
          "liberty_raw": 0.3,
          "liberty_salience": 0.2,
          "liberty_confidence": 0.7,
          "oppression_raw": 0.8,
          "oppression_salience": 0.8,
          "oppression_confidence": 0.9,
          "care_harm_tension": 0.07999999999999999,
          "fairness_cheating_tension": 0.07000000000000006,
          "loyalty_betrayal_tension": 0.18000000000000002,
          "authority_subversion_tension": 0.0,
          "sanctity_degradation_tension": 0.14000000000000004,
          "liberty_oppression_tension": 0.18000000000000002,
          "individualizing_tension": 0.15000000000000005,
          "binding_tension": 0.32000000000000006,
          "liberty_tension": 0.18000000000000002,
          "moral_strategic_contradiction_index": 0.10833333333333335,
          "moral_salience_concentration": 0.20207259421636903,
          "individualizing_foundations_mean": 0.8,
          "binding_foundations_mean": 0.7999999999999999,
          "liberty_foundation_mean": 0.55
        }
      ],
      "columns": [
        "document_name",
        "care_raw",
        "care_salience",
        "care_confidence",
        "harm_raw",
        "harm_salience",
        "harm_confidence",
        "fairness_raw",
        "fairness_salience",
        "fairness_confidence",
        "cheating_raw",
        "cheating_salience",
        "cheating_confidence",
        "loyalty_raw",
        "loyalty_salience",
        "loyalty_confidence",
        "betrayal_raw",
        "betrayal_salience",
        "betrayal_confidence",
        "authority_raw",
        "authority_salience",
        "authority_confidence",
        "subversion_raw",
        "subversion_salience",
        "subversion_confidence",
        "sanctity_raw",
        "sanctity_salience",
        "sanctity_confidence",
        "degradation_raw",
        "degradation_salience",
        "degradation_confidence",
        "liberty_raw",
        "liberty_salience",
        "liberty_confidence",
        "oppression_raw",
        "oppression_salience",
        "oppression_confidence",
        "care_harm_tension",
        "fairness_cheating_tension",
        "loyalty_betrayal_tension",
        "authority_subversion_tension",
        "sanctity_degradation_tension",
        "liberty_oppression_tension",
        "individualizing_tension",
        "binding_tension",
        "liberty_tension",
        "moral_strategic_contradiction_index",
        "moral_salience_concentration",
        "individualizing_foundations_mean",
        "binding_foundations_mean",
        "liberty_foundation_mean"
      ],
      "index": [
        0,
        1,
        2,
        3,
        4,
        5,
        6,
        7
      ],
      "shape": [
        8,
        51
      ]
    },
    "descriptive_stats_summary": {
      "raw_scores_stats": {
        "care_raw": {
          "count": 8.0,
          "mean": 0.49375,
          "std": 0.30288788590594284,
          "min": 0.1,
          "25%": 0.275,
          "50%": 0.44999999999999996,
          "75%": 0.8,
          "max": 0.85
        },
        "harm_raw": {
          "count": 8.0,
          "mean": 0.8125,
          "std": 0.17268882005337974,
          "min": 0.4,
          "25%": 0.8,
          "50%": 0.9,
          "75%": 0.9,
          "max": 0.9
        },
        "fairness_raw": {
          "count": 8.0,
          "mean": 0.80625,
          "std": 0.11475906437899737,
          "min": 0.7,
          "25%": 0.7,
          "50%": 0.8,
          "75%": 0.9,
          "max": 0.95
        },
        "cheating_raw": {
          "count": 8.0,
          "mean": 0.7562500000000001,
          "std": 0.2718159829212193,
          "min": 0.1,
          "25%": 0.8,
          "50%": 0.8,
          "75%": 0.9,
          "max": 0.95
        },
        "loyalty_raw": {
          "count": 8.0,
          "mean": 0.7,
          "std": 0.11952286093343939,
          "min": 0.6,
          "25%": 0.6,
          "50%": 0.6499999999999999,
          "75%": 0.8,
          "max": 0.9
        },
        "betrayal_raw": {
          "count": 8.0,
          "mean": 0.5375,
          "std": 0.30676887530703445,
          "min": 0.1,
          "25%": 0.325,
          "50%": 0.6499999999999999,
          "75%": 0.725,
          "max": 0.9
        },
        "authority_raw": {
          "count": 8.0,
          "mean": 0.47500000000000003,
          "std": 0.30589447293376937,
          "min": 0.1,
          "25%": 0.275,
          "50%": 0.4,
          "75%": 0.675,
          "max": 0.9
        },
        "subversion_raw": {
          "count": 8.0,
          "mean": 0.74375,
          "std": 0.2770217887253121,
          "min": 0.1,
          "25%": 0.7,
          "50%": 0.8500000000000001,
          "75%": 0.9,
          "max": 0.95
        },
        "sanctity_raw": {
          "count": 8.0,
          "mean": 0.4625,
          "std": 0.30207614933986426,
          "min": 0.0,
          "25%": 0.2,
          "50%": 0.55,
          "75%": 0.625,
          "max": 0.9
        },
        "degradation_raw": {
          "count": 8.0,
          "mean": 0.575,
          "std": 0.29640705601780615,
          "min": 0.0,
          "25%": 0.45,
          "50%": 0.7,
          "75%": 0.725,
          "max": 0.9
        },
        "liberty_raw": {
          "count": 8.0,
          "mean": 0.675,
          "std": 0.2251983252919207,
          "min": 0.3,
          "25%": 0.5,
          "50%": 0.75,
          "75%": 0.8,
          "max": 1.0
        },
        "oppression_raw": {
          "count": 8.0,
          "mean": 0.7875000000000001,
          "std": 0.17268882005337974,
          "min": 0.4,
          "25%": 0.775,
          "50%": 0.8500000000000001,
          "75%": 0.9,
          "max": 0.9
        }
      },
      "salience_scores_stats": {
        "care_salience": {
          "count": 8.0,
          "mean": 0.45625,
          "std": 0.32451887464367923,
          "min": 0.05,
          "25%": 0.17500000000000002,
          "50%": 0.5,
          "75%": 0.725,
          "max": 0.8
        },
        "harm_salience": {
          "count": 8.0,
          "mean": 0.7875000000000001,
          "std": 0.20310096011589904,
          "min": 0.3,
          "25%": 0.8,
          "50%": 0.8500000000000001,
          "75%": 0.9,
          "max": 0.9
        },
        "fairness_salience": {
          "count": 8.0,
          "mean": 0.8,
          "std": 0.11649647450214351,
          "min": 0.7,
          "25%": 0.7,
          "50%": 0.75,
          "75%": 0.9125,
          "max": 0.95
        },
        "cheating_salience": {
          "count": 8.0,
          "mean": 0.7375,
          "std": 0.27091116097875545,
          "min": 0.1,
          "25%": 0.7375,
          "50%": 0.8,
          "75%": 0.9,
          "max": 0.95
        },
        "loyalty_salience": {
          "count": 8.0,
          "mean": 0.675,
          "std": 0.15811388300841897,
          "min": 0.4,
          "25%": 0.6,
          "50%": 0.6499999999999999,
          "75%": 0.8,
          "max": 0.9
        },
        "betrayal_salience": {
          "count": 8.0,
          "mean": 0.53125,
          "std": 0.3034768760312955,
          "min": 0.1,
          "25%": 0.325,
          "50%": 0.625,
          "75%": 0.725,
          "max": 0.9
        },
        "authority_salience": {
          "count": 8.0,
          "mean": 0.45,
          "std": 0.3070597894314954,
          "min": 0.1,
          "25%": 0.2,
          "50%": 0.4,
          "75%": 0.6,
          "max": 0.9
        },
        "subversion_salience": {
          "count": 8.0,
          "mean": 0.71875,
          "std": 0.2724721479868565,
          "min": 0.1,
          "25%": 0.6749999999999999,
          "50%": 0.825,
          "75%": 0.9,
          "max": 0.9
        },
        "sanctity_salience": {
          "count": 8.0,
          "mean": 0.4,
          "std": 0.31167748898959186,
          "min": 0.0,
          "25%": 0.1,
          "50%": 0.45,
          "75%": 0.6,
          "max": 0.9
        },
        "degradation_salience": {
          "count": 8.0,
          "mean": 0.5249999999999999,
          "std": 0.30589447293376937,
          "min": 0.0,
          "25%": 0.35000000000000003,
          "50%": 0.6,
          "75%": 0.725,
          "max": 0.9
        },
        "liberty_salience": {
          "count": 8.0,
          "mean": 0.625,
          "std": 0.25495097567963926,
          "min": 0.2,
          "25%": 0.475,
          "50%": 0.6499999999999999,
          "75%": 0.8,
          "max": 1.0
        },
        "oppression_salience": {
          "count": 8.0,
          "mean": 0.7625,
          "std": 0.1995530720671285,
          "min": 0.3,
          "25%": 0.775,
          "50%": 0.8,
          "75%": 0.9,
          "max": 0.9
        }
      },
      "derived_metrics_stats": {
        "individualizing_tension": {
          "count": 8.0,
          "mean": 0.17375000000000004,
          "std": 0.08786149814988847,
          "min": 0.07500000000000001,
          "25%": 0.12250000000000008,
          "50%": 0.15500000000000008,
          "75%": 0.19625,
          "max": 0.35000000000000003
        },
        "binding_tension": {
          "count": 8.0,
          "mean": 0.224375,
          "std": 0.10946093562806516,
          "min": 0.07999999999999999,
          "25%": 0.15500000000000003,
          "50%": 0.21999999999999997,
          "75%": 0.2750000000000001,
          "max": 0.4149999999999999
        },
        "liberty_tension": {
          "count": 8.0,
          "mean": 0.1125,
          "std": 0.0745941399460153,
          "min": 0.0,
          "25%": 0.08000000000000004,
          "50%": 0.09499999999999997,
          "75%": 0.135,
          "max": 0.25
        },
        "moral_strategic_contradiction_index": {
          "count": 8.0,
          "mean": 0.08510416666666668,
          "std": 0.0361199432117362,
          "min": 0.025833333333333333,
          "25%": 0.07000000000000002,
          "50%": 0.08041666666666666,
          "75%": 0.11000000000000001,
          "max": 0.14083333333333334
        },
        "moral_salience_concentration": {
          "count": 8.0,
          "mean": 0.24677307370443458,
          "std": 0.06657451070737319,
          "min": 0.13789543689024492,
          "25%": 0.20207259421636903,
          "50%": 0.2468521184005298,
          "75%": 0.2900047330183947,
          "max": 0.34410622038709343
        },
        "individualizing_foundations_mean": {
          "count": 8.0,
          "mean": 0.7171875,
          "std": 0.16406356292172686,
          "min": 0.35000000000000003,
          "25%": 0.69375,
          "50%": 0.7437499999999999,
          "75%": 0.8125,
          "max": 0.875
        },
        "binding_foundations_mean": {
          "count": 8.0,
          "mean": 0.5822916666666667,
          "std": 0.17764092885136185,
          "min": 0.26666666666666666,
          "25%": 0.5249999999999999,
          "50%": 0.5625,
          "75%": 0.7,
          "max": 0.7999999999999999
        },
        "liberty_foundation_mean": {
          "count": 8.0,
          "mean": 0.73125,
          "std": 0.15338443765351722,
          "min": 0.55,
          "25%": 0.5875,
          "50%": 0.75,
          "75%": 0.8500000000000001,
          "max": 0.95
        }
      }
    },
    "foundation_correlation_matrix": {
      "care": {
        "care": 1.0,
        "harm": 0.4796695736584126,
        "fairness": 0.13485665338248654,
        "cheating": 0.20442614458110103,
        "loyalty": -0.13811381462675745,
        "betrayal": 0.1335683556816719,
        "authority": -0.1175677682145689,
        "subversion": 0.357008592385679,
        "sanctity": 0.32495847677420053,
        "degradation": 0.26056304858649293,
        "liberty": -0.05497740126582434,
        "oppression": 0.367006969169248
      },
      "harm": {
        "care": 0.4796695736584126,
        "harm": 1.0,
        "fairness": 0.3919674549505147,
        "cheating": 0.9415599221110923,
        "loyalty": -0.41527712412358986,
        "betrayal": 0.47528659922960415,
        "authority": -0.23663237409127155,
        "subversion": 0.9574612164803596,
        "sanctity": 0.4758237491665646,
        "degradation": 0.6768009240349397,
        "liberty": -0.027550776571673558,
        "oppression": 0.9640718562874251
      },
      "fairness": {
        "care": 0.13485665338248654,
        "harm": 0.3919674549505147,
        "fairness": 1.0,
        "cheating": 0.39929529813395637,
        "loyalty": -0.6769823371002843,
        "betrayal": -0.027898211960599405,
        "authority": -0.05595591855027287,
        "subversion": 0.3833660946362351,
        "sanctity": 0.296194085608515,
        "degradation": 0.4042287153920744,
        "liberty": 0.5320476665708265,
        "oppression": 0.5451501384943942
      },
      "cheating": {
        "care": 0.20442614458110103,
        "harm": 0.9415599221110923,
        "fairness": 0.39929529813395637,
        "cheating": 1.0,
        "loyalty": -0.3737618027519328,
        "betrayal": 0.5707197508015226,
        "authority": -0.18684634453955393,
        "subversion": 0.9397067125900258,
        "sanctity": 0.5252156780835264,
        "degradation": 0.7557929285857361,
        "liberty": -0.043758566425944334,
        "oppression": 0.9149299445160312
      },
      "loyalty": {
        "care": -0.13811381462675745,
        "harm": -0.41527712412358986,
        "fairness": -0.6769823371002843,
        "cheating": -0.3737618027519328,
        "loyalty": 1.0,
        "betrayal": -0.3116948831626101,
        "authority": -0.3907323325822816,
        "subversion": -0.49617501462943997,
        "sanctity": -0.3561041647120187,
        "degradation": -0.4838867031273072,
        "liberty": -0.05307448924342783,
        "oppression": -0.48448997814418826
      },
      "betrayal": {
        "care": 0.1335683556816719,
        "harm": 0.47528659922960415,
        "fairness": -0.027898211960599405,
        "cheating": 0.5707197508015226,
        "loyalty": -0.3116948831626101,
        "betrayal": 1.0,
        "authority": 0.17887798847962993,
        "subversion": 0.6251348631916587,
        "sanctity": 0.5106579721113261,
        "degradation": 0.6559316814843623,
        "liberty": -0.5221399122150566,
        "oppression": 0.30674525198506386
      },
      "authority": {
        "care": -0.1175677682145689,
        "harm": -0.23663237409127155,
        "fairness": -0.05595591855027287,
        "cheating": -0.18684634453955393,
        "loyalty": -0.3907323325822816,
        "betrayal": 0.17887798847962993,
        "authority": 1.0,
        "subversion": -0.21283732287343568,
        "sanctity": 0.5295103693351758,
        "degradation": 0.3860182846127358,
        "liberty": -0.36291358219720904,
        "oppression": -0.30424162383163467
      },
      "subversion": {
        "care": 0.357008592385679,
        "harm": 0.9574612164803596,
        "fairness": 0.3833660946362351,
        "cheating": 0.9397067125900258,
        "loyalty": -0.49617501462943997,
        "betrayal": 0.6251348631916587,
        "authority": -0.21283732287343568,
        "subversion": 1.0,
        "sanctity": 0.46626524910780776,
        "degradation": 0.6502504991752872,
        "liberty": -0.014312084498264979,
        "oppression": 0.92386608607754
      },
      "sanctity": {
        "care": 0.32495847677420053,
        "harm": 0.4758237491665646,
        "fairness": 0.296194085608515,
        "cheating": 0.5252156780835264,
        "loyalty": -0.3561041647120187,
        "betrayal": 0.5106579721113261,
        "authority": 0.5295103693351758,
        "subversion": 0.46626524910780776,
        "sanctity": 1.0,
        "degradation": 0.8496040962847614,
        "liberty": -0.057750142570840296,
        "oppression": 0.37312797596514796
      },
      "degradation": {
        "care": 0.26056304858649293,
        "harm": 0.6768009240349397,
        "fairness": 0.4042287153920744,
        "cheating": 0.7557929285857361,
        "loyalty": -0.4838867031273072,
        "betrayal": 0.6559316814843623,
        "authority": 0.3860182846127358,
        "subversion": 0.6502504991752872,
        "sanctity": 0.8496040962847614,
        "degradation": 1.0,
        "liberty": -0.3103246454103617,
        "oppression": 0.5791183164422683
      },
      "liberty": {
        "care": -0.05497740126582434,
        "harm": -0.027550776571673558,
        "fairness": 0.5320476665708265,
        "cheating": -0.043758566425944334,
        "loyalty": -0.05307448924342783,
        "betrayal": -0.5221399122150566,
        "authority": -0.36291358219720904,
        "subversion": -0.014312084498264979,
        "sanctity": -0.057750142570840296,
        "degradation": -0.3103246454103617,
        "liberty": 1.0,
        "oppression": 0.1744882516206003
      },
      "oppression": {
        "care": 0.367006969169248,
        "harm": 0.9640718562874251,
        "fairness": 0.5451501384943942,
        "cheating": 0.9149299445160312,
        "loyalty": -0.48448997814418826,
        "betrayal": 0.30674525198506386,
        "authority": -0.30424162383163467,
        "subversion": 0.92386608607754,
        "sanctity": 0.37312797596514796,
        "degradation": 0.5791183164422683,
        "liberty": 0.1744882516206003,
        "oppression": 1.0
      }
    },
    "generate_statistical_summary_report": "STATISTICAL ANALYSIS SUMMARY REPORT\n==================================================\nAnalysis Timestamp: Unknown\nSample Size: Unknown\nAlpha Level: Unknown\nVariables: 0\n",
    "perform_statistical_analysis": {
      "analysis_metadata": {
        "timestamp": "2025-08-27T23:00:49.952108",
        "sample_size": 8,
        "alpha_level": 0.05,
        "variables_analyzed": [
          "care_raw",
          "care_salience",
          "care_confidence",
          "harm_raw",
          "harm_salience",
          "harm_confidence",
          "fairness_raw",
          "fairness_salience",
          "fairness_confidence",
          "cheating_raw",
          "cheating_salience",
          "cheating_confidence",
          "loyalty_raw",
          "loyalty_salience",
          "loyalty_confidence",
          "betrayal_raw",
          "betrayal_salience",
          "betrayal_confidence",
          "authority_raw",
          "authority_salience",
          "authority_confidence",
          "subversion_raw",
          "subversion_salience",
          "subversion_confidence",
          "sanctity_raw",
          "sanctity_salience",
          "sanctity_confidence",
          "degradation_raw",
          "degradation_salience",
          "degradation_confidence",
          "liberty_raw",
          "liberty_salience",
          "liberty_confidence",
          "oppression_raw",
          "oppression_salience",
          "oppression_confidence"
        ]
      }
    },
    "run_complete_statistical_analysis": {
      "analysis_metadata": {
        "timestamp": "2025-08-27T23:00:49.957332",
        "sample_size": 8,
        "alpha_level": 0.05,
        "variables_analyzed": [
          "care_raw",
          "care_salience",
          "care_confidence",
          "harm_raw",
          "harm_salience",
          "harm_confidence",
          "fairness_raw",
          "fairness_salience",
          "fairness_confidence",
          "cheating_raw",
          "cheating_salience",
          "cheating_confidence",
          "loyalty_raw",
          "loyalty_salience",
          "loyalty_confidence",
          "betrayal_raw",
          "betrayal_salience",
          "betrayal_confidence",
          "authority_raw",
          "authority_salience",
          "authority_confidence",
          "subversion_raw",
          "subversion_salience",
          "subversion_confidence",
          "sanctity_raw",
          "sanctity_salience",
          "sanctity_confidence",
          "degradation_raw",
          "degradation_salience",
          "degradation_confidence",
          "liberty_raw",
          "liberty_salience",
          "liberty_confidence",
          "oppression_raw",
          "oppression_salience",
          "oppression_confidence"
        ]
      }
    },
    "summarize_analyst_confidence": {
      "document_confidence_summary": [
        {
          "document_name": "alexandria_ocasio_cortez_2025_fighting_oligarchy.txt",
          "mean_confidence": 0.8374999999999999
        },
        {
          "document_name": "bernie_sanders_2025_fighting_oligarchy.txt",
          "mean_confidence": 0.8333333333333334
        },
        {
          "document_name": "cory_booker_2018_first_step_act.txt",
          "mean_confidence": 0.8500000000000001
        },
        {
          "document_name": "jd_vance_2022_natcon_conference.txt",
          "mean_confidence": 0.8333333333333335
        },
        {
          "document_name": "john_lewis_1963_march_on_washington.txt",
          "mean_confidence": 0.8624999999999999
        },
        {
          "document_name": "john_mccain_2008_concession.txt",
          "mean_confidence": 0.7833333333333333
        },
        {
          "document_name": "mitt_romney_2020_impeachment.txt",
          "mean_confidence": 0.9083333333333333
        },
        {
          "document_name": "steve_king_2017_house_floor.txt",
          "mean_confidence": 0.8583333333333334
        }
      ],
      "dataset_confidence_summary": {
        "overall_mean_confidence": 0.8458333333333333,
        "overall_std_dev_confidence": 0.0351442605217274,
        "min_mean_confidence": 0.7833333333333333,
        "max_mean_confidence": 0.9083333333333333,
        "confidence_per_dimension": {
          "care_confidence": 0.825,
          "harm_confidence": 0.90625,
          "fairness_confidence": 0.89375,
          "cheating_confidence": 0.875,
          "loyalty_confidence": 0.84375,
          "betrayal_confidence": 0.8,
          "authority_confidence": 0.79375,
          "subversion_confidence": 0.8625,
          "sanctity_confidence": 0.7749999999999999,
          "degradation_confidence": 0.8375,
          "liberty_confidence": 0.84375,
          "oppression_confidence": 0.89375
        }
      }
    }
  },
  "status": "success_with_data",
  "validation_passed": true
}