{'generation_metadata': {'status': 'success', 'functions_generated': 6, 'output_file': 'automatedstatisticalanalysisagent_functions.py', 'module_size': 22459}, 'statistical_data': {'add_speaker_metadata': {'type': 'dataframe', 'data': [{'document_name': 'alexandria_ocasio_cortez_2025_fighting_oligarchy.txt', 'care_raw': 0.9, 'care_salience': 0.8, 'care_confidence': 0.95, 'harm_raw': 0.8, 'harm_salience': 0.8, 'harm_confidence': 0.95, 'fairness_raw': 0.8, 'fairness_salience': 0.85, 'fairness_confidence': 0.95, 'cheating_raw': 1.0, 'cheating_salience': 1.0, 'cheating_confidence': 1.0, 'loyalty_raw': 0.9, 'loyalty_salience': 0.9, 'loyalty_confidence': 0.95, 'betrayal_raw': 0.6, 'betrayal_salience': 0.5, 'betrayal_confidence': 0.9, 'authority_raw': 0.0, 'authority_salience': 0.0, 'authority_confidence': 1.0, 'subversion_raw': 0.9, 'subversion_salience': 0.95, 'subversion_confidence': 0.95, 'sanctity_raw': 0.1, 'sanctity_salience': 0.1, 'sanctity_confidence': 0.7, 'degradation_raw': 0.0, 'degradation_salience': 0.0, 'degradation_confidence': 1.0, 'liberty_raw': 0.0, 'liberty_salience': 0.0, 'liberty_confidence': 1.0, 'oppression_raw': 0.9, 'oppression_salience': 0.9, 'oppression_confidence': 0.95, 'speaker': 'Unknown', 'ideology': 'Unknown', 'party': 'Unknown'}, {'document_name': 'bernie_sanders_2025_fighting_oligarchy.txt', 'care_raw': 0.8, 'care_salience': 0.9, 'care_confidence': 0.95, 'harm_raw': 0.9, 'harm_salience': 0.9, 'harm_confidence': 0.95, 'fairness_raw': 0.9, 'fairness_salience': 1.0, 'fairness_confidence': 1.0, 'cheating_raw': 0.9, 'cheating_salience': 1.0, 'cheating_confidence': 1.0, 'loyalty_raw': 0.7, 'loyalty_salience': 0.6, 'loyalty_confidence': 0.9, 'betrayal_raw': 0.7, 'betrayal_salience': 0.6, 'betrayal_confidence': 0.9, 'authority_raw': 0.1, 'authority_salience': 0.1, 'authority_confidence': 0.9, 'subversion_raw': 0.9, 'subversion_salience': 0.9, 'subversion_confidence': 1.0, 'sanctity_raw': 0.3, 'sanctity_salience': 0.2, 'sanctity_confidence': 0.7, 'degradation_raw': 0.5, 'degradation_salience': 0.5, 'degradation_confidence': 0.8, 'liberty_raw': 0.1, 'liberty_salience': 0.1, 'liberty_confidence': 0.9, 'oppression_raw': 0.9, 'oppression_salience': 0.9, 'oppression_confidence': 1.0, 'speaker': 'Bernie Sanders', 'ideology': 'Progressive', 'party': 'Independent'}, {'document_name': 'cory_booker_2018_first_step_act.txt', 'care_raw': 0.9, 'care_salience': 0.9, 'care_confidence': 1.0, 'harm_raw': 0.9, 'harm_salience': 0.9, 'harm_confidence': 1.0, 'fairness_raw': 1.0, 'fairness_salience': 1.0, 'fairness_confidence': 1.0, 'cheating_raw': 0.8, 'cheating_salience': 0.8, 'cheating_confidence': 1.0, 'loyalty_raw': 0.7, 'loyalty_salience': 0.7, 'loyalty_confidence': 0.9, 'betrayal_raw': 0.0, 'betrayal_salience': 0.0, 'betrayal_confidence': 1.0, 'authority_raw': 0.4, 'authority_salience': 0.3, 'authority_confidence': 0.9, 'subversion_raw': 0.8, 'subversion_salience': 0.8, 'subversion_confidence': 1.0, 'sanctity_raw': 0.9, 'sanctity_salience': 0.8, 'sanctity_confidence': 0.95, 'degradation_raw': 0.9, 'degradation_salience': 0.9, 'degradation_confidence': 1.0, 'liberty_raw': 0.8, 'liberty_salience': 0.7, 'liberty_confidence': 1.0, 'oppression_raw': 1.0, 'oppression_salience': 1.0, 'oppression_confidence': 1.0, 'speaker': 'Cory Booker', 'ideology': 'Progressive', 'party': 'Democrat'}, {'document_name': 'jd_vance_2022_natcon_conference.txt', 'care_raw': 0.7, 'care_salience': 0.8, 'care_confidence': 0.95, 'harm_raw': 0.6, 'harm_salience': 0.7, 'harm_confidence': 0.9, 'fairness_raw': 0.7, 'fairness_salience': 0.8, 'fairness_confidence': 0.9, 'cheating_raw': 0.8, 'cheating_salience': 0.9, 'cheating_confidence': 0.95, 'loyalty_raw': 0.9, 'loyalty_salience': 1.0, 'loyalty_confidence': 1.0, 'betrayal_raw': 0.8, 'betrayal_salience': 0.9, 'betrayal_confidence': 0.95, 'authority_raw': 0.4, 'authority_salience': 0.5, 'authority_confidence': 0.9, 'subversion_raw': 0.8, 'subversion_salience': 0.8, 'subversion_confidence': 0.9, 'sanctity_raw': 0.5, 'sanctity_salience': 0.6, 'sanctity_confidence': 0.85, 'degradation_raw': 0.6, 'degradation_salience': 0.6, 'degradation_confidence': 0.85, 'liberty_raw': 0.0, 'liberty_salience': 0.0, 'liberty_confidence': 1.0, 'oppression_raw': 0.8, 'oppression_salience': 0.8, 'oppression_confidence': 0.95, 'speaker': 'JD Vance', 'ideology': 'Conservative', 'party': 'Republican'}, {'document_name': 'john_lewis_1963_march_on_washington.txt', 'care_raw': 0.9, 'care_salience': 0.8, 'care_confidence': 0.95, 'harm_raw': 0.9, 'harm_salience': 0.9, 'harm_confidence': 0.95, 'fairness_raw': 1.0, 'fairness_salience': 1.0, 'fairness_confidence': 1.0, 'cheating_raw': 0.8, 'cheating_salience': 0.8, 'cheating_confidence': 0.9, 'loyalty_raw': 0.6, 'loyalty_salience': 0.5, 'loyalty_confidence': 0.85, 'betrayal_raw': 0.5, 'betrayal_salience': 0.4, 'betrayal_confidence': 0.8, 'authority_raw': 0.0, 'authority_salience': 0.0, 'authority_confidence': 0.9, 'subversion_raw': 0.9, 'subversion_salience': 0.9, 'subversion_confidence': 0.95, 'sanctity_raw': 0.7, 'sanctity_salience': 0.6, 'sanctity_confidence': 0.85, 'degradation_raw': 0.6, 'degradation_salience': 0.5, 'degradation_confidence': 0.8, 'liberty_raw': 1.0, 'liberty_salience': 1.0, 'liberty_confidence': 1.0, 'oppression_raw': 1.0, 'oppression_salience': 1.0, 'oppression_confidence': 1.0, 'speaker': 'John Lewis', 'ideology': 'Progressive', 'party': 'Democrat'}, {'document_name': 'john_mccain_2008_concession.txt', 'care_raw': 0.6, 'care_salience': 0.5, 'care_confidence': 0.95, 'harm_raw': 0.4, 'harm_salience': 0.3, 'harm_confidence': 0.9, 'fairness_raw': 0.9, 'fairness_salience': 0.9, 'fairness_confidence': 1.0, 'cheating_raw': 0.1, 'cheating_salience': 0.1, 'cheating_confidence': 0.85, 'loyalty_raw': 0.9, 'loyalty_salience': 0.9, 'loyalty_confidence': 1.0, 'betrayal_raw': 0.0, 'betrayal_salience': 0.0, 'betrayal_confidence': 1.0, 'authority_raw': 0.8, 'authority_salience': 0.8, 'authority_confidence': 1.0, 'subversion_raw': 0.0, 'subversion_salience': 0.0, 'subversion_confidence': 1.0, 'sanctity_raw': 0.5, 'sanctity_salience': 0.4, 'sanctity_confidence': 0.9, 'degradation_raw': 0.3, 'degradation_salience': 0.2, 'degradation_confidence': 0.9, 'liberty_raw': 0.3, 'liberty_salience': 0.2, 'liberty_confidence': 0.9, 'oppression_raw': 0.6, 'oppression_salience': 0.6, 'oppression_confidence': 0.95, 'speaker': 'John McCain', 'ideology': 'Conservative', 'party': 'Republican'}, {'document_name': 'mitt_romney_2020_impeachment.txt', 'care_raw': 0.1, 'care_salience': 0.1, 'care_confidence': 0.9, 'harm_raw': 0.7, 'harm_salience': 0.6, 'harm_confidence': 0.9, 'fairness_raw': 0.9, 'fairness_salience': 0.9, 'fairness_confidence': 1.0, 'cheating_raw': 0.9, 'cheating_salience': 0.9, 'cheating_confidence': 1.0, 'loyalty_raw': 0.8, 'loyalty_salience': 0.8, 'loyalty_confidence': 1.0, 'betrayal_raw': 0.5, 'betrayal_salience': 0.5, 'betrayal_confidence': 0.9, 'authority_raw': 0.9, 'authority_salience': 0.9, 'authority_confidence': 1.0, 'subversion_raw': 0.8, 'subversion_salience': 0.8, 'subversion_confidence': 0.9, 'sanctity_raw': 0.9, 'sanctity_salience': 0.9, 'sanctity_confidence': 1.0, 'degradation_raw': 0.8, 'degradation_salience': 0.7, 'degradation_confidence': 0.9, 'liberty_raw': 0.6, 'liberty_salience': 0.5, 'liberty_confidence': 0.9, 'oppression_raw': 0.7, 'oppression_salience': 0.6, 'oppression_confidence': 0.9, 'speaker': 'Mitt Romney', 'ideology': 'Conservative', 'party': 'Republican'}, {'document_name': 'steve_king_2017_house_floor.txt', 'care_raw': 0.8, 'care_salience': 0.7, 'care_confidence': 0.95, 'harm_raw': 0.9, 'harm_salience': 0.8, 'harm_confidence': 1.0, 'fairness_raw': 0.7, 'fairness_salience': 0.6, 'fairness_confidence': 0.9, 'cheating_raw': 0.8, 'cheating_salience': 0.7, 'cheating_confidence': 0.9, 'loyalty_raw': 0.8, 'loyalty_salience': 0.7, 'loyalty_confidence': 0.9, 'betrayal_raw': 0.7, 'betrayal_salience': 0.6, 'betrayal_confidence': 0.9, 'authority_raw': 1.0, 'authority_salience': 1.0, 'authority_confidence': 1.0, 'subversion_raw': 1.0, 'subversion_salience': 1.0, 'subversion_confidence': 1.0, 'sanctity_raw': 0.7, 'sanctity_salience': 0.5, 'sanctity_confidence': 0.9, 'degradation_raw': 0.8, 'degradation_salience': 0.6, 'degradation_confidence': 0.9, 'liberty_raw': 0.4, 'liberty_salience': 0.2, 'liberty_confidence': 0.8, 'oppression_raw': 0.7, 'oppression_salience': 0.6, 'oppression_confidence': 0.9, 'speaker': 'Steve King', 'ideology': 'Conservative', 'party': 'Republican'}], 'columns': ['document_name', 'care_raw', 'care_salience', 'care_confidence', 'harm_raw', 'harm_salience', 'harm_confidence', 'fairness_raw', 'fairness_salience', 'fairness_confidence', 'cheating_raw', 'cheating_salience', 'cheating_confidence', 'loyalty_raw', 'loyalty_salience', 'loyalty_confidence', 'betrayal_raw', 'betrayal_salience', 'betrayal_confidence', 'authority_raw', 'authority_salience', 'authority_confidence', 'subversion_raw', 'subversion_salience', 'subversion_confidence', 'sanctity_raw', 'sanctity_salience', 'sanctity_confidence', 'degradation_raw', 'degradation_salience', 'degradation_confidence', 'liberty_raw', 'liberty_salience', 'liberty_confidence', 'oppression_raw', 'oppression_salience', 'oppression_confidence', 'speaker', 'ideology', 'party'], 'index': [0, 1, 2, 3, 4, 5, 6, 7], 'shape': (8, 40)}, 'calculate_derived_metrics': {'type': 'dataframe', 'data': [{'document_name': 'alexandria_ocasio_cortez_2025_fighting_oligarchy.txt', 'care_raw': 0.9, 'care_salience': 0.8, 'care_confidence': 0.95, 'harm_raw': 0.8, 'harm_salience': 0.8, 'harm_confidence': 0.95, 'fairness_raw': 0.8, 'fairness_salience': 0.85, 'fairness_confidence': 0.95, 'cheating_raw': 1.0, 'cheating_salience': 1.0, 'cheating_confidence': 1.0, 'loyalty_raw': 0.9, 'loyalty_salience': 0.9, 'loyalty_confidence': 0.95, 'betrayal_raw': 0.6, 'betrayal_salience': 0.5, 'betrayal_confidence': 0.9, 'authority_raw': 0.0, 'authority_salience': 0.0, 'authority_confidence': 1.0, 'subversion_raw': 0.9, 'subversion_salience': 0.95, 'subversion_confidence': 0.95, 'sanctity_raw': 0.1, 'sanctity_salience': 0.1, 'sanctity_confidence': 0.7, 'degradation_raw': 0.0, 'degradation_salience': 0.0, 'degradation_confidence': 1.0, 'liberty_raw': 0.0, 'liberty_salience': 0.0, 'liberty_confidence': 1.0, 'oppression_raw': 0.9, 'oppression_salience': 0.9, 'oppression_confidence': 0.95, 'care_harm_tension': 0.0, 'fairness_cheating_tension': 0.12000000000000002, 'loyalty_betrayal_tension': 0.24, 'authority_subversion_tension': 0.0, 'sanctity_degradation_tension': 0.0, 'liberty_oppression_tension': 0.0, 'individualizing_tension': 0.12000000000000002, 'binding_tension': 0.24, 'liberty_tension': 0.0, 'moral_strategic_contradiction_index': 0.06, 'moral_salience_concentration': 0.41905376815637946, 'individualizing_foundations_mean': 0.875, 'binding_foundations_mean': 0.4166666666666667, 'liberty_foundation_mean': 0.45, 'individualizing_salience_mean': 0.8625, 'binding_salience_mean': 0.40833333333333327, 'liberty_salience_mean': 0.45}, {'document_name': 'bernie_sanders_2025_fighting_oligarchy.txt', 'care_raw': 0.8, 'care_salience': 0.9, 'care_confidence': 0.95, 'harm_raw': 0.9, 'harm_salience': 0.9, 'harm_confidence': 0.95, 'fairness_raw': 0.9, 'fairness_salience': 1.0, 'fairness_confidence': 1.0, 'cheating_raw': 0.9, 'cheating_salience': 1.0, 'cheating_confidence': 1.0, 'loyalty_raw': 0.7, 'loyalty_salience': 0.6, 'loyalty_confidence': 0.9, 'betrayal_raw': 0.7, 'betrayal_salience': 0.6, 'betrayal_confidence': 0.9, 'authority_raw': 0.1, 'authority_salience': 0.1, 'authority_confidence': 0.9, 'subversion_raw': 0.9, 'subversion_salience': 0.9, 'subversion_confidence': 1.0, 'sanctity_raw': 0.3, 'sanctity_salience': 0.2, 'sanctity_confidence': 0.7, 'degradation_raw': 0.5, 'degradation_salience': 0.5, 'degradation_confidence': 0.8, 'liberty_raw': 0.1, 'liberty_salience': 0.1, 'liberty_confidence': 0.9, 'oppression_raw': 0.9, 'oppression_salience': 0.9, 'oppression_confidence': 1.0, 'care_harm_tension': 0.0, 'fairness_cheating_tension': 0.0, 'loyalty_betrayal_tension': 0.0, 'authority_subversion_tension': 0.08000000000000002, 'sanctity_degradation_tension': 0.09, 'liberty_oppression_tension': 0.08000000000000002, 'individualizing_tension': 0.0, 'binding_tension': 0.17, 'liberty_tension': 0.08000000000000002, 'moral_strategic_contradiction_index': 0.041666666666666664, 'moral_salience_concentration': 0.3476108935769035, 'individualizing_foundations_mean': 0.875, 'binding_foundations_mean': 0.5333333333333333, 'liberty_foundation_mean': 0.5, 'individualizing_salience_mean': 0.95, 'binding_salience_mean': 0.4833333333333334, 'liberty_salience_mean': 0.5}, {'document_name': 'cory_booker_2018_first_step_act.txt', 'care_raw': 0.9, 'care_salience': 0.9, 'care_confidence': 1.0, 'harm_raw': 0.9, 'harm_salience': 0.9, 'harm_confidence': 1.0, 'fairness_raw': 1.0, 'fairness_salience': 1.0, 'fairness_confidence': 1.0, 'cheating_raw': 0.8, 'cheating_salience': 0.8, 'cheating_confidence': 1.0, 'loyalty_raw': 0.7, 'loyalty_salience': 0.7, 'loyalty_confidence': 0.9, 'betrayal_raw': 0.0, 'betrayal_salience': 0.0, 'betrayal_confidence': 1.0, 'authority_raw': 0.4, 'authority_salience': 0.3, 'authority_confidence': 0.9, 'subversion_raw': 0.8, 'subversion_salience': 0.8, 'subversion_confidence': 1.0, 'sanctity_raw': 0.9, 'sanctity_salience': 0.8, 'sanctity_confidence': 0.95, 'degradation_raw': 0.9, 'degradation_salience': 0.9, 'degradation_confidence': 1.0, 'liberty_raw': 0.8, 'liberty_salience': 0.7, 'liberty_confidence': 1.0, 'oppression_raw': 1.0, 'oppression_salience': 1.0, 'oppression_confidence': 1.0, 'care_harm_tension': 0.0, 'fairness_cheating_tension': 0.15999999999999998, 'loyalty_betrayal_tension': 0.0, 'authority_subversion_tension': 0.2, 'sanctity_degradation_tension': 0.08999999999999998, 'liberty_oppression_tension': 0.24000000000000005, 'individualizing_tension': 0.15999999999999998, 'binding_tension': 0.29, 'liberty_tension': 0.24000000000000005, 'moral_strategic_contradiction_index': 0.11499999999999999, 'moral_salience_concentration': 0.2964435660944388, 'individualizing_foundations_mean': 0.8999999999999999, 'binding_foundations_mean': 0.6166666666666667, 'liberty_foundation_mean': 0.9, 'individualizing_salience_mean': 0.8999999999999999, 'binding_salience_mean': 0.5833333333333334, 'liberty_salience_mean': 0.85}, {'document_name': 'jd_vance_2022_natcon_conference.txt', 'care_raw': 0.7, 'care_salience': 0.8, 'care_confidence': 0.95, 'harm_raw': 0.6, 'harm_salience': 0.7, 'harm_confidence': 0.9, 'fairness_raw': 0.7, 'fairness_salience': 0.8, 'fairness_confidence': 0.9, 'cheating_raw': 0.8, 'cheating_salience': 0.9, 'cheating_confidence': 0.95, 'loyalty_raw': 0.9, 'loyalty_salience': 1.0, 'loyalty_confidence': 1.0, 'betrayal_raw': 0.8, 'betrayal_salience': 0.9, 'betrayal_confidence': 0.95, 'authority_raw': 0.4, 'authority_salience': 0.5, 'authority_confidence': 0.9, 'subversion_raw': 0.8, 'subversion_salience': 0.8, 'subversion_confidence': 0.9, 'sanctity_raw': 0.5, 'sanctity_salience': 0.6, 'sanctity_confidence': 0.85, 'degradation_raw': 0.6, 'degradation_salience': 0.6, 'degradation_confidence': 0.85, 'liberty_raw': 0.0, 'liberty_salience': 0.0, 'liberty_confidence': 1.0, 'oppression_raw': 0.8, 'oppression_salience': 0.8, 'oppression_confidence': 0.95, 'care_harm_tension': 0.06000000000000005, 'fairness_cheating_tension': 0.06999999999999998, 'loyalty_betrayal_tension': 0.07999999999999999, 'authority_subversion_tension': 0.12000000000000002, 'sanctity_degradation_tension': 0.0, 'liberty_oppression_tension': 0.0, 'individualizing_tension': 0.13000000000000003, 'binding_tension': 0.2, 'liberty_tension': 0.0, 'moral_strategic_contradiction_index': 0.055000000000000014, 'moral_salience_concentration': 0.26285149626910836, 'individualizing_foundations_mean': 0.7, 'binding_foundations_mean': 0.6666666666666666, 'liberty_foundation_mean': 0.4, 'individualizing_salience_mean': 0.7999999999999999, 'binding_salience_mean': 0.7333333333333334, 'liberty_salience_mean': 0.4}, {'document_name': 'john_lewis_1963_march_on_washington.txt', 'care_raw': 0.9, 'care_salience': 0.8, 'care_confidence': 0.95, 'harm_raw': 0.9, 'harm_salience': 0.9, 'harm_confidence': 0.95, 'fairness_raw': 1.0, 'fairness_salience': 1.0, 'fairness_confidence': 1.0, 'cheating_raw': 0.8, 'cheating_salience': 0.8, 'cheating_confidence': 0.9, 'loyalty_raw': 0.6, 'loyalty_salience': 0.5, 'loyalty_confidence': 0.85, 'betrayal_raw': 0.5, 'betrayal_salience': 0.4, 'betrayal_confidence': 0.8, 'authority_raw': 0.0, 'authority_salience': 0.0, 'authority_confidence': 0.9, 'subversion_raw': 0.9, 'subversion_salience': 0.9, 'subversion_confidence': 0.95, 'sanctity_raw': 0.7, 'sanctity_salience': 0.6, 'sanctity_confidence': 0.85, 'degradation_raw': 0.6, 'degradation_salience': 0.5, 'degradation_confidence': 0.8, 'liberty_raw': 1.0, 'liberty_salience': 1.0, 'liberty_confidence': 1.0, 'oppression_raw': 1.0, 'oppression_salience': 1.0, 'oppression_confidence': 1.0, 'care_harm_tension': 0.08999999999999998, 'fairness_cheating_tension': 0.15999999999999998, 'loyalty_betrayal_tension': 0.04999999999999999, 'authority_subversion_tension': 0.0, 'sanctity_degradation_tension': 0.059999999999999984, 'liberty_oppression_tension': 0.0, 'individualizing_tension': 0.24999999999999994, 'binding_tension': 0.10999999999999997, 'liberty_tension': 0.0, 'moral_strategic_contradiction_index': 0.05999999999999999, 'moral_salience_concentration': 0.30748244591432294, 'individualizing_foundations_mean': 0.8999999999999999, 'binding_foundations_mean': 0.55, 'liberty_foundation_mean': 1.0, 'individualizing_salience_mean': 0.875, 'binding_salience_mean': 0.48333333333333334, 'liberty_salience_mean': 1.0}, {'document_name': 'john_mccain_2008_concession.txt', 'care_raw': 0.6, 'care_salience': 0.5, 'care_confidence': 0.95, 'harm_raw': 0.4, 'harm_salience': 0.3, 'harm_confidence': 0.9, 'fairness_raw': 0.9, 'fairness_salience': 0.9, 'fairness_confidence': 1.0, 'cheating_raw': 0.1, 'cheating_salience': 0.1, 'cheating_confidence': 0.85, 'loyalty_raw': 0.9, 'loyalty_salience': 0.9, 'loyalty_confidence': 1.0, 'betrayal_raw': 0.0, 'betrayal_salience': 0.0, 'betrayal_confidence': 1.0, 'authority_raw': 0.8, 'authority_salience': 0.8, 'authority_confidence': 1.0, 'subversion_raw': 0.0, 'subversion_salience': 0.0, 'subversion_confidence': 1.0, 'sanctity_raw': 0.5, 'sanctity_salience': 0.4, 'sanctity_confidence': 0.9, 'degradation_raw': 0.3, 'degradation_salience': 0.2, 'degradation_confidence': 0.9, 'liberty_raw': 0.3, 'liberty_salience': 0.2, 'liberty_confidence': 0.9, 'oppression_raw': 0.6, 'oppression_salience': 0.6, 'oppression_confidence': 0.95, 'care_harm_tension': 0.08000000000000002, 'fairness_cheating_tension': 0.08000000000000002, 'loyalty_betrayal_tension': 0.0, 'authority_subversion_tension': 0.0, 'sanctity_degradation_tension': 0.06, 'liberty_oppression_tension': 0.11999999999999998, 'individualizing_tension': 0.16000000000000003, 'binding_tension': 0.06, 'liberty_tension': 0.11999999999999998, 'moral_strategic_contradiction_index': 0.05666666666666667, 'moral_salience_concentration': 0.33154825052206566, 'individualizing_foundations_mean': 0.5, 'binding_foundations_mean': 0.4166666666666667, 'liberty_foundation_mean': 0.44999999999999996, 'individualizing_salience_mean': 0.45000000000000007, 'binding_salience_mean': 0.38333333333333336, 'liberty_salience_mean': 0.4}, {'document_name': 'mitt_romney_2020_impeachment.txt', 'care_raw': 0.1, 'care_salience': 0.1, 'care_confidence': 0.9, 'harm_raw': 0.7, 'harm_salience': 0.6, 'harm_confidence': 0.9, 'fairness_raw': 0.9, 'fairness_salience': 0.9, 'fairness_confidence': 1.0, 'cheating_raw': 0.9, 'cheating_salience': 0.9, 'cheating_confidence': 1.0, 'loyalty_raw': 0.8, 'loyalty_salience': 0.8, 'loyalty_confidence': 1.0, 'betrayal_raw': 0.5, 'betrayal_salience': 0.5, 'betrayal_confidence': 0.9, 'authority_raw': 0.9, 'authority_salience': 0.9, 'authority_confidence': 1.0, 'subversion_raw': 0.8, 'subversion_salience': 0.8, 'subversion_confidence': 0.9, 'sanctity_raw': 0.9, 'sanctity_salience': 0.9, 'sanctity_confidence': 1.0, 'degradation_raw': 0.8, 'degradation_salience': 0.7, 'degradation_confidence': 0.9, 'liberty_raw': 0.6, 'liberty_salience': 0.5, 'liberty_confidence': 0.9, 'oppression_raw': 0.7, 'oppression_salience': 0.6, 'oppression_confidence': 0.9, 'care_harm_tension': 0.05, 'fairness_cheating_tension': 0.0, 'loyalty_betrayal_tension': 0.15000000000000002, 'authority_subversion_tension': 0.07999999999999999, 'sanctity_degradation_tension': 0.16000000000000006, 'liberty_oppression_tension': 0.059999999999999984, 'individualizing_tension': 0.05, 'binding_tension': 0.39000000000000007, 'liberty_tension': 0.059999999999999984, 'moral_strategic_contradiction_index': 0.08333333333333333, 'moral_salience_concentration': 0.24058010698889443, 'individualizing_foundations_mean': 0.65, 'binding_foundations_mean': 0.7833333333333333, 'liberty_foundation_mean': 0.6499999999999999, 'individualizing_salience_mean': 0.625, 'binding_salience_mean': 0.7666666666666666, 'liberty_salience_mean': 0.55}, {'document_name': 'steve_king_2017_house_floor.txt', 'care_raw': 0.8, 'care_salience': 0.7, 'care_confidence': 0.95, 'harm_raw': 0.9, 'harm_salience': 0.8, 'harm_confidence': 1.0, 'fairness_raw': 0.7, 'fairness_salience': 0.6, 'fairness_confidence': 0.9, 'cheating_raw': 0.8, 'cheating_salience': 0.7, 'cheating_confidence': 0.9, 'loyalty_raw': 0.8, 'loyalty_salience': 0.7, 'loyalty_confidence': 0.9, 'betrayal_raw': 0.7, 'betrayal_salience': 0.6, 'betrayal_confidence': 0.9, 'authority_raw': 1.0, 'authority_salience': 1.0, 'authority_confidence': 1.0, 'subversion_raw': 1.0, 'subversion_salience': 1.0, 'subversion_confidence': 1.0, 'sanctity_raw': 0.7, 'sanctity_salience': 0.5, 'sanctity_confidence': 0.9, 'degradation_raw': 0.8, 'degradation_salience': 0.6, 'degradation_confidence': 0.9, 'liberty_raw': 0.4, 'liberty_salience': 0.2, 'liberty_confidence': 0.8, 'oppression_raw': 0.7, 'oppression_salience': 0.6, 'oppression_confidence': 0.9, 'care_harm_tension': 0.08000000000000007, 'fairness_cheating_tension': 0.06999999999999998, 'loyalty_betrayal_tension': 0.06999999999999998, 'authority_subversion_tension': 0.0, 'sanctity_degradation_tension': 0.06999999999999998, 'liberty_oppression_tension': 0.16, 'individualizing_tension': 0.15000000000000005, 'binding_tension': 0.13999999999999996, 'liberty_tension': 0.16, 'moral_strategic_contradiction_index': 0.07500000000000001, 'moral_salience_concentration': 0.21461734799546392, 'individualizing_foundations_mean': 0.8, 'binding_foundations_mean': 0.8333333333333334, 'liberty_foundation_mean': 0.55, 'individualizing_salience_mean': 0.7, 'binding_salience_mean': 0.7333333333333333, 'liberty_salience_mean': 0.4}], 'columns': ['document_name', 'care_raw', 'care_salience', 'care_confidence', 'harm_raw', 'harm_salience', 'harm_confidence', 'fairness_raw', 'fairness_salience', 'fairness_confidence', 'cheating_raw', 'cheating_salience', 'cheating_confidence', 'loyalty_raw', 'loyalty_salience', 'loyalty_confidence', 'betrayal_raw', 'betrayal_salience', 'betrayal_confidence', 'authority_raw', 'authority_salience', 'authority_confidence', 'subversion_raw', 'subversion_salience', 'subversion_confidence', 'sanctity_raw', 'sanctity_salience', 'sanctity_confidence', 'degradation_raw', 'degradation_salience', 'degradation_confidence', 'liberty_raw', 'liberty_salience', 'liberty_confidence', 'oppression_raw', 'oppression_salience', 'oppression_confidence', 'care_harm_tension', 'fairness_cheating_tension', 'loyalty_betrayal_tension', 'authority_subversion_tension', 'sanctity_degradation_tension', 'liberty_oppression_tension', 'individualizing_tension', 'binding_tension', 'liberty_tension', 'moral_strategic_contradiction_index', 'moral_salience_concentration', 'individualizing_foundations_mean', 'binding_foundations_mean', 'liberty_foundation_mean', 'individualizing_salience_mean', 'binding_salience_mean', 'liberty_salience_mean'], 'index': [0, 1, 2, 3, 4, 5, 6, 7], 'shape': (8, 54)}, 'compare_ideological_groups': {'test_type': 'independent_t-test', 'groups': ['Progressive', 'Conservative'], 'care_raw': {'statistic': 1.7021372647521071, 'p_value': 0.14946440335595423}, 'care_salience': {'statistic': 1.8443039066380762, 'p_value': 0.12445687771230088}, 'care_confidence': {'statistic': 1.4348601079588792, 'p_value': 0.2107957540029115}, 'harm_raw': {'statistic': 2.0299948573528757, 'p_value': 0.09811182125573001}, 'harm_salience': {'statistic': 2.347382389307854, 'p_value': 0.06577373774618067}, 'harm_confidence': {'statistic': 1.2741179785940628, 'p_value': 0.25862863951997106}, 'fairness_raw': {'statistic': 2.2587697572631273, 'p_value': 0.07346486022623269}, 'fairness_salience': {'statistic': 2.3904572186687867, 'p_value': 0.062352416002150426}, 'fairness_confidence': {'statistic': 1.4638501094228016, 'p_value': 0.2031106637200543}, 'cheating_raw': {'statistic': 0.8315218406203, 'p_value': 0.44356363137016885}, 'cheating_salience': {'statistic': 0.9386832123229737, 'p_value': 0.39098867894781464}, 'cheating_confidence': {'statistic': 0.8811342210628035, 'p_value': 0.4185802320330524}, 'loyalty_raw': {'statistic': -4.1576092031015035, 'p_value': 0.008844553648948737}, 'loyalty_salience': {'statistic': -2.7664166758624416, 'p_value': 0.039532796638626716}, 'loyalty_confidence': {'statistic': -2.8030595529069413, 'p_value': 0.0378567346768761}, 'betrayal_raw': {'statistic': -0.3659625273557001, 'p_value': 0.7293667752072224}, 'betrayal_salience': {'statistic': -0.6264700137907352, 'p_value': 0.5584932603112229}, 'betrayal_confidence': {'statistic': -0.6697051466046114, 'p_value': 0.5327244503997572}, 'authority_raw': {'statistic': -3.2837606142580387, 'p_value': 0.021863539782013827}, 'authority_salience': {'statistic': -4.5175395145262565, 'p_value': 0.006297102328449345}, 'authority_confidence': {'statistic': -2.535462764185549, 'p_value': 0.052181400457057936}, 'subversion_raw': {'statistic': 0.8212073334045126, 'p_value': 0.44889719124153704}, 'subversion_salience': {'statistic': 0.8212073334045126, 'p_value': 0.44889719124153704}, 'subversion_confidence': {'statistic': 0.9035079029052541, 'p_value': 0.4076765325135334}, 'sanctity_raw': {'statistic': -0.08958617182905795, 'p_value': 0.9320939432187586}, 'sanctity_salience': {'statistic': -0.3414938883812553, 'p_value': 0.7466156365497775}, 'sanctity_confidence': {'statistic': -1.110750302167968, 'p_value': 0.3172150136920414}, 'degradation_raw': {'statistic': 0.2419669592479134, 'p_value': 0.818416743628447}, 'degradation_salience': {'statistic': 0.6291140940445752, 'p_value': 0.5568944905049955}, 'degradation_confidence': {'statistic': -0.36103200473960384, 'p_value': 0.7328283474517815}, 'liberty_raw': {'statistic': 1.1335622585594545, 'p_value': 0.30838669125064516}, 'liberty_salience': {'statistic': 1.483767352966745, 'p_value': 0.19798712026943463}, 'liberty_confidence': {'statistic': 1.1952286093343958, 'p_value': 0.28559094064520024}, 'oppression_raw': {'statistic': 4.780914437337575, 'p_value': 0.004966620866137675}, 'oppression_salience': {'statistic': 4.841648318657442, 'p_value': 0.004708143918551085}, 'oppression_confidence': {'statistic': 4.3915503282684085, 'p_value': 0.007077597925498215}, 'care_harm_tension': {'statistic': -1.4085904245475291, 'p_value': 0.21799835760206687}, 'fairness_cheating_tension': {'statistic': 1.0397066299035833, 'p_value': 0.346116722431121}, 'loyalty_betrayal_tension': {'statistic': -1.4997857601931512, 'p_value': 0.1939570404814175}, 'authority_subversion_tension': {'statistic': 0.7197826499377717, 'p_value': 0.5038890661283286}, 'sanctity_degradation_tension': {'statistic': 0.18776913304124498, 'p_value': 0.8584399590580398}, 'liberty_oppression_tension': {'statistic': 0.3004786579277667, 'p_value': 0.7759046058956883}, 'individualizing_tension': {'statistic': 0.2085771817930101, 'p_value': 0.8430089999928239}, 'binding_tension': {'statistic': -0.07960986511089882, 'p_value': 0.9396356204977258}, 'liberty_tension': {'statistic': 0.3004786579277667, 'p_value': 0.7759046058956883}, 'moral_strategic_contradiction_index': {'statistic': 0.2339060747321447, 'p_value': 0.8243333944831164}, 'moral_salience_concentration': {'statistic': 1.69137794301362, 'p_value': 0.1515520201483777}, 'individualizing_foundations_mean': {'statistic': 3.085217194186693, 'p_value': 0.02730730903868319}, 'binding_foundations_mean': {'statistic': -0.967351878428884, 'p_value': 0.37779670155086054}, 'liberty_foundation_mean': {'statistic': 2.001387706351718, 'p_value': 0.10175899338535045}, 'individualizing_salience_mean': {'statistic': 2.9621519993771037, 'p_value': 0.03144063794203128}, 'binding_salience_mean': {'statistic': -1.2410927338335922, 'p_value': 0.2696224373925094}, 'liberty_salience_mean': {'statistic': 2.6270558731418516, 'p_value': 0.046698683329114365}}, 'generate_statistical_summary_report': 'STATISTICAL ANALYSIS SUMMARY REPORT\n==================================================\nAnalysis Timestamp: Unknown\nSample Size: Unknown\nAlpha Level: Unknown\nVariables: 0\n', 'get_correlation_matrix': {'care_raw': {'care_raw': 1.0, 'harm_raw': 0.4698, 'fairness_raw': 0.0167, 'cheating_raw': 0.1218, 'loyalty_raw': -0.2765, 'betrayal_raw': 0.0213, 'authority_raw': -0.6183, 'subversion_raw': 0.2579, 'sanctity_raw': -0.3905, 'degradation_raw': -0.2428, 'liberty_raw': -0.0283, 'oppression_raw': 0.6321, 'moral_strategic_contradiction_index': -0.0894, 'moral_salience_concentration': 0.432, 'individualizing_foundations_mean': 0.6725, 'binding_foundations_mean': -0.4068, 'liberty_foundation_mean': 0.187}, 'harm_raw': {'care_raw': 0.4698, 'harm_raw': 1.0, 'fairness_raw': 0.1872, 'cheating_raw': 0.7493, 'loyalty_raw': -0.7127, 'betrayal_raw': 0.3051, 'authority_raw': -0.3895, 'subversion_raw': 0.8539, 'sanctity_raw': 0.1168, 'degradation_raw': 0.335, 'liberty_raw': 0.3515, 'oppression_raw': 0.7148, 'moral_strategic_contradiction_index': 0.2611, 'moral_salience_concentration': -0.0223, 'individualizing_foundations_mean': 0.9286, 'binding_foundations_mean': 0.2786, 'liberty_foundation_mean': 0.5328}, 'fairness_raw': {'care_raw': 0.0167, 'harm_raw': 0.1872, 'fairness_raw': 1.0, 'cheating_raw': -0.1355, 'loyalty_raw': -0.681, 'betrayal_raw': -0.6488, 'authority_raw': -0.3102, 'subversion_raw': -0.1951, 'sanctity_raw': 0.3524, 'degradation_raw': 0.1568, 'liberty_raw': 0.7072, 'oppression_raw': 0.4648, 'moral_strategic_contradiction_index': 0.3177, 'moral_salience_concentration': 0.2541, 'individualizing_foundations_mean': 0.2063, 'binding_foundations_mean': -0.3456, 'liberty_foundation_mean': 0.7477}, 'cheating_raw': {'care_raw': 0.1218, 'harm_raw': 0.7493, 'fairness_raw': -0.1355, 'cheating_raw': 1.0, 'loyalty_raw': -0.2915, 'betrayal_raw': 0.6343, 'authority_raw': -0.4111, 'subversion_raw': 0.9433, 'sanctity_raw': -0.0686, 'degradation_raw': 0.1537, 'liberty_raw': -0.0275, 'oppression_raw': 0.5796, 'moral_strategic_contradiction_index': 0.1209, 'moral_salience_concentration': 0.0103, 'individualizing_foundations_mean': 0.7418, 'binding_foundations_mean': 0.3467, 'liberty_foundation_mean': 0.1701}, 'loyalty_raw': {'care_raw': -0.2765, 'harm_raw': -0.7127, 'fairness_raw': -0.681, 'cheating_raw': -0.2915, 'loyalty_raw': 1.0, 'betrayal_raw': 0.0715, 'authority_raw': 0.3584, 'subversion_raw': -0.4166, 'sanctity_raw': -0.4168, 'degradation_raw': -0.4854, 'liberty_raw': -0.746, 'oppression_raw': -0.6608, 'moral_strategic_contradiction_index': -0.2048, 'moral_salience_concentration': 0.111, 'individualizing_foundations_mean': -0.631, 'binding_foundations_mean': -0.1215, 'liberty_foundation_mean': -0.8455}, 'betrayal_raw': {'care_raw': 0.0213, 'harm_raw': 0.3051, 'fairness_raw': -0.6488, 'cheating_raw': 0.6343, 'loyalty_raw': 0.0715, 'betrayal_raw': 1.0, 'authority_raw': -0.1921, 'subversion_raw': 0.6734, 'sanctity_raw': -0.3676, 'degradation_raw': -0.0735, 'liberty_raw': -0.4549, 'oppression_raw': 0.0464, 'moral_strategic_contradiction_index': -0.5334, 'moral_salience_concentration': -0.1434, 'individualizing_foundations_mean': 0.2761, 'binding_foundations_mean': 0.3538, 'liberty_foundation_mean': -0.3658}, 'authority_raw': {'care_raw': -0.6183, 'harm_raw': -0.3895, 'fairness_raw': -0.3102, 'cheating_raw': -0.4111, 'loyalty_raw': 0.3584, 'betrayal_raw': -0.1921, 'authority_raw': 1.0, 'subversion_raw': -0.3277, 'sanctity_raw': 0.5359, 'degradation_raw': 0.4544, 'liberty_raw': 0.0657, 'oppression_raw': -0.8254, 'moral_strategic_contradiction_index': 0.3322, 'moral_salience_concentration': -0.7237, 'individualizing_foundations_mean': -0.668, 'binding_foundations_mean': 0.6155, 'liberty_foundation_mean': -0.2201}, 'subversion_raw': {'care_raw': 0.2579, 'harm_raw': 0.8539, 'fairness_raw': -0.1951, 'cheating_raw': 0.9433, 'loyalty_raw': -0.4166, 'betrayal_raw': 0.6734, 'authority_raw': -0.3277, 'subversion_raw': 1.0, 'sanctity_raw': 0.0361, 'degradation_raw': 0.3023, 'liberty_raw': 0.0846, 'oppression_raw': 0.5697, 'moral_strategic_contradiction_index': 0.1261, 'moral_salience_concentration': -0.1677, 'individualizing_foundations_mean': 0.7987, 'binding_foundations_mean': 0.4801, 'liberty_foundation_mean': 0.2608}, 'sanctity_raw': {'care_raw': -0.3905, 'harm_raw': 0.1168, 'fairness_raw': 0.3524, 'cheating_raw': -0.0686, 'loyalty_raw': -0.4168, 'betrayal_raw': -0.3676, 'authority_raw': 0.5359, 'subversion_raw': 0.0361, 'sanctity_raw': 1.0, 'degradation_raw': 0.8916, 'liberty_raw': 0.7864, 'oppression_raw': -0.0511, 'moral_strategic_contradiction_index': 0.7371, 'moral_salience_concentration': -0.775, 'individualizing_foundations_mean': -0.1044, 'binding_foundations_mean': 0.664, 'liberty_foundation_mean': 0.6421}, 'degradation_raw': {'care_raw': -0.2428, 'harm_raw': 0.335, 'fairness_raw': 0.1568, 'cheating_raw': 0.1537, 'loyalty_raw': -0.4854, 'betrayal_raw': -0.0735, 'authority_raw': 0.4544, 'subversion_raw': 0.3023, 'sanctity_raw': 0.8916, 'degradation_raw': 1.0, 'liberty_raw': 0.5907, 'oppression_raw': 0.0565, 'moral_strategic_contradiction_index': 0.6241, 'moral_salience_concentration': -0.8484, 'individualizing_foundations_mean': 0.0989, 'binding_foundations_mean': 0.811, 'liberty_foundation_mean': 0.514}, 'liberty_raw': {'care_raw': -0.0283, 'harm_raw': 0.3515, 'fairness_raw': 0.7072, 'cheating_raw': -0.0275, 'loyalty_raw': -0.746, 'betrayal_raw': -0.4549, 'authority_raw': 0.0657, 'subversion_raw': 0.0846, 'sanctity_raw': 0.7864, 'degradation_raw': 0.5907, 'liberty_raw': 1.0, 'oppression_raw': 0.3336, 'moral_strategic_contradiction_index': 0.5799, 'moral_salience_concentration': -0.3462, 'individualizing_foundations_mean': 0.2292, 'binding_foundations_mean': 0.2431, 'liberty_foundation_mean': 0.9493}, 'oppression_raw': {'care_raw': 0.6321, 'harm_raw': 0.7148, 'fairness_raw': 0.4648, 'cheating_raw': 0.5796, 'loyalty_raw': -0.6608, 'betrayal_raw': 0.0464, 'authority_raw': -0.8254, 'subversion_raw': 0.5697, 'sanctity_raw': -0.0511, 'degradation_raw': 0.0565, 'liberty_raw': 0.3336, 'oppression_raw': 1.0, 'moral_strategic_contradiction_index': 0.1832, 'moral_salience_concentration': 0.3705, 'individualizing_foundations_mean': 0.8891, 'binding_foundations_mean': -0.2306, 'liberty_foundation_mean': 0.6129}, 'moral_strategic_contradiction_index': {'care_raw': -0.0894, 'harm_raw': 0.2611, 'fairness_raw': 0.3177, 'cheating_raw': 0.1209, 'loyalty_raw': -0.2048, 'betrayal_raw': -0.5334, 'authority_raw': 0.3322, 'subversion_raw': 0.1261, 'sanctity_raw': 0.7371, 'degradation_raw': 0.6241, 'liberty_raw': 0.5799, 'oppression_raw': 0.1832, 'moral_strategic_contradiction_index': 1.0, 'moral_salience_concentration': -0.3695, 'individualizing_foundations_mean': 0.1636, 'binding_foundations_mean': 0.4096, 'liberty_foundation_mean': 0.5471}, 'moral_salience_concentration': {'care_raw': 0.432, 'harm_raw': -0.0223, 'fairness_raw': 0.2541, 'cheating_raw': 0.0103, 'loyalty_raw': 0.111, 'betrayal_raw': -0.1434, 'authority_raw': -0.7237, 'subversion_raw': -0.1677, 'sanctity_raw': -0.775, 'degradation_raw': -0.8484, 'liberty_raw': -0.3462, 'oppression_raw': 0.3705, 'moral_strategic_contradiction_index': -0.3695, 'moral_salience_concentration': 1.0, 'individualizing_foundations_mean': 0.2493, 'binding_foundations_mean': -0.9175, 'liberty_foundation_mean': -0.1667}, 'individualizing_foundations_mean': {'care_raw': 0.6725, 'harm_raw': 0.9286, 'fairness_raw': 0.2063, 'cheating_raw': 0.7418, 'loyalty_raw': -0.631, 'betrayal_raw': 0.2761, 'authority_raw': -0.668, 'subversion_raw': 0.7987, 'sanctity_raw': -0.1044, 'degradation_raw': 0.0989, 'liberty_raw': 0.2292, 'oppression_raw': 0.8891, 'moral_strategic_contradiction_index': 0.1636, 'moral_salience_concentration': 0.2493, 'individualizing_foundations_mean': 1.0, 'binding_foundations_mean': -0.0053, 'liberty_foundation_mean': 0.4884}, 'binding_foundations_mean': {'care_raw': -0.4068, 'harm_raw': 0.2786, 'fairness_raw': -0.3456, 'cheating_raw': 0.3467, 'loyalty_raw': -0.1215, 'betrayal_raw': 0.3538, 'authority_raw': 0.6155, 'subversion_raw': 0.4801, 'sanctity_raw': 0.664, 'degradation_raw': 0.811, 'liberty_raw': 0.2431, 'oppression_raw': -0.2306, 'moral_strategic_contradiction_index': 0.4096, 'moral_salience_concentration': -0.9175, 'individualizing_foundations_mean': -0.0053, 'binding_foundations_mean': 1.0, 'liberty_foundation_mean': 0.1269}, 'liberty_foundation_mean': {'care_raw': 0.187, 'harm_raw': 0.5328, 'fairness_raw': 0.7477, 'cheating_raw': 0.1701, 'loyalty_raw': -0.8455, 'betrayal_raw': -0.3658, 'authority_raw': -0.2201, 'subversion_raw': 0.2608, 'sanctity_raw': 0.6421, 'degradation_raw': 0.514, 'liberty_raw': 0.9493, 'oppression_raw': 0.6129, 'moral_strategic_contradiction_index': 0.5471, 'moral_salience_concentration': -0.1667, 'individualizing_foundations_mean': 0.4884, 'binding_foundations_mean': 0.1269, 'liberty_foundation_mean': 1.0}}, 'get_descriptive_statistics': {'care_raw': {'count': 8.0, 'mean': 0.7125, 'std': 0.2696, 'min': 0.1, '25%': 0.675, '50%': 0.8, '75%': 0.9, 'max': 0.9}, 'care_salience': {'count': 8.0, 'mean': 0.6875, 'std': 0.2696, 'min': 0.1, '25%': 0.65, '50%': 0.8, '75%': 0.825, 'max': 0.9}, 'care_confidence': {'count': 8.0, 'mean': 0.95, 'std': 0.0267, 'min': 0.9, '25%': 0.95, '50%': 0.95, '75%': 0.95, 'max': 1.0}, 'harm_raw': {'count': 8.0, 'mean': 0.7625, 'std': 0.1847, 'min': 0.4, '25%': 0.675, '50%': 0.85, '75%': 0.9, 'max': 0.9}, 'harm_salience': {'count': 8.0, 'mean': 0.7375, 'std': 0.2066, 'min': 0.3, '25%': 0.675, '50%': 0.8, '75%': 0.9, 'max': 0.9}, 'harm_confidence': {'count': 8.0, 'mean': 0.9438, 'std': 0.0417, 'min': 0.9, '25%': 0.9, '50%': 0.95, '75%': 0.9625, 'max': 1.0}, 'fairness_raw': {'count': 8.0, 'mean': 0.8625, 'std': 0.1188, 'min': 0.7, '25%': 0.775, '50%': 0.9, '75%': 0.925, 'max': 1.0}, 'fairness_salience': {'count': 8.0, 'mean': 0.8812, 'std': 0.1361, 'min': 0.6, '25%': 0.8375, '50%': 0.9, '75%': 1.0, 'max': 1.0}, 'fairness_confidence': {'count': 8.0, 'mean': 0.9688, 'std': 0.0458, 'min': 0.9, '25%': 0.9375, '50%': 1.0, '75%': 1.0, 'max': 1.0}, 'cheating_raw': {'count': 8.0, 'mean': 0.7625, 'std': 0.2774, 'min': 0.1, '25%': 0.8, '50%': 0.8, '75%': 0.9, 'max': 1.0}, 'cheating_salience': {'count': 8.0, 'mean': 0.775, 'std': 0.2915, 'min': 0.1, '25%': 0.775, '50%': 0.85, '75%': 0.925, 'max': 1.0}, 'cheating_confidence': {'count': 8.0, 'mean': 0.95, 'std': 0.0598, 'min': 0.85, '25%': 0.9, '50%': 0.975, '75%': 1.0, 'max': 1.0}, 'loyalty_raw': {'count': 8.0, 'mean': 0.7875, 'std': 0.1126, 'min': 0.6, '25%': 0.7, '50%': 0.8, '75%': 0.9, 'max': 0.9}, 'loyalty_salience': {'count': 8.0, 'mean': 0.7625, 'std': 0.1685, 'min': 0.5, '25%': 0.675, '50%': 0.75, '75%': 0.9, 'max': 1.0}, 'loyalty_confidence': {'count': 8.0, 'mean': 0.9375, 'std': 0.0582, 'min': 0.85, '25%': 0.9, '50%': 0.925, '75%': 1.0, 'max': 1.0}, 'betrayal_raw': {'count': 8.0, 'mean': 0.475, 'std': 0.3105, 'min': 0.0, '25%': 0.375, '50%': 0.55, '75%': 0.7, 'max': 0.8}, 'betrayal_salience': {'count': 8.0, 'mean': 0.4375, 'std': 0.3068, 'min': 0.0, '25%': 0.3, '50%': 0.5, '75%': 0.6, 'max': 0.9}, 'betrayal_confidence': {'count': 8.0, 'mean': 0.9188, 'std': 0.0651, 'min': 0.8, '25%': 0.9, '50%': 0.9, '75%': 0.9625, 'max': 1.0}, 'authority_raw': {'count': 8.0, 'mean': 0.45, 'std': 0.4071, 'min': 0.0, '25%': 0.075, '50%': 0.4, '75%': 0.825, 'max': 1.0}, 'authority_salience': {'count': 8.0, 'mean': 0.45, 'std': 0.4106, 'min': 0.0, '25%': 0.075, '50%': 0.4, '75%': 0.825, 'max': 1.0}, 'authority_confidence': {'count': 8.0, 'mean': 0.95, 'std': 0.0535, 'min': 0.9, '25%': 0.9, '50%': 0.95, '75%': 1.0, 'max': 1.0}, 'subversion_raw': {'count': 8.0, 'mean': 0.7625, 'std': 0.3159, 'min': 0.0, '25%': 0.8, '50%': 0.85, '75%': 0.9, 'max': 1.0}, 'subversion_salience': {'count': 8.0, 'mean': 0.7688, 'std': 0.3195, 'min': 0.0, '25%': 0.8, '50%': 0.85, '75%': 0.9125, 'max': 1.0}, 'subversion_confidence': {'count': 8.0, 'mean': 0.9625, 'std': 0.0443, 'min': 0.9, '25%': 0.9375, '50%': 0.975, '75%': 1.0, 'max': 1.0}, 'sanctity_raw': {'count': 8.0, 'mean': 0.575, 'std': 0.2816, 'min': 0.1, '25%': 0.45, '50%': 0.6, '75%': 0.75, 'max': 0.9}, 'sanctity_salience': {'count': 8.0, 'mean': 0.5125, 'std': 0.2748, 'min': 0.1, '25%': 0.35, '50%': 0.55, '75%': 0.65, 'max': 0.9}, 'sanctity_confidence': {'count': 8.0, 'mean': 0.8562, 'std': 0.1084, 'min': 0.7, '25%': 0.8125, '50%': 0.875, '75%': 0.9125, 'max': 1.0}, 'degradation_raw': {'count': 8.0, 'mean': 0.5625, 'std': 0.2973, 'min': 0.0, '25%': 0.45, '50%': 0.6, '75%': 0.8, 'max': 0.9}, 'degradation_salience': {'count': 8.0, 'mean': 0.5, 'std': 0.2828, 'min': 0.0, '25%': 0.425, '50%': 0.55, '75%': 0.625, 'max': 0.9}, 'degradation_confidence': {'count': 8.0, 'mean': 0.8938, 'std': 0.0776, 'min': 0.8, '25%': 0.8375, '50%': 0.9, '75%': 0.925, 'max': 1.0}, 'liberty_raw': {'count': 8.0, 'mean': 0.4, 'std': 0.3742, 'min': 0.0, '25%': 0.075, '50%': 0.35, '75%': 0.65, 'max': 1.0}, 'liberty_salience': {'count': 8.0, 'mean': 0.3375, 'std': 0.3623, 'min': 0.0, '25%': 0.075, '50%': 0.2, '75%': 0.55, 'max': 1.0}, 'liberty_confidence': {'count': 8.0, 'mean': 0.9375, 'std': 0.0744, 'min': 0.8, '25%': 0.9, '50%': 0.95, '75%': 1.0, 'max': 1.0}, 'oppression_raw': {'count': 8.0, 'mean': 0.825, 'std': 0.1488, 'min': 0.6, '25%': 0.7, '50%': 0.85, '75%': 0.925, 'max': 1.0}, 'oppression_salience': {'count': 8.0, 'mean': 0.8, 'std': 0.1773, 'min': 0.6, '25%': 0.6, '50%': 0.85, '75%': 0.925, 'max': 1.0}, 'oppression_confidence': {'count': 8.0, 'mean': 0.9562, 'std': 0.0417, 'min': 0.9, '25%': 0.9375, '50%': 0.95, '75%': 1.0, 'max': 1.0}, 'care_harm_tension': {'count': 8.0, 'mean': 0.045, 'std': 0.0393, 'min': 0.0, '25%': 0.0, '50%': 0.055, '75%': 0.08, 'max': 0.09}, 'fairness_cheating_tension': {'count': 8.0, 'mean': 0.0825, 'std': 0.0625, 'min': 0.0, '25%': 0.0525, '50%': 0.075, '75%': 0.13, 'max': 0.16}, 'loyalty_betrayal_tension': {'count': 8.0, 'mean': 0.0738, 'std': 0.0848, 'min': 0.0, '25%': 0.0, '50%': 0.06, '75%': 0.0975, 'max': 0.24}, 'authority_subversion_tension': {'count': 8.0, 'mean': 0.06, 'std': 0.0741, 'min': 0.0, '25%': 0.0, '50%': 0.04, '75%': 0.09, 'max': 0.2}, 'sanctity_degradation_tension': {'count': 8.0, 'mean': 0.0662, 'std': 0.0518, 'min': 0.0, '25%': 0.045, '50%': 0.065, '75%': 0.09, 'max': 0.16}, 'liberty_oppression_tension': {'count': 8.0, 'mean': 0.0825, 'std': 0.0871, 'min': 0.0, '25%': 0.0, '50%': 0.07, '75%': 0.13, 'max': 0.24}, 'individualizing_tension': {'count': 8.0, 'mean': 0.1275, 'std': 0.0755, 'min': 0.0, '25%': 0.1025, '50%': 0.14, '75%': 0.16, 'max': 0.25}, 'binding_tension': {'count': 8.0, 'mean': 0.2, 'std': 0.1056, 'min': 0.06, '25%': 0.1325, '50%': 0.185, '75%': 0.2525, 'max': 0.39}, 'liberty_tension': {'count': 8.0, 'mean': 0.0825, 'std': 0.0871, 'min': 0.0, '25%': 0.0, '50%': 0.07, '75%': 0.13, 'max': 0.24}, 'moral_strategic_contradiction_index': {'count': 8.0, 'mean': 0.0683, 'std': 0.0227, 'min': 0.0417, '25%': 0.0563, '50%': 0.06, '75%': 0.0771, 'max': 0.115}, 'moral_salience_concentration': {'count': 8.0, 'mean': 0.3025, 'std': 0.0651, 'min': 0.2146, '25%': 0.2573, '50%': 0.302, '75%': 0.3356, 'max': 0.4191}, 'individualizing_foundations_mean': {'count': 8.0, 'mean': 0.775, 'std': 0.1458, 'min': 0.5, '25%': 0.6875, '50%': 0.8375, '75%': 0.8812, 'max': 0.9}, 'binding_foundations_mean': {'count': 8.0, 'mean': 0.6021, 'std': 0.1544, 'min': 0.4167, '25%': 0.5042, '50%': 0.5833, '75%': 0.6958, 'max': 0.8333}, 'liberty_foundation_mean': {'count': 8.0, 'mean': 0.6125, 'std': 0.2232, 'min': 0.4, '25%': 0.45, '50%': 0.525, '75%': 0.7125, 'max': 1.0}, 'individualizing_salience_mean': {'count': 8.0, 'mean': 0.7703, 'std': 0.1682, 'min': 0.45, '25%': 0.6812, '50%': 0.8312, '75%': 0.8812, 'max': 0.95}, 'binding_salience_mean': {'count': 8.0, 'mean': 0.5719, 'std': 0.155, 'min': 0.3833, '25%': 0.4646, '50%': 0.5333, '75%': 0.7333, 'max': 0.7667}, 'liberty_salience_mean': {'count': 8.0, 'mean': 0.5688, 'std': 0.2298, 'min': 0.4, '25%': 0.4, '50%': 0.475, '75%': 0.625, 'max': 1.0}}, 'get_inter_rater_reliability': {'message': 'No documents with multiple ratings found for IRR analysis.'}, 'perform_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-08-29T08:29:45.972361', 'sample_size': 8, 'alpha_level': 0.05, 'variables_analyzed': ['care_raw', 'care_salience', 'care_confidence', 'harm_raw', 'harm_salience', 'harm_confidence', 'fairness_raw', 'fairness_salience', 'fairness_confidence', 'cheating_raw', 'cheating_salience', 'cheating_confidence', 'loyalty_raw', 'loyalty_salience', 'loyalty_confidence', 'betrayal_raw', 'betrayal_salience', 'betrayal_confidence', 'authority_raw', 'authority_salience', 'authority_confidence', 'subversion_raw', 'subversion_salience', 'subversion_confidence', 'sanctity_raw', 'sanctity_salience', 'sanctity_confidence', 'degradation_raw', 'degradation_salience', 'degradation_confidence', 'liberty_raw', 'liberty_salience', 'liberty_confidence', 'oppression_raw', 'oppression_salience', 'oppression_confidence']}}, 'run_complete_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-08-29T08:29:45.984016', 'sample_size': 8, 'alpha_level': 0.05, 'variables_analyzed': ['care_raw', 'care_salience', 'care_confidence', 'harm_raw', 'harm_salience', 'harm_confidence', 'fairness_raw', 'fairness_salience', 'fairness_confidence', 'cheating_raw', 'cheating_salience', 'cheating_confidence', 'loyalty_raw', 'loyalty_salience', 'loyalty_confidence', 'betrayal_raw', 'betrayal_salience', 'betrayal_confidence', 'authority_raw', 'authority_salience', 'authority_confidence', 'subversion_raw', 'subversion_salience', 'subversion_confidence', 'sanctity_raw', 'sanctity_salience', 'sanctity_confidence', 'degradation_raw', 'degradation_salience', 'degradation_confidence', 'liberty_raw', 'liberty_salience', 'liberty_confidence', 'oppression_raw', 'oppression_salience', 'oppression_confidence']}}}, 'status': 'success_with_data', 'validation_passed': True}