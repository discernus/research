{'generation_metadata': {'status': 'success', 'functions_generated': 6, 'output_file': 'automatedstatisticalanalysisagent_functions.py', 'module_size': 23815}, 'statistical_data': {'analyze_descriptive_statistics': {'care_raw': {'count': 8.0, 'mean': 0.6875, 'std': 0.17268882005337977, 'min': 0.3, '25%': 0.6749999999999999, '50%': 0.75, '75%': 0.8, 'max': 0.8}, 'care_salience': {'count': 8.0, 'mean': 0.625, 'std': 0.249284690951645, 'min': 0.1, '25%': 0.575, '50%': 0.6499999999999999, '75%': 0.8, 'max': 0.9}, 'care_confidence': {'count': 8.0, 'mean': 0.9375, 'std': 0.05175491695067656, 'min': 0.85, '25%': 0.9, '50%': 0.95, '75%': 0.9624999999999999, 'max': 1.0}, 'harm_raw': {'count': 8.0, 'mean': 0.7750000000000001, 'std': 0.21213203435596426, 'min': 0.3, '25%': 0.775, '50%': 0.8, '75%': 0.9, 'max': 1.0}, 'harm_salience': {'count': 8.0, 'mean': 0.7250000000000001, 'std': 0.23754698783308417, 'min': 0.2, '25%': 0.6749999999999999, '50%': 0.8, '75%': 0.9, 'max': 0.9}, 'harm_confidence': {'count': 8.0, 'mean': 0.94375, 'std': 0.056299581322980165, 'min': 0.85, '25%': 0.9, '50%': 0.95, '75%': 1.0, 'max': 1.0}, 'fairness_raw': {'count': 8.0, 'mean': 0.8250000000000001, 'std': 0.12817398889233117, 'min': 0.6, '25%': 0.775, '50%': 0.8500000000000001, '75%': 0.9, 'max': 1.0}, 'fairness_salience': {'count': 8.0, 'mean': 0.8, 'std': 0.16035674514745463, 'min': 0.5, '25%': 0.7, '50%': 0.8500000000000001, '75%': 0.9, 'max': 1.0}, 'fairness_confidence': {'count': 8.0, 'mean': 0.96875, 'std': 0.04580626906564521, 'min': 0.9, '25%': 0.9375, '50%': 1.0, '75%': 1.0, 'max': 1.0}, 'cheating_raw': {'count': 8.0, 'mean': 0.7375, 'std': 0.3113908889391045, 'min': 0.0, '25%': 0.775, '50%': 0.8, '75%': 0.9, 'max': 1.0}, 'cheating_salience': {'count': 8.0, 'mean': 0.6875, 'std': 0.3090885217631258, 'min': 0.0, '25%': 0.6749999999999999, '50%': 0.7, '75%': 0.9, 'max': 1.0}, 'cheating_confidence': {'count': 8.0, 'mean': 0.9550000000000001, 'std': 0.048403069560278324, 'min': 0.9, '25%': 0.9, '50%': 0.97, '75%': 1.0, 'max': 1.0}, 'loyalty_raw': {'count': 8.0, 'mean': 0.6625, 'std': 0.19226098333849673, 'min': 0.3, '25%': 0.6, '50%': 0.6499999999999999, '75%': 0.75, 'max': 0.9}, 'loyalty_salience': {'count': 8.0, 'mean': 0.6312500000000001, 'std': 0.2685111064475998, 'min': 0.2, '25%': 0.475, '50%': 0.65, '75%': 0.8250000000000001, 'max': 0.95}, 'loyalty_confidence': {'count': 8.0, 'mean': 0.9312499999999999, 'std': 0.05303300858899106, 'min': 0.85, '25%': 0.9, '50%': 0.925, '75%': 0.9624999999999999, 'max': 1.0}, 'betrayal_raw': {'count': 8.0, 'mean': 0.5249999999999999, 'std': 0.3011881234615431, 'min': 0.0, '25%': 0.425, '50%': 0.55, '75%': 0.725, 'max': 0.9}, 'betrayal_salience': {'count': 8.0, 'mean': 0.4375, 'std': 0.2973093626895345, 'min': 0.0, '25%': 0.25, '50%': 0.45, '75%': 0.65, 'max': 0.8}, 'betrayal_confidence': {'count': 8.0, 'mean': 0.8987499999999999, 'std': 0.0687516233574576, 'min': 0.8, '25%': 0.875, '50%': 0.9, '75%': 0.95, 'max': 0.99}, 'authority_raw': {'count': 8.0, 'mean': 0.53125, 'std': 0.3432382554436495, 'min': 0.0, '25%': 0.275, '50%': 0.55, '75%': 0.8625, 'max': 0.9}, 'authority_salience': {'count': 8.0, 'mean': 0.475, 'std': 0.36154430670982185, 'min': 0.0, '25%': 0.17500000000000002, '50%': 0.45, '75%': 0.8250000000000001, 'max': 0.9}, 'authority_confidence': {'count': 8.0, 'mean': 0.95625, 'std': 0.05629958132298017, 'min': 0.85, '25%': 0.9375, '50%': 0.975, '75%': 1.0, 'max': 1.0}, 'subversion_raw': {'count': 8.0, 'mean': 0.675, 'std': 0.42003401222826164, 'min': 0.0, '25%': 0.6000000000000001, '50%': 0.9, '75%': 0.9, 'max': 1.0}, 'subversion_salience': {'count': 8.0, 'mean': 0.64375, 'std': 0.40305751096196824, 'min': 0.0, '25%': 0.5249999999999999, '50%': 0.875, '75%': 0.9, 'max': 0.9}, 'subversion_confidence': {'count': 8.0, 'mean': 0.9737499999999999, 'std': 0.03700868623908254, 'min': 0.9, '25%': 0.95, '50%': 0.995, '75%': 1.0, 'max': 1.0}, 'sanctity_raw': {'count': 8.0, 'mean': 0.5125, 'std': 0.33990544903798514, 'min': 0.0, '25%': 0.325, '50%': 0.55, '75%': 0.725, 'max': 1.0}, 'sanctity_salience': {'count': 8.0, 'mean': 0.4625, 'std': 0.337797487937881, 'min': 0.0, '25%': 0.25, '50%': 0.45, '75%': 0.65, 'max': 1.0}, 'sanctity_confidence': {'count': 8.0, 'mean': 0.8875, 'std': 0.08762745818846641, 'min': 0.7, '25%': 0.8875, '50%': 0.9, '75%': 0.9125, 'max': 1.0}, 'degradation_raw': {'count': 8.0, 'mean': 0.625, 'std': 0.2815771906346718, 'min': 0.0, '25%': 0.575, '50%': 0.7, '75%': 0.8, 'max': 0.9}, 'degradation_salience': {'count': 8.0, 'mean': 0.5375, 'std': 0.25035688811888407, 'min': 0.0, '25%': 0.475, '50%': 0.6, '75%': 0.7, 'max': 0.8}, 'degradation_confidence': {'count': 8.0, 'mean': 0.9237500000000001, 'std': 0.05153015760559191, 'min': 0.85, '25%': 0.9, '50%': 0.9, '75%': 0.96, 'max': 1.0}, 'liberty_raw': {'count': 8.0, 'mean': 0.425, 'std': 0.3370036032024414, 'min': 0.0, '25%': 0.17500000000000002, '50%': 0.4, '75%': 0.625, 'max': 1.0}, 'liberty_salience': {'count': 8.0, 'mean': 0.3375, 'std': 0.3020761493398643, 'min': 0.0, '25%': 0.1, '50%': 0.30000000000000004, '75%': 0.45, 'max': 0.9}, 'liberty_confidence': {'count': 8.0, 'mean': 0.9125000000000001, 'std': 0.08345229603962799, 'min': 0.8, '25%': 0.875, '50%': 0.9, '75%': 1.0, 'max': 1.0}, 'oppression_raw': {'count': 8.0, 'mean': 0.7, 'std': 0.37032803990902063, 'min': 0.0, '25%': 0.525, '50%': 0.9, '75%': 0.925, 'max': 1.0}, 'oppression_salience': {'count': 8.0, 'mean': 0.6375, 'std': 0.36620642110310253, 'min': 0.0, '25%': 0.425, '50%': 0.8, '75%': 0.9, 'max': 1.0}, 'oppression_confidence': {'count': 8.0, 'mean': 0.9625, 'std': 0.058248237251071755, 'min': 0.85, '25%': 0.9375, '50%': 1.0, '75%': 1.0, 'max': 1.0}}, 'analyze_foundation_correlations': {'care_raw': {'care_raw': 1.0, 'harm_raw': 0.49721207348800506, 'fairness_raw': -0.17748862770716783, 'cheating_raw': 0.16936046044042136, 'loyalty_raw': -0.36035569867818923, 'betrayal_raw': -0.21286369558324145, 'authority_raw': -0.6070537069492714, 'subversion_raw': 0.23141480808612278, 'sanctity_raw': -0.6054003957260042, 'degradation_raw': -0.11017207626882315, 'liberty_raw': -0.23933587923442762, 'oppression_raw': 0.20104517322394674}, 'harm_raw': {'care_raw': 0.49721207348800506, 'harm_raw': 1.0, 'fairness_raw': 0.07881104062390991, 'cheating_raw': 0.859660426219745, 'loyalty_raw': -0.7268129301982819, 'betrayal_raw': 0.570161700269707, 'authority_raw': -0.4684285938157121, 'subversion_raw': 0.6974298018536287, 'sanctity_raw': -0.03467173901780799, 'degradation_raw': 0.7294542700564115, 'liberty_raw': 0.20982172726556322, 'oppression_raw': 0.41825095628565223}, 'fairness_raw': {'care_raw': -0.17748862770716783, 'harm_raw': 0.07881104062390991, 'fairness_raw': 1.0, 'cheating_raw': 0.18791240976820936, 'loyalty_raw': -0.36231884057971026, 'betrayal_raw': -0.01850266226206333, 'authority_raw': -0.4424282856539418, 'subversion_raw': -0.11940709007280514, 'sanctity_raw': -0.4016798273384066, 'degradation_raw': -0.05937393699957592, 'liberty_raw': 0.3803341794400215, 'oppression_raw': 0.6621221919717305}, 'cheating_raw': {'care_raw': 0.16936046044042136, 'harm_raw': 0.859660426219745, 'fairness_raw': 0.18791240976820936, 'cheating_raw': 1.0, 'loyalty_raw': -0.4503932361111056, 'betrayal_raw': 0.7501782196666089, 'authority_raw': -0.4469245094367799, 'subversion_raw': 0.7509038005111889, 'sanctity_raw': -0.018558404227930807, 'degradation_raw': 0.786132778273854, 'liberty_raw': 0.0034033101036090763, 'oppression_raw': 0.45836472351908586}, 'loyalty_raw': {'care_raw': -0.36035569867818923, 'harm_raw': -0.7268129301982819, 'fairness_raw': -0.36231884057971026, 'cheating_raw': -0.4503932361111056, 'loyalty_raw': 1.0, 'betrayal_raw': -0.20352928488269525, 'authority_raw': 0.3666607504960803, 'subversion_raw': -0.3493763005833928, 'sanctity_raw': 0.07377792747031968, 'degradation_raw': -0.4288117672191595, 'liberty_raw': -0.4685276123536497, 'oppression_raw': -0.28090032386679487}, 'betrayal_raw': {'care_raw': -0.21286369558324145, 'harm_raw': 0.570161700269707, 'fairness_raw': -0.01850266226206333, 'cheating_raw': 0.7501782196666089, 'loyalty_raw': -0.20352928488269525, 'betrayal_raw': 1.0, 'authority_raw': -0.1122772588186659, 'subversion_raw': 0.8412710251405798, 'sanctity_raw': 0.331413049168514, 'degradation_raw': 0.6822156360562188, 'liberty_raw': 0.20407865119803223, 'oppression_raw': 0.30738931174713596}, 'authority_raw': {'care_raw': -0.6070537069492714, 'harm_raw': -0.4684285938157121, 'fairness_raw': -0.4424282856539418, 'cheating_raw': -0.4469245094367799, 'loyalty_raw': 0.3666607504960803, 'betrayal_raw': -0.1122772588186659, 'authority_raw': 1.0, 'subversion_raw': -0.31584354732312336, 'sanctity_raw': 0.7982013775875606, 'degradation_raw': 0.00554293862993277, 'liberty_raw': 0.0849071829742074, 'oppression_raw': -0.7754767263356722}, 'subversion_raw': {'care_raw': 0.23141480808612278, 'harm_raw': 0.6974298018536287, 'fairness_raw': -0.11940709007280514, 'cheating_raw': 0.7509038005111889, 'loyalty_raw': -0.3493763005833928, 'betrayal_raw': 0.8412710251405798, 'authority_raw': -0.31584354732312336, 'subversion_raw': 1.0, 'sanctity_raw': -0.027516428772849814, 'degradation_raw': 0.46502977317253613, 'liberty_raw': -0.09587526164678441, 'oppression_raw': 0.20204754855192736}, 'sanctity_raw': {'care_raw': -0.6054003957260042, 'harm_raw': -0.03467173901780799, 'fairness_raw': -0.4016798273384066, 'cheating_raw': -0.018558404227930807, 'loyalty_raw': 0.07377792747031968, 'betrayal_raw': 0.331413049168514, 'authority_raw': 0.7982013775875606, 'subversion_raw': -0.027516428772849814, 'sanctity_raw': 1.0, 'degradation_raw': 0.5336080930306415, 'liberty_raw': 0.4084327825758261, 'oppression_raw': -0.44261063374781917}, 'degradation_raw': {'care_raw': -0.11017207626882315, 'harm_raw': 0.7294542700564115, 'fairness_raw': -0.05937393699957592, 'cheating_raw': 0.786132778273854, 'loyalty_raw': -0.4288117672191595, 'betrayal_raw': 0.6822156360562188, 'authority_raw': 0.00554293862993277, 'subversion_raw': 0.46502977317253613, 'sanctity_raw': 0.5336080930306415, 'degradation_raw': 1.0, 'liberty_raw': 0.30861981398771565, 'oppression_raw': 0.17809890291413716}, 'liberty_raw': {'care_raw': -0.23933587923442762, 'harm_raw': 0.20982172726556322, 'fairness_raw': 0.3803341794400215, 'cheating_raw': 0.0034033101036090763, 'loyalty_raw': -0.4685276123536497, 'betrayal_raw': 0.20407865119803223, 'authority_raw': 0.0849071829742074, 'subversion_raw': -0.09587526164678441, 'sanctity_raw': 0.4084327825758261, 'degradation_raw': 0.30861981398771565, 'liberty_raw': 1.0, 'oppression_raw': 0.4006349781836763}, 'oppression_raw': {'care_raw': 0.20104517322394674, 'harm_raw': 0.41825095628565223, 'fairness_raw': 0.6621221919717305, 'cheating_raw': 0.45836472351908586, 'loyalty_raw': -0.28090032386679487, 'betrayal_raw': 0.30738931174713596, 'authority_raw': -0.7754767263356722, 'subversion_raw': 0.20204754855192736, 'sanctity_raw': -0.44261063374781917, 'degradation_raw': 0.17809890291413716, 'liberty_raw': 0.4006349781836763, 'oppression_raw': 1.0}}, 'analyze_ideological_differences': {'individualizing_foundations_mean': {'f_statistic': 3.0643022870401047, 'p_value': 0.14042916001696185, 'groups_compared': ['Conservative', 'Progressive']}, 'binding_foundations_mean': {'f_statistic': 2.8494136154825176, 'p_value': 0.15220939755170737, 'groups_compared': ['Conservative', 'Progressive']}, 'liberty_foundation_mean': {'f_statistic': 1.1043142305215714, 'p_value': 0.3414349830952684, 'groups_compared': ['Conservative', 'Progressive']}, 'moral_strategic_contradiction_index': {'f_statistic': 0.5844206383179816, 'p_value': 0.479090069044979, 'groups_compared': ['Conservative', 'Progressive']}, 'moral_salience_concentration': {'f_statistic': 1.824544852098355, 'p_value': 0.2346826271264898, 'groups_compared': ['Conservative', 'Progressive']}}, 'analyze_inter_rater_reliability': {'error': 'Insufficient documents with multiple evaluations for ICC.'}, 'calculate_derived_metrics': {'type': 'dataframe', 'data': [{'document_name': 'alexandria_ocasio_cortez_2025_fighting_oligarchy.txt', 'care_raw': 0.8, 'care_salience': 0.9, 'care_confidence': 1.0, 'harm_raw': 0.8, 'harm_salience': 0.8, 'harm_confidence': 1.0, 'fairness_raw': 0.8, 'fairness_salience': 0.8, 'fairness_confidence': 1.0, 'cheating_raw': 0.9, 'cheating_salience': 0.9, 'cheating_confidence': 1.0, 'loyalty_raw': 0.7, 'loyalty_salience': 0.8, 'loyalty_confidence': 0.9, 'betrayal_raw': 0.6, 'betrayal_salience': 0.5, 'betrayal_confidence': 0.9, 'authority_raw': 0.0, 'authority_salience': 0.0, 'authority_confidence': 1.0, 'subversion_raw': 0.9, 'subversion_salience': 0.9, 'subversion_confidence': 1.0, 'sanctity_raw': 0.1, 'sanctity_salience': 0.1, 'sanctity_confidence': 0.7, 'degradation_raw': 0.6, 'degradation_salience': 0.5, 'degradation_confidence': 0.9, 'liberty_raw': 0.0, 'liberty_salience': 0.0, 'liberty_confidence': 1.0, 'oppression_raw': 0.9, 'oppression_salience': 0.9, 'oppression_confidence': 1.0, 'care_harm_tension': 0.07999999999999999, 'fairness_cheating_tension': 0.07999999999999999, 'loyalty_betrayal_tension': 0.18000000000000002, 'authority_subversion_tension': 0.0, 'sanctity_degradation_tension': 0.04000000000000001, 'liberty_oppression_tension': 0.0, 'individualizing_tension': 0.15999999999999998, 'binding_tension': 0.22000000000000003, 'liberty_tension': 0.0, 'moral_strategic_contradiction_index': 0.06333333333333334, 'moral_salience_concentration': 0.36545944516540557, 'individualizing_foundations_mean': 0.8250000000000001, 'binding_foundations_mean': 0.48333333333333334, 'liberty_foundation_mean': 0.45, 'individualizing_salience_mean': 0.85, 'binding_salience_mean': 0.46666666666666673, 'liberty_salience_mean': 0.45}, {'document_name': 'bernie_sanders_2025_fighting_oligarchy.txt', 'care_raw': 0.8, 'care_salience': 0.8, 'care_confidence': 0.95, 'harm_raw': 0.9, 'harm_salience': 0.9, 'harm_confidence': 0.95, 'fairness_raw': 1.0, 'fairness_salience': 1.0, 'fairness_confidence': 1.0, 'cheating_raw': 1.0, 'cheating_salience': 1.0, 'cheating_confidence': 1.0, 'loyalty_raw': 0.6, 'loyalty_salience': 0.5, 'loyalty_confidence': 0.9, 'betrayal_raw': 0.5, 'betrayal_salience': 0.4, 'betrayal_confidence': 0.9, 'authority_raw': 0.2, 'authority_salience': 0.1, 'authority_confidence': 0.9, 'subversion_raw': 0.9, 'subversion_salience': 0.9, 'subversion_confidence': 0.95, 'sanctity_raw': 0.0, 'sanctity_salience': 0.0, 'sanctity_confidence': 0.95, 'degradation_raw': 0.5, 'degradation_salience': 0.4, 'degradation_confidence': 0.9, 'liberty_raw': 0.2, 'liberty_salience': 0.1, 'liberty_confidence': 0.9, 'oppression_raw': 1.0, 'oppression_salience': 1.0, 'oppression_confidence': 1.0, 'care_harm_tension': 0.07999999999999999, 'fairness_cheating_tension': 0.0, 'loyalty_betrayal_tension': 0.04999999999999999, 'authority_subversion_tension': 0.16000000000000003, 'sanctity_degradation_tension': 0.0, 'liberty_oppression_tension': 0.18000000000000002, 'individualizing_tension': 0.07999999999999999, 'binding_tension': 0.21000000000000002, 'liberty_tension': 0.18000000000000002, 'moral_strategic_contradiction_index': 0.07833333333333335, 'moral_salience_concentration': 0.3872005196590553, 'individualizing_foundations_mean': 0.925, 'binding_foundations_mean': 0.45, 'liberty_foundation_mean': 0.6, 'individualizing_salience_mean': 0.925, 'binding_salience_mean': 0.3833333333333333, 'liberty_salience_mean': 0.55}, {'document_name': 'cory_booker_2018_first_step_act.txt', 'care_raw': 0.7, 'care_salience': 0.6, 'care_confidence': 1.0, 'harm_raw': 0.8, 'harm_salience': 0.8, 'harm_confidence': 1.0, 'fairness_raw': 0.9, 'fairness_salience': 0.9, 'fairness_confidence': 1.0, 'cheating_raw': 0.7, 'cheating_salience': 0.6, 'cheating_confidence': 0.9, 'loyalty_raw': 0.6, 'loyalty_salience': 0.5, 'loyalty_confidence': 1.0, 'betrayal_raw': 0.2, 'betrayal_salience': 0.1, 'betrayal_confidence': 0.8, 'authority_raw': 0.5, 'authority_salience': 0.4, 'authority_confidence': 1.0, 'subversion_raw': 0.0, 'subversion_salience': 0.0, 'subversion_confidence': 1.0, 'sanctity_raw': 0.6, 'sanctity_salience': 0.5, 'sanctity_confidence': 0.9, 'degradation_raw': 0.8, 'degradation_salience': 0.7, 'degradation_confidence': 1.0, 'liberty_raw': 0.7, 'liberty_salience': 0.6, 'liberty_confidence': 1.0, 'oppression_raw': 0.9, 'oppression_salience': 0.8, 'oppression_confidence': 1.0, 'care_harm_tension': 0.14000000000000004, 'fairness_cheating_tension': 0.21000000000000002, 'loyalty_betrayal_tension': 0.08000000000000002, 'authority_subversion_tension': 0.0, 'sanctity_degradation_tension': 0.11999999999999997, 'liberty_oppression_tension': 0.14000000000000004, 'individualizing_tension': 0.3500000000000001, 'binding_tension': 0.19999999999999998, 'liberty_tension': 0.14000000000000004, 'moral_strategic_contradiction_index': 0.115, 'moral_salience_concentration': 0.2712205856136405, 'individualizing_foundations_mean': 0.7749999999999999, 'binding_foundations_mean': 0.45, 'liberty_foundation_mean': 0.8, 'individualizing_salience_mean': 0.725, 'binding_salience_mean': 0.3666666666666667, 'liberty_salience_mean': 0.7}, {'document_name': 'jd_vance_2022_natcon_conference.txt', 'care_raw': 0.7, 'care_salience': 0.6, 'care_confidence': 0.9, 'harm_raw': 0.8, 'harm_salience': 0.7, 'harm_confidence': 0.9, 'fairness_raw': 0.7, 'fairness_salience': 0.7, 'fairness_confidence': 0.9, 'cheating_raw': 0.8, 'cheating_salience': 0.7, 'cheating_confidence': 0.95, 'loyalty_raw': 0.9, 'loyalty_salience': 0.95, 'loyalty_confidence': 1.0, 'betrayal_raw': 0.8, 'betrayal_salience': 0.8, 'betrayal_confidence': 0.95, 'authority_raw': 0.6, 'authority_salience': 0.5, 'authority_confidence': 0.85, 'subversion_raw': 0.9, 'subversion_salience': 0.85, 'subversion_confidence': 0.95, 'sanctity_raw': 0.7, 'sanctity_salience': 0.6, 'sanctity_confidence': 0.9, 'degradation_raw': 0.7, 'degradation_salience': 0.6, 'degradation_confidence': 0.85, 'liberty_raw': 0.5, 'liberty_salience': 0.4, 'liberty_confidence': 0.8, 'oppression_raw': 0.9, 'oppression_salience': 0.8, 'oppression_confidence': 0.95, 'care_harm_tension': 0.06999999999999998, 'fairness_cheating_tension': 0.0, 'loyalty_betrayal_tension': 0.11999999999999994, 'authority_subversion_tension': 0.21, 'sanctity_degradation_tension': 0.0, 'liberty_oppression_tension': 0.2, 'individualizing_tension': 0.06999999999999998, 'binding_tension': 0.32999999999999996, 'liberty_tension': 0.2, 'moral_strategic_contradiction_index': 0.09999999999999998, 'moral_salience_concentration': 0.154233196128067, 'individualizing_foundations_mean': 0.75, 'binding_foundations_mean': 0.7666666666666667, 'liberty_foundation_mean': 0.7, 'individualizing_salience_mean': 0.6749999999999999, 'binding_salience_mean': 0.7166666666666667, 'liberty_salience_mean': 0.6000000000000001}, {'document_name': 'john_lewis_1963_march_on_washington.txt', 'care_raw': 0.8, 'care_salience': 0.7, 'care_confidence': 0.95, 'harm_raw': 1.0, 'harm_salience': 0.9, 'harm_confidence': 1.0, 'fairness_raw': 0.9, 'fairness_salience': 0.9, 'fairness_confidence': 1.0, 'cheating_raw': 0.8, 'cheating_salience': 0.7, 'cheating_confidence': 0.9, 'loyalty_raw': 0.3, 'loyalty_salience': 0.2, 'loyalty_confidence': 0.9, 'betrayal_raw': 0.7, 'betrayal_salience': 0.6, 'betrayal_confidence': 0.9, 'authority_raw': 0.3, 'authority_salience': 0.2, 'authority_confidence': 0.95, 'subversion_raw': 1.0, 'subversion_salience': 0.9, 'subversion_confidence': 1.0, 'sanctity_raw': 0.5, 'sanctity_salience': 0.4, 'sanctity_confidence': 0.85, 'degradation_raw': 0.7, 'degradation_salience': 0.6, 'degradation_confidence': 0.9, 'liberty_raw': 1.0, 'liberty_salience': 0.9, 'liberty_confidence': 1.0, 'oppression_raw': 1.0, 'oppression_salience': 0.9, 'oppression_confidence': 1.0, 'care_harm_tension': 0.16000000000000006, 'fairness_cheating_tension': 0.16000000000000006, 'loyalty_betrayal_tension': 0.11999999999999998, 'authority_subversion_tension': 0.21, 'sanctity_degradation_tension': 0.09999999999999998, 'liberty_oppression_tension': 0.0, 'individualizing_tension': 0.3200000000000001, 'binding_tension': 0.42999999999999994, 'liberty_tension': 0.0, 'moral_strategic_contradiction_index': 0.12500000000000003, 'moral_salience_concentration': 0.2678477631835372, 'individualizing_foundations_mean': 0.875, 'binding_foundations_mean': 0.5833333333333334, 'liberty_foundation_mean': 1.0, 'individualizing_salience_mean': 0.8, 'binding_salience_mean': 0.48333333333333334, 'liberty_salience_mean': 0.9}, {'document_name': 'john_mccain_2008_concession.txt', 'care_raw': 0.6, 'care_salience': 0.5, 'care_confidence': 0.9, 'harm_raw': 0.3, 'harm_salience': 0.2, 'harm_confidence': 0.85, 'fairness_raw': 0.8, 'fairness_salience': 0.7, 'fairness_confidence': 0.95, 'cheating_raw': 0.0, 'cheating_salience': 0.0, 'cheating_confidence': 0.99, 'loyalty_raw': 0.9, 'loyalty_salience': 0.9, 'loyalty_confidence': 0.95, 'betrayal_raw': 0.0, 'betrayal_salience': 0.0, 'betrayal_confidence': 0.99, 'authority_raw': 0.85, 'authority_salience': 0.8, 'authority_confidence': 0.95, 'subversion_raw': 0.0, 'subversion_salience': 0.0, 'subversion_confidence': 0.99, 'sanctity_raw': 0.4, 'sanctity_salience': 0.3, 'sanctity_confidence': 0.9, 'degradation_raw': 0.0, 'degradation_salience': 0.0, 'degradation_confidence': 0.99, 'liberty_raw': 0.3, 'liberty_salience': 0.2, 'liberty_confidence': 0.8, 'oppression_raw': 0.3, 'oppression_salience': 0.2, 'oppression_confidence': 0.9, 'care_harm_tension': 0.09, 'fairness_cheating_tension': 0.0, 'loyalty_betrayal_tension': 0.0, 'authority_subversion_tension': 0.0, 'sanctity_degradation_tension': 0.0, 'liberty_oppression_tension': 0.0, 'individualizing_tension': 0.09, 'binding_tension': 0.0, 'liberty_tension': 0.0, 'moral_strategic_contradiction_index': 0.015, 'moral_salience_concentration': 0.32983007562664557, 'individualizing_foundations_mean': 0.425, 'binding_foundations_mean': 0.35833333333333334, 'liberty_foundation_mean': 0.3, 'individualizing_salience_mean': 0.35, 'binding_salience_mean': 0.3333333333333333, 'liberty_salience_mean': 0.2}, {'document_name': 'mitt_romney_2020_impeachment.txt', 'care_raw': 0.3, 'care_salience': 0.1, 'care_confidence': 0.85, 'harm_raw': 0.7, 'harm_salience': 0.6, 'harm_confidence': 0.9, 'fairness_raw': 0.9, 'fairness_salience': 0.9, 'fairness_confidence': 1.0, 'cheating_raw': 0.9, 'cheating_salience': 0.9, 'cheating_confidence': 1.0, 'loyalty_raw': 0.7, 'loyalty_salience': 0.8, 'loyalty_confidence': 0.95, 'betrayal_raw': 0.9, 'betrayal_salience': 0.8, 'betrayal_confidence': 0.95, 'authority_raw': 0.9, 'authority_salience': 0.9, 'authority_confidence': 1.0, 'subversion_raw': 0.8, 'subversion_salience': 0.7, 'subversion_confidence': 0.9, 'sanctity_raw': 1.0, 'sanctity_salience': 1.0, 'sanctity_confidence': 1.0, 'degradation_raw': 0.9, 'degradation_salience': 0.8, 'degradation_confidence': 0.95, 'liberty_raw': 0.6, 'liberty_salience': 0.4, 'liberty_confidence': 0.9, 'oppression_raw': 0.6, 'oppression_salience': 0.5, 'oppression_confidence': 0.85, 'care_harm_tension': 0.15, 'fairness_cheating_tension': 0.0, 'loyalty_betrayal_tension': 0.0, 'authority_subversion_tension': 0.16000000000000006, 'sanctity_degradation_tension': 0.17999999999999997, 'liberty_oppression_tension': 0.059999999999999984, 'individualizing_tension': 0.15, 'binding_tension': 0.34, 'liberty_tension': 0.059999999999999984, 'moral_strategic_contradiction_index': 0.09166666666666666, 'moral_salience_concentration': 0.2593698657761292, 'individualizing_foundations_mean': 0.7, 'binding_foundations_mean': 0.8666666666666667, 'liberty_foundation_mean': 0.6, 'individualizing_salience_mean': 0.625, 'binding_salience_mean': 0.8333333333333334, 'liberty_salience_mean': 0.45}, {'document_name': 'steve_king_2017_house_floor.txt', 'care_raw': 0.8, 'care_salience': 0.8, 'care_confidence': 0.95, 'harm_raw': 0.9, 'harm_salience': 0.9, 'harm_confidence': 0.95, 'fairness_raw': 0.6, 'fairness_salience': 0.5, 'fairness_confidence': 0.9, 'cheating_raw': 0.8, 'cheating_salience': 0.7, 'cheating_confidence': 0.9, 'loyalty_raw': 0.6, 'loyalty_salience': 0.4, 'loyalty_confidence': 0.85, 'betrayal_raw': 0.5, 'betrayal_salience': 0.3, 'betrayal_confidence': 0.8, 'authority_raw': 0.9, 'authority_salience': 0.9, 'authority_confidence': 1.0, 'subversion_raw': 0.9, 'subversion_salience': 0.9, 'subversion_confidence': 1.0, 'sanctity_raw': 0.8, 'sanctity_salience': 0.8, 'sanctity_confidence': 0.9, 'degradation_raw': 0.8, 'degradation_salience': 0.7, 'degradation_confidence': 0.9, 'liberty_raw': 0.1, 'liberty_salience': 0.1, 'liberty_confidence': 0.9, 'oppression_raw': 0.0, 'oppression_salience': 0.0, 'oppression_confidence': 1.0, 'care_harm_tension': 0.07999999999999999, 'fairness_cheating_tension': 0.11999999999999997, 'loyalty_betrayal_tension': 0.05000000000000002, 'authority_subversion_tension': 0.0, 'sanctity_degradation_tension': 0.08000000000000007, 'liberty_oppression_tension': 0.0, 'individualizing_tension': 0.19999999999999996, 'binding_tension': 0.1300000000000001, 'liberty_tension': 0.0, 'moral_strategic_contradiction_index': 0.055000000000000014, 'moral_salience_concentration': 0.3186144245246149, 'individualizing_foundations_mean': 0.7750000000000001, 'binding_foundations_mean': 0.75, 'liberty_foundation_mean': 0.05, 'individualizing_salience_mean': 0.7250000000000001, 'binding_salience_mean': 0.6666666666666666, 'liberty_salience_mean': 0.05}], 'columns': ['document_name', 'care_raw', 'care_salience', 'care_confidence', 'harm_raw', 'harm_salience', 'harm_confidence', 'fairness_raw', 'fairness_salience', 'fairness_confidence', 'cheating_raw', 'cheating_salience', 'cheating_confidence', 'loyalty_raw', 'loyalty_salience', 'loyalty_confidence', 'betrayal_raw', 'betrayal_salience', 'betrayal_confidence', 'authority_raw', 'authority_salience', 'authority_confidence', 'subversion_raw', 'subversion_salience', 'subversion_confidence', 'sanctity_raw', 'sanctity_salience', 'sanctity_confidence', 'degradation_raw', 'degradation_salience', 'degradation_confidence', 'liberty_raw', 'liberty_salience', 'liberty_confidence', 'oppression_raw', 'oppression_salience', 'oppression_confidence', 'care_harm_tension', 'fairness_cheating_tension', 'loyalty_betrayal_tension', 'authority_subversion_tension', 'sanctity_degradation_tension', 'liberty_oppression_tension', 'individualizing_tension', 'binding_tension', 'liberty_tension', 'moral_strategic_contradiction_index', 'moral_salience_concentration', 'individualizing_foundations_mean', 'binding_foundations_mean', 'liberty_foundation_mean', 'individualizing_salience_mean', 'binding_salience_mean', 'liberty_salience_mean'], 'index': [0, 1, 2, 3, 4, 5, 6, 7], 'shape': (8, 54)}, 'classify_ideological_profile': {'profile_counts': {'Balanced': 5, 'Libertarian': 3}, 'document_profiles': [{'document_name': 'alexandria_ocasio_cortez_2025_fighting_oligarchy.txt', 'ideological_profile': 'Balanced'}, {'document_name': 'bernie_sanders_2025_fighting_oligarchy.txt', 'ideological_profile': 'Balanced'}, {'document_name': 'cory_booker_2018_first_step_act.txt', 'ideological_profile': 'Libertarian'}, {'document_name': 'jd_vance_2022_natcon_conference.txt', 'ideological_profile': 'Libertarian'}, {'document_name': 'john_lewis_1963_march_on_washington.txt', 'ideological_profile': 'Libertarian'}, {'document_name': 'john_mccain_2008_concession.txt', 'ideological_profile': 'Balanced'}, {'document_name': 'mitt_romney_2020_impeachment.txt', 'ideological_profile': 'Balanced'}, {'document_name': 'steve_king_2017_house_floor.txt', 'ideological_profile': 'Balanced'}]}, 'generate_statistical_summary_report': 'STATISTICAL ANALYSIS SUMMARY REPORT\n==================================================\nAnalysis Timestamp: Unknown\nSample Size: Unknown\nAlpha Level: Unknown\nVariables: 0\n', 'perform_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-08-29T08:10:49.549001', 'sample_size': 8, 'alpha_level': 0.05, 'variables_analyzed': ['care_raw', 'care_salience', 'care_confidence', 'harm_raw', 'harm_salience', 'harm_confidence', 'fairness_raw', 'fairness_salience', 'fairness_confidence', 'cheating_raw', 'cheating_salience', 'cheating_confidence', 'loyalty_raw', 'loyalty_salience', 'loyalty_confidence', 'betrayal_raw', 'betrayal_salience', 'betrayal_confidence', 'authority_raw', 'authority_salience', 'authority_confidence', 'subversion_raw', 'subversion_salience', 'subversion_confidence', 'sanctity_raw', 'sanctity_salience', 'sanctity_confidence', 'degradation_raw', 'degradation_salience', 'degradation_confidence', 'liberty_raw', 'liberty_salience', 'liberty_confidence', 'oppression_raw', 'oppression_salience', 'oppression_confidence']}}, 'run_complete_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-08-29T08:10:49.562114', 'sample_size': 8, 'alpha_level': 0.05, 'variables_analyzed': ['care_raw', 'care_salience', 'care_confidence', 'harm_raw', 'harm_salience', 'harm_confidence', 'fairness_raw', 'fairness_salience', 'fairness_confidence', 'cheating_raw', 'cheating_salience', 'cheating_confidence', 'loyalty_raw', 'loyalty_salience', 'loyalty_confidence', 'betrayal_raw', 'betrayal_salience', 'betrayal_confidence', 'authority_raw', 'authority_salience', 'authority_confidence', 'subversion_raw', 'subversion_salience', 'subversion_confidence', 'sanctity_raw', 'sanctity_salience', 'sanctity_confidence', 'degradation_raw', 'degradation_salience', 'degradation_confidence', 'liberty_raw', 'liberty_salience', 'liberty_confidence', 'oppression_raw', 'oppression_salience', 'oppression_confidence']}}}, 'status': 'success_with_data', 'validation_passed': True}