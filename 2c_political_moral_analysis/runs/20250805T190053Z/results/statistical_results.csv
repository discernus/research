test_name,test_type,statistic_name,statistic_value,p_value,effect_size,degrees_of_freedom,sample_size,dependent_variable,grouping_variable,significance_level,interpretation,notes
calculate_all_derived_metrics,derived_metrics_calculation,result_value,"{'type': 'derived_metrics_calculation', 'success': True, 'calculated_metrics': {'individualizing_foundations_mean': [0.8125, 0.775, 0.775, 0.4375, 0.825, 0.425, 0.7249999999999999, 0.575], 'individualizing_salience_mean': [0.775, 0.825, 0.8, 0.48750000000000004, 0.8499999999999999, 0.525, 0.7249999999999999, 0.575], 'binding_foundations_mean': [0.3416666666666666, 0.2583333333333333, 0.20000000000000004, 0.4666666666666666, 0.5166666666666666, 0.3666666666666667, 0.4666666666666666, 0.4166666666666667], 'binding_salience_mean': [0.2916666666666667, 0.2333333333333333, 0.1416666666666667, 0.5166666666666666, 0.5333333333333333, 0.44999999999999996, 0.4666666666666666, 0.41666666666666674], 'individualizing_tension': [0.28500000000000003, 0.14499999999999996, 0.3075, 0.04500000000000003, 0.22000000000000003, 0.17250000000000001, 0.07000000000000006, 0.21999999999999997], 'binding_tension': [0.04499999999999995, 0.0825, 0.04, 0.08000000000000006, 0.18, 0.15750000000000003, 0.16999999999999998, 0.17000000000000004]}, 'successful_calculations': ['individualizing_foundations_mean', 'individualizing_salience_mean', 'binding_foundations_mean', 'binding_salience_mean', 'individualizing_tension', 'binding_tension'], 'failed_calculations': [{'metric': 'moral_strategic_contradiction_index', 'formula': '(individualizing_tension + binding_tension + foundation_tension) / 3', 'error': ""name 'individualizing_tension' is not defined""}, {'metric': 'foundation_tension', 'formula': 'abs(individualizing_foundations_mean - binding_foundations_mean) * abs(individualizing_salience_mean - binding_salience_mean)', 'error': ""name 'binding_salience_mean' is not defined""}], 'formulas_used': ['individualizing_foundations_mean', 'binding_foundations_mean', 'individualizing_salience_mean', 'binding_salience_mean', 'individualizing_tension', 'binding_tension', 'foundation_tension', 'moral_strategic_contradiction_index'], 'input_columns': ['care_score', 'care_salience', 'harm_score', 'harm_salience', 'fairness_score', 'fairness_salience', 'cheating_score', 'cheating_salience', 'loyalty_score', 'loyalty_salience', 'betrayal_score', 'betrayal_salience', 'authority_score', 'authority_salience', 'subversion_score', 'subversion_salience', 'sanctity_score', 'sanctity_salience', 'degradation_score', 'degradation_salience'], 'total_metrics': 8, 'success_rate': 0.75}",,,,,,,,Generic derived_metrics_calculation result,
validate_derived_metrics,metric_validation,result_value,"{'type': 'metric_validation', 'validation_rules': ['missing_data_check', 'range_check', 'consistency_check'], 'results': {'missing_data_check': {'status': 'completed', 'missing_data_by_column': {'aid': 0, 'speaker': 0, 'ideology': 0, 'date': 0, 'context': 0, 'category': 0, 'care_score': 0, 'care_salience': 0, 'care_confidence': 0, 'harm_score': 0, 'harm_salience': 0, 'harm_confidence': 0, 'fairness_score': 0, 'fairness_salience': 0, 'fairness_confidence': 0, 'cheating_score': 0, 'cheating_salience': 0, 'cheating_confidence': 0, 'loyalty_score': 0, 'loyalty_salience': 0, 'loyalty_confidence': 0, 'betrayal_score': 0, 'betrayal_salience': 0, 'betrayal_confidence': 0, 'authority_score': 0, 'authority_salience': 0, 'authority_confidence': 0, 'subversion_score': 0, 'subversion_salience': 0, 'subversion_confidence': 0, 'sanctity_score': 0, 'sanctity_salience': 0, 'sanctity_confidence': 0, 'degradation_score': 0, 'degradation_salience': 0, 'degradation_confidence': 0, 'gasket_version': 0, 'extraction_time_seconds': 0, 'individualizing_foundations_mean': 0, 'individualizing_salience_mean': 0, 'binding_foundations_mean': 0, 'binding_salience_mean': 0, 'individualizing_tension': 0, 'binding_tension': 0}, 'total_missing': 0}, 'range_check': {'status': 'completed', 'ranges': {'care_score': {'min': 0.2, 'max': 0.9, 'mean': 0.6625000000000001}, 'care_salience': {'min': 0.25, 'max': 0.95, 'mean': 0.68125}, 'care_confidence': {'min': 0.7, 'max': 0.95, 'mean': 0.84375}, 'harm_score': {'min': 0.1, 'max': 0.85, 'mean': 0.5874999999999999}, 'harm_salience': {'min': 0.15, 'max': 0.85, 'mean': 0.6375}, 'harm_confidence': {'min': 0.65, 'max': 0.9, 'mean': 0.81875}, 'fairness_score': {'min': 0.7, 'max': 0.9, 'mean': 0.8}, 'fairness_salience': {'min': 0.7, 'max': 0.95, 'mean': 0.8375}, 'fairness_confidence': {'min': 0.8, 'max': 0.98, 'mean': 0.90375}, 'cheating_score': {'min': 0.15, 'max': 0.8, 'mean': 0.625}, 'cheating_salience': {'min': 0.25, 'max': 0.8, 'mean': 0.625}, 'cheating_confidence': {'min': 0.7, 'max': 0.9, 'mean': 0.79375}, 'loyalty_score': {'min': 0.2, 'max': 0.75, 'mean': 0.48750000000000004}, 'loyalty_salience': {'min': 0.15, 'max': 0.85, 'mean': 0.46875}, 'loyalty_confidence': {'min': 0.6, 'max': 0.9, 'mean': 0.7375}, 'betrayal_score': {'min': 0.05, 'max': 0.7, 'mean': 0.40625}, 'betrayal_salience': {'min': 0.1, 'max': 0.7, 'mean': 0.4125}, 'betrayal_confidence': {'min': 0.6, 'max': 0.8, 'mean': 0.7250000000000001}, 'authority_score': {'min': 0.1, 'max': 0.7, 'mean': 0.3375}, 'authority_salience': {'min': 0.1, 'max': 0.8, 'mean': 0.35625}, 'authority_confidence': {'min': 0.5, 'max': 0.95, 'mean': 0.6812499999999999}, 'subversion_score': {'min': 0.1, 'max': 0.7, 'mean': 0.4125}, 'subversion_salience': {'min': 0.15, 'max': 0.7, 'mean': 0.43125}, 'subversion_confidence': {'min': 0.5, 'max': 0.9, 'mean': 0.7125}, 'sanctity_score': {'min': 0.1, 'max': 0.6, 'mean': 0.325}, 'sanctity_salience': {'min': 0.05, 'max': 0.65, 'mean': 0.3125}, 'sanctity_confidence': {'min': 0.4, 'max': 0.8, 'mean': 0.64375}, 'degradation_score': {'min': 0.0, 'max': 0.7, 'mean': 0.30625}, 'degradation_salience': {'min': 0.0, 'max': 0.7, 'mean': 0.30625}, 'degradation_confidence': {'min': 0.3, 'max': 0.8, 'mean': 0.65625}, 'extraction_time_seconds': {'min': 1.0994267463684082, 'max': 1.357212781906128, 'mean': 1.185482531785965}, 'individualizing_foundations_mean': {'min': 0.425, 'max': 0.825, 'mean': 0.66875}, 'individualizing_salience_mean': {'min': 0.48750000000000004, 'max': 0.8499999999999999, 'mean': 0.6953125}, 'binding_foundations_mean': {'min': 0.20000000000000004, 'max': 0.5166666666666666, 'mean': 0.37916666666666665}, 'binding_salience_mean': {'min': 0.1416666666666667, 'max': 0.5333333333333333, 'mean': 0.38125}, 'individualizing_tension': {'min': 0.04500000000000003, 'max': 0.3075, 'mean': 0.183125}, 'binding_tension': {'min': 0.04, 'max': 0.18, 'mean': 0.115625}}}, 'consistency_check': {'status': 'completed', 'notes': 'Basic consistency check completed'}}, 'quality_thresholds': {'moral_strategic_contradiction_index': {'min': 0.0, 'max': 1.0}}}",,,,,,,,Generic metric_validation result,
