{'generation_metadata': {'status': 'success', 'functions_generated': 10, 'output_file': 'automatedstatisticalanalysisagent_functions.py', 'module_size': 35704, 'function_code_content': '"""\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: bolsonaro_2018_populist_discourse_analysis\nDescription: Statistical analysis experiment\nGenerated: 2025-09-01T05:05:57.916583+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n"""\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings(\'ignore\', category=RuntimeWarning)\n\n\ndef _prepare_data_for_analysis(data):\n    """\n    Internal helper function to calculate derived metrics and add grouping variables.\n    This function is not intended to be called directly by the user.\n    """\n    import pandas as pd\n    import numpy as np\n\n    df = data.copy()\n\n    # --- Calculate Derived Metrics ---\n    # Define dimension names for easier access\n    dims = [\n        "manichaean_people_elite_framing", "crisis_restoration_narrative",\n        "popular_sovereignty_claims", "anti_pluralist_exclusion",\n        "elite_conspiracy_systemic_corruption", "authenticity_vs_political_class",\n        "homogeneous_people_construction", "nationalist_exclusion",\n        "economic_populist_appeals"\n    ]\n    \n    core_dims = dims[:4]\n    mech_dims = dims[4:7]\n    bound_dims = dims[7:]\n\n    # Helper to get scores and salience\n    def get_rs(dim_name):\n        return df[f\'{dim_name}_raw\'], df[f\'{dim_name}_salience\']\n\n    # Salience-Weighted Indices\n    # Overall\n    numerator = sum(get_rs(d)[0] * get_rs(d)[1] for d in dims)\n    denominator = sum(get_rs(d)[1] for d in dims) + 0.001\n    df[\'salience_weighted_overall_populism_index\'] = numerator / denominator\n\n    # Core\n    numerator_core = sum(get_rs(d)[0] * get_rs(d)[1] for d in core_dims)\n    denominator_core = sum(get_rs(d)[1] for d in core_dims) + 0.001\n    df[\'salience_weighted_core_populism_index\'] = numerator_core / denominator_core\n\n    # Mechanisms\n    numerator_mech = sum(get_rs(d)[0] * get_rs(d)[1] for d in mech_dims)\n    denominator_mech = sum(get_rs(d)[1] for d in mech_dims) + 0.001\n    df[\'salience_weighted_populism_mechanisms_index\'] = numerator_mech / denominator_mech\n\n    # Boundary\n    numerator_bound = sum(get_rs(d)[0] * get_rs(d)[1] for d in bound_dims)\n    denominator_bound = sum(get_rs(d)[1] for d in bound_dims) + 0.001\n    df[\'salience_weighted_boundary_distinctions_index\'] = numerator_bound / denominator_bound\n\n    # Tension Indices\n    psc_r, psc_s = get_rs(\'popular_sovereignty_claims\')\n    ape_r, ape_s = get_rs(\'anti_pluralist_exclusion\')\n    df[\'democratic_authoritarian_tension\'] = np.minimum(psc_r, ape_r) * abs(psc_s - ape_s)\n\n    hpc_r, hpc_s = get_rs(\'homogeneous_people_construction\')\n    ne_r, ne_s = get_rs(\'nationalist_exclusion\')\n    df[\'internal_external_focus_tension\'] = np.minimum(hpc_r, ne_r) * abs(hpc_s - ne_s)\n\n    crn_r, crn_s = get_rs(\'crisis_restoration_narrative\')\n    ecs_r, ecs_s = get_rs(\'elite_conspiracy_systemic_corruption\')\n    df[\'crisis_elite_attribution_tension\'] = np.minimum(crn_r, ecs_r) * abs(crn_s - ecs_s)\n\n    # Populist Strategic Contradiction Index (PSCI)\n    df[\'populist_strategic_contradiction_index\'] = (\n        df[\'democratic_authoritarian_tension\'] +\n        df[\'internal_external_focus_tension\'] +\n        df[\'crisis_elite_attribution_tension\']\n    ) / 3\n\n    # --- Add Grouping Variables ---\n    df[\'date\'] = pd.to_datetime(df[\'document_name\'].str.extract(r\'(\\d{4}-\\d{2}-\\d{2})\')[0], errors=\'coerce\')\n    \n    # Pre/Post Stabbing Groups\n    stabbing_date = pd.to_datetime(\'2018-09-06\')\n    df[\'pre_post_stabbing\'] = np.where(df[\'date\'] < stabbing_date, \'pre_stabbing\', \'post_stabbing\')\n    \n    # Metadata mapping based on experiment spec\n    doc_metadata_map = {\n        \'2018-07-22\': {\'campaign_stage\': \'early_campaign\', \'audience\': \'mass_public\', \'electoral_proximity\': \'distant\'},\n        \'2018-08-23\': {\'campaign_stage\': \'mid_campaign\', \'audience\': \'mass_public\', \'electoral_proximity\': \'distant\'},\n        \'2018-08-28\': {\'campaign_stage\': \'mid_campaign\', \'audience\': \'business_leaders\', \'electoral_proximity\': \'distant\'},\n        \'2018-08-31\': {\'campaign_stage\': \'mid_campaign\', \'audience\': \'mass_public\', \'electoral_proximity\': \'distant\'},\n        \'2018-09-06\': {\'campaign_stage\': \'mid_campaign\', \'audience\': \'national_audience\', \'electoral_proximity\': \'approaching\'},\n        \'2018-09-16\': {\'campaign_stage\': \'campaign_interruption\', \'audience\': \'online_supporters\', \'electoral_proximity\': \'approaching\'},\n        \'2018-09-30\': {\'campaign_stage\': \'late_campaign\', \'audience\': \'mass_public\', \'electoral_proximity\': \'approaching\'},\n        \'2018-10-06\': {\'campaign_stage\': \'final_campaign\', \'audience\': \'national_audience\', \'electoral_proximity\': \'imminent\'},\n        \'2018-10-07\': {\'campaign_stage\': \'post_first_round\', \'audience\': \'online_supporters\', \'electoral_proximity\': \'inter_round\'}, # Simplified date for mapping\n        \'2018-10-16\': {\'campaign_stage\': \'post_first_round\', \'audience\': \'national_audience\', \'electoral_proximity\': \'inter_round\'},\n        \'2018-10-22\': {\'campaign_stage\': \'pre_second_round\', \'audience\': \'mass_public\', \'electoral_proximity\': \'final_push\'},\n        \'2018-10-27\': {\'campaign_stage\': \'pre_second_round\', \'audience\': \'online_supporters\', \'electoral_proximity\': \'final_push\'},\n    }\n    \n    # Handle the two Oct 7 speeches by checking for a keyword if available\n    # For now, we map all Oct 7 to the same group as a fallback\n    df[\'date_str\'] = df[\'date\'].dt.strftime(\'%Y-%m-%d\')\n    \n    df[\'campaign_stage\'] = df[\'date_str\'].map({k: v[\'campaign_stage\'] for k, v in doc_metadata_map.items()})\n    df[\'audience\'] = df[\'date_str\'].map({k: v[\'audience\'] for k, v in doc_metadata_map.items()})\n    df[\'electoral_proximity\'] = df[\'date_str\'].map({k: v[\'electoral_proximity\'] for k, v in doc_metadata_map.items()})\n\n    # A more robust mapping for the two Oct 7 speeches if name contains clues\n    is_oct7 = df[\'date_str\'] == \'2018-10-07\'\n    is_morning = df[\'document_name\'].str.contains(\'morning\', case=False, na=False)\n    df.loc[is_oct7 & is_morning, \'campaign_stage\'] = \'election_day\'\n    df.loc[is_oct7 & is_morning, \'audience\'] = \'national_audience\'\n\n    return df.dropna(subset=[\'date\'])\n\ndef get_descriptive_statistics(data, **kwargs):\n    """\n    Calculates descriptive statistics for all raw scores, salience scores, and derived indices.\n    This addresses RQ1, RQ5 and provides foundational data for hypotheses H1, H5, H14.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with analysis scores.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing descriptive statistics (mean, std, min, max) for key metrics.\n              Returns None if the data is empty or an error occurs.\n    """\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        if data.empty:\n            return None\n            \n        df = _prepare_data_for_analysis(data)\n        if df.empty:\n            return {"error": "No valid data after preparation."}\n\n        metrics_to_describe = [\n            \'salience_weighted_overall_populism_index\',\n            \'populist_strategic_contradiction_index\'\n        ]\n        dims = [\n            "manichaean_people_elite_framing", "crisis_restoration_narrative",\n            "popular_sovereignty_claims", "anti_pluralist_exclusion",\n            "elite_conspiracy_systemic_corruption", "authenticity_vs_political_class",\n            "homogeneous_people_construction", "nationalist_exclusion",\n            "economic_populist_appeals"\n        ]\n        for dim in dims:\n            metrics_to_describe.append(f\'{dim}_raw\')\n            metrics_to_describe.append(f\'{dim}_salience\')\n\n        # Filter out columns that might not exist if preparation failed\n        existing_metrics = [m for m in metrics_to_describe if m in df.columns]\n        \n        if not existing_metrics:\n            return {"error": "No descriptive metrics columns found in the data."}\n\n        desc_stats = df[existing_metrics].describe().to_dict()\n\n        # Specific check for H1\n        h1_avg = df[\'salience_weighted_overall_populism_index\'].mean()\n        h1_result = {\n            "hypothesis": "H1: Bolsonaro\'s Salience-Weighted Overall Populism Index will average >= 0.5",\n            "mean_score": h1_avg,\n            "is_confirmed": h1_avg >= 0.5\n        }\n        \n        # Specific check for H5\n        salience_cols = [f\'{d}_salience\' for d in dims]\n        avg_salience = df[salience_cols].mean().sort_values(ascending=False)\n        h5_result = {\n            "hypothesis": "H5: Anti-Pluralist Exclusion and Crisis-Restoration dimensions will show highest salience scores (> 0.7)",\n            "average_salience_scores": avg_salience.to_dict(),\n            "top_2_dimensions": list(avg_salience.head(2).index),\n            "is_confirmed": (\'anti_pluralist_exclusion_salience\' in avg_salience.index and \n                             \'crisis_restoration_narrative_salience\' in avg_salience.index and\n                             avg_salience[\'anti_pluralist_exclusion_salience\'] > 0.7 and \n                             avg_salience[\'crisis_restoration_narrative_salience\'] > 0.7)\n        }\n\n        return {\n            "descriptive_statistics": desc_stats,\n            "hypothesis_checks": {\n                "H1": h1_result,\n                "H5": h5_result\n            }\n        }\n\n    except Exception as e:\n        return {"error": f"An error occurred: {str(e)}"}\n\ndef perform_temporal_trend_analysis(data, **kwargs):\n    """\n    Performs a linear regression to analyze the temporal trend of populism scores over the campaign.\n    This directly tests hypothesis H9.\n\n    Methodology:\n    - Converts document dates to ordinal numbers for regression.\n    - Fits an Ordinary Least Squares (OLS) model from statsmodels.\n    - Reports the slope (beta coefficient), p-value, and R-squared.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with analysis scores.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary with regression results (slope, p-value, r_squared) or None on error.\n    """\n    import pandas as pd\n    import numpy as np\n    import statsmodels.api as sm\n\n    try:\n        df = _prepare_data_for_analysis(data)\n        if df.shape[0] < 2:\n            return {"error": "Insufficient data for trend analysis (need at least 2 data points)."}\n\n        df = df.sort_values(\'date\').reset_index()\n        df[\'time_ordinal\'] = df.index\n\n        y = df[\'salience_weighted_overall_populism_index\']\n        X = sm.add_constant(df[\'time_ordinal\'])\n\n        model = sm.OLS(y, X).fit()\n        \n        slope = model.params[\'time_ordinal\']\n        p_value = model.pvalues[\'time_ordinal\']\n\n        h9_result = {\n            "hypothesis": "H9: Linear trend analysis will show significant positive slope for overall populism (β > 0, p < 0.05)",\n            "slope_beta": slope,\n            "p_value": p_value,\n            "is_confirmed": slope > 0 and p_value < 0.05\n        }\n\n        return {\n            "analysis_type": "Temporal Trend Linear Regression",\n            "dependent_variable": "salience_weighted_overall_populism_index",\n            "independent_variable": "time_ordinal",\n            "slope": slope,\n            "intercept": model.params[\'const\'],\n            "p_value_slope": p_value,\n            "r_squared": model.rsquared,\n            "n_observations": model.nobs,\n            "hypothesis_H9_check": h9_result\n        }\n\n    except Exception as e:\n        return {"error": f"An error occurred during trend analysis: {str(e)}"}\n\ndef compare_campaign_phases(data, **kwargs):\n    """\n    Compares populist scores between different campaign phases using t-tests.\n    Specifically tests hypotheses H2 (early vs. late), H4 (pre vs. post-stabbing), and H8 (pre vs. post-stabbing).\n\n    Methodology:\n    - Groups data into phases (early/late, pre/post).\n    - Performs independent t-tests between groups.\n    - Reports t-statistic, p-value, and Cohen\'s d for effect size.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with analysis scores.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing results for each t-test comparison, or None on error.\n    """\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import ttest_ind\n    from pingouin import compute_effsize\n\n    try:\n        df = _prepare_data_for_analysis(data)\n        if df.empty:\n            return {"error": "No valid data after preparation."}\n\n        results = {}\n\n        # H2: Early vs. Late Campaign\n        early_campaign_docs = [\'early_campaign\', \'mid_campaign\']\n        late_campaign_docs = [\'late_campaign\', \'final_campaign\', \'post_first_round\', \'pre_second_round\']\n        \n        early_scores = df[df[\'campaign_stage\'].isin(early_campaign_docs)][\'salience_weighted_overall_populism_index\']\n        late_scores = df[df[\'campaign_stage\'].isin(late_campaign_docs)][\'salience_weighted_overall_populism_index\']\n\n        if len(early_scores) > 1 and len(late_scores) > 1:\n            t_stat, p_val = ttest_ind(late_scores, early_scores, nan_policy=\'omit\', equal_var=False) # Welch\'s t-test\n            cohen_d = compute_effsize(late_scores, early_scores, eftype=\'cohen\')\n            results[\'H2_early_vs_late_campaign\'] = {\n                "hypothesis": "H2: μ_late_campaign > μ_early_campaign on Salience-Weighted Overall Populism Index",\n                "mean_early": early_scores.mean(),\n                "mean_late": late_scores.mean(),\n                "t_statistic": t_stat,\n                "p_value": p_val,\n                "cohen_d": cohen_d,\n                "is_confirmed": late_scores.mean() > early_scores.mean() and p_val / 2 < 0.05 # One-tailed test\n            }\n\n        # H4 & H8: Pre vs. Post Stabbing\n        pre_stabbing = df[df[\'pre_post_stabbing\'] == \'pre_stabbing\']\n        post_stabbing = df[df[\'pre_post_stabbing\'] == \'post_stabbing\']\n\n        if len(pre_stabbing) > 1 and len(post_stabbing) > 1:\n            # H4 Test\n            pre_h4 = pre_stabbing[\'manichaean_people_elite_framing_raw\']\n            post_h4 = post_stabbing[\'manichaean_people_elite_framing_raw\']\n            t_stat_h4, p_val_h4 = ttest_ind(post_h4, pre_h4, nan_policy=\'omit\', equal_var=False)\n            cohen_d_h4 = compute_effsize(post_h4, pre_h4, eftype=\'cohen\')\n            results[\'H4_pre_vs_post_stabbing_manichaean\'] = {\n                "hypothesis": "H4: μ_post_stabbing > μ_pre_stabbing on Manichaean People-Elite Framing",\n                "mean_pre_stabbing": pre_h4.mean(),\n                "mean_post_stabbing": post_h4.mean(),\n                "t_statistic": t_stat_h4,\n                "p_value": p_val_h4,\n                "cohen_d": cohen_d_h4,\n                "is_confirmed": post_h4.mean() > pre_h4.mean() and p_val_h4 / 2 < 0.05\n            }\n\n            # H8 Test\n            pre_h8 = pre_stabbing[\'elite_conspiracy_systemic_corruption_raw\']\n            post_h8 = post_stabbing[\'elite_conspiracy_systemic_corruption_raw\']\n            t_stat_h8, p_val_h8 = ttest_ind(post_h8, pre_h8, nan_policy=\'omit\', equal_var=False)\n            cohen_d_h8 = compute_effsize(post_h8, pre_h8, eftype=\'cohen\')\n            results[\'H8_pre_vs_post_stabbing_conspiracy\'] = {\n                "hypothesis": "H8: Elite Conspiracy dimension will show significant increase after stabbing incident",\n                "mean_pre_stabbing": pre_h8.mean(),\n                "mean_post_stabbing": post_h8.mean(),\n                "t_statistic": t_stat_h8,\n                "p_value": p_val_h8,\n                "cohen_d": cohen_d_h8,\n                "is_confirmed": post_h8.mean() > pre_h8.mean() and p_val_h8 / 2 < 0.05\n            }\n\n        return results if results else {"error": "Insufficient data for any phase comparison."}\n\n    except Exception as e:\n        return {"error": f"An error occurred during phase comparison: {str(e)}"}\n\ndef analyze_audience_effects(data, **kwargs):\n    """\n    Compares populist scores between different audience types using t-tests.\n    Specifically tests hypothesis H6 (Business leaders vs. Mass rallies).\n\n    Methodology:\n    - Groups data by the \'audience\' variable.\n    - Performs an independent t-test between the \'business_leaders\' and \'mass_public\' groups.\n    - Reports t-statistic, p-value, and Cohen\'s d for effect size.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with analysis scores.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing results for the t-test, or None on error.\n    """\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import ttest_ind\n    from pingouin import compute_effsize\n\n    try:\n        df = _prepare_data_for_analysis(data)\n        if df.empty:\n            return {"error": "No valid data after preparation."}\n\n        business_scores = df[df[\'audience\'] == \'business_leaders\'][\'economic_populist_appeals_raw\']\n        mass_rally_scores = df[df[\'audience\'] == \'mass_public\'][\'economic_populist_appeals_raw\']\n\n        if len(business_scores) < 1 or len(mass_rally_scores) < 1:\n            return {"error": "Insufficient data for audience comparison (need at least one speech for each audience type)."}\n\n        # Use Welch\'s t-test as sample sizes are small and likely unequal variances\n        t_stat, p_val = ttest_ind(business_scores, mass_rally_scores, nan_policy=\'omit\', equal_var=False)\n        cohen_d = compute_effsize(business_scores, mass_rally_scores, eftype=\'cohen\')\n\n        h6_result = {\n            "hypothesis": "H6: Business audience speeches will score lower on Economic Populist Appeals than mass rally speeches",\n            "mean_business_audience": business_scores.mean(),\n            "mean_mass_rally": mass_rally_scores.mean(),\n            "t_statistic": t_stat,\n            "p_value": p_val,\n            "cohen_d": cohen_d,\n            "is_confirmed": business_scores.mean() < mass_rally_scores.mean() and p_val / 2 < 0.05 # One-tailed\n        }\n\n        return h6_result\n\n    except Exception as e:\n        return {"error": f"An error occurred during audience effects analysis: {str(e)}"}\n\ndef perform_dimensional_anova(data, **kwargs):\n    """\n    Performs a one-way ANOVA to test for significant differences between campaign stages across PDAF dimensions.\n    This directly tests hypothesis H11.\n\n    Methodology:\n    - Uses the \'campaign_stage\' as the independent variable.\n    - Runs a separate ANOVA for each of the 9 raw score dimensions.\n    - Reports F-statistic, p-value, and eta-squared for effect size.\n    - If ANOVA is significant, performs a post-hoc Tukey HSD test.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with analysis scores.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary with ANOVA and Tukey HSD results for each dimension, or None on error.\n    """\n    import pandas as pd\n    import numpy as np\n    import pingouin as pg\n\n    try:\n        df = _prepare_data_for_analysis(data)\n        if df.empty:\n            return {"error": "No valid data after preparation."}\n            \n        if df[\'campaign_stage\'].nunique() < 2:\n            return {"error": "ANOVA requires at least 2 campaign_stage groups."}\n\n        dims = [\n            "manichaean_people_elite_framing", "crisis_restoration_narrative",\n            "popular_sovereignty_claims", "anti_pluralist_exclusion",\n            "elite_conspiracy_systemic_corruption", "authenticity_vs_political_class",\n            "homogeneous_people_construction", "nationalist_exclusion",\n            "economic_populist_appeals"\n        ]\n        \n        results = {}\n        significant_dims_count = 0\n\n        for dim in dims:\n            dim_col = f\'{dim}_raw\'\n            if dim_col not in df.columns:\n                continue\n            \n            # Drop rows with NaN in the dimension or grouping variable\n            anova_data = df.dropna(subset=[dim_col, \'campaign_stage\'])\n            if anova_data[\'campaign_stage\'].nunique() < 2:\n                continue\n\n            anova_res = pg.anova(data=anova_data, dv=dim_col, between=\'campaign_stage\', detailed=True)\n            f_val = anova_res.loc[0, \'F\']\n            p_val = anova_res.loc[0, \'p-unc\']\n            eta_sq = anova_res.loc[0, \'np2\'] # Partial eta-squared\n\n            dim_results = {\n                "f_statistic": f_val,\n                "p_value": p_val,\n                "eta_squared": eta_sq,\n                "is_significant": p_val < 0.05\n            }\n            \n            if p_val < 0.05:\n                significant_dims_count += 1\n                tukey_res = pg.pairwise_tukey(data=anova_data, dv=dim_col, between=\'campaign_stage\')\n                dim_results["tukey_hsd_posthoc"] = tukey_res.to_dict(\'records\')\n            \n            results[dim] = dim_results\n\n        h11_result = {\n            "hypothesis": "H11: At least 5 of 9 PDAF dimensions will show significant differences between campaign_stage groups (ANOVA)",\n            "significant_dimensions_count": significant_dims_count,\n            "is_confirmed": significant_dims_count >= 5\n        }\n\n        return {\n            "anova_results_by_dimension": results,\n            "hypothesis_H11_check": h11_result\n        }\n\n    except Exception as e:\n        return {"error": f"An error occurred during ANOVA: {str(e)}"}\n\ndef analyze_dimensional_correlations(data, **kwargs):\n    """\n    Calculates a Pearson correlation matrix for all PDAF dimensions.\n    This tests hypotheses H3 and H13.\n\n    Methodology:\n    - Selects all 9 raw score dimension columns.\n    - Computes the Pearson correlation matrix.\n    - Extracts specific correlation values required by hypotheses.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with analysis scores.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing the full correlation matrix and specific hypothesis test results.\n    """\n    import pandas as pd\n    import numpy as np\n\n    try:\n        df = _prepare_data_for_analysis(data)\n        if df.shape[0] < 2:\n            return {"error": "Insufficient data for correlation analysis."}\n\n        dims = [\n            "manichaean_people_elite_framing", "crisis_restoration_narrative",\n            "popular_sovereignty_claims", "anti_pluralist_exclusion",\n            "elite_conspiracy_systemic_corruption", "authenticity_vs_political_class",\n            "homogeneous_people_construction", "nationalist_exclusion",\n            "economic_populist_appeals"\n        ]\n        dim_cols = [f\'{d}_raw\' for d in dims]\n        \n        # Ensure all columns exist\n        existing_dim_cols = [col for col in dim_cols if col in df.columns]\n        if len(existing_dim_cols) < 2:\n            return {"error": "Fewer than 2 dimension columns exist for correlation."}\n\n        corr_matrix = df[existing_dim_cols].corr(method=\'pearson\')\n\n        results = {"correlation_matrix": corr_matrix.to_dict()}\n        hypothesis_checks = {}\n\n        # H3: Patriotic/nationalist framing will show negative correlation with people-centric populist dimensions\n        # Assuming "people-centric" are the core dimensions minus anti-pluralist\n        people_centric_dims = [\n            \'manichaean_people_elite_framing_raw\',\n            \'crisis_restoration_narrative_raw\',\n            \'popular_sovereignty_claims_raw\'\n        ]\n        nationalist_dim = \'nationalist_exclusion_raw\'\n        if nationalist_dim in corr_matrix.columns:\n            h3_corrs = corr_matrix.loc[people_centric_dims, nationalist_dim]\n            h3_result = {\n                "hypothesis": "H3: Patriotic/nationalist framing will show negative correlation with people-centric populist dimensions (r < -0.3)",\n                "correlations": h3_corrs.to_dict(),\n                "is_confirmed": (h3_corrs < -0.3).all()\n            }\n            hypothesis_checks["H3"] = h3_result\n\n        # H13: Nationalist Exclusion will correlate positively with Anti-Pluralist Exclusion\n        if \'nationalist_exclusion_raw\' in corr_matrix.columns and \'anti_pluralist_exclusion_raw\' in corr_matrix.columns:\n            h13_corr = corr_matrix.loc[\'nationalist_exclusion_raw\', \'anti_pluralist_exclusion_raw\']\n            h13_result = {\n                "hypothesis": "H13: Nationalist Exclusion will correlate positively with Anti-Pluralist Exclusion (r > 0.5)",\n                "correlation_coefficient": h13_corr,\n                "is_confirmed": h13_corr > 0.5\n            }\n            hypothesis_checks["H13"] = h13_result\n        \n        results["hypothesis_checks"] = hypothesis_checks\n        return results\n\n    except Exception as e:\n        return {"error": f"An error occurred during correlation analysis: {str(e)}"}\n\ndef test_dimensional_reliability(data, **kwargs):\n    """\n    Tests the internal consistency of dimensional groupings using Cronbach\'s alpha.\n    This directly tests hypothesis H12.\n\n    Methodology:\n    - Groups dimensions into \'core\' and \'auxiliary\' sets.\n    - Calculates Cronbach\'s alpha for each set using the pingouin library.\n    - Compares the alpha values as per the hypothesis.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with analysis scores.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary with Cronbach\'s alpha for each dimension set and the hypothesis test result.\n    """\n    import pandas as pd\n    import numpy as np\n    import pingouin as pg\n\n    try:\n        df = _prepare_data_for_analysis(data)\n        if df.shape[0] < 2:\n            return {"error": "Insufficient data for reliability analysis."}\n\n        core_dims = [\n            \'manichaean_people_elite_framing_raw\', \'crisis_restoration_narrative_raw\',\n            \'popular_sovereignty_claims_raw\', \'anti_pluralist_exclusion_raw\'\n        ]\n        aux_dims = [\n            \'elite_conspiracy_systemic_corruption_raw\', \'authenticity_vs_political_class_raw\',\n            \'homogeneous_people_construction_raw\', \'nationalist_exclusion_raw\',\n            \'economic_populist_appeals_raw\'\n        ]\n        \n        core_alpha = pg.cronbach_alpha(data=df[core_dims])\n        aux_alpha = pg.cronbach_alpha(data=df[aux_dims])\n\n        h12_result = {\n            "hypothesis": "H12: Core populist dimensions will show higher internal consistency (Cronbach\'s α > 0.8) than auxiliary dimensions",\n            "core_dimensions_alpha": core_alpha[0],\n            "auxiliary_dimensions_alpha": aux_alpha[0],\n            "is_confirmed": core_alpha[0] > 0.8 and core_alpha[0] > aux_alpha[0]\n        }\n\n        return {\n            "reliability_analysis": {\n                "core_dimensions": {\n                    "dimensions": core_dims,\n                    "cronbach_alpha": core_alpha[0],\n                    "confidence_interval_95": core_alpha[1]\n                },\n                "auxiliary_dimensions": {\n                    "dimensions": aux_dims,\n                    "cronbach_alpha": aux_alpha[0],\n                    "confidence_interval_95": aux_alpha[1]\n                }\n            },\n            "hypothesis_H12_check": h12_result\n        }\n\n    except Exception as e:\n        return {"error": f"An error occurred during reliability testing: {str(e)}"}\n\ndef test_variance_homogeneity(data, **kwargs):\n    """\n    Tests for homogeneity of variances between campaign months.\n    This directly tests hypothesis H10.\n\n    Methodology:\n    - Groups data by month.\n    - Performs Levene\'s test to compare the variance of the \'salience_weighted_overall_populism_index\'\n      between the final campaign month (October) and earlier months.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with analysis scores.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary with Levene\'s test results and the hypothesis check.\n    """\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import levene\n\n    try:\n        df = _prepare_data_for_analysis(data)\n        if df.empty:\n            return {"error": "No valid data after preparation."}\n\n        df[\'month\'] = df[\'date\'].dt.month\n        \n        october_scores = df[df[\'month\'] == 10][\'salience_weighted_overall_populism_index\'].dropna()\n        earlier_scores = df[df[\'month\'] < 10][\'salience_weighted_overall_populism_index\'].dropna()\n\n        if len(october_scores) < 2 or len(earlier_scores) < 2:\n            return {"error": "Insufficient data for Levene\'s test (need at least 2 data points in each group)."}\n\n        stat, p_val = levene(october_scores, earlier_scores)\n\n        h10_result = {\n            "hypothesis": "H10: Variance in populist scores will increase in final campaign month (Levene\'s test, October vs. earlier months)",\n            "variance_october": october_scores.var(),\n            "variance_earlier": earlier_scores.var(),\n            "levene_statistic": stat,\n            "p_value": p_val,\n            "is_confirmed": october_scores.var() > earlier_scores.var() and p_val < 0.05\n        }\n\n        return h10_result\n\n    except Exception as e:\n        return {"error": f"An error occurred during variance testing: {str(e)}"}\n\ndef analyze_strategic_contradiction(data, **kwargs):\n    """\n    Analyzes the Populist Strategic Contradiction Index (PSCI) over the campaign.\n    This directly tests hypothesis H7.\n\n    Methodology:\n    - Calculates the mean PSCI for October speeches and earlier speeches.\n    - Performs an independent t-test to check for a significant difference.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with analysis scores.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary with the t-test results and hypothesis check.\n    """\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import ttest_ind\n    from pingouin import compute_effsize\n\n    try:\n        df = _prepare_data_for_analysis(data)\n        if df.empty:\n            return {"error": "No valid data after preparation."}\n\n        df[\'month\'] = df[\'date\'].dt.month\n        \n        october_psci = df[df[\'month\'] == 10][\'populist_strategic_contradiction_index\'].dropna()\n        earlier_psci = df[df[\'month\'] < 10][\'populist_strategic_contradiction_index\'].dropna()\n\n        if len(october_psci) < 2 or len(earlier_psci) < 2:\n            return {"error": "Insufficient data for strategic contradiction analysis."}\n\n        t_stat, p_val = ttest_ind(october_psci, earlier_psci, equal_var=False) # Welch\'s t-test\n        cohen_d = compute_effsize(october_psci, earlier_psci, eftype=\'cohen\')\n\n        h7_result = {\n            "hypothesis": "H7: Populist Strategic Contradiction Index will be highest in October speeches",\n            "mean_psci_october": october_psci.mean(),\n            "mean_psci_earlier": earlier_psci.mean(),\n            "t_statistic": t_stat,\n            "p_value": p_val,\n            "cohen_d": cohen_d,\n            "is_confirmed": october_psci.mean() > earlier_psci.mean() and p_val / 2 < 0.05 # One-tailed\n        }\n\n        return h7_result\n\n    except Exception as e:\n        return {"error": f"An error occurred during strategic contradiction analysis: {str(e)}"}\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    """\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    """\n    results = {\n        \'analysis_metadata\': {\n            \'timestamp\': pd.Timestamp.now().isoformat(),\n            \'sample_size\': len(data),\n            \'alpha_level\': alpha,\n            \'variables_analyzed\': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith((\'calculate_\', \'perform_\', \'test_\')) and \n            name != \'run_complete_statistical_analysis\'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if \'alpha\' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {\'error\': f\'Analysis failed: {str(e)}\'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    """\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    """\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    """\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    """\n    report_lines = []\n    report_lines.append("STATISTICAL ANALYSIS SUMMARY REPORT")\n    report_lines.append("=" * 50)\n    \n    metadata = analysis_results.get(\'analysis_metadata\', {})\n    report_lines.append(f"Analysis Timestamp: {metadata.get(\'timestamp\', \'Unknown\')}")\n    report_lines.append(f"Sample Size: {metadata.get(\'sample_size\', \'Unknown\')}")\n    report_lines.append(f"Alpha Level: {metadata.get(\'alpha_level\', \'Unknown\')}")\n    report_lines.append(f"Variables: {len(metadata.get(\'variables_analyzed\', []))}")\n    report_lines.append("")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != \'analysis_metadata\' and isinstance(result, dict):\n            if \'error\' not in result:\n                report_lines.append(f"{analysis_name.replace(\'_\', \' \').title()}:")\n                \n                # Extract key statistics based on analysis type\n                if \'p_value\' in result:\n                    p_val = result[\'p_value\']\n                    significance = "significant" if p_val < metadata.get(\'alpha_level\', 0.05) else "not significant"\n                    report_lines.append(f"  - p-value: {p_val:.4f} ({significance})")\n                \n                if \'effect_size\' in result:\n                    report_lines.append(f"  - Effect size: {result[\'effect_size\']:.4f}")\n                \n                if \'correlation_matrix\' in result:\n                    report_lines.append(f"  - Correlation matrix generated with {len(result[\'correlation_matrix\'])} variables")\n                \n                if \'cronbach_alpha\' in result:\n                    alpha_val = result[\'cronbach_alpha\']\n                    reliability = "excellent" if alpha_val > 0.9 else "good" if alpha_val > 0.8 else "acceptable" if alpha_val > 0.7 else "questionable"\n                    report_lines.append(f"  - Cronbach\'s α: {alpha_val:.3f} ({reliability})")\n                \n                report_lines.append("")\n            else:\n                report_lines.append(f"{analysis_name}: ERROR - {result[\'error\']}")\n                report_lines.append("")\n    \n    return "\\n".join(report_lines)\n', 'cached_with_code': True}, 'statistical_data': {'analyze_audience_effects': {'error': 'Insufficient data for audience comparison (need at least one speech for each audience type).'}, 'analyze_dimensional_correlations': {'correlation_matrix': {'manichaean_people_elite_framing_raw': {'manichaean_people_elite_framing_raw': 1.0, 'crisis_restoration_narrative_raw': 0.40532174168888896, 'popular_sovereignty_claims_raw': -0.03297787560273696, 'anti_pluralist_exclusion_raw': 0.5733412928572632, 'elite_conspiracy_systemic_corruption_raw': 0.31754264805429394, 'authenticity_vs_political_class_raw': 0.17748239349298917, 'homogeneous_people_construction_raw': 0.16666666666666652, 'nationalist_exclusion_raw': -0.17398353448033582, 'economic_populist_appeals_raw': -0.1134568593126501}, 'crisis_restoration_narrative_raw': {'manichaean_people_elite_framing_raw': 0.40532174168888896, 'crisis_restoration_narrative_raw': 1.0, 'popular_sovereignty_claims_raw': -0.1401238185942367, 'anti_pluralist_exclusion_raw': 0.24077170617153865, 'elite_conspiracy_systemic_corruption_raw': 0.18822753949743573, 'authenticity_vs_political_class_raw': 0.3023459003827584, 'homogeneous_people_construction_raw': -0.3563483225498993, 'nationalist_exclusion_raw': -0.4790713941627621, 'economic_populist_appeals_raw': 0.01627430347634376}, 'popular_sovereignty_claims_raw': {'manichaean_people_elite_framing_raw': -0.03297787560273696, 'crisis_restoration_narrative_raw': -0.1401238185942367, 'popular_sovereignty_claims_raw': 1.0, 'anti_pluralist_exclusion_raw': -0.2565709143399018, 'elite_conspiracy_systemic_corruption_raw': -0.2538638047538965, 'authenticity_vs_political_class_raw': -0.008671099695241706, 'homogeneous_people_construction_raw': -0.2138897698969528, 'nationalist_exclusion_raw': 0.3952573957056182, 'economic_populist_appeals_raw': -0.5441223393400084}, 'anti_pluralist_exclusion_raw': {'manichaean_people_elite_framing_raw': 0.5733412928572632, 'crisis_restoration_narrative_raw': 0.24077170617153865, 'popular_sovereignty_claims_raw': -0.2565709143399018, 'anti_pluralist_exclusion_raw': 1.0, 'elite_conspiracy_systemic_corruption_raw': 0.14085904245475273, 'authenticity_vs_political_class_raw': -0.24537386440559086, 'homogeneous_people_construction_raw': 0.4801287609333052, 'nationalist_exclusion_raw': 0.07958931749587678, 'economic_populist_appeals_raw': -0.10814761408717492}, 'elite_conspiracy_systemic_corruption_raw': {'manichaean_people_elite_framing_raw': 0.31754264805429394, 'crisis_restoration_narrative_raw': 0.18822753949743573, 'popular_sovereignty_claims_raw': -0.2538638047538965, 'anti_pluralist_exclusion_raw': 0.14085904245475273, 'elite_conspiracy_systemic_corruption_raw': 1.0, 'authenticity_vs_political_class_raw': -0.34644452589672914, 'homogeneous_people_construction_raw': -0.5049193032800973, 'nationalist_exclusion_raw': -0.5775842826840111, 'economic_populist_appeals_raw': -0.19346633092388718}, 'authenticity_vs_political_class_raw': {'manichaean_people_elite_framing_raw': 0.17748239349298917, 'crisis_restoration_narrative_raw': 0.3023459003827584, 'popular_sovereignty_claims_raw': -0.008671099695241706, 'anti_pluralist_exclusion_raw': -0.24537386440559086, 'elite_conspiracy_systemic_corruption_raw': -0.34644452589672914, 'authenticity_vs_political_class_raw': 1.0, 'homogeneous_people_construction_raw': -0.052270837348931475, 'nationalist_exclusion_raw': -0.08822575465125684, 'economic_populist_appeals_raw': 0.2497562164217851}, 'homogeneous_people_construction_raw': {'manichaean_people_elite_framing_raw': 0.16666666666666652, 'crisis_restoration_narrative_raw': -0.3563483225498993, 'popular_sovereignty_claims_raw': -0.2138897698969528, 'anti_pluralist_exclusion_raw': 0.4801287609333052, 'elite_conspiracy_systemic_corruption_raw': -0.5049193032800973, 'authenticity_vs_political_class_raw': -0.052270837348931475, 'homogeneous_people_construction_raw': 1.0, 'nationalist_exclusion_raw': 0.582671582316751, 'economic_populist_appeals_raw': 0.4002539477671929}, 'nationalist_exclusion_raw': {'manichaean_people_elite_framing_raw': -0.17398353448033582, 'crisis_restoration_narrative_raw': -0.4790713941627621, 'popular_sovereignty_claims_raw': 0.3952573957056182, 'anti_pluralist_exclusion_raw': 0.07958931749587678, 'elite_conspiracy_systemic_corruption_raw': -0.5775842826840111, 'authenticity_vs_political_class_raw': -0.08822575465125684, 'homogeneous_people_construction_raw': 0.582671582316751, 'nationalist_exclusion_raw': 1.0, 'economic_populist_appeals_raw': -0.41889321330827817}, 'economic_populist_appeals_raw': {'manichaean_people_elite_framing_raw': -0.1134568593126501, 'crisis_restoration_narrative_raw': 0.01627430347634376, 'popular_sovereignty_claims_raw': -0.5441223393400084, 'anti_pluralist_exclusion_raw': -0.10814761408717492, 'elite_conspiracy_systemic_corruption_raw': -0.19346633092388718, 'authenticity_vs_political_class_raw': 0.2497562164217851, 'homogeneous_people_construction_raw': 0.4002539477671929, 'nationalist_exclusion_raw': -0.41889321330827817, 'economic_populist_appeals_raw': 1.0}}, 'hypothesis_checks': {'H3': {'hypothesis': 'H3: Patriotic/nationalist framing will show negative correlation with people-centric populist dimensions (r < -0.3)', 'correlations': {'manichaean_people_elite_framing_raw': -0.17398353448033582, 'crisis_restoration_narrative_raw': -0.4790713941627621, 'popular_sovereignty_claims_raw': 0.3952573957056182}, 'is_confirmed': np.False_}, 'H13': {'hypothesis': 'H13: Nationalist Exclusion will correlate positively with Anti-Pluralist Exclusion (r > 0.5)', 'correlation_coefficient': np.float64(0.07958931749587678), 'is_confirmed': np.False_}}}, 'analyze_strategic_contradiction': {'hypothesis': 'H7: Populist Strategic Contradiction Index will be highest in October speeches', 'mean_psci_october': np.float64(0.1286666666666667), 'mean_psci_earlier': np.float64(0.10000000000000002), 't_statistic': np.float64(1.4779337758227409), 'p_value': np.float64(0.1814807095063949), 'cohen_d': np.float64(0.934727392498511), 'is_confirmed': np.False_}, 'compare_campaign_phases': {'H2_early_vs_late_campaign': {'hypothesis': 'H2: μ_late_campaign > μ_early_campaign on Salience-Weighted Overall Populism Index', 'mean_early': np.float64(0.7902603123102098), 'mean_late': np.float64(0.7547278451426902), 't_statistic': np.float64(-1.6416446730049328), 'p_value': np.float64(0.16917818368883827), 'cohen_d': np.float64(-0.9812054014753459), 'is_confirmed': np.False_}, 'H4_pre_vs_post_stabbing_manichaean': {'hypothesis': 'H4: μ_post_stabbing > μ_pre_stabbing on Manichaean People-Elite Framing', 'mean_pre_stabbing': np.float64(0.8500000000000001), 'mean_post_stabbing': np.float64(0.8222222222222223), 't_statistic': np.float64(-0.5976143046671971), 'p_value': np.float64(0.5629623552988746), 'cohen_d': np.float64(-0.28356543640780035), 'is_confirmed': np.False_}, 'H8_pre_vs_post_stabbing_conspiracy': {'hypothesis': 'H8: Elite Conspiracy dimension will show significant increase after stabbing incident', 'mean_pre_stabbing': np.float64(0.5750000000000001), 'mean_post_stabbing': np.float64(0.7222222222222222), 't_statistic': np.float64(1.492513700145803), 'p_value': np.float64(0.19436213270052913), 'cohen_d': np.float64(0.9520197811670859), 'is_confirmed': np.False_}}, 'generate_statistical_summary_report': 'STATISTICAL ANALYSIS SUMMARY REPORT\n==================================================\nAnalysis Timestamp: Unknown\nSample Size: Unknown\nAlpha Level: Unknown\nVariables: 0\n', 'get_descriptive_statistics': {'descriptive_statistics': {'salience_weighted_overall_populism_index': {'count': 10.0, 'mean': 0.7696618071920331, 'std': 0.03659338437711589, 'min': 0.6998772145237678, '25%': 0.7588057463735922, '50%': 0.7718259599469585, '75%': 0.7920658204348527, 'max': 0.8248625229128477}, 'populist_strategic_contradiction_index': {'count': 10.0, 'mean': 0.11433333333333336, 'std': 0.03262393274352927, 'min': 0.06666666666666671, '25%': 0.08750000000000002, '50%': 0.12000000000000002, '75%': 0.13083333333333338, 'max': 0.16333333333333336}, 'manichaean_people_elite_framing_raw': {'count': 13.0, 'mean': 0.8307692307692309, 'std': 0.0947330933431342, 'min': 0.6, '25%': 0.8, '50%': 0.9, '75%': 0.9, 'max': 0.9}, 'manichaean_people_elite_framing_salience': {'count': 13.0, 'mean': 0.8230769230769232, 'std': 0.10127393670836668, 'min': 0.7, '25%': 0.7, '50%': 0.8, '75%': 0.9, 'max': 1.0}, 'crisis_restoration_narrative_raw': {'count': 13.0, 'mean': 0.7615384615384617, 'std': 0.07679476477883049, 'min': 0.7, '25%': 0.7, '50%': 0.7, '75%': 0.8, 'max': 0.9}, 'crisis_restoration_narrative_salience': {'count': 13.0, 'mean': 0.7615384615384617, 'std': 0.09607689228305231, 'min': 0.6, '25%': 0.7, '50%': 0.8, '75%': 0.8, 'max': 0.9}, 'popular_sovereignty_claims_raw': {'count': 13.0, 'mean': 0.5923076923076923, 'std': 0.184668795692624, 'min': 0.2, '25%': 0.5, '50%': 0.6, '75%': 0.7, 'max': 0.9}, 'popular_sovereignty_claims_salience': {'count': 13.0, 'mean': 0.576923076923077, 'std': 0.2006400016357911, 'min': 0.2, '25%': 0.4, '50%': 0.6, '75%': 0.7, 'max': 0.9}, 'anti_pluralist_exclusion_raw': {'count': 13.0, 'mean': 0.8846153846153848, 'std': 0.055470019622522904, 'min': 0.8, '25%': 0.9, '50%': 0.9, '75%': 0.9, 'max': 1.0}, 'anti_pluralist_exclusion_salience': {'count': 13.0, 'mean': 0.8230769230769232, 'std': 0.08320502943378437, 'min': 0.7, '25%': 0.8, '50%': 0.8, '75%': 0.9, 'max': 1.0}, 'elite_conspiracy_systemic_corruption_raw': {'count': 13.0, 'mean': 0.676923076923077, 'std': 0.1640825308284734, 'min': 0.4, '25%': 0.6, '50%': 0.7, '75%': 0.8, 'max': 0.9}, 'elite_conspiracy_systemic_corruption_salience': {'count': 13.0, 'mean': 0.6076923076923078, 'std': 0.1800996874952798, 'min': 0.4, '25%': 0.5, '50%': 0.6, '75%': 0.7, 'max': 0.9}, 'authenticity_vs_political_class_raw': {'count': 13.0, 'mean': 0.7846153846153846, 'std': 0.0800640769025436, 'min': 0.6, '25%': 0.8, '50%': 0.8, '75%': 0.8, 'max': 0.9}, 'authenticity_vs_political_class_salience': {'count': 13.0, 'mean': 0.7307692307692307, 'std': 0.1250640861359713, 'min': 0.5, '25%': 0.6, '50%': 0.8, '75%': 0.8, 'max': 0.9}, 'homogeneous_people_construction_raw': {'count': 10.0, 'mean': 0.78, 'std': 0.1549193338482967, 'min': 0.5, '25%': 0.65, '50%': 0.8500000000000001, '75%': 0.9, 'max': 0.9}, 'homogeneous_people_construction_salience': {'count': 10.0, 'mean': 0.69, 'std': 0.15951314818673867, 'min': 0.4, '25%': 0.55, '50%': 0.8, '75%': 0.8, 'max': 0.8}, 'nationalist_exclusion_raw': {'count': 13.0, 'mean': 0.7076923076923076, 'std': 0.2177978361598146, 'min': 0.1, '25%': 0.7, '50%': 0.8, '75%': 0.8, 'max': 0.9}, 'nationalist_exclusion_salience': {'count': 13.0, 'mean': 0.6461538461538462, 'std': 0.18980421761815786, 'min': 0.1, '25%': 0.6, '50%': 0.7, '75%': 0.7, 'max': 0.9}, 'economic_populist_appeals_raw': {'count': 13.0, 'mean': 0.5076923076923077, 'std': 0.2564551242795331, 'min': 0.0, '25%': 0.3, '50%': 0.6, '75%': 0.7, 'max': 0.8}, 'economic_populist_appeals_salience': {'count': 13.0, 'mean': 0.423076923076923, 'std': 0.227866357593825, 'min': 0.1, '25%': 0.2, '50%': 0.5, '75%': 0.6, 'max': 0.8}}, 'hypothesis_checks': {'H1': {'hypothesis': "H1: Bolsonaro's Salience-Weighted Overall Populism Index will average >= 0.5", 'mean_score': np.float64(0.7696618071920331), 'is_confirmed': np.True_}, 'H5': {'hypothesis': 'H5: Anti-Pluralist Exclusion and Crisis-Restoration dimensions will show highest salience scores (> 0.7)', 'average_salience_scores': {'manichaean_people_elite_framing_salience': 0.8230769230769232, 'anti_pluralist_exclusion_salience': 0.8230769230769232, 'crisis_restoration_narrative_salience': 0.7615384615384617, 'authenticity_vs_political_class_salience': 0.7307692307692307, 'homogeneous_people_construction_salience': 0.69, 'nationalist_exclusion_salience': 0.6461538461538462, 'elite_conspiracy_systemic_corruption_salience': 0.6076923076923078, 'popular_sovereignty_claims_salience': 0.576923076923077, 'economic_populist_appeals_salience': 0.423076923076923}, 'top_2_dimensions': ['manichaean_people_elite_framing_salience', 'anti_pluralist_exclusion_salience'], 'is_confirmed': np.True_}}}, 'perform_dimensional_anova': {'anova_results_by_dimension': {'manichaean_people_elite_framing': {'f_statistic': np.float64(2.9160839160839163), 'p_value': np.float64(0.10924625415581611), 'eta_squared': np.float64(0.7446428571428572), 'is_significant': np.False_}, 'crisis_restoration_narrative': {'f_statistic': np.float64(1.0712945590994372), 'p_value': np.float64(0.4677564564876915), 'eta_squared': np.float64(0.5172101449275364), 'is_significant': np.False_}, 'popular_sovereignty_claims': {'f_statistic': np.float64(2.897435897435897), 'p_value': np.float64(0.11057572080181782), 'eta_squared': np.float64(0.743421052631579), 'is_significant': np.False_}, 'anti_pluralist_exclusion': {'f_statistic': np.float64(6.384615384615382), 'p_value': np.float64(0.020061431843557496), 'eta_squared': np.float64(0.8645833333333333), 'is_significant': np.True_, 'tukey_hsd_posthoc': [{'A': 'campaign_interruption', 'B': 'early_campaign', 'mean(A)': 0.8, 'mean(B)': 0.8, 'diff': 0.0, 'se': 0.04082482904638629, 'T': 0.0, 'p-tukey': 1.0, 'hedges': array([nan])}, {'A': 'campaign_interruption', 'B': 'final_campaign', 'mean(A)': 0.8, 'mean(B)': 0.8, 'diff': 0.0, 'se': 0.04082482904638629, 'T': 0.0, 'p-tukey': 1.0, 'hedges': array([nan])}, {'A': 'campaign_interruption', 'B': 'late_campaign', 'mean(A)': 0.8, 'mean(B)': 0.9, 'diff': -0.09999999999999998, 'se': 0.04082482904638629, 'T': -2.4494897427831783, 'p-tukey': 0.31545894622855564, 'hedges': array([nan])}, {'A': 'campaign_interruption', 'B': 'mid_campaign', 'mean(A)': 0.8, 'mean(B)': 0.9, 'diff': -0.09999999999999998, 'se': 0.03227486121839514, 'T': -3.098386676965933, 'p-tukey': 0.15663617153522313, 'hedges': nan}, {'A': 'campaign_interruption', 'B': 'post_first_round', 'mean(A)': 0.8, 'mean(B)': 0.9, 'diff': -0.09999999999999998, 'se': 0.033333333333333326, 'T': -3.0, 'p-tukey': 0.17440632521022925, 'hedges': nan}, {'A': 'campaign_interruption', 'B': 'pre_second_round', 'mean(A)': 0.8, 'mean(B)': 0.95, 'diff': -0.1499999999999999, 'se': 0.03535533905932737, 'T': -4.242640687119284, 'p-tukey': 0.046354709121357196, 'hedges': nan}, {'A': 'early_campaign', 'B': 'final_campaign', 'mean(A)': 0.8, 'mean(B)': 0.8, 'diff': 0.0, 'se': 0.04082482904638629, 'T': 0.0, 'p-tukey': 1.0, 'hedges': array([nan])}, {'A': 'early_campaign', 'B': 'late_campaign', 'mean(A)': 0.8, 'mean(B)': 0.9, 'diff': -0.09999999999999998, 'se': 0.04082482904638629, 'T': -2.4494897427831783, 'p-tukey': 0.31545894622855564, 'hedges': array([nan])}, {'A': 'early_campaign', 'B': 'mid_campaign', 'mean(A)': 0.8, 'mean(B)': 0.9, 'diff': -0.09999999999999998, 'se': 0.03227486121839514, 'T': -3.098386676965933, 'p-tukey': 0.15663617153522313, 'hedges': nan}, {'A': 'early_campaign', 'B': 'post_first_round', 'mean(A)': 0.8, 'mean(B)': 0.9, 'diff': -0.09999999999999998, 'se': 0.033333333333333326, 'T': -3.0, 'p-tukey': 0.17440632521022925, 'hedges': nan}, {'A': 'early_campaign', 'B': 'pre_second_round', 'mean(A)': 0.8, 'mean(B)': 0.95, 'diff': -0.1499999999999999, 'se': 0.03535533905932737, 'T': -4.242640687119284, 'p-tukey': 0.046354709121357196, 'hedges': nan}, {'A': 'final_campaign', 'B': 'late_campaign', 'mean(A)': 0.8, 'mean(B)': 0.9, 'diff': -0.09999999999999998, 'se': 0.04082482904638629, 'T': -2.4494897427831783, 'p-tukey': 0.31545894622855564, 'hedges': array([nan])}, {'A': 'final_campaign', 'B': 'mid_campaign', 'mean(A)': 0.8, 'mean(B)': 0.9, 'diff': -0.09999999999999998, 'se': 0.03227486121839514, 'T': -3.098386676965933, 'p-tukey': 0.15663617153522313, 'hedges': nan}, {'A': 'final_campaign', 'B': 'post_first_round', 'mean(A)': 0.8, 'mean(B)': 0.9, 'diff': -0.09999999999999998, 'se': 0.033333333333333326, 'T': -3.0, 'p-tukey': 0.17440632521022925, 'hedges': nan}, {'A': 'final_campaign', 'B': 'pre_second_round', 'mean(A)': 0.8, 'mean(B)': 0.95, 'diff': -0.1499999999999999, 'se': 0.03535533905932737, 'T': -4.242640687119284, 'p-tukey': 0.046354709121357196, 'hedges': nan}, {'A': 'late_campaign', 'B': 'mid_campaign', 'mean(A)': 0.9, 'mean(B)': 0.9, 'diff': 0.0, 'se': 0.03227486121839514, 'T': 0.0, 'p-tukey': 1.0, 'hedges': nan}, {'A': 'late_campaign', 'B': 'post_first_round', 'mean(A)': 0.9, 'mean(B)': 0.9, 'diff': 0.0, 'se': 0.033333333333333326, 'T': 0.0, 'p-tukey': 1.0, 'hedges': nan}, {'A': 'late_campaign', 'B': 'pre_second_round', 'mean(A)': 0.9, 'mean(B)': 0.95, 'diff': -0.04999999999999993, 'se': 0.03535533905932737, 'T': -1.4142135623730934, 'p-tukey': 0.7802248643430605, 'hedges': nan}, {'A': 'mid_campaign', 'B': 'post_first_round', 'mean(A)': 0.9, 'mean(B)': 0.9, 'diff': 0.0, 'se': 0.022047927592204915, 'T': 0.0, 'p-tukey': 1.0, 'hedges': nan}, {'A': 'mid_campaign', 'B': 'pre_second_round', 'mean(A)': 0.9, 'mean(B)': 0.95, 'diff': -0.04999999999999993, 'se': 0.024999999999999994, 'T': -1.9999999999999978, 'p-tukey': 0.49349173119864875, 'hedges': -1.1313708498984747}, {'A': 'post_first_round', 'B': 'pre_second_round', 'mean(A)': 0.9, 'mean(B)': 0.95, 'diff': -0.04999999999999993, 'se': 0.02635231383473649, 'T': -1.8973665961010253, 'p-tukey': 0.5416559037352993, 'hedges': -0.8907235428302457}]}, 'elite_conspiracy_systemic_corruption': {'f_statistic': np.float64(1.095634095634096), 'p_value': np.float64(0.457276622807895), 'eta_squared': np.float64(0.5228174603174605), 'is_significant': np.False_}, 'authenticity_vs_political_class': {'f_statistic': np.float64(0.9639934533551556), 'p_value': np.float64(0.5171836491151884), 'eta_squared': np.float64(0.4908333333333334), 'is_significant': np.False_}, 'homogeneous_people_construction': {'f_statistic': np.float64(1.047872340425532), 'p_value': np.float64(0.4670715959555242), 'eta_squared': np.float64(0.4560185185185185), 'is_significant': np.False_}, 'nationalist_exclusion': {'f_statistic': np.float64(0.2602895259721827), 'p_value': np.float64(0.9369408252316753), 'eta_squared': np.float64(0.20653153153153145), 'is_significant': np.False_}, 'economic_populist_appeals': {'f_statistic': np.float64(1.3795902589872437), 'p_value': np.float64(0.3529684019480487), 'eta_squared': np.float64(0.5797595841455491), 'is_significant': np.False_}}, 'hypothesis_H11_check': {'hypothesis': 'H11: At least 5 of 9 PDAF dimensions will show significant differences between campaign_stage groups (ANOVA)', 'significant_dimensions_count': 1, 'is_confirmed': False}}, 'perform_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-09-01T01:18:20.284491', 'sample_size': 13, 'alpha_level': 0.05, 'variables_analyzed': ['manichaean_people_elite_framing_raw', 'manichaean_people_elite_framing_salience', 'manichaean_people_elite_framing_confidence', 'crisis_restoration_narrative_raw', 'crisis_restoration_narrative_salience', 'crisis_restoration_narrative_confidence', 'popular_sovereignty_claims_raw', 'popular_sovereignty_claims_salience', 'popular_sovereignty_claims_confidence', 'anti_pluralist_exclusion_raw', 'anti_pluralist_exclusion_salience', 'anti_pluralist_exclusion_confidence', 'elite_conspiracy_systemic_corruption_raw', 'elite_conspiracy_systemic_corruption_salience', 'elite_conspiracy_systemic_corruption_confidence', 'authenticity_vs_political_class_raw', 'authenticity_vs_political_class_salience', 'authenticity_vs_political_class_confidence', 'homogenous_people_construction_raw', 'homogenous_people_construction_salience', 'homogenous_people_construction_confidence', 'nationalist_exclusion_raw', 'nationalist_exclusion_salience', 'nationalist_exclusion_confidence', 'economic_populist_appeals_raw', 'economic_populist_appeals_salience', 'economic_populist_appeals_confidence', 'homogeneous_people_construction_raw', 'homogeneous_people_construction_salience', 'homogeneous_people_construction_confidence']}}, 'perform_temporal_trend_analysis': {'analysis_type': 'Temporal Trend Linear Regression', 'dependent_variable': 'salience_weighted_overall_populism_index', 'independent_variable': 'time_ordinal', 'slope': np.float64(nan), 'intercept': np.float64(nan), 'p_value_slope': np.float64(nan), 'r_squared': np.float64(nan), 'n_observations': 13.0, 'hypothesis_H9_check': {'hypothesis': 'H9: Linear trend analysis will show significant positive slope for overall populism (β > 0, p < 0.05)', 'slope_beta': np.float64(nan), 'p_value': np.float64(nan), 'is_confirmed': np.False_}}, 'run_complete_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-09-01T01:18:20.546278', 'sample_size': 13, 'alpha_level': 0.05, 'variables_analyzed': ['manichaean_people_elite_framing_raw', 'manichaean_people_elite_framing_salience', 'manichaean_people_elite_framing_confidence', 'crisis_restoration_narrative_raw', 'crisis_restoration_narrative_salience', 'crisis_restoration_narrative_confidence', 'popular_sovereignty_claims_raw', 'popular_sovereignty_claims_salience', 'popular_sovereignty_claims_confidence', 'anti_pluralist_exclusion_raw', 'anti_pluralist_exclusion_salience', 'anti_pluralist_exclusion_confidence', 'elite_conspiracy_systemic_corruption_raw', 'elite_conspiracy_systemic_corruption_salience', 'elite_conspiracy_systemic_corruption_confidence', 'authenticity_vs_political_class_raw', 'authenticity_vs_political_class_salience', 'authenticity_vs_political_class_confidence', 'homogenous_people_construction_raw', 'homogenous_people_construction_salience', 'homogenous_people_construction_confidence', 'nationalist_exclusion_raw', 'nationalist_exclusion_salience', 'nationalist_exclusion_confidence', 'economic_populist_appeals_raw', 'economic_populist_appeals_salience', 'economic_populist_appeals_confidence', 'homogeneous_people_construction_raw', 'homogeneous_people_construction_salience', 'homogeneous_people_construction_confidence']}}, 'test_dimensional_reliability': {'reliability_analysis': {'core_dimensions': {'dimensions': ['manichaean_people_elite_framing_raw', 'crisis_restoration_narrative_raw', 'popular_sovereignty_claims_raw', 'anti_pluralist_exclusion_raw'], 'cronbach_alpha': np.float64(0.08602150537634401), 'confidence_interval_95': array([-1.129,  0.688])}, 'auxiliary_dimensions': {'dimensions': ['elite_conspiracy_systemic_corruption_raw', 'authenticity_vs_political_class_raw', 'homogeneous_people_construction_raw', 'nationalist_exclusion_raw', 'economic_populist_appeals_raw'], 'cronbach_alpha': np.float64(-1.0063099330559062), 'confidence_interval_95': array([-3.47 ,  0.303])}}, 'hypothesis_H12_check': {'hypothesis': "H12: Core populist dimensions will show higher internal consistency (Cronbach's α > 0.8) than auxiliary dimensions", 'core_dimensions_alpha': np.float64(0.08602150537634401), 'auxiliary_dimensions_alpha': np.float64(-1.0063099330559062), 'is_confirmed': np.False_}}, 'test_variance_homogeneity': {'hypothesis': "H10: Variance in populist scores will increase in final campaign month (Levene's test, October vs. earlier months)", 'variance_october': np.float64(0.002223707112745045), 'variance_earlier': np.float64(0.0002316553364124631), 'levene_statistic': np.float64(2.484897779376317), 'p_value': np.float64(0.15359388124261356), 'is_confirmed': np.False_}}, 'status': 'success_with_data', 'validation_passed': True}