{
  "status": "success",
  "functions_generated": 11,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 34886,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: bolsonaro_2018_populist_discourse_analysis\nDescription: Statistical analysis experiment\nGenerated: 2025-09-02T01:52:59.372737+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculate comprehensive descriptive statistics for all PDAF dimensions.\n    Includes mean, median, standard deviation, min, max, and quartiles for raw scores and salience.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data\n        **kwargs: Additional parameters\n        \n    Returns:\n        dict: Descriptive statistics for all dimensions\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # Extract all dimension columns\n        dimension_columns = [\n            'manichaean_people_elite_framing_raw', 'manichaean_people_elite_framing_salience',\n            'crisis_restoration_narrative_raw', 'crisis_restoration_narrative_salience',\n            'popular_sovereignty_claims_raw', 'popular_sovereignty_claims_salience',\n            'anti_pluralist_exclusion_raw', 'anti_pluralist_exclusion_salience',\n            'elite_conspiracy_systemic_corruption_raw', 'elite_conspiracy_systemic_corruption_salience',\n            'authenticity_vs_political_class_raw', 'authenticity_vs_political_class_salience',\n            'homogeneous_people_construction_raw', 'homogeneous_people_construction_salience',\n            'nationalist_exclusion_raw', 'nationalist_exclusion_salience',\n            'economic_populist_appeals_raw', 'economic_populist_appeals_salience'\n        ]\n        \n        results = {}\n        \n        for col in dimension_columns:\n            if col in data.columns:\n                stats = {\n                    'mean': data[col].mean(),\n                    'median': data[col].median(),\n                    'std': data[col].std(),\n                    'min': data[col].min(),\n                    'max': data[col].max(),\n                    'q1': data[col].quantile(0.25),\n                    'q3': data[col].quantile(0.75),\n                    'count': data[col].count()\n                }\n                results[col] = stats\n        \n        return results\n        \n    except Exception:\n        return None\n\ndef calculate_correlations(data, **kwargs):\n    \"\"\"\n    Calculate Pearson correlations between all PDAF dimension raw scores.\n    Tests H\u2081\u2082 and H\u2081\u2083 hypotheses about dimensional relationships.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data\n        **kwargs: Additional parameters\n        \n    Returns:\n        dict: Correlation matrix and significance values\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy import stats\n    \n    try:\n        # Extract raw score columns\n        raw_score_columns = [\n            'manichaean_people_elite_framing_raw',\n            'crisis_restoration_narrative_raw',\n            'popular_sovereignty_claims_raw',\n            'anti_pluralist_exclusion_raw',\n            'elite_conspiracy_systemic_corruption_raw',\n            'authenticity_vs_political_class_raw',\n            'homogeneous_people_construction_raw',\n            'nationalist_exclusion_raw',\n            'economic_populist_appeals_raw'\n        ]\n        \n        # Filter to only columns that exist in data\n        valid_columns = [col for col in raw_score_columns if col in data.columns]\n        \n        if len(valid_columns) < 2:\n            return None\n            \n        # Calculate correlation matrix\n        corr_matrix = data[valid_columns].corr()\n        \n        # Calculate p-values for correlations\n        p_values = pd.DataFrame(index=valid_columns, columns=valid_columns)\n        for i, col1 in enumerate(valid_columns):\n            for j, col2 in enumerate(valid_columns):\n                if i <= j:\n                    corr, p_value = stats.pearsonr(data[col1].dropna(), data[col2].dropna())\n                    p_values.loc[col1, col2] = p_value\n                    p_values.loc[col2, col1] = p_value\n        \n        return {\n            'correlation_matrix': corr_matrix.to_dict(),\n            'p_values': p_values.to_dict(),\n            'dimension_names': valid_columns\n        }\n        \n    except Exception:\n        return None\n\ndef calculate_salience_weighted_indices(data, **kwargs):\n    \"\"\"\n    Calculate salience-weighted populism indices as defined in PDAF v10.0.2.\n    Includes core, mechanisms, boundary, and overall populism indices.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data\n        **kwargs: Additional parameters\n        \n    Returns:\n        dict: Salience-weighted indices for each document\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        results = {}\n        \n        for idx, row in data.iterrows():\n            # Core Populism Index\n            core_numerator = (\n                row.get('manichaean_people_elite_framing_raw', 0) * row.get('manichaean_people_elite_framing_salience', 0) +\n                row.get('crisis_restoration_narrative_raw', 0) * row.get('crisis_restoration_narrative_salience', 0) +\n                row.get('popular_sovereignty_claims_raw', 0) * row.get('popular_sovereignty_claims_salience', 0) +\n                row.get('anti_pluralist_exclusion_raw', 0) * row.get('anti_pluralist_exclusion_salience', 0)\n            )\n            core_denominator = (\n                row.get('manichaean_people_elite_framing_salience', 0) +\n                row.get('crisis_restoration_narrative_salience', 0) +\n                row.get('popular_sovereignty_claims_salience', 0) +\n                row.get('anti_pluralist_exclusion_salience', 0) + 0.001\n            )\n            core_index = core_numerator / core_denominator\n            \n            # Mechanisms Index\n            mech_numerator = (\n                row.get('elite_conspiracy_systemic_corruption_raw', 0) * row.get('elite_conspiracy_systemic_corruption_salience', 0) +\n                row.get('authenticity_vs_political_class_raw', 0) * row.get('authenticity_vs_political_class_salience', 0) +\n                row.get('homogeneous_people_construction_raw', 0) * row.get('homogeneous_people_construction_salience', 0)\n            )\n            mech_denominator = (\n                row.get('elite_conspiracy_systemic_corruption_salience', 0) +\n                row.get('authenticity_vs_political_class_salience', 0) +\n                row.get('homogeneous_people_construction_salience', 0) + 0.001\n            )\n            mechanisms_index = mech_numerator / mech_denominator\n            \n            # Boundary Index\n            boundary_numerator = (\n                row.get('nationalist_exclusion_raw', 0) * row.get('nationalist_exclusion_salience', 0) +\n                row.get('economic_populist_appeals_raw', 0) * row.get('economic_populist_appeals_salience', 0)\n            )\n            boundary_denominator = (\n                row.get('nationalist_exclusion_salience', 0) +\n                row.get('economic_populist_appeals_salience', 0) + 0.001\n            )\n            boundary_index = boundary_numerator / boundary_denominator\n            \n            # Overall Index\n            overall_numerator = (\n                row.get('manichaean_people_elite_framing_raw', 0) * row.get('manichaean_people_elite_framing_salience', 0) +\n                row.get('crisis_restoration_narrative_raw', 0) * row.get('crisis_restoration_narrative_salience', 0) +\n                row.get('popular_sovereignty_claims_raw', 0) * row.get('popular_sovereignty_claims_salience', 0) +\n                row.get('anti_pluralist_exclusion_raw', 0) * row.get('anti_pluralist_exclusion_salience', 0) +\n                row.get('elite_conspiracy_systemic_corruption_raw', 0) * row.get('elite_conspiracy_systemic_corruption_salience', 0) +\n                row.get('authenticity_vs_political_class_raw', 0) * row.get('authenticity_vs_political_class_salience', 0) +\n                row.get('homogeneous_people_construction_raw', 0) * row.get('homogeneous_people_construction_salience', 0) +\n                row.get('nationalist_exclusion_raw', 0) * row.get('nationalist_exclusion_salience', 0) +\n                row.get('economic_populist_appeals_raw', 0) * row.get('economic_populist_appeals_salience', 0)\n            )\n            overall_denominator = (\n                row.get('manichaean_people_elite_framing_salience', 0) +\n                row.get('crisis_restoration_narrative_salience', 0) +\n                row.get('popular_sovereignty_claims_salience', 0) +\n                row.get('anti_pluralist_exclusion_salience', 0) +\n                row.get('elite_conspiracy_systemic_corruption_salience', 0) +\n                row.get('authenticity_vs_political_class_salience', 0) +\n                row.get('homogeneous_people_construction_salience', 0) +\n                row.get('nationalist_exclusion_salience', 0) +\n                row.get('economic_populist_appeals_salience', 0) + 0.001\n            )\n            overall_index = overall_numerator / overall_denominator\n            \n            results[row.get('document_name', f'doc_{idx}')] = {\n                'salience_weighted_core_populism_index': core_index,\n                'salience_weighted_populism_mechanisms_index': mechanisms_index,\n                'salience_weighted_boundary_distinctions_index': boundary_index,\n                'salience_weighted_overall_populism_index': overall_index\n            }\n        \n        return results\n        \n    except Exception:\n        return None\n\ndef calculate_strategic_tensions(data, **kwargs):\n    \"\"\"\n    Calculate populist strategic tension indices as defined in PDAF v10.0.2.\n    Includes democratic-authoritarian, internal-external, and crisis-elite attribution tensions.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data\n        **kwargs: Additional parameters\n        \n    Returns:\n        dict: Strategic tension indices for each document\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        results = {}\n        \n        for idx, row in data.iterrows():\n            # Democratic-Authoritarian Tension\n            da_tension = (\n                min(row.get('popular_sovereignty_claims_raw', 0), row.get('anti_pluralist_exclusion_raw', 0)) *\n                abs(row.get('popular_sovereignty_claims_salience', 0) - row.get('anti_pluralist_exclusion_salience', 0))\n            )\n            \n            # Internal-External Focus Tension\n            ie_tension = (\n                min(row.get('homogeneous_people_construction_raw', 0), row.get('nationalist_exclusion_raw', 0)) *\n                abs(row.get('homogeneous_people_construction_salience', 0) - row.get('nationalist_exclusion_salience', 0))\n            )\n            \n            # Crisis-Elite Attribution Tension\n            ce_tension = (\n                min(row.get('crisis_restoration_narrative_raw', 0), row.get('elite_conspiracy_systemic_corruption_raw', 0)) *\n                abs(row.get('crisis_restoration_narrative_salience', 0) - row.get('elite_conspiracy_systemic_corruption_salience', 0))\n            )\n            \n            # Populist Strategic Contradiction Index (PSCI)\n            psci = (da_tension + ie_tension + ce_tension) / 3\n            \n            results[row.get('document_name', f'doc_{idx}')] = {\n                'democratic_authoritarian_tension': da_tension,\n                'internal_external_focus_tension': ie_tension,\n                'crisis_elite_attribution_tension': ce_tension,\n                'populist_strategic_contradiction_index': psci\n            }\n        \n        return results\n        \n    except Exception:\n        return None\n\ndef group_by_campaign_stage(data, **kwargs):\n    \"\"\"\n    Group documents by campaign stage based on document names and known timeline.\n    Implements the campaign stage grouping structure from the experiment specification.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data\n        **kwargs: Additional parameters\n        \n    Returns:\n        dict: Documents grouped by campaign stage\n    \"\"\"\n    try:\n        # Map document names to campaign stages based on the experiment specification\n        campaign_mapping = {\n            '2018-07-22': 'early_campaign',\n            '2018-08-23': 'mid_campaign',\n            '2018-08-31': 'mid_campaign',\n            '2018-09-06': 'mid_campaign',\n            '2018-09-16': 'campaign_interruption',\n            '2018-09-30': 'late_campaign',\n            '2018-10-06': 'final_campaign',\n            '2018-10-07_morning': 'election_day',\n            '2018-10-07_evening': 'post_first_round',\n            '2018-10-16': 'post_first_round',\n            '2018-10-22': 'pre_second_round',\n            '2018-10-27': 'pre_second_round'\n        }\n        \n        grouped_data = {\n            'early_campaign': [],\n            'mid_campaign': [],\n            'campaign_interruption': [],\n            'late_campaign': [],\n            'final_campaign': [],\n            'election_day': [],\n            'post_first_round': [],\n            'pre_second_round': []\n        }\n        \n        for idx, row in data.iterrows():\n            doc_name = row.get('document_name', '')\n            for date_prefix, stage in campaign_mapping.items():\n                if date_prefix in doc_name:\n                    grouped_data[stage].append(row.to_dict())\n                    break\n        \n        return grouped_data\n        \n    except Exception:\n        return None\n\ndef group_by_pre_post_stabbing(data, **kwargs):\n    \"\"\"\n    Group documents by pre/post stabbing incident (September 6, 2018).\n    \n    Args:\n        data: pandas DataFrame containing the analysis data\n        **kwargs: Additional parameters\n        \n    Returns:\n        dict: Documents grouped by pre/post stabbing\n    \"\"\"\n    try:\n        pre_stabbing = []\n        post_stabbing = []\n        \n        for idx, row in data.iterrows():\n            doc_name = row.get('document_name', '')\n            \n            # Extract date from document name\n            date_str = doc_name.split('_')[0] if '_' in doc_name else doc_name[:10]\n            \n            try:\n                from datetime import datetime\n                doc_date = datetime.strptime(date_str, '%Y-%m-%d')\n                stabbing_date = datetime(2018, 9, 6)\n                \n                if doc_date < stabbing_date:\n                    pre_stabbing.append(row.to_dict())\n                else:\n                    post_stabbing.append(row.to_dict())\n            except:\n                # If date parsing fails, use string comparison\n                if '2018-07' in doc_name or '2018-08' in doc_name or '2018-09-06' in doc_name:\n                    pre_stabbing.append(row.to_dict())\n                else:\n                    post_stabbing.append(row.to_dict())\n        \n        return {\n            'pre_stabbing': pre_stabbing,\n            'post_stabbing': post_stabbing\n        }\n        \n    except Exception:\n        return None\n\ndef calculate_temporal_trend_analysis(data, **kwargs):\n    \"\"\"\n    Perform linear regression analysis on populism scores over time.\n    Tests H\u2089 hypothesis about temporal trends in populist discourse.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data\n        **kwargs: Additional parameters\n        \n    Returns:\n        dict: Regression results for overall populism index over time\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy import stats\n    from datetime import datetime\n    \n    try:\n        # Extract dates from document names and convert to ordinal\n        dates = []\n        overall_indices = []\n        \n        for idx, row in data.iterrows():\n            doc_name = row.get('document_name', '')\n            \n            # Extract date from document name\n            date_str = doc_name.split('_')[0] if '_' in doc_name else doc_name[:10]\n            \n            try:\n                doc_date = datetime.strptime(date_str, '%Y-%m-%d')\n                dates.append(doc_date.toordinal())\n                \n                # Calculate overall populism index for this row\n                overall_numerator = (\n                    row.get('manichaean_people_elite_framing_raw', 0) * row.get('manichaean_people_elite_framing_salience', 0) +\n                    row.get('crisis_restoration_narrative_raw', 0) * row.get('crisis_restoration_narrative_salience', 0) +\n                    row.get('popular_sovereignty_claims_raw', 0) * row.get('popular_sovereignty_claims_salience', 0) +\n                    row.get('anti_pluralist_exclusion_raw', 0) * row.get('anti_pluralist_exclusion_salience', 0) +\n                    row.get('elite_conspiracy_systemic_corruption_raw', 0) * row.get('elite_conspiracy_systemic_corruption_salience', 0) +\n                    row.get('authenticity_vs_political_class_raw', 0) * row.get('authenticity_vs_political_class_salience', 0) +\n                    row.get('homogeneous_people_construction_raw', 0) * row.get('homogeneous_people_construction_salience', 0) +\n                    row.get('nationalist_exclusion_raw', 0) * row.get('nationalist_exclusion_salience', 0) +\n                    row.get('economic_populist_appeals_raw', 0) * row.get('economic_populist_appeals_salience', 0)\n                )\n                overall_denominator = (\n                    row.get('manichaean_people_elite_framing_salience', 0) +\n                    row.get('crisis_restoration_narrative_salience', 0) +\n                    row.get('popular_sovereignty_claims_salience', 0) +\n                    row.get('anti_pluralist_exclusion_salience', 0) +\n                    row.get('elite_conspiracy_systemic_corruption_salience', 0) +\n                    row.get('authenticity_vs_political_class_salience', 0) +\n                    row.get('homogeneous_people_construction_salience', 0) +\n                    row.get('nationalist_exclusion_salience', 0) +\n                    row.get('economic_populist_appeals_salience', 0) + 0.001\n                )\n                overall_index = overall_numerator / overall_denominator\n                overall_indices.append(overall_index)\n                \n            except:\n                continue\n        \n        if len(dates) < 2:\n            return None\n        \n        # Perform linear regression\n        slope, intercept, r_value, p_value, std_err = stats.linregress(dates, overall_indices)\n        \n        return {\n            'slope': slope,\n            'intercept': intercept,\n            'r_squared': r_value**2,\n            'p_value': p_value,\n            'std_err': std_err,\n            'n': len(dates),\n            'trend_direction': 'increasing' if slope > 0 else 'decreasing',\n            'statistical_significance': p_value < 0.05\n        }\n        \n    except Exception:\n        return None\n\ndef perform_anova_across_dimensions(data, **kwargs):\n    \"\"\"\n    Perform one-way ANOVA across PDAF dimensions to test for significant differences.\n    Tests H\u2081\u2081 hypothesis about dimensional differences.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data\n        **kwargs: Additional parameters\n        \n    Returns:\n        dict: ANOVA results for each dimension across campaign stages\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy import stats\n    \n    try:\n        # First group by campaign stage\n        grouped_data = group_by_campaign_stage(data)\n        if not grouped_data:\n            return None\n        \n        results = {}\n        \n        # For each dimension, perform ANOVA across campaign stages\n        dimensions = [\n            'manichaean_people_elite_framing_raw',\n            'crisis_restoration_narrative_raw',\n            'popular_sovereignty_claims_raw',\n            'anti_pluralist_exclusion_raw',\n            'elite_conspiracy_systemic_corruption_raw',\n            'authenticity_vs_political_class_raw',\n            'homogeneous_people_construction_raw',\n            'nationalist_exclusion_raw',\n            'economic_populist_appeals_raw'\n        ]\n        \n        for dimension in dimensions:\n            if dimension not in data.columns:\n                continue\n                \n            # Extract scores by group\n            group_scores = []\n            group_labels = []\n            \n            for stage, documents in grouped_data.items():\n                if documents:  # Only include groups with data\n                    scores = [doc[dimension] for doc in documents if dimension in doc]\n                    if scores:\n                        group_scores.append(scores)\n                        group_labels.append(stage)\n            \n            if len(group_scores) >= 2:  # Need at least 2 groups for ANOVA\n                f_stat, p_value = stats.f_oneway(*group_scores)\n                \n                results[dimension] = {\n                    'f_statistic': f_stat,\n                    'p_value': p_value,\n                    'significant': p_value < 0.05,\n                    'groups_with_data': group_labels,\n                    'group_means': {label: np.mean(scores) for label, scores in zip(group_labels, group_scores)},\n                    'group_stds': {label: np.std(scores) for label, scores in zip(group_labels, group_scores)}\n                }\n        \n        return results\n        \n    except Exception:\n        return None\n\ndef calculate_reliability_analysis(data, **kwargs):\n    \"\"\"\n    Calculate Cronbach's alpha for internal consistency of core populist dimensions.\n    Tests H\u2081\u2082 hypothesis about dimensional consistency.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data\n        **kwargs: Additional parameters\n        \n    Returns:\n        dict: Reliability analysis results\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.preprocessing import scale\n    from scipy import stats\n    \n    try:\n        # Core populist dimensions for reliability analysis\n        core_dimensions = [\n            'manichaean_people_elite_framing_raw',\n            'crisis_restoration_narrative_raw',\n            'popular_sovereignty_claims_raw',\n            'anti_pluralist_exclusion_raw'\n        ]\n        \n        # Filter to only columns that exist\n        valid_dimensions = [dim for dim in core_dimensions if dim in data.columns]\n        \n        if len(valid_dimensions) < 2:\n            return None\n        \n        # Extract data for reliability analysis\n        reliability_data = data[valid_dimensions].dropna()\n        \n        if len(reliability_data) < 3:  # Need sufficient data\n            return None\n        \n        # Calculate Cronbach's alpha\n        scaled_data = scale(reliability_data)\n        item_variances = np.var(scaled_data, axis=0, ddof=1)\n        total_variance = np.var(np.sum(scaled_data, axis=1), ddof=1)\n        \n        k = len(valid_dimensions)\n        cronbach_alpha = (k / (k - 1)) * (1 - np.sum(item_variances) / total_variance)\n        \n        # Calculate inter-item correlations\n        corr_matrix = reliability_data.corr()\n        avg_inter_item_corr = corr_matrix.values[np.triu_indices_from(corr_matrix, k=1)].mean()\n        \n        return {\n            'cronbach_alpha': cronbach_alpha,\n            'number_of_items': k,\n            'sample_size': len(reliability_data),\n            'average_inter_item_correlation': avg_inter_item_corr,\n            'item_variance': item_variances.tolist(),\n            'total_variance': total_variance,\n            'dimensions_included': valid_dimensions\n        }\n        \n    except Exception:\n        return None\n\ndef perform_pairwise_comparisons(data, **kwargs):\n    \"\"\"\n    Perform pairwise t-tests between different campaign stages and groups.\n    Tests multiple hypotheses including H\u2082, H\u2084, H\u2086.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data\n        **kwargs: Additional parameters\n        \n    Returns:\n        dict: Results of pairwise comparisons with effect sizes\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy import stats\n    \n    try:\n        # First calculate overall populism indices\n        overall_indices = calculate_salience_weighted_indices(data)\n        if not overall_indices:\n            return None\n        \n        # Add overall indices to data\n        data_with_indices = data.copy()\n        for idx, row in data.iterrows():\n            doc_name = row.get('document_name', f'doc_{idx}')\n            if doc_name in overall_indices:\n                data_with_indices.at[idx, 'overall_populism_index'] = overall_indices[doc_name]['salience_weighted_overall_populism_index']\n        \n        # Group by campaign stage\n        grouped_data = group_by_campaign_stage(data_with_indices)\n        if not grouped_data:\n            return None\n        \n        results = {}\n        \n        # Compare early vs late campaign (H\u2082)\n        early_data = [doc.get('overall_populism_index', 0) for doc in grouped_data.get('early_campaign', [])]\n        late_data = [doc.get('overall_populism_index', 0) for doc in grouped_data.get('late_campaign', [])]\n        \n        if early_data and late_data:\n            t_stat, p_value = stats.ttest_ind(early_data, late_data)\n            cohens_d = (np.mean(late_data) - np.mean(early_data)) / np.sqrt((np.std(early_data)**2 + np.std(late_data)**2) / 2)\n            \n            results['early_vs_late_campaign'] = {\n                't_statistic': t_stat,\n                'p_value': p_value,\n                'cohens_d': cohens_d,\n                'mean_early': np.mean(early_data),\n                'mean_late': np.mean(late_data),\n                'n_early': len(early_data),\n                'n_late': len(late_data),\n                'significant': p_value < 0.05\n            }\n        \n        # Compare pre vs post stabbing (H\u2084) - using Manichaean dimension\n        pre_post_data = group_by_pre_post_stabbing(data_with_indices)\n        if pre_post_data:\n            pre_data = [doc.get('manichaean_people_elite_framing_raw', 0) for doc in pre_post_data.get('pre_stabbing', [])]\n            post_data = [doc.get('manichaean_people_elite_framing_raw', 0) for doc in pre_post_data.get('post_stabbing', [])]\n            \n            if pre_data and post_data:\n                t_stat, p_value = stats.ttest_ind(pre_data, post_data)\n                cohens_d = (np.mean(post_data) - np.mean(pre_data)) / np.sqrt((np.std(pre_data)**2 + np.std(post_data)**2) / 2)\n                \n                results['pre_vs_post_stabbing_manichaean'] = {\n                    't_statistic': t_stat,\n                    'p_value': p_value,\n                    'cohens_d': cohens_d,\n                    'mean_pre': np.mean(pre_data),\n                    'mean_post': np.mean(post_data),\n                    'n_pre': len(pre_data),\n                    'n_post': len(post_data),\n                    'significant': p_value < 0.05\n                }\n        \n        return results\n        \n    except Exception:\n        return None\n\ndef calculate_variance_analysis(data, **kwargs):\n    \"\"\"\n    Analyze variance patterns across different campaign phases.\n    Tests H\u2081\u2080 hypothesis about variance changes in final campaign month.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data\n        **kwargs: Additional parameters\n        \n    Returns:\n        dict: Variance analysis results\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy import stats\n    \n    try:\n        # First calculate overall populism indices\n        overall_indices = calculate_salience_weighted_indices(data)\n        if not overall_indices:\n            return None\n        \n        # Add overall indices to data\n        data_with_indices = data.copy()\n        for idx, row in data.iterrows():\n            doc_name = row.get('document_name', f'doc_{idx}')\n            if doc_name in overall_indices:\n                data_with_indices.at[idx, 'overall_populism_index'] = overall_indices[doc_name]['salience_weighted_overall_populism_index']\n        \n        # Group by campaign stage\n        grouped_data = group_by_campaign_stage(data_with_indices)\n        if not grouped_data:\n            return None\n        \n        # Extract October data (final campaign month)\n        october_data = []\n        other_data = []\n        \n        for stage, documents in grouped_data.items():\n            for doc in documents:\n                populism_index = doc.get('overall_populism_index', 0)\n                doc_name = doc.get('document_name', '')\n                \n                if '2018-10' in doc_name:\n                    october_data.append(populism_index)\n                elif any(month in doc_name for month in ['2018-07', '2018-08', '2018-09']):\n                    other_data.append(populism_index)\n        \n        if len(october_data) >= 2 and len(other_data) >= 2:\n            # Levene's test for equal variances\n            levene_stat, levene_p = stats.levene(october_data, other_data)\n            \n            # F-test for variance ratio\n            october_var = np.var(october_data, ddof=1)\n            other_var = np.var(other_data, ddof=1)\n            f_stat = max(october_var, other_var) / min(october_var, other_var)\n            f_p_value = stats.f.cdf(f_stat, len(october_data)-1, len(other_data)-1)\n            \n            return {\n                'levene_statistic': levene_stat,\n                'levene_p_value': levene_p,\n                'f_statistic': f_stat,\n                'f_p_value': f_p_value,\n                'october_variance': october_var,\n                'other_variance': other_var,\n                'variance_ratio': october_var / other_var,\n                'n_october': len(october_data),\n                'n_other': len(other_data),\n                'significant_variance_difference': levene_p < 0.05\n            }\n        \n        return None\n        \n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}