{
  "status": "success",
  "functions_generated": 9,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 29123,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: bolsonaro_2018_populist_discourse_analysis\nDescription: Statistical analysis experiment\nGenerated: 2025-09-02T02:13:22.983222+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef _preprocess_data(data):\n    \"\"\"\n    Internal helper function to preprocess the raw analysis data.\n    It calculates derived metrics and adds grouping variables based on document names.\n    This function is not intended to be called directly by the user.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if data is None or data.empty:\n        return pd.DataFrame()\n\n    df = data.copy()\n\n    # --- 1. Define dimension names and check for required columns ---\n    dimensions = [\n        \"manichaean_people_elite_framing\", \"crisis_restoration_narrative\",\n        \"popular_sovereignty_claims\", \"anti_pluralist_exclusion\",\n        \"elite_conspiracy_systemic_corruption\", \"authenticity_vs_political_class\",\n        \"homogeneous_people_construction\", \"nationalist_exclusion\",\n        \"economic_populist_appeals\"\n    ]\n    \n    required_cols = [f\"{dim}_{suffix}\" for dim in dimensions for suffix in ['raw', 'salience']]\n    if not all(col in df.columns for col in required_cols):\n        # Silently return an empty frame if data is malformed\n        return pd.DataFrame()\n\n    # --- 2. Calculate Derived Metrics from the Framework ---\n    # Helper function for safe division\n    def safe_div(numerator, denominator):\n        return numerator / (denominator + 0.001)\n\n    # Rename columns for easier formula application\n    score_cols = {f\"{dim}_raw\": f\"{dim}_raw_score\" for dim in dimensions}\n    salience_cols = {f\"{dim}_salience\": f\"{dim}_salience\" for dim in dimensions}\n    df.rename(columns={**score_cols, **salience_cols}, inplace=True)\n\n    # Tension Metrics\n    df['democratic_authoritarian_tension'] = (\n        np.minimum(df['popular_sovereignty_claims_raw_score'], df['anti_pluralist_exclusion_raw_score']) *\n        abs(df['popular_sovereignty_claims_salience'] - df['anti_pluralist_exclusion_salience'])\n    )\n    df['internal_external_focus_tension'] = (\n        np.minimum(df['homogeneous_people_construction_raw_score'], df['nationalist_exclusion_raw_score']) *\n        abs(df['homogeneous_people_construction_salience'] - df['nationalist_exclusion_salience'])\n    )\n    df['crisis_elite_attribution_tension'] = (\n        np.minimum(df['crisis_restoration_narrative_raw_score'], df['elite_conspiracy_systemic_corruption_raw_score']) *\n        abs(df['crisis_restoration_narrative_salience'] - df['elite_conspiracy_systemic_corruption_salience'])\n    )\n\n    # Populist Strategic Contradiction Index (PSCI)\n    df['populist_strategic_contradiction_index'] = (\n        df['democratic_authoritarian_tension'] +\n        df['internal_external_focus_tension'] +\n        df['crisis_elite_attribution_tension']\n    ) / 3\n\n    # Salience-Weighted Indices\n    core_dims = [\"manichaean_people_elite_framing\", \"crisis_restoration_narrative\", \"popular_sovereignty_claims\", \"anti_pluralist_exclusion\"]\n    mech_dims = [\"elite_conspiracy_systemic_corruption\", \"authenticity_vs_political_class\", \"homogeneous_people_construction\"]\n    bound_dims = [\"nationalist_exclusion\", \"economic_populist_appeals\"]\n\n    df['salience_weighted_core_populism_index'] = safe_div(\n        sum(df[f'{dim}_raw_score'] * df[f'{dim}_salience'] for dim in core_dims),\n        sum(df[f'{dim}_salience'] for dim in core_dims)\n    )\n    df['salience_weighted_populism_mechanisms_index'] = safe_div(\n        sum(df[f'{dim}_raw_score'] * df[f'{dim}_salience'] for dim in mech_dims),\n        sum(df[f'{dim}_salience'] for dim in mech_dims)\n    )\n    df['salience_weighted_boundary_distinctions_index'] = safe_div(\n        sum(df[f'{dim}_raw_score'] * df[f'{dim}_salience'] for dim in bound_dims),\n        sum(df[f'{dim}_salience'] for dim in bound_dims)\n    )\n    df['salience_weighted_overall_populism_index'] = safe_div(\n        sum(df[f'{dim}_raw_score'] * df[f'{dim}_salience'] for dim in dimensions),\n        sum(df[f'{dim}_salience'] for dim in dimensions)\n    )\n    \n    # Revert column names to original format for consistency\n    df.rename(columns={v: k for k, v in {**score_cols, **salience_cols}.items()}, inplace=True)\n\n    # --- 3. Add Grouping Variables based on document_name/date ---\n    if 'document_name' not in df.columns:\n        return df # Cannot proceed with grouping\n\n    df['date'] = pd.to_datetime(df['document_name'].str.slice(0, 10), errors='coerce')\n    df.dropna(subset=['date'], inplace=True)\n\n    # Campaign Stage Groups\n    def map_campaign_stage(date):\n        if date <= pd.Timestamp('2018-07-22'): return \"early_campaign\"\n        if date <= pd.Timestamp('2018-09-06'): return \"mid_campaign\"\n        if date <= pd.Timestamp('2018-09-16'): return \"campaign_interruption\"\n        if date <= pd.Timestamp('2018-09-30'): return \"late_campaign\"\n        if date <= pd.Timestamp('2018-10-06'): return \"final_campaign\"\n        if date <= pd.Timestamp('2018-10-07'): return \"election_day\"\n        if date <= pd.Timestamp('2018-10-16'): return \"post_first_round\"\n        return \"pre_second_round\"\n    df['campaign_stage'] = df['date'].apply(map_campaign_stage)\n\n    # Pre/Post Stabbing Groups\n    stabbing_date = pd.Timestamp('2018-09-06')\n    df['pre_post_stabbing'] = np.where(df['date'] < stabbing_date, 'pre_stabbing', 'post_stabbing')\n\n    # Electoral Proximity Groups\n    first_round_date = pd.Timestamp('2018-10-07')\n    def map_electoral_proximity(date):\n        days_to_election = (first_round_date - date).days\n        if days_to_election > 30: return \"distant\"\n        if 7 <= days_to_election <= 30: return \"approaching\"\n        if 0 <= days_to_election < 7: return \"imminent\"\n        if date < pd.Timestamp('2018-10-22'): return \"inter_round\"\n        return \"final_push\"\n    df['electoral_proximity'] = df['date'].apply(map_electoral_proximity)\n\n    # Audience Type Groups (inferred from spec, may be incomplete)\n    # This is a best-effort mapping based on typical speech types and dates.\n    def map_audience(doc_name):\n        doc_name_lower = doc_name.lower()\n        if 'business' in doc_name_lower or 'empresarios' in doc_name_lower:\n            return \"business_leaders\"\n        if 'live' in doc_name_lower:\n            return \"online_supporters\"\n        if 'paulista' in doc_name_lower or 'rall' in doc_name_lower or 'aracatuba' in doc_name_lower or 'porto_velho' in doc_name_lower:\n            return \"mass_public\"\n        return \"national_audience\" # Default\n    df['audience'] = df['document_name'].apply(map_audience)\n\n    return df\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates and returns descriptive statistics for all PDAF dimensions and derived metrics.\n\n    This function computes the mean, standard deviation, min, and max for each numerical column.\n    It can also provide statistics grouped by a specified categorical variable from the experiment design.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing analysis scores.\n        **kwargs:\n            group_by (str, optional): A column name to group the statistics by (e.g., 'campaign_stage', 'audience').\n\n    Returns:\n        dict: A dictionary containing pandas DataFrames for overall and, if requested, grouped descriptive statistics.\n              Returns None if the data is invalid or processing fails.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        processed_data = _preprocess_data(data)\n        if processed_data.empty:\n            return None\n\n        # Select only numeric columns for description\n        numeric_cols = processed_data.select_dtypes(include=np.number).columns.tolist()\n        \n        results = {\n            \"overall_descriptives\": processed_data[numeric_cols].describe().transpose()\n        }\n\n        group_by_col = kwargs.get('group_by')\n        if group_by_col and group_by_col in processed_data.columns:\n            # Ensure there are at least 2 groups to make grouping meaningful\n            if processed_data[group_by_col].nunique() > 1:\n                grouped_descriptives = processed_data.groupby(group_by_col)[numeric_cols].describe()\n                results[f\"grouped_by_{group_by_col}\"] = grouped_descriptives\n            else:\n                results[f\"grouped_by_{group_by_col}\"] = \"Not enough groups to provide grouped statistics.\"\n\n        return results\n\n    except Exception as e:\n        # In a production environment, you might log the error `e`\n        return None\n\ndef test_populism_intensification_trend(data, **kwargs):\n    \"\"\"\n    Performs a linear regression to test for a temporal trend in populist intensity.\n\n    This analysis addresses hypotheses H2 and H9 by modeling the 'salience_weighted_overall_populism_index'\n    as a function of time (days since the first speech). A significant positive slope would support\n    the populist intensification hypothesis.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing analysis scores.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A dictionary containing the regression model summary, including coefficients,\n              R-squared, and p-values. Returns None if the analysis cannot be performed.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import statsmodels.api as sm\n\n    try:\n        processed_data = _preprocess_data(data)\n        if processed_data.empty or 'date' not in processed_data.columns or processed_data.shape[0] < 2:\n            return None\n\n        df = processed_data.sort_values('date').copy()\n        \n        # Create a numeric time variable (days from start)\n        df['days_from_start'] = (df['date'] - df['date'].min()).dt.days\n\n        # Define dependent and independent variables\n        Y = df['salience_weighted_overall_populism_index']\n        X = df['days_from_start']\n        X = sm.add_constant(X) # Adds an intercept\n\n        # Fit the Ordinary Least Squares model\n        model = sm.OLS(Y, X).fit()\n\n        # Format results\n        results = {\n            'model': 'Linear Regression (OLS)',\n            'dependent_variable': 'salience_weighted_overall_populism_index',\n            'independent_variable': 'days_from_start',\n            'n_observations': int(model.nobs),\n            'r_squared': model.rsquared,\n            'adj_r_squared': model.rsquared_adj,\n            'f_statistic': model.fvalue,\n            'prob_f_statistic': model.f_pvalue,\n            'coefficients': model.params.to_dict(),\n            'p_values': model.pvalues.to_dict(),\n            'summary_table': str(model.summary())\n        }\n        return results\n\n    except Exception as e:\n        return None\n\ndef compare_campaign_phases_anova(data, **kwargs):\n    \"\"\"\n    Performs a one-way ANOVA to compare populist scores across different campaign stages.\n\n    This function addresses hypothesis H11. It tests if there are significant differences in the\n    'salience_weighted_overall_populism_index' across the 'campaign_stage' groups. If the ANOVA\n    is significant, a Tukey HSD post-hoc test is performed to identify which specific groups differ.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing analysis scores.\n        **kwargs:\n            metric (str, optional): The metric to analyze. Defaults to 'salience_weighted_overall_populism_index'.\n\n    Returns:\n        dict: A dictionary with the ANOVA table, eta-squared effect size, and Tukey HSD results.\n              Returns None if data is insufficient for ANOVA.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import f_oneway\n    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n\n    try:\n        processed_data = _preprocess_data(data)\n        metric_to_test = kwargs.get('metric', 'salience_weighted_overall_populism_index')\n\n        if processed_data.empty or metric_to_test not in processed_data.columns or 'campaign_stage' not in processed_data.columns:\n            return None\n\n        df = processed_data.dropna(subset=[metric_to_test, 'campaign_stage'])\n        \n        groups = df.groupby('campaign_stage')[metric_to_test].apply(list)\n        \n        # ANOVA requires at least 2 groups with at least 2 samples each for a meaningful result\n        valid_groups = [g for g in groups if len(g) > 1]\n        if len(valid_groups) < 2:\n            return {\"message\": \"Insufficient data for ANOVA (need at least 2 groups with >1 sample).\"}\n\n        # Unpack groups for f_oneway\n        f_stat, p_value = f_oneway(*groups)\n\n        # Calculate Eta-squared effect size\n        ss_between = sum(len(g) * (np.mean(g) - df[metric_to_test].mean())**2 for g in groups)\n        ss_total = sum((x - df[metric_to_test].mean())**2 for x in df[metric_to_test])\n        eta_squared = ss_between / ss_total if ss_total > 0 else 0\n\n        results = {\n            'test': 'One-Way ANOVA',\n            'dependent_variable': metric_to_test,\n            'grouping_variable': 'campaign_stage',\n            'f_statistic': f_stat,\n            'p_value': p_value,\n            'eta_squared_effect_size': eta_squared,\n            'tukey_hsd_results': None\n        }\n\n        # If ANOVA is significant, perform Tukey HSD\n        if p_value < 0.05 and df['campaign_stage'].nunique() > 1:\n            tukey_results = pairwise_tukeyhsd(endog=df[metric_to_test], groups=df['campaign_stage'], alpha=0.05)\n            results['tukey_hsd_results'] = str(tukey_results)\n\n        return results\n\n    except Exception as e:\n        return None\n\ndef compare_crisis_impact_ttest(data, **kwargs):\n    \"\"\"\n    Compares populist rhetoric before and after the September 6 stabbing incident using t-tests.\n\n    This function addresses hypotheses H4 and H8. It performs independent samples t-tests on the\n    'manichaean_people_elite_framing_raw' and 'elite_conspiracy_systemic_corruption_raw' scores,\n    comparing the 'pre_stabbing' and 'post_stabbing' groups. Cohen's d is calculated as an effect size measure.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing analysis scores.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing the t-test results (t-statistic, p-value, Cohen's d) for each\n              tested dimension. Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import ttest_ind\n\n    def cohen_d(x, y):\n        nx, ny = len(x), len(y)\n        if nx < 2 or ny < 2: return 0\n        s = np.sqrt(((nx - 1) * np.std(x, ddof=1)**2 + (ny - 1) * np.std(y, ddof=1)**2) / (nx + ny - 2))\n        return (np.mean(x) - np.mean(y)) / s if s > 0 else 0\n\n    try:\n        processed_data = _preprocess_data(data)\n        if processed_data.empty or 'pre_post_stabbing' not in processed_data.columns:\n            return None\n\n        df = processed_data.dropna(subset=['pre_post_stabbing'])\n        \n        group1 = df[df['pre_post_stabbing'] == 'pre_stabbing']\n        group2 = df[df['pre_post_stabbing'] == 'post_stabbing']\n\n        if group1.empty or group2.empty:\n            return {\"message\": \"Insufficient data: both 'pre_stabbing' and 'post_stabbing' groups must be present.\"}\n\n        results = {}\n        dimensions_to_test = {\n            'manichaean_people_elite_framing_raw': 'H4',\n            'elite_conspiracy_systemic_corruption_raw': 'H8'\n        }\n\n        for dim, hypo in dimensions_to_test.items():\n            if dim not in df.columns: continue\n            \n            g1_scores = group1[dim].dropna()\n            g2_scores = group2[dim].dropna()\n\n            if len(g1_scores) < 2 or len(g2_scores) < 2:\n                results[dim] = \"Insufficient data for t-test.\"\n                continue\n\n            t_stat, p_value = ttest_ind(g2_scores, g1_scores, equal_var=False, nan_policy='omit') # Welch's t-test\n            \n            results[dim] = {\n                'hypothesis': hypo,\n                'group1_mean': g1_scores.mean(),\n                'group2_mean': g2_scores.mean(),\n                't_statistic': t_stat,\n                'p_value': p_value,\n                'cohens_d': cohen_d(g2_scores, g1_scores)\n            }\n        \n        return results if results else None\n\n    except Exception as e:\n        return None\n\ndef compare_audience_effects_ttest(data, **kwargs):\n    \"\"\"\n    Compares economic populist appeals between speeches to business leaders and mass rallies.\n\n    This function addresses hypothesis H6 by performing an independent samples t-test on the\n    'economic_populist_appeals_raw' score. It compares the 'business_leaders' audience group\n    against the 'mass_public' group.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing analysis scores.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary with the t-test results, or a message if data is insufficient.\n              Returns None on failure.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import ttest_ind\n\n    def cohen_d(x, y):\n        nx, ny = len(x), len(y)\n        if nx < 2 or ny < 2: return 0\n        s = np.sqrt(((nx - 1) * np.std(x, ddof=1)**2 + (ny - 1) * np.std(y, ddof=1)**2) / (nx + ny - 2))\n        return (np.mean(x) - np.mean(y)) / s if s > 0 else 0\n\n    try:\n        processed_data = _preprocess_data(data)\n        dim_to_test = 'economic_populist_appeals_raw'\n\n        if processed_data.empty or 'audience' not in processed_data.columns or dim_to_test not in processed_data.columns:\n            return None\n\n        df = processed_data.dropna(subset=['audience', dim_to_test])\n        \n        group1 = df[df['audience'] == 'business_leaders'][dim_to_test]\n        group2 = df[df['audience'] == 'mass_public'][dim_to_test]\n\n        if group1.empty or group2.empty or len(group1) < 2 or len(group2) < 2:\n            return {\"message\": \"Insufficient data for one or both audience groups ('business_leaders', 'mass_public').\"}\n\n        t_stat, p_value = ttest_ind(group1, group2, equal_var=False, nan_policy='omit')\n\n        return {\n            'hypothesis': 'H6',\n            'test': 'Independent Samples T-test',\n            'dimension': dim_to_test,\n            'group1': 'business_leaders',\n            'group2': 'mass_public',\n            'group1_mean': group1.mean(),\n            'group2_mean': group2.mean(),\n            't_statistic': t_stat,\n            'p_value': p_value,\n            'cohens_d': cohen_d(group1, group2)\n        }\n\n    except Exception as e:\n        return None\n\ndef analyze_discourse_interaction_correlation(data, **kwargs):\n    \"\"\"\n    Analyzes the interaction between populist dimensions using Pearson correlation.\n\n    This function addresses hypotheses H3 and H13. It computes a correlation matrix for key\n    populist dimensions, specifically focusing on the relationship between 'nationalist_exclusion'\n    and other dimensions, including 'anti_pluralist_exclusion'.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing analysis scores.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing the correlation matrix and p-value matrix as pandas DataFrames.\n              Returns None on failure.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import pearsonr\n\n    try:\n        processed_data = _preprocess_data(data)\n        if processed_data.empty or processed_data.shape[0] < 2:\n            return None\n\n        # Select raw scores of all dimensions for correlation\n        dims_to_correlate = [col for col in processed_data.columns if col.endswith('_raw')]\n        \n        if not dims_to_correlate:\n            return None\n            \n        df_corr = processed_data[dims_to_correlate].copy()\n        \n        # Calculate correlation and p-value matrices\n        corr_matrix = df_corr.corr(method='pearson')\n        \n        pval_matrix = df_corr.corr(method=lambda x, y: pearsonr(x, y)[1])\n\n        return {\n            \"correlation_matrix\": corr_matrix.to_dict(),\n            \"p_value_matrix\": pval_matrix.to_dict()\n        }\n\n    except Exception as e:\n        return None\n\ndef analyze_dimensional_consistency(data, **kwargs):\n    \"\"\"\n    Calculates Cronbach's alpha to measure the internal consistency of core populist dimensions.\n\n    This function addresses hypothesis H12, which posits that the four core populist dimensions\n    (People-Elite, Crisis-Restoration, Popular Sovereignty, Anti-Pluralist) will show high\n    internal consistency (alpha > 0.8).\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing analysis scores.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing the Cronbach's alpha score for the core dimensions.\n              Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import pingouin as pg\n\n    try:\n        processed_data = _preprocess_data(data)\n        if processed_data.empty:\n            return None\n\n        core_dimensions_raw = [\n            \"manichaean_people_elite_framing_raw\",\n            \"crisis_restoration_narrative_raw\",\n            \"popular_sovereignty_claims_raw\",\n            \"anti_pluralist_exclusion_raw\"\n        ]\n\n        if not all(col in processed_data.columns for col in core_dimensions_raw):\n            return {\"message\": \"One or more core dimension columns are missing.\"}\n            \n        df_core = processed_data[core_dimensions_raw].dropna()\n\n        if df_core.shape[0] < 2 or df_core.shape[1] < 2:\n            return {\"message\": \"Insufficient data to calculate Cronbach's alpha.\"}\n\n        # Calculate Cronbach's alpha\n        alpha_results = pg.cronbach_alpha(data=df_core)\n        \n        return {\n            'hypothesis': 'H12',\n            'dimensions_analyzed': core_dimensions_raw,\n            'cronbach_alpha': alpha_results[0],\n            'confidence_interval_95': list(alpha_results[1])\n        }\n\n    except Exception as e:\n        return None\n\ndef analyze_score_variance_over_time(data, **kwargs):\n    \"\"\"\n    Tests for homogeneity of variances in populist scores across campaign months.\n\n    This function addresses hypothesis H10, which predicts that the variance in populist scores\n    will increase in the final campaign month. It uses Levene's test to compare the variance of\n    'salience_weighted_overall_populism_index' between October and earlier months.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing analysis scores.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary with the Levene's test statistic and p-value.\n              Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import levene\n\n    try:\n        processed_data = _preprocess_data(data)\n        metric_to_test = 'salience_weighted_overall_populism_index'\n\n        if processed_data.empty or 'date' not in processed_data.columns or metric_to_test not in processed_data.columns:\n            return None\n\n        df = processed_data.dropna(subset=['date', metric_to_test])\n        \n        # Create a group for 'October' vs 'Earlier'\n        df['month_group'] = np.where(df['date'].dt.month == 10, 'October', 'Earlier')\n\n        group1 = df[df['month_group'] == 'October'][metric_to_test]\n        group2 = df[df['month_group'] == 'Earlier'][metric_to_test]\n\n        if group1.empty or group2.empty or len(group1) < 2 or len(group2) < 2:\n            return {\"message\": \"Insufficient data for both 'October' and 'Earlier' groups.\"}\n\n        # Perform Levene's test\n        stat, p_value = levene(group1, group2)\n\n        return {\n            'hypothesis': 'H10',\n            'test': \"Levene's Test for Homogeneity of Variances\",\n            'dependent_variable': metric_to_test,\n            'groups': ['October', 'Earlier'],\n            'group1_variance': group1.var(),\n            'group2_variance': group2.var(),\n            'levene_statistic': stat,\n            'p_value': p_value\n        }\n\n    except Exception as e:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}