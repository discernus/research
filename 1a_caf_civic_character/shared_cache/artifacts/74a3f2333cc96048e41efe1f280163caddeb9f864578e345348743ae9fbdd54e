{"stage_1_raw_data": {"analysis_plan": {"stage": "raw_data_collection", "experiment_summary": "This plan outlines the collection and validation of raw data from the LLM analysis phase for the speaker_character_pattern_analysis experiment. It focuses on capturing and verifying the integrity of the 10 dimensional scores, salience values, and confidence ratings as specified by the Character Assessment Framework v6.1, ensuring a complete and valid dataset for subsequent analysis.", "tasks": {"validate_dimensional_scores": {"tool": "calculate_descriptive_stats", "parameters": {"columns": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score"]}, "purpose": "To verify that all 10 raw dimensional scores have been collected for each document and fall within the expected 0.0-1.0 range. This provides a preliminary check on the distribution and validity of the core character assessment data."}, "validate_salience_scores": {"tool": "calculate_descriptive_stats", "parameters": {"columns": ["dignity_salience", "truth_salience", "justice_salience", "hope_salience", "pragmatism_salience", "tribalism_salience", "manipulation_salience", "resentment_salience", "fear_salience", "fantasy_salience"]}, "purpose": "To validate that salience scores, which are critical for Stage 2 tension calculations, are captured correctly for all dimensions and fall within the required 0.0-1.0 range."}, "validate_assessment_confidence": {"tool": "calculate_descriptive_stats", "parameters": {"columns": ["dignity_confidence", "truth_confidence", "justice_confidence", "hope_confidence", "pragmatism_confidence", "tribalism_confidence", "manipulation_confidence", "resentment_confidence", "fear_confidence", "fantasy_confidence"]}, "purpose": "To assess the LLM's confidence in its ratings across all dimensions. This helps identify any dimensions where the model consistently reports low confidence, flagging potential issues with evidence quality or framework interpretation."}, "summarize_data_completeness": {"tool": "create_summary_statistics", "parameters": {"metrics": ["dignity_score", "dignity_salience", "dignity_confidence", "truth_score", "truth_salience", "truth_confidence", "justice_score", "justice_salience", "justice_confidence", "hope_score", "hope_salience", "hope_confidence", "pragmatism_score", "pragmatism_salience", "pragmatism_confidence", "tribalism_score", "tribalism_salience", "tribalism_confidence", "manipulation_score", "manipulation_salience", "manipulation_confidence", "resentment_score", "resentment_salience", "resentment_confidence", "fear_score", "fear_salience", "fear_confidence", "fantasy_score", "fantasy_salience", "fantasy_confidence"], "summary_types": ["count", "missing"]}, "purpose": "To perform a final check for data completeness across all 30 expected raw data columns from the LLM analysis, ensuring no values are null or missing before proceeding to the statistical analysis stage."}}}, "results": {"validate_dimensional_scores": {"type": "descriptive_stats", "columns_analyzed": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score"], "results": {"dignity_score": {"count": 7, "mean": 0.7071428571428572, "std": 0.11700630833624397, "min": 0.6, "max": 0.9, "median": 0.7, "q25": 0.6, "q75": 0.775, "skewness": 0.6354172449685459, "kurtosis": -0.7777542533081299}, "truth_score": {"count": 8, "mean": 0.6125, "std": 0.15294723646688466, "min": 0.4, "max": 0.85, "median": 0.625, "q25": 0.55, "q75": 0.7, "skewness": -0.23457673001622417, "kurtosis": -0.27672047083503326}, "justice_score": {"count": 8, "mean": 0.73125, "std": 0.15797264681854625, "min": 0.5, "max": 0.9, "median": 0.775, "q25": 0.6499999999999999, "q75": 0.8250000000000001, "skewness": -0.70039978049317, "kurtosis": -0.8331514556085038}, "hope_score": {"count": 8, "mean": 0.6475, "std": 0.23614462880676687, "min": 0.2, "max": 0.9, "median": 0.7, "q25": 0.5, "q75": 0.8200000000000001, "skewness": -0.9189397658129868, "kurtosis": 0.45756744270958816}, "pragmatism_score": {"count": 8, "mean": 0.5249999999999999, "std": 0.12817398889233114, "min": 0.3, "max": 0.7, "median": 0.55, "q25": 0.475, "q75": 0.6, "skewness": -0.6105830850825585, "kurtosis": -0.021172022684313063}, "tribalism_score": {"count": 8, "mean": 0.4125, "std": 0.18850918886280923, "min": 0.1, "max": 0.7, "median": 0.4, "q25": 0.3, "q75": 0.525, "skewness": -0.06664297982574521, "kurtosis": -0.01725208959369784}, "manipulation_score": {"count": 8, "mean": 0.40625, "std": 0.2111828929760038, "min": 0.15, "max": 0.7, "median": 0.4, "q25": 0.2, "q75": 0.6, "skewness": 0.1116263407578763, "kurtosis": -1.763594224855487}, "resentment_score": {"count": 8, "mean": 0.39375000000000004, "std": 0.2043063176423368, "min": 0.05, "max": 0.7, "median": 0.35, "q25": 0.3, "q75": 0.525, "skewness": -0.09815396884072479, "kurtosis": 0.005367954473961767}, "fear_score": {"count": 8, "mean": 0.25625000000000003, "std": 0.1347948176197544, "min": 0.1, "max": 0.5, "median": 0.2, "q25": 0.1875, "q75": 0.325, "skewness": 0.9323479781707691, "kurtosis": -0.004124383485562433}, "fantasy_score": {"count": 8, "mean": 0.175, "std": 0.10350983390135313, "min": 0.0, "max": 0.3, "median": 0.2, "q25": 0.1, "q75": 0.225, "skewness": -0.3864367132317181, "kurtosis": -0.4479999999999986}}}, "validate_salience_scores": {"type": "descriptive_stats", "columns_analyzed": ["dignity_salience", "truth_salience", "justice_salience", "hope_salience", "pragmatism_salience", "tribalism_salience", "manipulation_salience", "resentment_salience", "fear_salience", "fantasy_salience"], "results": {"dignity_salience": {"count": 7, "mean": 0.7571428571428572, "std": 0.09322272357358044, "min": 0.7, "max": 0.95, "median": 0.7, "q25": 0.7, "q75": 0.775, "skewness": 1.8735510780128417, "kurtosis": 3.432351285419381}, "truth_salience": {"count": 8, "mean": 0.675, "std": 0.13887301496588275, "min": 0.4, "max": 0.8, "median": 0.7, "q25": 0.6, "q75": 0.8, "skewness": -1.1201280219470378, "kurtosis": 1.1061728395061703}, "justice_salience": {"count": 8, "mean": 0.7875, "std": 0.18850918886280923, "min": 0.5, "max": 0.95, "median": 0.875, "q25": 0.6875, "q75": 0.9125, "skewness": -1.0209704509304138, "kurtosis": -0.719779803540316}, "hope_salience": {"count": 8, "mean": 0.6625, "std": 0.24892626561752318, "min": 0.2, "max": 0.9, "median": 0.75, "q25": 0.55, "q75": 0.85, "skewness": -1.0870897317668808, "kurtosis": 0.14808195400676105}, "pragmatism_salience": {"count": 8, "mean": 0.51875, "std": 0.14623244705409455, "min": 0.3, "max": 0.75, "median": 0.55, "q25": 0.4, "q75": 0.6, "skewness": 0.009279703405700527, "kurtosis": -0.6426401558570607}, "tribalism_salience": {"count": 8, "mean": 0.42500000000000004, "std": 0.17525491637693283, "min": 0.1, "max": 0.7, "median": 0.45, "q25": 0.375, "q75": 0.5, "skewness": -0.5042488670228694, "kurtosis": 1.3568415359653878}, "manipulation_salience": {"count": 8, "mean": 0.4749999999999999, "std": 0.21380899352993948, "min": 0.15, "max": 0.7, "median": 0.55, "q25": 0.2875, "q75": 0.625, "skewness": -0.4750150979302847, "kurtosis": -1.5741455078125002}, "resentment_salience": {"count": 8, "mean": 0.41875, "std": 0.19628241315585487, "min": 0.05, "max": 0.7, "median": 0.4, "q25": 0.375, "q75": 0.525, "skewness": -0.5817875780138718, "kurtosis": 1.121535402252242}, "fear_salience": {"count": 8, "mean": 0.28125, "std": 0.09977653603356425, "min": 0.1, "max": 0.4, "median": 0.3, "q25": 0.2375, "q75": 0.325, "skewness": -0.6044898304046372, "kurtosis": 0.3646323071045048}, "fantasy_salience": {"count": 8, "mean": 0.20625, "std": 0.126596941962615, "min": 0.0, "max": 0.4, "median": 0.2, "q25": 0.1375, "q75": 0.3, "skewness": -0.1155158663790897, "kurtosis": -0.21386550383687286}}}, "validate_assessment_confidence": {"type": "descriptive_stats", "columns_analyzed": ["dignity_confidence", "truth_confidence", "justice_confidence", "hope_confidence", "pragmatism_confidence", "tribalism_confidence", "manipulation_confidence", "resentment_confidence", "fear_confidence", "fantasy_confidence"], "results": {"dignity_confidence": {"count": 7, "mean": 0.7928571428571428, "std": 0.10177004891982151, "min": 0.7, "max": 0.9, "median": 0.75, "q25": 0.7, "q75": 0.9, "skewness": 0.267675800488283, "kurtosis": -2.6951248513674186}, "truth_confidence": {"count": 8, "mean": 0.7687499999999999, "std": 0.11933596033288303, "min": 0.6, "max": 0.95, "median": 0.8, "q25": 0.7125, "q75": 0.8125, "skewness": -0.34017919805586205, "kurtosis": -0.2550132172443309}, "justice_confidence": {"count": 8, "mean": 0.8187499999999999, "std": 0.14126343172547218, "min": 0.5, "max": 0.95, "median": 0.85, "q25": 0.8, "q75": 0.875, "skewness": -1.8774322889140391, "kurtosis": 4.501490923832252}, "hope_confidence": {"count": 8, "mean": 0.765, "std": 0.1461897006338975, "min": 0.6, "max": 0.95, "median": 0.75, "q25": 0.6375, "q75": 0.905, "skewness": 0.10845917943952225, "kurtosis": -2.079485580085219}, "pragmatism_confidence": {"count": 8, "mean": 0.65625, "std": 0.11783008347374015, "min": 0.5, "max": 0.85, "median": 0.7, "q25": 0.575, "q75": 0.7, "skewness": -0.023195425908955142, "kurtosis": -0.22939382347163573}, "tribalism_confidence": {"count": 8, "mean": 0.6499999999999999, "std": 0.13093073414159542, "min": 0.4, "max": 0.8, "median": 0.6499999999999999, "q25": 0.6, "q75": 0.725, "skewness": -0.7637626158259697, "kurtosis": 0.8749999999999947}, "manipulation_confidence": {"count": 8, "mean": 0.6499999999999999, "std": 0.13627702877384937, "min": 0.5, "max": 0.85, "median": 0.675, "q25": 0.5, "q75": 0.75, "skewness": 0.0, "kurtosis": -1.5946745562130182}, "resentment_confidence": {"count": 8, "mean": 0.64375, "std": 0.14252192813739223, "min": 0.4, "max": 0.8, "median": 0.7, "q25": 0.5375000000000001, "q75": 0.75, "skewness": -0.7702689769562403, "kurtosis": -0.823885038038882}, "fear_confidence": {"count": 8, "mean": 0.5187499999999999, "std": 0.16889874396894047, "min": 0.3, "max": 0.7, "median": 0.55, "q25": 0.375, "q75": 0.6625, "skewness": -0.3127134054837214, "kurtosis": -1.8339571072758938}, "fantasy_confidence": {"count": 8, "mean": 0.5062500000000001, "std": 0.20430631764233684, "min": 0.3, "max": 0.9, "median": 0.45, "q25": 0.375, "q75": 0.6125, "skewness": 1.0027409456768288, "kurtosis": 0.6916462009208111}}}, "summarize_data_completeness": {"type": "summary_statistics", "metrics": ["dignity_score", "dignity_salience", "dignity_confidence", "truth_score", "truth_salience", "truth_confidence", "justice_score", "justice_salience", "justice_confidence", "hope_score", "hope_salience", "hope_confidence", "pragmatism_score", "pragmatism_salience", "pragmatism_confidence", "tribalism_score", "tribalism_salience", "tribalism_confidence", "manipulation_score", "manipulation_salience", "manipulation_confidence", "resentment_score", "resentment_salience", "resentment_confidence", "fear_score", "fear_salience", "fear_confidence", "fantasy_score", "fantasy_salience", "fantasy_confidence"], "summary_types": ["count", "missing"], "results": {"dignity_score": {"count": 7}, "dignity_salience": {"count": 7}, "dignity_confidence": {"count": 7}, "truth_score": {"count": 8}, "truth_salience": {"count": 8}, "truth_confidence": {"count": 8}, "justice_score": {"count": 8}, "justice_salience": {"count": 8}, "justice_confidence": {"count": 8}, "hope_score": {"count": 8}, "hope_salience": {"count": 8}, "hope_confidence": {"count": 8}, "pragmatism_score": {"count": 8}, "pragmatism_salience": {"count": 8}, "pragmatism_confidence": {"count": 8}, "tribalism_score": {"count": 8}, "tribalism_salience": {"count": 8}, "tribalism_confidence": {"count": 8}, "manipulation_score": {"count": 8}, "manipulation_salience": {"count": 8}, "manipulation_confidence": {"count": 8}, "resentment_score": {"count": 8}, "resentment_salience": {"count": 8}, "resentment_confidence": {"count": 8}, "fear_score": {"count": 8}, "fear_salience": {"count": 8}, "fear_confidence": {"count": 8}, "fantasy_score": {"count": 8}, "fantasy_salience": {"count": 8}, "fantasy_confidence": {"count": 8}}, "missing_metrics": []}}, "errors": []}, "stage_2_derived_metrics": {"analysis_plan": {"stage": "derived_metrics_analysis", "experiment_summary": "This plan outlines the calculation of derived metrics (Character Tensions, MC-SCI) from raw CAF v6.1 scores and the subsequent statistical analysis to test the experiment's hypotheses. The analysis will use ANOVA to test for differentiation between speakers (using 'aid' as a proxy), analyze character signatures through descriptive statistics and correlations, and examine MC-SCI coherence patterns.", "tasks": {"task_01_calculate_derived_metrics": {"tool": "calculate_derived_metrics", "parameters": {"metric_formulas": {"dignity_tribalism_tension": "np.minimum(dignity_score, tribalism_score) * abs(dignity_salience - tribalism_salience)", "truth_manipulation_tension": "np.minimum(truth_score, manipulation_score) * abs(truth_salience - manipulation_salience)", "justice_resentment_tension": "np.minimum(justice_score, resentment_score) * abs(justice_salience - resentment_salience)", "hope_fear_tension": "np.minimum(hope_score, fear_score) * abs(hope_salience - fear_salience)", "pragmatism_fantasy_tension": "np.minimum(pragmatism_score, fantasy_score) * abs(pragmatism_salience - fantasy_salience)", "mc_sci": "(dignity_tribalism_tension + truth_manipulation_tension + justice_resentment_tension + hope_fear_tension + pragmatism_fantasy_tension) / 5"}, "input_columns": ["dignity_score", "dignity_salience", "tribalism_score", "tribalism_salience", "truth_score", "truth_salience", "manipulation_score", "manipulation_salience", "justice_score", "justice_salience", "resentment_score", "resentment_salience", "hope_score", "hope_salience", "fear_score", "fear_salience", "pragmatism_score", "pragmatism_salience", "fantasy_score", "fantasy_salience"]}, "purpose": "To calculate the core derived metrics (Character Tensions and MC-SCI) as specified by the Character Assessment Framework v6.1. These metrics are essential for testing H3 and for deeper character analysis."}, "task_02_validate_derived_metrics": {"tool": "validate_calculated_metrics", "parameters": {"validation_rules": [{"rule": "missing_data_check", "columns": ["mc_sci", "dignity_tribalism_tension", "truth_manipulation_tension"]}, {"rule": "range_check", "columns": ["mc_sci", "dignity_tribalism_tension"], "min_value": 0.0, "max_value": 1.0}]}, "purpose": "To ensure the quality and reliability of the newly calculated derived metrics by checking for missing values and verifying that they fall within expected logical ranges (0.0 to 1.0)."}, "task_03_speaker_differentiation_anova": {"tool": "perform_one_way_anova", "parameters": {"grouping_variable": "aid", "dependent_variable": "dignity_score"}, "purpose": "To test hypothesis H1 (Speaker Differentiation) by determining if there are statistically significant differences in the 'Dignity' dimension across individual documents (speakers). This will be repeated for all 10 dimensions."}, "task_04_mc_sci_coherence_anova": {"tool": "perform_one_way_anova", "parameters": {"grouping_variable": "aid", "dependent_variable": "mc_sci"}, "purpose": "To test hypothesis H3 (MC-SCI Patterns) by determining if the calculated MC-SCI scores, representing character coherence, vary meaningfully across different speakers (documents)."}, "task_05_character_signature_descriptives": {"tool": "calculate_descriptive_stats", "parameters": {"columns": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score", "mc_sci"], "grouping_variable": "aid"}, "purpose": "To address hypothesis H2 (Character Signatures) by generating mean scores for each dimension per speaker (document). This provides the quantitative basis for defining each speaker's unique character profile."}, "task_06_dimensional_correlation_analysis": {"tool": "generate_correlation_matrix", "parameters": {"dimensions": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score"], "correlation_method": "pearson"}, "purpose": "To further investigate character signatures (H2) by examining the relationships between different character dimensions across the entire corpus, revealing underlying patterns of co-occurrence between virtues and vices."}, "task_07_overall_summary_statistics": {"tool": "create_summary_statistics", "parameters": {"metrics": ["dignity_score", "tribalism_score", "mc_sci"], "summary_types": ["mean", "std", "min", "max"]}, "purpose": "To generate a high-level summary of key performance indicators for the entire dataset, providing an overall view of the central tendency and variance of primary dimensions and the main derived metric."}}}, "results": {"task_01_calculate_derived_metrics": {"type": "derived_metrics_calculation", "success": true, "calculated_metrics": {"hope_fear_tension": [0.11999999999999998, 0.06, 0.09749999999999999, 0.1, 0.22499999999999998, 0.08000000000000002, 0.030000000000000006, 0.04000000000000001], "dignity_tribalism_tension": [0.15, 0.08999999999999998, 0.12000000000000002, 0.0, 0.16, 0.085, 0.08999999999999998, NaN], "truth_manipulation_tension": [0.059999999999999984, 0.08000000000000003, 0.08, 0.039999999999999994, 0.11000000000000001, 0.08249999999999999, 0.06000000000000005, 0.07999999999999999], "justice_resentment_tension": [0.175, 0.12000000000000004, 0.16499999999999998, 0.029999999999999992, 0.24499999999999997, 0.034999999999999996, 0.18000000000000002, 0.039999999999999994], "pragmatism_fantasy_tension": [0.04000000000000001, 0.04000000000000001, 0.06, 0.059999999999999984, 0.030000000000000006, 0.0, 0.08, 0.0], "mc_sci": [0.10899999999999999, 0.078, 0.1045, 0.046, 0.154, 0.056499999999999995, 0.08800000000000002, NaN]}, "successful_calculations": ["hope_fear_tension", "dignity_tribalism_tension", "truth_manipulation_tension", "justice_resentment_tension", "pragmatism_fantasy_tension", "mc_sci"], "failed_calculations": [], "formulas_used": ["dignity_tribalism_tension", "truth_manipulation_tension", "justice_resentment_tension", "hope_fear_tension", "pragmatism_fantasy_tension", "mc_sci"], "input_columns": ["dignity_score", "dignity_salience", "tribalism_score", "tribalism_salience", "truth_score", "truth_salience", "manipulation_score", "manipulation_salience", "justice_score", "justice_salience", "resentment_score", "resentment_salience", "hope_score", "hope_salience", "fear_score", "fear_salience", "pragmatism_score", "pragmatism_salience", "fantasy_score", "fantasy_salience"], "total_metrics": 6, "success_rate": 1.0}, "task_05_character_signature_descriptives": {"type": "descriptive_stats_grouped", "grouping_variable": "aid", "groups": {"{artifact_id}": {"dignity_score": {"count": 7, "mean": 0.7071428571428572, "std": 0.11700630833624397, "min": 0.6, "max": 0.9}, "truth_score": {"count": 8, "mean": 0.6125, "std": 0.15294723646688466, "min": 0.4, "max": 0.85}, "justice_score": {"count": 8, "mean": 0.73125, "std": 0.15797264681854625, "min": 0.5, "max": 0.9}, "hope_score": {"count": 8, "mean": 0.6475, "std": 0.23614462880676687, "min": 0.2, "max": 0.9}, "pragmatism_score": {"count": 8, "mean": 0.5249999999999999, "std": 0.12817398889233114, "min": 0.3, "max": 0.7}, "tribalism_score": {"count": 8, "mean": 0.4125, "std": 0.18850918886280923, "min": 0.1, "max": 0.7}, "manipulation_score": {"count": 8, "mean": 0.40625, "std": 0.2111828929760038, "min": 0.15, "max": 0.7}, "resentment_score": {"count": 8, "mean": 0.39375000000000004, "std": 0.2043063176423368, "min": 0.05, "max": 0.7}, "fear_score": {"count": 8, "mean": 0.25625000000000003, "std": 0.1347948176197544, "min": 0.1, "max": 0.5}, "fantasy_score": {"count": 8, "mean": 0.175, "std": 0.10350983390135313, "min": 0.0, "max": 0.3}, "mc_sci": {"count": 7, "mean": 0.09085714285714284, "std": 0.03621315861869629, "min": 0.046, "max": 0.154}}}}, "task_06_dimensional_correlation_analysis": {"type": "correlation_matrix", "dimensions": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score"], "method": "pearson", "matrix": {"dignity_score": {"dignity_score": 1.0, "truth_score": 0.576770389298682, "justice_score": 0.4843884930464273, "hope_score": 0.8893427293057098, "pragmatism_score": 0.5421374765483945, "tribalism_score": -0.49854861812834383, "manipulation_score": -0.6651662082893139, "resentment_score": -0.2881837200291647, "fear_score": 0.061952247412989436, "fantasy_score": -0.9174634218511292}, "truth_score": {"dignity_score": 0.576770389298682, "truth_score": 1.0, "justice_score": 0.897975984067584, "hope_score": 0.6140643187067323, "pragmatism_score": 0.6740655909056726, "tribalism_score": -0.6007718129713515, "manipulation_score": -0.4671628644330198, "resentment_score": 0.20858419942357415, "fear_score": 0.11693133599279322, "fantasy_score": -0.7895629608911476}, "justice_score": {"dignity_score": 0.4843884930464273, "truth_score": 0.897975984067584, "justice_score": 1.0, "hope_score": 0.6093690745575551, "pragmatism_score": 0.7672721428435879, "tribalism_score": -0.42275297305978327, "manipulation_score": -0.35996778188527995, "resentment_score": 0.19503268660627737, "fear_score": 0.0062895293220607915, "fantasy_score": -0.7316835130490091}, "hope_score": {"dignity_score": 0.8893427293057098, "truth_score": 0.6140643187067323, "justice_score": 0.6093690745575551, "hope_score": 1.0, "pragmatism_score": 0.7952870179044798, "tribalism_score": -0.20779310668060388, "manipulation_score": -0.6971738961522156, "resentment_score": -0.14397984649651582, "fear_score": -0.2126179447022601, "fantasy_score": -0.7510094540244864}, "pragmatism_score": {"dignity_score": 0.5421374765483945, "truth_score": 0.6740655909056726, "justice_score": 0.7672721428435879, "hope_score": 0.7952870179044798, "pragmatism_score": 1.0, "tribalism_score": -0.31040509310661096, "manipulation_score": -0.5607538494404394, "resentment_score": -0.2932234722018026, "fear_score": -0.3410773006656662, "fantasy_score": -0.6998964726756151}, "tribalism_score": {"dignity_score": -0.49854861812834383, "truth_score": -0.6007718129713515, "justice_score": -0.42275297305978327, "hope_score": -0.20779310668060388, "pragmatism_score": -0.31040509310661096, "tribalism_score": 1.0, "manipulation_score": 0.48220218903683554, "resentment_score": 0.31760565283189346, "fear_score": 0.3619207488858103, "fantasy_score": 0.7504325943927066}, "manipulation_score": {"dignity_score": -0.6651662082893139, "truth_score": -0.4671628644330198, "justice_score": -0.35996778188527995, "hope_score": -0.6971738961522156, "pragmatism_score": -0.5607538494404394, "tribalism_score": 0.48220218903683554, "manipulation_score": 1.0, "resentment_score": 0.191418178326564, "fear_score": 0.21171606805422713, "fantasy_score": 0.72704557073721}, "resentment_score": {"dignity_score": -0.2881837200291647, "truth_score": 0.20858419942357415, "justice_score": 0.19503268660627737, "hope_score": -0.14397984649651582, "pragmatism_score": -0.2932234722018026, "tribalism_score": 0.31760565283189346, "manipulation_score": 0.191418178326564, "resentment_score": 1.0, "fear_score": 0.7408208383886864, "fantasy_score": 0.2617642051708646}, "fear_score": {"dignity_score": 0.061952247412989436, "truth_score": 0.11693133599279322, "justice_score": 0.0062895293220607915, "hope_score": -0.2126179447022601, "pragmatism_score": -0.3410773006656662, "tribalism_score": 0.3619207488858103, "manipulation_score": 0.21171606805422713, "resentment_score": 0.7408208383886864, "fear_score": 1.0, "fantasy_score": 0.268767319880733}, "fantasy_score": {"dignity_score": -0.9174634218511292, "truth_score": -0.7895629608911476, "justice_score": -0.7316835130490091, "hope_score": -0.7510094540244864, "pragmatism_score": -0.6998964726756151, "tribalism_score": 0.7504325943927066, "manipulation_score": 0.72704557073721, "resentment_score": 0.2617642051708646, "fear_score": 0.268767319880733, "fantasy_score": 1.0}}, "missing_dimensions": []}, "task_07_overall_summary_statistics": {"type": "summary_statistics", "metrics": ["dignity_score", "tribalism_score", "mc_sci"], "summary_types": ["mean", "std", "min", "max"], "results": {"dignity_score": {"mean": 0.7071428571428572, "std": 0.11700630833624397, "min": 0.6, "max": 0.9}, "tribalism_score": {"mean": 0.4125, "std": 0.18850918886280923, "min": 0.1, "max": 0.7}, "mc_sci": {"mean": 0.09085714285714284, "std": 0.03621315861869629, "min": 0.046, "max": 0.154}}, "missing_metrics": []}}, "errors": ["Task 'task_02_validate_derived_metrics' failed: validate_calculated_metrics() missing 1 required positional argument: 'quality_thresholds'", "Task 'task_03_speaker_differentiation_anova' failed: ANOVA failed: Need at least 2 groups for ANOVA, found 1", "Task 'task_04_mc_sci_coherence_anova' failed: ANOVA failed: Need at least 2 groups for ANOVA, found 1"]}, "combined_summary": "Two-stage execution: 4 raw data results + 4 derived metrics results"}