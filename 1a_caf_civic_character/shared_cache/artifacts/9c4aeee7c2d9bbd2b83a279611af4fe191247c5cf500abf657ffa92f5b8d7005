import pandas as pd
import numpy as np
from scipy import stats
import json

# --- Configuration & Setup ---
# This section defines variables based on the CAF v6.0 framework specification.
# This makes the code adaptable and easy to read.

VIRTUES = ["dignity", "truth", "justice", "hope", "pragmatism"]
VICES = ["tribalism", "manipulation", "resentment", "fear", "fantasy"]
OPPOSING_PAIRS = {
    "dignity": "tribalism",
    "truth": "manipulation",
    "justice": "resentment",
    "hope": "fear",
    "pragmatism": "fantasy"
}
ALL_DIMENSIONS = VIRTUES + VICES

# Initialize the final results dictionary with error-handling defaults
result_data = {
    'descriptive_stats': {'status': 'not_run'},
    'hypothesis_tests': {'status': 'not_run'},
    'correlations': {'status': 'not_run'},
    'reliability_metrics': {'status': 'not_run'}
}

# --- Core Analysis Functions ---

def calculate_cronbach_alpha(df: pd.DataFrame):
    """
    Calculates Cronbach's alpha for a given set of items in a DataFrame.
    Items are columns, subjects are rows.
    """
    if df.shape[1] < 2:
        return np.nan  # Alpha is not defined for a single item
    
    # Drop rows with any NaN values for this calculation
    df = df.dropna()
    if df.shape[0] < 2:
        return np.nan # Not enough data to calculate variance

    k = df.shape[1]
    item_variances = df.var(axis=0, ddof=1).sum()
    total_scores = df.sum(axis=1)
    total_variance = total_scores.var(ddof=1)

    if total_variance == 0:
        return 1.0 if item_variances == 0 else 0.0

    return (k / (k - 1)) * (1 - (item_variances / total_variance))

def cohen_d(x, y):
    """
    Calculates Cohen's d for independent or paired samples.
    """
    if len(x) < 2 or len(y) < 2:
        return np.nan
        
    # For paired samples, the calculation is on the differences
    diff = np.array(x) - np.array(y)
    std_dev = np.std(diff, ddof=1)
    
    if std_dev == 0:
        return np.inf # Indicates a perfect effect, but could be due to no variance

    return np.mean(diff) / std_dev

# --- Main Analysis Script ---

try:
    # --- Step 1: Data Validation and Preparation ---
    # Ensure scores_df exists and has data. The script assumes it's pre-loaded.
    if 'scores_df' not in locals() or scores_df.empty:
        raise ValueError("scores_df is not defined or is empty.")

    # Create a copy to avoid modifying the original DataFrame
    analysis_df = scores_df.copy()

    # --- Step 2: Feature Engineering - Calculate Framework Metrics ---
    # Calculate Character Tension scores and MC-SCI for each artifact (row)
    
    tension_cols = []
    for virtue, vice in OPPOSING_PAIRS.items():
        virtue_score_col = f"{virtue}_score"
        vice_score_col = f"{vice}_score"
        virtue_salience_col = f"{virtue}_salience"
        vice_salience_col = f"{vice}_salience"
        tension_col_name = f"{virtue}_{vice}_tension"
        tension_cols.append(tension_col_name)

        # Apply the Character Tension formula
        min_score = analysis_df[[virtue_score_col, vice_score_col]].min(axis=1)
        salience_diff = abs(analysis_df[virtue_salience_col] - analysis_df[vice_salience_col])
        analysis_df[tension_col_name] = min_score * salience_diff

    # Calculate Moral Character Strategic Contradiction Index (MC-SCI)
    analysis_df['mc_sci'] = analysis_df[tension_cols].mean(axis=1)

    # --- Step 3: Descriptive Statistics ---
    descriptive_stats = {}
    
    # Stats for primary scores
    score_cols = [f"{dim}_score" for dim in ALL_DIMENSIONS]
    descriptive_stats['primary_scores'] = json.loads(analysis_df[score_cols].describe().to_json())

    # Stats for calculated metrics
    calculated_metrics_cols = tension_cols + ['mc_sci']
    descriptive_stats['calculated_metrics'] = json.loads(analysis_df[calculated_metrics_cols].describe().to_json())
    
    # Stats from evidence data (if available)
    if 'evidence_df' in locals() and not evidence_df.empty:
        evidence_summary = {
            'total_quotes': int(evidence_df.shape[0]),
            'quotes_per_dimension': evidence_df['dimension'].value_counts().to_dict(),
            'confidence_score_stats': json.loads(evidence_df['confidence_score'].describe().to_json())
        }
        descriptive_stats['evidence_summary'] = evidence_summary
        
    result_data['descriptive_stats'] = descriptive_stats
    
    # --- Step 4: Reliability Metrics ---
    reliability_metrics = {}
    try:
        # Cronbach's Alpha for Virtues scale
        virtue_score_df = analysis_df[[f"{v}_score" for v in VIRTUES]]
        reliability_metrics['virtues_cronbach_alpha'] = calculate_cronbach_alpha(virtue_score_df)

        # Cronbach's Alpha for Vices scale
        vice_score_df = analysis_df[[f"{v}_score" for v in VICES]]
        reliability_metrics['vices_cronbach_alpha'] = calculate_cronbach_alpha(vice_score_df)
    except Exception as e:
        reliability_metrics['error'] = f"Could not calculate Cronbach's alpha: {str(e)}"
    
    result_data['reliability_metrics'] = reliability_metrics

    # --- Step 5: Correlation Analysis ---
    correlations = {}
    try:
        # Select all numeric columns for correlation matrix
        numeric_cols = analysis_df.select_dtypes(include=np.number).columns.tolist()
        corr_matrix = analysis_df[numeric_cols].corr()
        
        # Storing the full matrix can be large, so we'll store key findings
        correlations['full_matrix_sample'] = json.loads(corr_matrix.head().to_json()) # Sample for review
        
        # Key correlation: MC-SCI vs. average virtue and vice scores
        analysis_df['avg_virtue_score'] = analysis_df[[f"{v}_score" for v in VIRTUES]].mean(axis=1)
        analysis_df['avg_vice_score'] = analysis_df[[f"{v}_score" for v in VICES]].mean(axis=1)
        
        key_corrs = {
            'mc_sci_vs_avg_virtue': analysis_df['mc_sci'].corr(analysis_df['avg_virtue_score']),
            'mc_sci_vs_avg_vice': analysis_df['mc_sci'].corr(analysis_df['avg_vice_score']),
            'avg_virtue_vs_avg_vice': analysis_df['avg_virtue_score'].corr(analysis_df['avg_vice_score'])
        }
        correlations['key_correlations'] = key_corrs
    except Exception as e:
        correlations['error'] = f"Could not perform correlation analysis: {str(e)}"
        
    result_data['correlations'] = correlations

    # --- Step 6: Hypothesis Testing ---
    hypothesis_tests = {}
    
    # Test 1: Paired T-test between average Virtue scores and average Vice scores
    try:
        avg_virtue_scores = analysis_df['avg_virtue_score'].dropna()
        avg_vice_scores = analysis_df['avg_vice_score'].dropna()
        
        if len(avg_virtue_scores) > 1 and len(avg_vice_scores) > 1:
            ttest_virtue_vs_vice = stats.ttest_rel(avg_virtue_scores, avg_vice_scores)
            hypothesis_tests['virtue_vs_vice_paired_ttest'] = {
                'description': "Paired t-test comparing average virtue scores to average vice scores per artifact.",
                't_statistic': ttest_virtue_vs_vice.statistic,
                'p_value': ttest_virtue_vs_vice.pvalue,
                'effect_size_cohens_d': cohen_d(avg_virtue_scores, avg_vice_scores),
                'interpretation': 'p < 0.05 suggests a significant difference between expressed virtues and vices.'
            }
        else:
            hypothesis_tests['virtue_vs_vice_paired_ttest'] = {'status': 'skipped', 'reason': 'Insufficient data for test.'}
    except Exception as e:
        hypothesis_tests['virtue_vs_vice_paired_ttest'] = {'status': 'failed', 'error': str(e)}

    # Test 2: Paired T-test for a specific opposing pair: Justice vs. Resentment
    try:
        justice_scores = analysis_df['justice_score'].dropna()
        resentment_scores = analysis_df['resentment_score'].dropna()

        if len(justice_scores) > 1 and len(resentment_scores) > 1:
            ttest_justice_vs_resentment = stats.ttest_rel(justice_scores, resentment_scores)
            hypothesis_tests['justice_vs_resentment_paired_ttest'] = {
                'description': "Paired t-test comparing Justice scores to Resentment scores per artifact.",
                't_statistic': ttest_justice_vs_resentment.statistic,
                'p_value': ttest_justice_vs_resentment.pvalue,
                'effect_size_cohens_d': cohen_d(justice_scores, resentment_scores),
                'interpretation': 'p < 0.05 suggests a significant difference between scores on this specific axis.'
            }
        else:
            hypothesis_tests['justice_vs_resentment_paired_ttest'] = {'status': 'skipped', 'reason': 'Insufficient data for test.'}
    except Exception as e:
        hypothesis_tests['justice_vs_resentment_paired_ttest'] = {'status': 'failed', 'error': str(e)}

    result_data['hypothesis_tests'] = hypothesis_tests

except Exception as e:
    # Catch-all for major script failures, like missing DataFrames
    result_data['error'] = f"A critical error occurred during analysis: {str(e)}"

# --- Final Output ---
# The 'result_data' dictionary now contains all the structured results
# and is ready for downstream processing.
# No print statement is needed; the variable is the specified output.
# (Final variable assignment is implicitly the last step)