{
  "status": "success",
  "functions_generated": 4,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 21503,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: caf_civic_character_pattern_analysis\nDescription: Statistical analysis experiment\nGenerated: 2025-08-27T14:49:51.025525+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived metrics as specified in the CAF v10.0 framework.\n\n    This function implements the formulas for the five Character Tension Indices and\n    the primary summary metric, the Salience-Weighted Civic Character Index. It\n    adds these new metrics as columns to the input DataFrame. This is a foundational\n    analysis step required for many other summary functions.\n\n    Statistical Methodology:\n    - Tension Index: For each axis, calculates `min(Virtue_Score, Vice_Score) * abs(Virtue_Salience - Vice_Salience)`.\n      This measures the degree of strategic contradiction, penalizing cases where a speaker\n      appeals to both a virtue and its opposing vice, especially when one is emphasized\n      (high salience) and the other is downplayed (low salience).\n    - Civic Character Index: Calculates `(Weighted_Virtue_Sum - Weighted_Vice_Sum) / Total_Salience`.\n      This provides a single, normalized score from -1.0 to +1.0, indicating the overall\n      civic character of the discourse, weighted by the rhetorical prominence of each dimension.\n      A small epsilon (0.001) is added to the denominator to prevent division by zero.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw and salience scores for all 10 dimensions.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pd.DataFrame: The original DataFrame with added columns for each derived metric, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = data.copy()\n\n        # Define virtue and vice dimensions based on the framework\n        virtues = ['dignity', 'truth', 'justice', 'hope', 'pragmatism']\n        vices = ['tribalism', 'manipulation', 'resentment', 'fear', 'fantasy']\n        axes = list(zip(virtues, vices))\n\n        # Calculate Tension Indices\n        for virtue, vice in axes:\n            virtue_raw = f\"{virtue}_raw\"\n            virtue_salience = f\"{virtue}_salience\"\n            vice_raw = f\"{vice}_raw\"\n            vice_salience = f\"{vice}_salience\"\n            tension_col = f\"{virtue.split('_')[0]}_tension\"\n\n            # Ensure required columns exist\n            required_cols = [virtue_raw, virtue_salience, vice_raw, vice_salience]\n            if not all(col in df.columns for col in required_cols):\n                # Silently skip if columns are missing, or raise an error\n                continue\n\n            min_score = np.minimum(df[virtue_raw], df[vice_raw])\n            salience_diff = np.abs(df[virtue_salience] - df[vice_salience])\n            df[tension_col] = min_score * salience_diff\n\n        # Calculate Salience-Weighted Civic Character Index\n        weighted_virtue_scores = []\n        weighted_vice_scores = []\n        all_salience_scores = []\n\n        for virtue in virtues:\n            raw_col = f\"{virtue}_raw\"\n            salience_col = f\"{virtue}_salience\"\n            if raw_col in df.columns and salience_col in df.columns:\n                weighted_virtue_scores.append(df[raw_col] * df[salience_col])\n                all_salience_scores.append(df[salience_col])\n\n        for vice in vices:\n            raw_col = f\"{vice}_raw\"\n            salience_col = f\"{vice}_salience\"\n            if raw_col in df.columns and salience_col in df.columns:\n                weighted_vice_scores.append(df[raw_col] * df[salience_col])\n                all_salience_scores.append(df[salience_col])\n\n        if not weighted_virtue_scores or not weighted_vice_scores:\n             return df # Return df without index if columns are missing\n\n        df['weighted_virtue_score'] = pd.concat(weighted_virtue_scores, axis=1).sum(axis=1)\n        df['weighted_vice_score'] = pd.concat(weighted_vice_scores, axis=1).sum(axis=1)\n        df['combined_salience_total'] = pd.concat(all_salience_scores, axis=1).sum(axis=1)\n\n        # Add epsilon to prevent division by zero\n        denominator = df['combined_salience_total'] + 0.001\n        \n        df['civic_character_index'] = (df['weighted_virtue_score'] - df['weighted_vice_score']) / denominator\n\n        return df\n\n    except Exception:\n        # In a production environment, you might log the error\n        # import traceback\n        # traceback.print_exc()\n        return None\n\ndef _get_speaker_metadata(df):\n    \"\"\"\n    Internal helper to enrich a DataFrame with speaker metadata.\n\n    This function attempts to find a corpus manifest file (`corpus_manifest.json`)\n    in the workspace to map document filenames to speaker metadata. If the manifest\n    is not found (as indicated in the project spec), it gracefully falls back to\n    parsing speaker names from the `document_name` column. This approach satisfies\n    the dual requirements of preferring a manifest while handling its absence.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to enrich, must contain 'document_name'.\n\n    Returns:\n        pd.DataFrame: The enriched DataFrame with 'speaker' and 'style' columns.\n    \"\"\"\n    import pandas as pd\n    import json\n    from pathlib import Path\n\n    # The prompt states \"No corpus manifest found\", so this logic will use the fallback.\n    # However, it is designed to prefer a manifest if one were to become available.\n    manifest_path = Path('corpus_manifest.json')\n    if manifest_path.exists():\n        # Primary Strategy: Use manifest if it exists.\n        try:\n            with open(manifest_path, 'r') as f:\n                manifest = json.load(f)\n            # Example manifest structure: {\"filename.txt\": {\"speaker\": \"Name\", \"style\": \"populist\"}}\n            # Create a mapping from filename to speaker/style\n            speaker_map = {k: v.get('speaker', 'Unknown') for k, v in manifest.items()}\n            style_map = {k: v.get('style', 'unknown') for k, v in manifest.items()}\n            df['speaker'] = df['document_name'].map(speaker_map).fillna('Unknown Speaker')\n            df['style'] = df['document_name'].map(style_map).fillna('unknown')\n        except (json.JSONDecodeError, KeyError):\n            # If manifest is malformed, proceed to fallback\n            df['speaker'] = 'Unknown Speaker'\n            df['style'] = 'unknown'\n    else:\n        # Fallback Strategy: Parse from filename.\n        def parse_speaker_from_filename(filename):\n            try:\n                # Assumes a pattern like \"firstname_lastname_...\"\n                parts = str(filename).split('_')\n                if len(parts) >= 2:\n                    return f\"{parts[0].capitalize()} {parts[1].capitalize()}\"\n                return 'Unknown Speaker'\n            except:\n                return 'Unknown Speaker'\n\n        def assign_style_from_name(speaker_name):\n            # This is a demonstrative mapping for the validation function.\n            # In a real scenario, this logic would be more robust or data-driven.\n            populist_speakers = ['Steve King', 'Bernie Sanders', 'Alexandria Ocasio-cortez', 'Donald Trump']\n            institutional_speakers = ['Cory Booker', 'Joe Biden', 'Hillary Clinton']\n            if speaker_name in populist_speakers:\n                return 'populist'\n            if speaker_name in institutional_speakers:\n                return 'institutional'\n            return 'unknown'\n\n        df['speaker'] = df['document_name'].apply(parse_speaker_from_filename)\n        df['style'] = df['speaker'].apply(assign_style_from_name)\n\n    return df\n\n\ndef analyze_speaker_character_signatures(data, **kwargs):\n    \"\"\"\n    Calculates the average character signature for each speaker in the dataset.\n\n    This function addresses the research question \"What distinct character signatures\n    emerge across the 5 virtues and 5 vices for each speaker?\". It first enriches\n    the data with speaker metadata, then calculates derived metrics, and finally\n    groups the data by speaker to compute the mean scores for all dimensions and\n    indices.\n\n    Statistical Methodology:\n    - Speaker Identification: Uses a helper function that first attempts to load a\n      corpus manifest. If unavailable, it falls back to parsing speaker names from\n      filenames.\n    - Aggregation: Uses the arithmetic mean to aggregate scores for each speaker across\n      all their documents. This provides a stable, central measure of a speaker's\n      typical rhetorical profile.\n    - The output is a DataFrame where each row represents a unique speaker and each\n      column represents their average score for a specific CAF dimension or derived metric.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pd.DataFrame: A DataFrame of average character signatures per speaker, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        # 1. Enrich data with speaker metadata\n        df_with_speakers = _get_speaker_metadata(data.copy())\n\n        # 2. Calculate derived metrics\n        df_full = calculate_derived_metrics(df_with_speakers)\n        if df_full is None:\n            return None # Propagate error from calculation\n\n        # 3. Define columns to aggregate\n        score_cols = [col for col in df_full.columns if '_raw' in col or '_salience' in col]\n        derived_cols = [\n            'identity_tension', 'truth_tension', 'justice_tension',\n            'emotional_tension', 'reality_tension', 'civic_character_index'\n        ]\n        all_metrics_cols = score_cols + [col for col in derived_cols if col in df_full.columns]\n\n        if 'speaker' not in df_full.columns or df_full['speaker'].nunique() == 0:\n            return None # Not enough speaker data to analyze\n\n        # 4. Group by speaker and calculate mean signature\n        speaker_signatures = df_full.groupby('speaker')[all_metrics_cols].mean().reset_index()\n        \n        return speaker_signatures.set_index('speaker')\n\n    except Exception:\n        return None\n\ndef classify_rhetorical_patterns(data, **kwargs):\n    \"\"\"\n    Classifies each document into rhetorical patterns based on CAF v10.0 guidance.\n\n    This function addresses the research question on Civic Character Coherence by\n    operationalizing the interpretive patterns from the framework specification. It\n    evaluates the relationships between virtue/vice scores and their salience to\n    classify the speaker's strategy.\n\n    Methodology:\n    - The function first calculates average virtue/vice scores and salience for each document.\n    - It then applies a set of rules with defined thresholds (high > 0.6, low < 0.4)\n      to classify each document's primary rhetorical pattern:\n      - Authentic Virtue: High virtue intensity and high virtue salience.\n      - Strategic Virtue Signaling: High virtue intensity but low virtue salience.\n      - Strategic Pathology: High vice intensity and high vice salience.\n      - Incoherent Messaging: High scores on a tension index ( > 0.3).\n      - Balanced/Mixed: Default category if no other pattern strongly matches.\n    - This provides a qualitative layer on top of the quantitative scores.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pd.DataFrame: The original DataFrame with an added 'rhetorical_pattern' column, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = calculate_derived_metrics(data.copy())\n        if df is None:\n            return None\n\n        virtues = ['dignity', 'truth', 'justice', 'hope', 'pragmatism']\n        vices = ['tribalism', 'manipulation', 'resentment', 'fear', 'fantasy']\n\n        virtue_raw_cols = [f\"{v}_raw\" for v in virtues]\n        virtue_sal_cols = [f\"{v}_salience\" for v in virtues]\n        vice_raw_cols = [f\"{v}_raw\" for v in vices]\n        vice_sal_cols = [f\"{v}_salience\" for v in vices]\n        tension_cols = [col for col in df.columns if '_tension' in col]\n\n        # Calculate average scores per document\n        df['avg_virtue_raw'] = df[virtue_raw_cols].mean(axis=1)\n        df['avg_virtue_salience'] = df[virtue_sal_cols].mean(axis=1)\n        df['avg_vice_raw'] = df[vice_raw_cols].mean(axis=1)\n        df['avg_vice_salience'] = df[vice_sal_cols].mean(axis=1)\n        df['max_tension'] = df[tension_cols].max(axis=1)\n\n        # Define thresholds\n        HIGH_THRESHOLD = 0.6\n        LOW_THRESHOLD = 0.4\n        TENSION_THRESHOLD = 0.3\n\n        def assign_pattern(row):\n            if row['max_tension'] > TENSION_THRESHOLD:\n                return \"Incoherent Messaging\"\n            if row['avg_vice_raw'] > HIGH_THRESHOLD and row['avg_vice_salience'] > HIGH_THRESHOLD:\n                return \"Strategic Pathology\"\n            if row['avg_virtue_raw'] > HIGH_THRESHOLD and row['avg_virtue_salience'] > HIGH_THRESHOLD:\n                return \"Authentic Virtue\"\n            if row['avg_virtue_raw'] > HIGH_THRESHOLD and row['avg_virtue_salience'] < LOW_THRESHOLD:\n                return \"Strategic Virtue Signaling\"\n            return \"Balanced/Mixed\"\n\n        df['rhetorical_pattern'] = df.apply(assign_pattern, axis=1)\n        \n        # Clean up intermediate columns before returning\n        df.drop(columns=['avg_virtue_raw', 'avg_virtue_salience', 'avg_vice_raw', 'avg_vice_salience', 'max_tension'], inplace=True)\n\n        return df\n\n    except Exception:\n        return None\n\ndef analyze_speaker_differentiation(data, **kwargs):\n    \"\"\"\n    Analyzes the variance in CAF dimensions to assess speaker differentiation.\n\n    This function addresses the research question: \"How effectively do the 10 CAF\n    dimensions distinguish between different speakers' character profiles?\". It\n    calculates the standard deviation of mean scores for each dimension across all\n    speakers. A higher standard deviation for a dimension indicates that it is more\n    effective at differentiating between speakers.\n\n    Statistical Methodology:\n    - First, it calculates the mean score for each dimension for each speaker.\n    - Then, it computes the standard deviation of these mean scores across the\n      speaker population.\n    - Dimensions with the highest standard deviation are the most powerful\n      differentiators in the given corpus.\n    - The output is a sorted series showing which dimensions vary the most,\n      providing insight into the key rhetorical battlegrounds in the dataset.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pd.Series: A series of standard deviations for each dimension, sorted\n                   descending, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        # Use the character signatures function to get per-speaker means\n        speaker_signatures = analyze_speaker_character_signatures(data, **kwargs)\n\n        if speaker_signatures is None or speaker_signatures.empty:\n            return None\n            \n        if len(speaker_signatures) < 2:\n            # Cannot calculate std dev with fewer than 2 speakers\n            return pd.Series(dtype=float, name=\"differentiation_std_dev\")\n\n        # Calculate the standard deviation of the mean scores across all speakers\n        # This measures how much each dimension's score varies from speaker to speaker\n        differentiation_std = speaker_signatures.std()\n\n        # We are interested in the raw scores for differentiation\n        raw_score_cols = [col for col in differentiation_std.index if '_raw' in col]\n        differentiation_summary = differentiation_std[raw_score_cols].sort_values(ascending=False)\n        differentiation_summary.name = \"differentiation_std_dev\"\n\n        return differentiation_summary\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}