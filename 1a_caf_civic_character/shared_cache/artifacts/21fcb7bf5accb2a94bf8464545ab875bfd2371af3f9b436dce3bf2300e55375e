import pandas as pd
import numpy as np
from scipy import stats
import json

# --- Main Analysis Function ---
def perform_character_analysis(scores_df, evidence_df):
    """
    Performs a comprehensive statistical analysis based on the Character Assessment Framework v6.0.

    Args:
        scores_df (pd.DataFrame): DataFrame containing character dimension scores.
        evidence_df (pd.DataFrame): DataFrame containing supporting evidence quotes.

    Returns:
        dict: A dictionary containing the structured results of the analysis.
    """
    # Initialize the main result dictionary with required keys
    result_data = {
        'descriptive_stats': {},
        'hypothesis_tests': {},
        'correlations': {},
        'reliability_metrics': {}
    }

    try:
        # --- 1. Framework Configuration & Data Validation ---
        # Define dimensions and opposing pairs based on the framework
        virtues = ["dignity", "truth", "justice", "hope", "pragmatism"]
        vices = ["tribalism", "manipulation", "resentment", "fear", "fantasy"]
        opposing_pairs = {
            "dignity": "tribalism",
            "truth": "manipulation",
            "justice": "resentment",
            "hope": "fear",
            "pragmatism": "fantasy"
        }
        all_dims = virtues + vices

        # Check for minimum required columns in scores_df
        required_score_cols = [f"{dim}_{metric}" for dim in all_dims for metric in ["score", "salience"]]
        if not all(col in scores_df.columns for col in required_score_cols):
            raise ValueError("Scores DataFrame is missing required score or salience columns.")

        # --- 2. Calculate Framework-Specific Metrics ---
        # Calculate Character Tension Scores for each opposing pair
        tension_cols = []
        for virtue, vice in opposing_pairs.items():
            tension_col_name = f"{virtue}_{vice}_tension"
            tension_cols.append(tension_col_name)

            virtue_score = scores_df[f"{virtue}_score"]
            vice_score = scores_df[f"{vice}_score"]
            virtue_salience = scores_df[f"{virtue}_salience"]
            vice_salience = scores_df[f"{vice}_salience"]

            # Formula: Character Tension = min(Virtue_score, Vice_score) * |Virtue_salience - Vice_salience|
            min_score = np.minimum(virtue_score, vice_score)
            salience_diff = np.abs(virtue_salience - vice_salience)
            scores_df[tension_col_name] = min_score * salience_diff

        # Calculate Moral Character Strategic Contradiction Index (MC-SCI)
        # Formula: MC-SCI = (Sum of all Character Tension Scores) / Number of Opposing Pairs
        if tension_cols:
            scores_df['mc_sci'] = scores_df[tension_cols].mean(axis=1)

        # --- 3. Descriptive Statistics ---
        scores_stats = scores_df.describe().to_dict()

        evidence_stats = {
            'record_count': int(evidence_df.shape[0]),
            'quotes_per_dimension': evidence_df['dimension'].value_counts().to_dict(),
            'confidence_score_stats': evidence_df['confidence_score'].describe().to_dict(),
            'quote_length_stats': evidence_df['quote_text'].str.len().describe().to_dict()
        }

        result_data['descriptive_stats'] = {
            'scores_and_metrics': scores_stats,
            'evidence': evidence_stats,
            'sample_size': int(scores_df.shape[0])
        }

        # --- 4. Reliability Analysis (Cronbach's Alpha) ---
        def calculate_cronbach_alpha(df_items):
            """Calculates Cronbach's Alpha for a set of items (columns)."""
            if df_items.shape[0] < 2 or df_items.shape[1] < 2:
                return np.nan # Not enough data to calculate
            
            k = df_items.shape[1]
            item_variances = df_items.var(axis=0, ddof=1).sum()
            total_variance = df_items.sum(axis=1).var(ddof=1)
            
            if total_variance == 0:
                return 1.0 if item_variances == 0 else 0.0
                
            return (k / (k - 1)) * (1 - (item_variances / total_variance))

        virtue_scores = scores_df[[f"{v}_score" for v in virtues]]
        vice_scores = scores_df[[f"{v}_score" for v in vices]]

        alpha_virtues = calculate_cronbach_alpha(virtue_scores)
        alpha_vices = calculate_cronbach_alpha(vice_scores)

        result_data['reliability_metrics'] = {
            'cronbach_alpha_virtues': alpha_virtues,
            'cronbach_alpha_vices': alpha_vices,
            'interpretation': "Cronbach's Alpha measures internal consistency. Values > 0.7 are generally considered good. This assesses if the virtue/vice dimensions cohere as a scale."
        }

        # --- 5. Correlation Analysis ---
        score_cols = [f"{dim}_score" for dim in all_dims]
        correlation_matrix = scores_df[score_cols].corr()

        opposing_pair_corrs = {}
        for virtue, vice in opposing_pairs.items():
            corr_val = correlation_matrix.loc[f"{virtue}_score", f"{vice}_score"]
            opposing_pair_corrs[f"{virtue}_vs_{vice}"] = corr_val

        result_data['correlations'] = {
            'full_correlation_matrix': correlation_matrix.to_dict(),
            'opposing_pair_correlations': opposing_pair_corrs
        }
        
        # --- 6. Hypothesis Testing ---
        # Test if there is a significant difference between mean virtue scores and mean vice scores.
        # H0: The mean of virtue scores is equal to the mean of vice scores.
        # H1: The mean of virtue scores is not equal to the mean of vice scores.
        scores_df['mean_virtue_score'] = virtue_scores.mean(axis=1)
        scores_df['mean_vice_score'] = vice_scores.mean(axis=1)

        # Paired t-test is appropriate as virtue/vice scores are from the same subject (artifact).
        if scores_df.shape[0] >= 2:
            ttest_result = stats.ttest_rel(scores_df['mean_virtue_score'], scores_df['mean_vice_score'], nan_policy='omit')
            
            # Calculate Cohen's d for effect size
            diff = scores_df['mean_virtue_score'] - scores_df['mean_vice_score']
            std_diff = np.std(diff, ddof=1)
            cohens_d = np.mean(diff) / std_diff if std_diff != 0 else 0.0

            result_data['hypothesis_tests'] = {
                'virtue_vs_vice_means_ttest': {
                    'description': "Paired t-test comparing the mean of all virtue scores to the mean of all vice scores per artifact.",
                    't_statistic': ttest_result.statistic,
                    'p_value': ttest_result.pvalue,
                    'degrees_of_freedom': ttest_result.df if hasattr(ttest_result, 'df') else scores_df.shape[0] - 1,
                    'cohens_d_effect_size': cohens_d,
                    'interpretation': f"A low p-value (e.g., < 0.05) suggests a statistically significant difference between the expression of virtues and vices. Cohen's d measures the size of this difference."
                }
            }
        else:
            result_data['hypothesis_tests']['virtue_vs_vice_means_ttest'] = {
                'status': 'skipped',
                'reason': 'Not enough data for a paired t-test (requires at least 2 samples).'
            }

    except (KeyError, ValueError, TypeError) as e:
        # Populate the result dictionary with error information for graceful failure
        result_data['error'] = {
            'type': type(e).__name__,
            'message': str(e),
            'details': "The analysis could not be completed due to missing data, incorrect data types, or other data-related issues. Please check the input DataFrames."
        }
    except Exception as e:
         result_data['error'] = {
            'type': type(e).__name__,
            'message': str(e),
            'details': "An unexpected error occurred during the analysis."
        }
    
    return result_data

# Execute the analysis using the pre-loaded DataFrames
# The 'scores_df' and 'evidence_df' variables are assumed to be pre-populated in the execution environment.
result_data = perform_character_analysis(scores_df, evidence_df)

# The 'result_data' variable now holds the complete analysis output as a dictionary.
# This dictionary will be captured by the downstream pipeline.