{"stage_1_raw_data": {"analysis_plan": {"stage": "raw_data_collection", "experiment_summary": "This plan outlines the collection and validation of raw data from the LLM analysis phase for the speaker_character_pattern_analysis experiment. The focus is on capturing the 10 dimensional scores (virtues and vices), along with their corresponding salience and confidence values, for each of the 8 documents as specified by the Character Assessment Framework v6.1. The plan includes steps to validate data completeness and ensure all collected values fall within their expected ranges (0.0-1.0) before proceeding to Stage 2 analysis.", "tasks": {"validate_dimensional_scores": {"tool": "calculate_descriptive_stats", "parameters": {"columns": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score"]}, "purpose": "To verify that all 10 raw dimensional scores specified in the CAF v6.1 output contract have been collected for each document and to perform an initial quality check on their statistical properties (min, max, mean) to ensure they are within the valid 0.0-1.0 range."}, "validate_salience_metadata": {"tool": "calculate_descriptive_stats", "parameters": {"columns": ["dignity_salience", "truth_salience", "justice_salience", "hope_salience", "pragmatism_salience", "tribalism_salience", "manipulation_salience", "resentment_salience", "fear_salience", "fantasy_salience"]}, "purpose": "To confirm the collection and validity of the salience scores for each of the 10 dimensions. This is a critical raw data point required for Stage 2 character tension calculations, and this step ensures the data is present and within the expected 0.0-1.0 range."}, "validate_confidence_metadata": {"tool": "calculate_descriptive_stats", "parameters": {"columns": ["dignity_confidence", "truth_confidence", "justice_confidence", "hope_confidence", "pragmatism_confidence", "tribalism_confidence", "manipulation_confidence", "resentment_confidence", "fear_confidence", "fantasy_confidence"]}, "purpose": "To validate the collection of the LLM's confidence scores for each dimensional assessment, as required by the framework's output contract. This step ensures the data is complete and values are within the 0.0-1.0 range, providing a measure of the reliability of the raw scores."}, "generate_overall_raw_data_summary": {"tool": "create_summary_statistics", "parameters": {"metrics": ["dignity_score", "dignity_salience", "dignity_confidence", "truth_score", "truth_salience", "truth_confidence", "justice_score", "justice_salience", "justice_confidence", "hope_score", "hope_salience", "hope_confidence", "pragmatism_score", "pragmatism_salience", "pragmatism_confidence", "tribalism_score", "tribalism_salience", "tribalism_confidence", "manipulation_score", "manipulation_salience", "manipulation_confidence", "resentment_score", "resentment_salience", "resentment_confidence", "fear_score", "fear_salience", "fear_confidence", "fantasy_score", "fantasy_salience", "fantasy_confidence"], "summary_types": ["count", "mean", "std", "min", "max"]}, "purpose": "To create a comprehensive summary of all 30 raw numerical data points collected from the LLM. This serves as a final validation check to confirm data completeness (count should equal the number of documents) and adherence to value constraints across the entire raw dataset."}}}, "results": {"validate_dimensional_scores": {"type": "descriptive_stats", "columns_analyzed": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score"], "results": {"dignity_score": {"count": 7, "mean": 0.7071428571428572, "std": 0.11700630833624397, "min": 0.6, "max": 0.9, "median": 0.7, "q25": 0.6, "q75": 0.775, "skewness": 0.6354172449685459, "kurtosis": -0.7777542533081299}, "truth_score": {"count": 8, "mean": 0.6125, "std": 0.15294723646688466, "min": 0.4, "max": 0.85, "median": 0.625, "q25": 0.55, "q75": 0.7, "skewness": -0.23457673001622417, "kurtosis": -0.27672047083503326}, "justice_score": {"count": 8, "mean": 0.73125, "std": 0.15797264681854625, "min": 0.5, "max": 0.9, "median": 0.775, "q25": 0.6499999999999999, "q75": 0.8250000000000001, "skewness": -0.70039978049317, "kurtosis": -0.8331514556085038}, "hope_score": {"count": 8, "mean": 0.6475, "std": 0.23614462880676687, "min": 0.2, "max": 0.9, "median": 0.7, "q25": 0.5, "q75": 0.8200000000000001, "skewness": -0.9189397658129868, "kurtosis": 0.45756744270958816}, "pragmatism_score": {"count": 8, "mean": 0.5249999999999999, "std": 0.12817398889233114, "min": 0.3, "max": 0.7, "median": 0.55, "q25": 0.475, "q75": 0.6, "skewness": -0.6105830850825585, "kurtosis": -0.021172022684313063}, "tribalism_score": {"count": 8, "mean": 0.4125, "std": 0.18850918886280923, "min": 0.1, "max": 0.7, "median": 0.4, "q25": 0.3, "q75": 0.525, "skewness": -0.06664297982574521, "kurtosis": -0.01725208959369784}, "manipulation_score": {"count": 8, "mean": 0.40625, "std": 0.2111828929760038, "min": 0.15, "max": 0.7, "median": 0.4, "q25": 0.2, "q75": 0.6, "skewness": 0.1116263407578763, "kurtosis": -1.763594224855487}, "resentment_score": {"count": 8, "mean": 0.39375000000000004, "std": 0.2043063176423368, "min": 0.05, "max": 0.7, "median": 0.35, "q25": 0.3, "q75": 0.525, "skewness": -0.09815396884072479, "kurtosis": 0.005367954473961767}, "fear_score": {"count": 8, "mean": 0.25625000000000003, "std": 0.1347948176197544, "min": 0.1, "max": 0.5, "median": 0.2, "q25": 0.1875, "q75": 0.325, "skewness": 0.9323479781707691, "kurtosis": -0.004124383485562433}, "fantasy_score": {"count": 8, "mean": 0.175, "std": 0.10350983390135313, "min": 0.0, "max": 0.3, "median": 0.2, "q25": 0.1, "q75": 0.225, "skewness": -0.3864367132317181, "kurtosis": -0.4479999999999986}}}, "validate_salience_metadata": {"type": "descriptive_stats", "columns_analyzed": ["dignity_salience", "truth_salience", "justice_salience", "hope_salience", "pragmatism_salience", "tribalism_salience", "manipulation_salience", "resentment_salience", "fear_salience", "fantasy_salience"], "results": {"dignity_salience": {"count": 7, "mean": 0.7571428571428572, "std": 0.09322272357358044, "min": 0.7, "max": 0.95, "median": 0.7, "q25": 0.7, "q75": 0.775, "skewness": 1.8735510780128417, "kurtosis": 3.432351285419381}, "truth_salience": {"count": 8, "mean": 0.675, "std": 0.13887301496588275, "min": 0.4, "max": 0.8, "median": 0.7, "q25": 0.6, "q75": 0.8, "skewness": -1.1201280219470378, "kurtosis": 1.1061728395061703}, "justice_salience": {"count": 8, "mean": 0.7875, "std": 0.18850918886280923, "min": 0.5, "max": 0.95, "median": 0.875, "q25": 0.6875, "q75": 0.9125, "skewness": -1.0209704509304138, "kurtosis": -0.719779803540316}, "hope_salience": {"count": 8, "mean": 0.6625, "std": 0.24892626561752318, "min": 0.2, "max": 0.9, "median": 0.75, "q25": 0.55, "q75": 0.85, "skewness": -1.0870897317668808, "kurtosis": 0.14808195400676105}, "pragmatism_salience": {"count": 8, "mean": 0.51875, "std": 0.14623244705409455, "min": 0.3, "max": 0.75, "median": 0.55, "q25": 0.4, "q75": 0.6, "skewness": 0.009279703405700527, "kurtosis": -0.6426401558570607}, "tribalism_salience": {"count": 8, "mean": 0.42500000000000004, "std": 0.17525491637693283, "min": 0.1, "max": 0.7, "median": 0.45, "q25": 0.375, "q75": 0.5, "skewness": -0.5042488670228694, "kurtosis": 1.3568415359653878}, "manipulation_salience": {"count": 8, "mean": 0.4749999999999999, "std": 0.21380899352993948, "min": 0.15, "max": 0.7, "median": 0.55, "q25": 0.2875, "q75": 0.625, "skewness": -0.4750150979302847, "kurtosis": -1.5741455078125002}, "resentment_salience": {"count": 8, "mean": 0.41875, "std": 0.19628241315585487, "min": 0.05, "max": 0.7, "median": 0.4, "q25": 0.375, "q75": 0.525, "skewness": -0.5817875780138718, "kurtosis": 1.121535402252242}, "fear_salience": {"count": 8, "mean": 0.28125, "std": 0.09977653603356425, "min": 0.1, "max": 0.4, "median": 0.3, "q25": 0.2375, "q75": 0.325, "skewness": -0.6044898304046372, "kurtosis": 0.3646323071045048}, "fantasy_salience": {"count": 8, "mean": 0.20625, "std": 0.126596941962615, "min": 0.0, "max": 0.4, "median": 0.2, "q25": 0.1375, "q75": 0.3, "skewness": -0.1155158663790897, "kurtosis": -0.21386550383687286}}}, "validate_confidence_metadata": {"type": "descriptive_stats", "columns_analyzed": ["dignity_confidence", "truth_confidence", "justice_confidence", "hope_confidence", "pragmatism_confidence", "tribalism_confidence", "manipulation_confidence", "resentment_confidence", "fear_confidence", "fantasy_confidence"], "results": {"dignity_confidence": {"count": 7, "mean": 0.7928571428571428, "std": 0.10177004891982151, "min": 0.7, "max": 0.9, "median": 0.75, "q25": 0.7, "q75": 0.9, "skewness": 0.267675800488283, "kurtosis": -2.6951248513674186}, "truth_confidence": {"count": 8, "mean": 0.7687499999999999, "std": 0.11933596033288303, "min": 0.6, "max": 0.95, "median": 0.8, "q25": 0.7125, "q75": 0.8125, "skewness": -0.34017919805586205, "kurtosis": -0.2550132172443309}, "justice_confidence": {"count": 8, "mean": 0.8187499999999999, "std": 0.14126343172547218, "min": 0.5, "max": 0.95, "median": 0.85, "q25": 0.8, "q75": 0.875, "skewness": -1.8774322889140391, "kurtosis": 4.501490923832252}, "hope_confidence": {"count": 8, "mean": 0.765, "std": 0.1461897006338975, "min": 0.6, "max": 0.95, "median": 0.75, "q25": 0.6375, "q75": 0.905, "skewness": 0.10845917943952225, "kurtosis": -2.079485580085219}, "pragmatism_confidence": {"count": 8, "mean": 0.65625, "std": 0.11783008347374015, "min": 0.5, "max": 0.85, "median": 0.7, "q25": 0.575, "q75": 0.7, "skewness": -0.023195425908955142, "kurtosis": -0.22939382347163573}, "tribalism_confidence": {"count": 8, "mean": 0.6499999999999999, "std": 0.13093073414159542, "min": 0.4, "max": 0.8, "median": 0.6499999999999999, "q25": 0.6, "q75": 0.725, "skewness": -0.7637626158259697, "kurtosis": 0.8749999999999947}, "manipulation_confidence": {"count": 8, "mean": 0.6499999999999999, "std": 0.13627702877384937, "min": 0.5, "max": 0.85, "median": 0.675, "q25": 0.5, "q75": 0.75, "skewness": 0.0, "kurtosis": -1.5946745562130182}, "resentment_confidence": {"count": 8, "mean": 0.64375, "std": 0.14252192813739223, "min": 0.4, "max": 0.8, "median": 0.7, "q25": 0.5375000000000001, "q75": 0.75, "skewness": -0.7702689769562403, "kurtosis": -0.823885038038882}, "fear_confidence": {"count": 8, "mean": 0.5187499999999999, "std": 0.16889874396894047, "min": 0.3, "max": 0.7, "median": 0.55, "q25": 0.375, "q75": 0.6625, "skewness": -0.3127134054837214, "kurtosis": -1.8339571072758938}, "fantasy_confidence": {"count": 8, "mean": 0.5062500000000001, "std": 0.20430631764233684, "min": 0.3, "max": 0.9, "median": 0.45, "q25": 0.375, "q75": 0.6125, "skewness": 1.0027409456768288, "kurtosis": 0.6916462009208111}}}, "generate_overall_raw_data_summary": {"type": "summary_statistics", "metrics": ["dignity_score", "dignity_salience", "dignity_confidence", "truth_score", "truth_salience", "truth_confidence", "justice_score", "justice_salience", "justice_confidence", "hope_score", "hope_salience", "hope_confidence", "pragmatism_score", "pragmatism_salience", "pragmatism_confidence", "tribalism_score", "tribalism_salience", "tribalism_confidence", "manipulation_score", "manipulation_salience", "manipulation_confidence", "resentment_score", "resentment_salience", "resentment_confidence", "fear_score", "fear_salience", "fear_confidence", "fantasy_score", "fantasy_salience", "fantasy_confidence"], "summary_types": ["count", "mean", "std", "min", "max"], "results": {"dignity_score": {"mean": 0.7071428571428572, "std": 0.11700630833624397, "min": 0.6, "max": 0.9, "count": 7}, "dignity_salience": {"mean": 0.7571428571428572, "std": 0.09322272357358044, "min": 0.7, "max": 0.95, "count": 7}, "dignity_confidence": {"mean": 0.7928571428571428, "std": 0.10177004891982151, "min": 0.7, "max": 0.9, "count": 7}, "truth_score": {"mean": 0.6125, "std": 0.15294723646688466, "min": 0.4, "max": 0.85, "count": 8}, "truth_salience": {"mean": 0.675, "std": 0.13887301496588275, "min": 0.4, "max": 0.8, "count": 8}, "truth_confidence": {"mean": 0.7687499999999999, "std": 0.11933596033288303, "min": 0.6, "max": 0.95, "count": 8}, "justice_score": {"mean": 0.73125, "std": 0.15797264681854625, "min": 0.5, "max": 0.9, "count": 8}, "justice_salience": {"mean": 0.7875, "std": 0.18850918886280923, "min": 0.5, "max": 0.95, "count": 8}, "justice_confidence": {"mean": 0.8187499999999999, "std": 0.14126343172547218, "min": 0.5, "max": 0.95, "count": 8}, "hope_score": {"mean": 0.6475, "std": 0.23614462880676687, "min": 0.2, "max": 0.9, "count": 8}, "hope_salience": {"mean": 0.6625, "std": 0.24892626561752318, "min": 0.2, "max": 0.9, "count": 8}, "hope_confidence": {"mean": 0.765, "std": 0.1461897006338975, "min": 0.6, "max": 0.95, "count": 8}, "pragmatism_score": {"mean": 0.5249999999999999, "std": 0.12817398889233114, "min": 0.3, "max": 0.7, "count": 8}, "pragmatism_salience": {"mean": 0.51875, "std": 0.14623244705409455, "min": 0.3, "max": 0.75, "count": 8}, "pragmatism_confidence": {"mean": 0.65625, "std": 0.11783008347374015, "min": 0.5, "max": 0.85, "count": 8}, "tribalism_score": {"mean": 0.4125, "std": 0.18850918886280923, "min": 0.1, "max": 0.7, "count": 8}, "tribalism_salience": {"mean": 0.42500000000000004, "std": 0.17525491637693283, "min": 0.1, "max": 0.7, "count": 8}, "tribalism_confidence": {"mean": 0.6499999999999999, "std": 0.13093073414159542, "min": 0.4, "max": 0.8, "count": 8}, "manipulation_score": {"mean": 0.40625, "std": 0.2111828929760038, "min": 0.15, "max": 0.7, "count": 8}, "manipulation_salience": {"mean": 0.4749999999999999, "std": 0.21380899352993948, "min": 0.15, "max": 0.7, "count": 8}, "manipulation_confidence": {"mean": 0.6499999999999999, "std": 0.13627702877384937, "min": 0.5, "max": 0.85, "count": 8}, "resentment_score": {"mean": 0.39375000000000004, "std": 0.2043063176423368, "min": 0.05, "max": 0.7, "count": 8}, "resentment_salience": {"mean": 0.41875, "std": 0.19628241315585487, "min": 0.05, "max": 0.7, "count": 8}, "resentment_confidence": {"mean": 0.64375, "std": 0.14252192813739223, "min": 0.4, "max": 0.8, "count": 8}, "fear_score": {"mean": 0.25625000000000003, "std": 0.1347948176197544, "min": 0.1, "max": 0.5, "count": 8}, "fear_salience": {"mean": 0.28125, "std": 0.09977653603356425, "min": 0.1, "max": 0.4, "count": 8}, "fear_confidence": {"mean": 0.5187499999999999, "std": 0.16889874396894047, "min": 0.3, "max": 0.7, "count": 8}, "fantasy_score": {"mean": 0.175, "std": 0.10350983390135313, "min": 0.0, "max": 0.3, "count": 8}, "fantasy_salience": {"mean": 0.20625, "std": 0.126596941962615, "min": 0.0, "max": 0.4, "count": 8}, "fantasy_confidence": {"mean": 0.5062500000000001, "std": 0.20430631764233684, "min": 0.3, "max": 0.9, "count": 8}}, "missing_metrics": []}}, "errors": []}, "stage_2_derived_metrics": {"analysis_plan": {"stage": "derived_metrics_analysis", "experiment_summary": "This plan outlines Stage 2 of the analysis, focusing on calculating derived metrics from raw CAF v6.1 scores and conducting statistical tests to evaluate the research hypotheses. Key activities include calculating Character Tension scores and the Moral Character Strategic Contradiction Index (MC-SCI), followed by ANOVA and correlation analyses to test for speaker differentiation (H1), unique character signatures (H2), and variations in character coherence (H3) based on dimensional scores.", "tasks": {"task_01_calculate_derived_metrics": {"tool": "calculate_derived_metrics", "parameters": {"metric_formulas": {"dignity_tribalism_tension": "min(dignity_score, tribalism_score) * abs(dignity_salience - tribalism_salience)", "truth_manipulation_tension": "min(truth_score, manipulation_score) * abs(truth_salience - manipulation_salience)", "justice_resentment_tension": "min(justice_score, resentment_score) * abs(justice_salience - resentment_salience)", "hope_fear_tension": "min(hope_score, fear_score) * abs(hope_salience - fear_salience)", "pragmatism_fantasy_tension": "min(pragmatism_score, fantasy_score) * abs(pragmatism_salience - fantasy_salience)", "mc_sci": "(dignity_tribalism_tension + truth_manipulation_tension + justice_resentment_tension + hope_fear_tension + pragmatism_fantasy_tension) / 5"}, "input_columns": ["dignity_score", "dignity_salience", "tribalism_score", "tribalism_salience", "truth_score", "truth_salience", "manipulation_score", "manipulation_salience", "justice_score", "justice_salience", "resentment_score", "resentment_salience", "hope_score", "hope_salience", "fear_score", "fear_salience", "pragmatism_score", "pragmatism_salience", "fantasy_score", "fantasy_salience"]}, "purpose": "To calculate the core derived metrics (Character Tensions and MC-SCI) as specified by the CAF v6.1 framework. These metrics are essential for testing H3 and understanding character coherence."}, "task_02_validate_calculated_metrics": {"tool": "validate_calculated_metrics", "parameters": {"validation_rules": ["missing_data_check", "range_check"], "quality_thresholds": {"mc_sci": [0.0, 1.0], "dignity_tribalism_tension": [0.0, 1.0], "truth_manipulation_tension": [0.0, 1.0], "justice_resentment_tension": [0.0, 1.0], "hope_fear_tension": [0.0, 1.0], "pragmatism_fantasy_tension": [0.0, 1.0]}}, "purpose": "To ensure the integrity and reliability of the newly calculated derived metrics by checking for missing values and verifying that they fall within their expected theoretical ranges (0.0 to 1.0)."}, "task_03_descriptive_statistics_overview": {"tool": "calculate_descriptive_stats", "parameters": {"columns": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score", "mc_sci"]}, "purpose": "To generate summary statistics for all raw dimensional scores and the calculated MC-SCI, providing a foundational understanding of the data's distribution, central tendency, and variance across the corpus."}, "task_04_test_hypothesis_h1_differentiation": {"tool": "perform_one_way_anova", "parameters": {"grouping_variable": "tribalism_score", "dependent_variable": "dignity_score"}, "purpose": "To test H1 (Speaker Differentiation) by performing an ANOVA. Since direct speaker metadata is unavailable, this test will determine if documents can be differentiated based on their character profiles, for example, by testing if the level of 'Tribalism' significantly affects the level of 'Dignity'."}, "task_05_test_hypothesis_h2_signatures": {"tool": "generate_correlation_matrix", "parameters": {"dimensions": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score"], "correlation_method": "pearson"}, "purpose": "To test H2 (Character Signatures) by analyzing the inter-correlations between the 10 CAF dimensions. This will reveal patterns of association between virtues and vices, helping to identify coherent character profiles within the data."}, "task_06_test_hypothesis_h3_coherence": {"tool": "perform_one_way_anova", "parameters": {"grouping_variable": "resentment_score", "dependent_variable": "mc_sci"}, "purpose": "To test H3 (MC-SCI Patterns) by using ANOVA to determine if the MC-SCI score, representing character coherence, varies meaningfully between groups of documents categorized by a key dimensional score, such as 'Resentment'."}, "task_07_explore_factorial_interactions": {"tool": "perform_two_way_anova", "parameters": {"factor1": "hope_score", "factor2": "fear_score", "dependent_variable": "mc_sci"}, "purpose": "To explore the interaction effects between character dimensions, reflecting the experiment's factorial design using dimensional data. This test examines if the relationship between a virtue (Hope) and character coherence (MC-SCI) is moderated by the presence of its opposing vice (Fear)."}}}, "results": {"task_01_calculate_derived_metrics": {"type": "derived_metrics_calculation", "success": false, "calculated_metrics": {}, "successful_calculations": [], "failed_calculations": [{"metric": "hope_fear_tension", "formula": "min(hope_score, fear_score) * abs(hope_salience - fear_salience)", "error": "name 'min' is not defined"}, {"metric": "dignity_tribalism_tension", "formula": "min(dignity_score, tribalism_score) * abs(dignity_salience - tribalism_salience)", "error": "name 'min' is not defined"}, {"metric": "truth_manipulation_tension", "formula": "min(truth_score, manipulation_score) * abs(truth_salience - manipulation_salience)", "error": "name 'min' is not defined"}, {"metric": "justice_resentment_tension", "formula": "min(justice_score, resentment_score) * abs(justice_salience - resentment_salience)", "error": "name 'min' is not defined"}, {"metric": "pragmatism_fantasy_tension", "formula": "min(pragmatism_score, fantasy_score) * abs(pragmatism_salience - fantasy_salience)", "error": "name 'min' is not defined"}, {"metric": "mc_sci", "formula": "(dignity_tribalism_tension + truth_manipulation_tension + justice_resentment_tension + hope_fear_tension + pragmatism_fantasy_tension) / 5", "error": "name 'dignity_tribalism_tension' is not defined"}], "formulas_used": ["dignity_tribalism_tension", "truth_manipulation_tension", "justice_resentment_tension", "hope_fear_tension", "pragmatism_fantasy_tension", "mc_sci"], "input_columns": ["dignity_score", "dignity_salience", "tribalism_score", "tribalism_salience", "truth_score", "truth_salience", "manipulation_score", "manipulation_salience", "justice_score", "justice_salience", "resentment_score", "resentment_salience", "hope_score", "hope_salience", "fear_score", "fear_salience", "pragmatism_score", "pragmatism_salience", "fantasy_score", "fantasy_salience"], "total_metrics": 6, "success_rate": 0.0}, "task_02_validate_calculated_metrics": {"type": "metric_validation", "validation_rules": ["missing_data_check", "range_check"], "results": {"missing_data_check": {"status": "completed", "missing_data_by_column": {"aid": 0, "dignity_score": 1, "dignity_raw_score": 1, "dignity_salience": 1, "dignity_confidence": 1, "truth_score": 0, "truth_raw_score": 0, "truth_salience": 0, "truth_confidence": 0, "justice_score": 0, "justice_raw_score": 0, "justice_salience": 0, "justice_confidence": 0, "hope_score": 0, "hope_raw_score": 0, "hope_salience": 0, "hope_confidence": 0, "pragmatism_score": 0, "pragmatism_raw_score": 0, "pragmatism_salience": 0, "pragmatism_confidence": 0, "tribalism_score": 0, "tribalism_raw_score": 0, "tribalism_salience": 0, "tribalism_confidence": 0, "manipulation_score": 0, "manipulation_raw_score": 0, "manipulation_salience": 0, "manipulation_confidence": 0, "resentment_score": 0, "resentment_raw_score": 0, "resentment_salience": 0, "resentment_confidence": 0, "fear_score": 0, "fear_raw_score": 0, "fear_salience": 0, "fear_confidence": 0, "fantasy_score": 0, "fantasy_raw_score": 0, "fantasy_salience": 0, "fantasy_confidence": 0, "diginity_score": 7, "diginity_raw_score": 7, "diginity_salience": 7, "diginity_confidence": 7}, "total_missing": 32}, "range_check": {"status": "completed", "ranges": {"dignity_score": {"min": 0.6, "max": 0.9, "mean": 0.7071428571428572}, "dignity_raw_score": {"min": 0.6, "max": 0.9, "mean": 0.7071428571428572}, "dignity_salience": {"min": 0.7, "max": 0.95, "mean": 0.7571428571428571}, "dignity_confidence": {"min": 0.7, "max": 0.9, "mean": 0.7928571428571428}, "truth_score": {"min": 0.4, "max": 0.85, "mean": 0.6125}, "truth_raw_score": {"min": 0.4, "max": 0.85, "mean": 0.6125}, "truth_salience": {"min": 0.4, "max": 0.8, "mean": 0.675}, "truth_confidence": {"min": 0.6, "max": 0.95, "mean": 0.7687499999999999}, "justice_score": {"min": 0.5, "max": 0.9, "mean": 0.73125}, "justice_raw_score": {"min": 0.5, "max": 0.9, "mean": 0.73125}, "justice_salience": {"min": 0.5, "max": 0.95, "mean": 0.7875}, "justice_confidence": {"min": 0.5, "max": 0.95, "mean": 0.8187499999999999}, "hope_score": {"min": 0.2, "max": 0.9, "mean": 0.6475}, "hope_raw_score": {"min": 0.2, "max": 0.9, "mean": 0.6475}, "hope_salience": {"min": 0.2, "max": 0.9, "mean": 0.6625}, "hope_confidence": {"min": 0.6, "max": 0.95, "mean": 0.765}, "pragmatism_score": {"min": 0.3, "max": 0.7, "mean": 0.5249999999999999}, "pragmatism_raw_score": {"min": 0.3, "max": 0.7, "mean": 0.5249999999999999}, "pragmatism_salience": {"min": 0.3, "max": 0.75, "mean": 0.51875}, "pragmatism_confidence": {"min": 0.5, "max": 0.85, "mean": 0.65625}, "tribalism_score": {"min": 0.1, "max": 0.7, "mean": 0.4125}, "tribalism_raw_score": {"min": 0.1, "max": 0.7, "mean": 0.4125}, "tribalism_salience": {"min": 0.1, "max": 0.7, "mean": 0.42500000000000004}, "tribalism_confidence": {"min": 0.4, "max": 0.8, "mean": 0.6499999999999999}, "manipulation_score": {"min": 0.15, "max": 0.7, "mean": 0.40625}, "manipulation_raw_score": {"min": 0.15, "max": 0.7, "mean": 0.40625}, "manipulation_salience": {"min": 0.15, "max": 0.7, "mean": 0.4749999999999999}, "manipulation_confidence": {"min": 0.5, "max": 0.85, "mean": 0.6499999999999999}, "resentment_score": {"min": 0.05, "max": 0.7, "mean": 0.39375000000000004}, "resentment_raw_score": {"min": 0.05, "max": 0.7, "mean": 0.39375000000000004}, "resentment_salience": {"min": 0.05, "max": 0.7, "mean": 0.41875}, "resentment_confidence": {"min": 0.4, "max": 0.8, "mean": 0.64375}, "fear_score": {"min": 0.1, "max": 0.5, "mean": 0.25625000000000003}, "fear_raw_score": {"min": 0.1, "max": 0.5, "mean": 0.25625000000000003}, "fear_salience": {"min": 0.1, "max": 0.4, "mean": 0.28125}, "fear_confidence": {"min": 0.3, "max": 0.7, "mean": 0.5187499999999999}, "fantasy_score": {"min": 0.0, "max": 0.3, "mean": 0.175}, "fantasy_raw_score": {"min": 0.0, "max": 0.3, "mean": 0.175}, "fantasy_salience": {"min": 0.0, "max": 0.4, "mean": 0.20625}, "fantasy_confidence": {"min": 0.3, "max": 0.9, "mean": 0.5062500000000001}, "diginity_score": {"min": 0.3, "max": 0.3, "mean": 0.3}, "diginity_raw_score": {"min": 0.3, "max": 0.3, "mean": 0.3}, "diginity_salience": {"min": 0.3, "max": 0.3, "mean": 0.3}, "diginity_confidence": {"min": 0.7, "max": 0.7, "mean": 0.7}}}}, "quality_thresholds": {"mc_sci": [0.0, 1.0], "dignity_tribalism_tension": [0.0, 1.0], "truth_manipulation_tension": [0.0, 1.0], "justice_resentment_tension": [0.0, 1.0], "hope_fear_tension": [0.0, 1.0], "pragmatism_fantasy_tension": [0.0, 1.0]}}, "task_04_test_hypothesis_h1_differentiation": {"type": "one_way_anova", "grouping_variable": "tribalism_score", "dependent_variable": "dignity_score", "groups": {"0.1": {"n": 1, "mean": 0.9, "std": 0.0}, "0.3": {"n": 2, "mean": 0.6, "std": 0.0}, "0.4": {"n": 2, "mean": 0.775, "std": 0.025000000000000022}, "0.6": {"n": 1, "mean": 0.7, "std": 0.0}, "0.7": {"n": 1, "mean": 0.6, "std": 0.0}}, "f_statistic": 32.35714285714284, "p_value": 0.030203213610586026, "significant": "True"}, "task_05_test_hypothesis_h2_signatures": {"type": "correlation_matrix", "dimensions": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score"], "method": "pearson", "matrix": {"dignity_score": {"dignity_score": 1.0, "truth_score": 0.576770389298682, "justice_score": 0.4843884930464273, "hope_score": 0.8893427293057098, "pragmatism_score": 0.5421374765483945, "tribalism_score": -0.49854861812834383, "manipulation_score": -0.6651662082893139, "resentment_score": -0.2881837200291647, "fear_score": 0.061952247412989436, "fantasy_score": -0.9174634218511292}, "truth_score": {"dignity_score": 0.576770389298682, "truth_score": 1.0, "justice_score": 0.897975984067584, "hope_score": 0.6140643187067323, "pragmatism_score": 0.6740655909056726, "tribalism_score": -0.6007718129713515, "manipulation_score": -0.4671628644330198, "resentment_score": 0.20858419942357415, "fear_score": 0.11693133599279322, "fantasy_score": -0.7895629608911476}, "justice_score": {"dignity_score": 0.4843884930464273, "truth_score": 0.897975984067584, "justice_score": 1.0, "hope_score": 0.6093690745575551, "pragmatism_score": 0.7672721428435879, "tribalism_score": -0.42275297305978327, "manipulation_score": -0.35996778188527995, "resentment_score": 0.19503268660627737, "fear_score": 0.0062895293220607915, "fantasy_score": -0.7316835130490091}, "hope_score": {"dignity_score": 0.8893427293057098, "truth_score": 0.6140643187067323, "justice_score": 0.6093690745575551, "hope_score": 1.0, "pragmatism_score": 0.7952870179044798, "tribalism_score": -0.20779310668060388, "manipulation_score": -0.6971738961522156, "resentment_score": -0.14397984649651582, "fear_score": -0.2126179447022601, "fantasy_score": -0.7510094540244864}, "pragmatism_score": {"dignity_score": 0.5421374765483945, "truth_score": 0.6740655909056726, "justice_score": 0.7672721428435879, "hope_score": 0.7952870179044798, "pragmatism_score": 1.0, "tribalism_score": -0.31040509310661096, "manipulation_score": -0.5607538494404394, "resentment_score": -0.2932234722018026, "fear_score": -0.3410773006656662, "fantasy_score": -0.6998964726756151}, "tribalism_score": {"dignity_score": -0.49854861812834383, "truth_score": -0.6007718129713515, "justice_score": -0.42275297305978327, "hope_score": -0.20779310668060388, "pragmatism_score": -0.31040509310661096, "tribalism_score": 1.0, "manipulation_score": 0.48220218903683554, "resentment_score": 0.31760565283189346, "fear_score": 0.3619207488858103, "fantasy_score": 0.7504325943927066}, "manipulation_score": {"dignity_score": -0.6651662082893139, "truth_score": -0.4671628644330198, "justice_score": -0.35996778188527995, "hope_score": -0.6971738961522156, "pragmatism_score": -0.5607538494404394, "tribalism_score": 0.48220218903683554, "manipulation_score": 1.0, "resentment_score": 0.191418178326564, "fear_score": 0.21171606805422713, "fantasy_score": 0.72704557073721}, "resentment_score": {"dignity_score": -0.2881837200291647, "truth_score": 0.20858419942357415, "justice_score": 0.19503268660627737, "hope_score": -0.14397984649651582, "pragmatism_score": -0.2932234722018026, "tribalism_score": 0.31760565283189346, "manipulation_score": 0.191418178326564, "resentment_score": 1.0, "fear_score": 0.7408208383886864, "fantasy_score": 0.2617642051708646}, "fear_score": {"dignity_score": 0.061952247412989436, "truth_score": 0.11693133599279322, "justice_score": 0.0062895293220607915, "hope_score": -0.2126179447022601, "pragmatism_score": -0.3410773006656662, "tribalism_score": 0.3619207488858103, "manipulation_score": 0.21171606805422713, "resentment_score": 0.7408208383886864, "fear_score": 1.0, "fantasy_score": 0.268767319880733}, "fantasy_score": {"dignity_score": -0.9174634218511292, "truth_score": -0.7895629608911476, "justice_score": -0.7316835130490091, "hope_score": -0.7510094540244864, "pragmatism_score": -0.6998964726756151, "tribalism_score": 0.7504325943927066, "manipulation_score": 0.72704557073721, "resentment_score": 0.2617642051708646, "fear_score": 0.268767319880733, "fantasy_score": 1.0}}, "missing_dimensions": []}}, "errors": ["Task 'task_03_descriptive_statistics_overview' failed: Descriptive stats calculation failed: Column 'mc_sci' not found in DataFrame. Available columns: ['aid', 'dignity_score', 'dignity_raw_score', 'dignity_salience', 'dignity_confidence', 'truth_score', 'truth_raw_score', 'truth_salience', 'truth_confidence', 'justice_score', 'justice_raw_score', 'justice_salience', 'justice_confidence', 'hope_score', 'hope_raw_score', 'hope_salience', 'hope_confidence', 'pragmatism_score', 'pragmatism_raw_score', 'pragmatism_salience', 'pragmatism_confidence', 'tribalism_score', 'tribalism_raw_score', 'tribalism_salience', 'tribalism_confidence', 'manipulation_score', 'manipulation_raw_score', 'manipulation_salience', 'manipulation_confidence', 'resentment_score', 'resentment_raw_score', 'resentment_salience', 'resentment_confidence', 'fear_score', 'fear_raw_score', 'fear_salience', 'fear_confidence', 'fantasy_score', 'fantasy_raw_score', 'fantasy_salience', 'fantasy_confidence', 'diginity_score', 'diginity_raw_score', 'diginity_salience', 'diginity_confidence']", "Task 'task_06_test_hypothesis_h3_coherence' failed: ANOVA failed: Dependent variable 'mc_sci' not found in DataFrame", "Task 'task_07_explore_factorial_interactions' failed: Two-way ANOVA failed: Column 'mc_sci' not found in DataFrame"]}, "combined_summary": "Two-stage execution: 4 raw data results + 4 derived metrics results"}