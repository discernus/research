{
  "status": "success",
  "functions_generated": 7,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 22842,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: caf_civic_character_pattern_analysis\nDescription: Statistical analysis experiment\nGenerated: 2025-08-25T12:53:24.327279+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef _get_speaker_metadata(data):\n    \"\"\"\n    Internal helper to identify speakers and add metadata to the DataFrame.\n\n    This function first attempts to load a `corpus_manifest.json` file from the\n    current directory. The manifest is expected to be a dictionary mapping\n    'document_name' to a dictionary of metadata (e.g., {'speaker': 'John Doe', 'style': 'populist'}).\n\n    If the manifest is not found or is invalid, it gracefully falls back to parsing\n    the speaker's name from the 'document_name' column. The parsing logic assumes\n    the name consists of the parts of the filename before a four-digit year.\n    This handles names like 'steve_king' and 'alexandria_ocasio_cortez'.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with a 'document_name' column.\n\n    Returns:\n        pd.DataFrame: The DataFrame with an added 'speaker' column and any other\n                      metadata found in the manifest. Returns the original DataFrame\n                      if the 'speaker' column cannot be created.\n    \"\"\"\n    import pandas as pd\n    import json\n    import re\n\n    if 'document_name' not in data.columns:\n        return data\n\n    data_with_meta = data.copy()\n\n    try:\n        with open('corpus_manifest.json', 'r') as f:\n            manifest = json.load(f)\n        \n        # Assuming manifest is a dict like: {\"doc_name.txt\": {\"speaker\": \"...\", \"style\": \"...\"}}\n        meta_df = pd.DataFrame.from_dict(manifest, orient='index')\n        meta_df.index.name = 'document_name'\n        data_with_meta = data_with_meta.merge(meta_df, on='document_name', how='left')\n\n    except (FileNotFoundError, json.JSONDecodeError, KeyError):\n        # Graceful fallback to filename parsing\n        def extract_speaker_from_filename(filename):\n            if not isinstance(filename, str):\n                return \"unknown_speaker\"\n            # Find all parts before a 4-digit year-like number\n            match = re.match(r'([a-zA-Z_]+?)_(\\d{4})', filename)\n            if match:\n                return match.group(1)\n            # Fallback for filenames without a year\n            return '_'.join(filename.split('_')[:2])\n\n        data_with_meta['speaker'] = data_with_meta['document_name'].apply(extract_speaker_from_filename)\n\n    return data_with_meta\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived metrics as specified in the CAF v10.0 framework.\n\n    This function computes the five Character Tension Indices and the final\n    Salience-Weighted Civic Character Index. It uses the exact formulas\n    from the framework specification, operating directly on the provided\n    DataFrame columns.\n\n    Statistical Methodology:\n    - Tension Indices: For each axis, `Tension = min(Virtue_Score, Vice_Score) * abs(Virtue_Salience - Vice_Salience)`.\n      This captures the degree of strategic contradiction, penalizing cases where two opposing, high-intensity\n      themes are used with very different levels of emphasis.\n    - Civic Character Index: `(Sum(Virtue * Salience) - Sum(Vice * Salience)) / Total_Salience`. This provides a\n      normalized, weighted summary of the speaker's overall civic character, ranging from -1.0 (vice-dominant)\n      to +1.0 (virtue-dominant). A small epsilon (0.001) is added to the denominator to prevent division by zero.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw and salience scores for all 10 dimensions.\n        **kwargs: Additional parameters (unused).\n\n    Returns:\n        pd.DataFrame: The original DataFrame with added columns for each derived metric, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        df = data.copy()\n\n        # Define virtue and vice dimensions\n        virtues = ['dignity', 'truth', 'justice', 'hope', 'pragmatism']\n        vices = ['tribalism', 'manipulation', 'resentment', 'fear', 'fantasy']\n        axes = list(zip(virtues, vices))\n\n        # Calculate Tension Indices\n        df['identity_tension'] = np.minimum(df['dignity_raw'], df['tribalism_raw']) * np.abs(df['dignity_salience'] - df['tribalism_salience'])\n        df['truth_tension'] = np.minimum(df['truth_raw'], df['manipulation_raw']) * np.abs(df['truth_salience'] - df['manipulation_salience'])\n        df['justice_tension'] = np.minimum(df['justice_raw'], df['resentment_raw']) * np.abs(df['justice_salience'] - df['resentment_salience'])\n        df['emotional_tension'] = np.minimum(df['hope_raw'], df['fear_raw']) * np.abs(df['hope_salience'] - df['fear_salience'])\n        df['reality_tension'] = np.minimum(df['pragmatism_raw'], df['fantasy_raw']) * np.abs(df['pragmatism_salience'] - df['fantasy_salience'])\n\n        # Calculate Salience-Weighted Civic Character Index\n        weighted_virtue_score = sum(df[f'{dim}_raw'] * df[f'{dim}_salience'] for dim in virtues)\n        weighted_vice_score = sum(df[f'{dim}_raw'] * df[f'{dim}_salience'] for dim in vices)\n        \n        virtue_salience_total = sum(df[f'{dim}_salience'] for dim in virtues)\n        vice_salience_total = sum(df[f'{dim}_salience'] for dim in vices)\n        combined_salience_total = virtue_salience_total + vice_salience_total\n\n        # Add intermediate calculations for clarity\n        df['weighted_virtue_score'] = weighted_virtue_score\n        df['weighted_vice_score'] = weighted_vice_score\n        df['combined_salience_total'] = combined_salience_total\n\n        # Add a small epsilon to the denominator to avoid division by zero\n        df['civic_character_index'] = (weighted_virtue_score - weighted_vice_score) / (combined_salience_total + 0.001)\n\n        return df\n\n    except Exception:\n        return None\n\ndef get_speaker_character_profiles(data, **kwargs):\n    \"\"\"\n    Generates character profiles for each speaker by aggregating their scores.\n\n    This function addresses Research Questions 1 and 2 by creating a \"character signature\"\n    for each speaker. It first identifies speakers using the helper function `_get_speaker_metadata`.\n    Then, it groups the data by speaker and calculates the mean for all raw scores, salience scores,\n    and derived metrics (tension and civic character index).\n\n    Statistical Methodology:\n    The function uses grouping and aggregation (mean) to summarize the central tendency of each\n    speaker's rhetorical patterns across multiple documents. The resulting table provides a\n    comparative overview of how different speakers score on each dimension of the CAF framework.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis results.\n        **kwargs: Additional parameters (unused).\n\n    Returns:\n        pd.DataFrame: A DataFrame where each row is a speaker and columns are the mean\n                      scores for each dimension and derived metric. Returns None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # First, calculate derived metrics as they are needed for the profile\n        data_with_derived = calculate_derived_metrics(data)\n        if data_with_derived is None:\n            return None\n\n        # Then, get speaker metadata\n        df = _get_speaker_metadata(data_with_derived)\n        if 'speaker' not in df.columns:\n            return None # Cannot proceed without speaker identification\n\n        # Define all numeric columns to aggregate\n        score_cols = [col for col in df.columns if '_raw' in col or '_salience' in col]\n        derived_cols = [\n            'identity_tension', 'truth_tension', 'justice_tension', \n            'emotional_tension', 'reality_tension', 'civic_character_index'\n        ]\n        all_metrics_cols = score_cols + derived_cols\n        \n        # Ensure all columns exist before grouping\n        cols_to_aggregate = [col for col in all_metrics_cols if col in df.columns]\n        if not cols_to_aggregate:\n            return None\n\n        speaker_profiles = df.groupby('speaker')[cols_to_aggregate].mean().reset_index()\n        \n        return speaker_profiles.sort_values(by='civic_character_index', ascending=False)\n\n    except Exception:\n        return None\n\ndef analyze_character_coherence(data, **kwargs):\n    \"\"\"\n    Analyzes the civic character coherence of speakers using derived metrics and pattern classification.\n\n    This function addresses Research Question 3. It calculates the mean Civic Character Index and\n    Tension Indices for each speaker. It then applies a rule-based classification based on the\n    \"Interpretive Guidance\" from the framework to categorize each speaker's dominant rhetorical pattern\n    (e.g., 'Authentic Virtue', 'Strategic Pathology').\n\n    Statistical Methodology:\n    - Aggregation: Mean scores for the Civic Character Index and Tension Indices are calculated per speaker.\n    - Classification: A rule-based model classifies speaker profiles. For example:\n        - 'Authentic Virtue': High virtue scores/salience, low vice scores/salience.\n        - 'Strategic Pathology': High vice scores/salience, low virtue scores/salience.\n        - 'Incoherent Messaging': High tension scores, Civic Character Index near zero.\n        - 'Strategic Virtue Signaling': High virtue scores but low virtue salience.\n    The output provides a summary of each speaker's coherence and strategic orientation.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis results.\n        **kwargs: Additional parameters (unused).\n\n    Returns:\n        pd.DataFrame: A DataFrame summarizing the coherence metrics and pattern classification\n                      for each speaker. Returns None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # Calculate derived metrics and get speaker profiles\n        profiles = get_speaker_character_profiles(data)\n        if profiles is None:\n            return None\n\n        # Calculate mean virtue/vice scores and salience from the profiles\n        virtues = ['dignity', 'truth', 'justice', 'hope', 'pragmatism']\n        vices = ['tribalism', 'manipulation', 'resentment', 'fear', 'fantasy']\n        \n        profiles['mean_virtue_raw'] = profiles[[f'{v}_raw' for v in virtues]].mean(axis=1)\n        profiles['mean_virtue_salience'] = profiles[[f'{v}_salience' for v in virtues]].mean(axis=1)\n        profiles['mean_vice_raw'] = profiles[[f'{v}_raw' for v in vices]].mean(axis=1)\n        profiles['mean_vice_salience'] = profiles[[f'{v}_salience' for v in vices]].mean(axis=1)\n        profiles['mean_tension'] = profiles[['identity_tension', 'truth_tension', 'justice_tension', 'emotional_tension', 'reality_tension']].mean(axis=1)\n\n        def classify_pattern(row):\n            cci = row['civic_character_index']\n            tension = row['mean_tension']\n            virtue_raw = row['mean_virtue_raw']\n            virtue_salience = row['mean_virtue_salience']\n            vice_raw = row['mean_vice_raw']\n            vice_salience = row['mean_vice_salience']\n\n            if cci > 0.3 and virtue_raw > 0.5 and virtue_salience > 0.5 and tension < 0.2:\n                return \"Authentic Virtue\"\n            elif cci < -0.3 and vice_raw > 0.5 and vice_salience > 0.5 and tension < 0.2:\n                return \"Strategic Pathology\"\n            elif tension > 0.25 and abs(cci) < 0.2:\n                return \"Incoherent Messaging\"\n            elif cci > 0.1 and virtue_raw > 0.5 and virtue_salience < 0.4:\n                return \"Strategic Virtue Signaling\"\n            elif cci > 0:\n                return \"Virtue-Leaning\"\n            elif cci < 0:\n                return \"Vice-Leaning\"\n            else:\n                return \"Ambiguous\"\n\n        profiles['pattern_classification'] = profiles.apply(classify_pattern, axis=1)\n        \n        # Select key columns for the final report\n        coherence_report = profiles[[\n            'speaker', 'civic_character_index', 'mean_tension', 'pattern_classification',\n            'mean_virtue_raw', 'mean_virtue_salience', 'mean_vice_raw', 'mean_vice_salience'\n        ]].sort_values(by='civic_character_index', ascending=False)\n\n        return coherence_report\n\n    except Exception:\n        return None\n\ndef get_dimension_correlations(data, **kwargs):\n    \"\"\"\n    Calculates the correlation matrix for the 10 raw CAF dimension scores.\n\n    This function provides an exploratory analysis of the relationships between\n    the different rhetorical dimensions. It helps to identify which virtues and\n    vices tend to co-occur in the analyzed discourse.\n\n    Statistical Methodology:\n    The function computes a Pearson correlation matrix for the 10 '_raw' score\n    columns. The resulting matrix shows the correlation coefficient (ranging\n    from -1 to +1) for each pair of dimensions, indicating the strength and\n    direction of their linear relationship across the entire dataset.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis results.\n        **kwargs: Additional parameters (unused).\n\n    Returns:\n        pd.DataFrame: A correlation matrix of the raw dimension scores, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        raw_score_cols = [col for col in data.columns if '_raw' in col]\n        if len(raw_score_cols) < 2:\n            return None # Not enough data to correlate\n\n        correlation_matrix = data[raw_score_cols].corr(method='pearson')\n        \n        # Clean up column names for readability\n        correlation_matrix.columns = [c.replace('_raw', '') for c in correlation_matrix.columns]\n        correlation_matrix.index = [i.replace('_raw', '') for i in correlation_matrix.index]\n        \n        return correlation_matrix\n\n    except Exception:\n        return None\n\ndef validate_framework_by_style(data, **kwargs):\n    \"\"\"\n    Validates the framework by comparing scores across different rhetorical styles.\n\n    This function addresses Research Question 4. It attempts to load speaker metadata,\n    specifically a 'style' attribute (e.g., 'populist', 'institutional'), from a\n    corpus manifest. If this metadata is available, it groups the data by style and\n    calculates the mean scores for all dimensions and derived metrics.\n\n    Statistical Methodology:\n    This function uses grouping and aggregation (mean) to test the hypothesis that the\n    CAF framework can successfully distinguish between predefined rhetorical styles.\n    Significant differences in the mean scores for key dimensions (e.g., higher\n    'tribalism' in populist style) would provide evidence for the framework's\n    construct validity. If the required 'style' metadata is not available, the\n    function returns a message indicating this limitation.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis results.\n        **kwargs: Additional parameters (unused).\n\n    Returns:\n        pd.DataFrame or dict: A DataFrame comparing metrics across styles, or a\n                              dictionary with an error message if style metadata is\n                              unavailable. Returns None on other errors.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # Calculate derived metrics and get speaker metadata\n        data_with_derived = calculate_derived_metrics(data)\n        if data_with_derived is None:\n            return None\n            \n        df = _get_speaker_metadata(data_with_derived)\n\n        if 'style' not in df.columns or df['style'].isnull().all():\n            return {\n                \"status\": \"error\",\n                \"message\": \"Cannot perform style validation. 'style' column not found in corpus manifest or is empty. Fallback to speaker parsing does not provide style information.\"\n            }\n\n        # Define all numeric columns to aggregate\n        all_metrics_cols = [col for col in df.columns if '_raw' in col or '_salience' in col or '_tension' in col or 'civic_character_index' in col]\n        \n        # Ensure all columns exist before grouping\n        cols_to_aggregate = [col for col in all_metrics_cols if col in df.columns]\n        if not cols_to_aggregate:\n            return None\n\n        style_comparison = df.groupby('style')[cols_to_aggregate].mean().reset_index()\n        \n        return style_comparison.sort_values(by='civic_character_index', ascending=False)\n\n    except Exception:\n        return None\n\ndef get_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Computes descriptive statistics for all raw score and salience columns.\n\n    This function provides a high-level overview of the dataset's distribution,\n    addressing the initial part of Research Question 1. It calculates common\n    statistical measures (mean, std, min, 25%, 50%, 75%, max) for each of the\n    20 primary score columns (10 raw scores, 10 salience scores).\n\n    Statistical Methodology:\n    The function uses the `.describe()` method from the pandas library to generate\n    summary statistics for all numeric columns related to the CAF dimensions. This\n    is a standard first step in any quantitative analysis to understand the data's\n    central tendency, dispersion, and range.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis results.\n        **kwargs: Additional parameters (unused).\n\n    Returns:\n        pd.DataFrame: A DataFrame containing descriptive statistics for each\n                      score and salience column, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        score_cols = [col for col in data.columns if '_raw' in col or '_salience' in col]\n        if not score_cols:\n            return None # No score columns found\n\n        descriptive_stats = data[score_cols].describe()\n        return descriptive_stats\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}