{"stage_1_raw_data": {"analysis_plan": {"stage": "raw_data_collection", "experiment_summary": "This plan outlines the collection and initial validation of raw data from the LLM analysis phase for the speaker_character_pattern_analysis experiment. The focus is on capturing the 10 dimensional scores (virtues and vices), associated salience and confidence metadata, and supporting textual evidence for each document, as specified by the Character Assessment Framework v6.1. The plan includes descriptive statistical checks to ensure data integrity and completeness before proceeding to Stage 2 analysis.", "tasks": {"validate_raw_dimensional_scores": {"tool": "calculate_descriptive_stats", "parameters": {"columns": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score"]}, "purpose": "To verify that all raw dimensional scores generated by the LLM fall within the expected 0.0-1.0 range and to perform a preliminary check for anomalies or biases in the initial data distribution across all 10 CAF dimensions."}, "validate_raw_salience_scores": {"tool": "calculate_descriptive_stats", "parameters": {"columns": ["dignity_salience", "truth_salience", "justice_salience", "hope_salience", "pragmatism_salience", "tribalism_salience", "manipulation_salience", "resentment_salience", "fear_salience", "fantasy_salience"]}, "purpose": "To validate that the salience scores, a critical component for Stage 2 tension calculations, are captured correctly and fall within the required 0.0-1.0 range. This ensures the LLM has provided the necessary inputs for subsequent analysis."}, "validate_raw_confidence_scores": {"tool": "calculate_descriptive_stats", "parameters": {"columns": ["dignity_confidence", "truth_confidence", "justice_confidence", "hope_confidence", "pragmatism_confidence", "tribalism_confidence", "manipulation_confidence", "resentment_confidence", "fear_confidence", "fantasy_confidence"]}, "purpose": "To assess the LLM's self-reported confidence for each dimensional assessment. This provides a quantitative measure of the reliability of the raw scores and helps identify assessments that may require further qualitative review."}, "generate_initial_data_summary": {"tool": "create_summary_statistics", "parameters": {"metrics": ["dignity_score", "tribalism_score", "hope_score", "fear_score", "dignity_salience", "tribalism_salience"], "summary_types": ["mean", "std", "min", "max", "count"]}, "purpose": "To create a high-level overview of the entire raw dataset. This summary provides a quick health check on data completeness ('count') and the central tendency and dispersion of key raw scores and salience values before any grouping or hypothesis testing."}}}, "results": {"validate_raw_dimensional_scores": {"type": "descriptive_stats", "columns_analyzed": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score"], "results": {"dignity_score": {"count": 7, "mean": 0.7071428571428572, "std": 0.11700630833624397, "min": 0.6, "max": 0.9, "median": 0.7, "q25": 0.6, "q75": 0.775, "skewness": 0.6354172449685459, "kurtosis": -0.7777542533081299}, "truth_score": {"count": 8, "mean": 0.6125, "std": 0.15294723646688466, "min": 0.4, "max": 0.85, "median": 0.625, "q25": 0.55, "q75": 0.7, "skewness": -0.23457673001622417, "kurtosis": -0.27672047083503326}, "justice_score": {"count": 8, "mean": 0.73125, "std": 0.15797264681854625, "min": 0.5, "max": 0.9, "median": 0.775, "q25": 0.6499999999999999, "q75": 0.8250000000000001, "skewness": -0.70039978049317, "kurtosis": -0.8331514556085038}, "hope_score": {"count": 8, "mean": 0.6475, "std": 0.23614462880676687, "min": 0.2, "max": 0.9, "median": 0.7, "q25": 0.5, "q75": 0.8200000000000001, "skewness": -0.9189397658129868, "kurtosis": 0.45756744270958816}, "pragmatism_score": {"count": 8, "mean": 0.5249999999999999, "std": 0.12817398889233114, "min": 0.3, "max": 0.7, "median": 0.55, "q25": 0.475, "q75": 0.6, "skewness": -0.6105830850825585, "kurtosis": -0.021172022684313063}, "tribalism_score": {"count": 8, "mean": 0.4125, "std": 0.18850918886280923, "min": 0.1, "max": 0.7, "median": 0.4, "q25": 0.3, "q75": 0.525, "skewness": -0.06664297982574521, "kurtosis": -0.01725208959369784}, "manipulation_score": {"count": 8, "mean": 0.40625, "std": 0.2111828929760038, "min": 0.15, "max": 0.7, "median": 0.4, "q25": 0.2, "q75": 0.6, "skewness": 0.1116263407578763, "kurtosis": -1.763594224855487}, "resentment_score": {"count": 8, "mean": 0.39375000000000004, "std": 0.2043063176423368, "min": 0.05, "max": 0.7, "median": 0.35, "q25": 0.3, "q75": 0.525, "skewness": -0.09815396884072479, "kurtosis": 0.005367954473961767}, "fear_score": {"count": 8, "mean": 0.25625000000000003, "std": 0.1347948176197544, "min": 0.1, "max": 0.5, "median": 0.2, "q25": 0.1875, "q75": 0.325, "skewness": 0.9323479781707691, "kurtosis": -0.004124383485562433}, "fantasy_score": {"count": 8, "mean": 0.175, "std": 0.10350983390135313, "min": 0.0, "max": 0.3, "median": 0.2, "q25": 0.1, "q75": 0.225, "skewness": -0.3864367132317181, "kurtosis": -0.4479999999999986}}}, "validate_raw_salience_scores": {"type": "descriptive_stats", "columns_analyzed": ["dignity_salience", "truth_salience", "justice_salience", "hope_salience", "pragmatism_salience", "tribalism_salience", "manipulation_salience", "resentment_salience", "fear_salience", "fantasy_salience"], "results": {"dignity_salience": {"count": 7, "mean": 0.7571428571428572, "std": 0.09322272357358044, "min": 0.7, "max": 0.95, "median": 0.7, "q25": 0.7, "q75": 0.775, "skewness": 1.8735510780128417, "kurtosis": 3.432351285419381}, "truth_salience": {"count": 8, "mean": 0.675, "std": 0.13887301496588275, "min": 0.4, "max": 0.8, "median": 0.7, "q25": 0.6, "q75": 0.8, "skewness": -1.1201280219470378, "kurtosis": 1.1061728395061703}, "justice_salience": {"count": 8, "mean": 0.7875, "std": 0.18850918886280923, "min": 0.5, "max": 0.95, "median": 0.875, "q25": 0.6875, "q75": 0.9125, "skewness": -1.0209704509304138, "kurtosis": -0.719779803540316}, "hope_salience": {"count": 8, "mean": 0.6625, "std": 0.24892626561752318, "min": 0.2, "max": 0.9, "median": 0.75, "q25": 0.55, "q75": 0.85, "skewness": -1.0870897317668808, "kurtosis": 0.14808195400676105}, "pragmatism_salience": {"count": 8, "mean": 0.51875, "std": 0.14623244705409455, "min": 0.3, "max": 0.75, "median": 0.55, "q25": 0.4, "q75": 0.6, "skewness": 0.009279703405700527, "kurtosis": -0.6426401558570607}, "tribalism_salience": {"count": 8, "mean": 0.42500000000000004, "std": 0.17525491637693283, "min": 0.1, "max": 0.7, "median": 0.45, "q25": 0.375, "q75": 0.5, "skewness": -0.5042488670228694, "kurtosis": 1.3568415359653878}, "manipulation_salience": {"count": 8, "mean": 0.4749999999999999, "std": 0.21380899352993948, "min": 0.15, "max": 0.7, "median": 0.55, "q25": 0.2875, "q75": 0.625, "skewness": -0.4750150979302847, "kurtosis": -1.5741455078125002}, "resentment_salience": {"count": 8, "mean": 0.41875, "std": 0.19628241315585487, "min": 0.05, "max": 0.7, "median": 0.4, "q25": 0.375, "q75": 0.525, "skewness": -0.5817875780138718, "kurtosis": 1.121535402252242}, "fear_salience": {"count": 8, "mean": 0.28125, "std": 0.09977653603356425, "min": 0.1, "max": 0.4, "median": 0.3, "q25": 0.2375, "q75": 0.325, "skewness": -0.6044898304046372, "kurtosis": 0.3646323071045048}, "fantasy_salience": {"count": 8, "mean": 0.20625, "std": 0.126596941962615, "min": 0.0, "max": 0.4, "median": 0.2, "q25": 0.1375, "q75": 0.3, "skewness": -0.1155158663790897, "kurtosis": -0.21386550383687286}}}, "validate_raw_confidence_scores": {"type": "descriptive_stats", "columns_analyzed": ["dignity_confidence", "truth_confidence", "justice_confidence", "hope_confidence", "pragmatism_confidence", "tribalism_confidence", "manipulation_confidence", "resentment_confidence", "fear_confidence", "fantasy_confidence"], "results": {"dignity_confidence": {"count": 7, "mean": 0.7928571428571428, "std": 0.10177004891982151, "min": 0.7, "max": 0.9, "median": 0.75, "q25": 0.7, "q75": 0.9, "skewness": 0.267675800488283, "kurtosis": -2.6951248513674186}, "truth_confidence": {"count": 8, "mean": 0.7687499999999999, "std": 0.11933596033288303, "min": 0.6, "max": 0.95, "median": 0.8, "q25": 0.7125, "q75": 0.8125, "skewness": -0.34017919805586205, "kurtosis": -0.2550132172443309}, "justice_confidence": {"count": 8, "mean": 0.8187499999999999, "std": 0.14126343172547218, "min": 0.5, "max": 0.95, "median": 0.85, "q25": 0.8, "q75": 0.875, "skewness": -1.8774322889140391, "kurtosis": 4.501490923832252}, "hope_confidence": {"count": 8, "mean": 0.765, "std": 0.1461897006338975, "min": 0.6, "max": 0.95, "median": 0.75, "q25": 0.6375, "q75": 0.905, "skewness": 0.10845917943952225, "kurtosis": -2.079485580085219}, "pragmatism_confidence": {"count": 8, "mean": 0.65625, "std": 0.11783008347374015, "min": 0.5, "max": 0.85, "median": 0.7, "q25": 0.575, "q75": 0.7, "skewness": -0.023195425908955142, "kurtosis": -0.22939382347163573}, "tribalism_confidence": {"count": 8, "mean": 0.6499999999999999, "std": 0.13093073414159542, "min": 0.4, "max": 0.8, "median": 0.6499999999999999, "q25": 0.6, "q75": 0.725, "skewness": -0.7637626158259697, "kurtosis": 0.8749999999999947}, "manipulation_confidence": {"count": 8, "mean": 0.6499999999999999, "std": 0.13627702877384937, "min": 0.5, "max": 0.85, "median": 0.675, "q25": 0.5, "q75": 0.75, "skewness": 0.0, "kurtosis": -1.5946745562130182}, "resentment_confidence": {"count": 8, "mean": 0.64375, "std": 0.14252192813739223, "min": 0.4, "max": 0.8, "median": 0.7, "q25": 0.5375000000000001, "q75": 0.75, "skewness": -0.7702689769562403, "kurtosis": -0.823885038038882}, "fear_confidence": {"count": 8, "mean": 0.5187499999999999, "std": 0.16889874396894047, "min": 0.3, "max": 0.7, "median": 0.55, "q25": 0.375, "q75": 0.6625, "skewness": -0.3127134054837214, "kurtosis": -1.8339571072758938}, "fantasy_confidence": {"count": 8, "mean": 0.5062500000000001, "std": 0.20430631764233684, "min": 0.3, "max": 0.9, "median": 0.45, "q25": 0.375, "q75": 0.6125, "skewness": 1.0027409456768288, "kurtosis": 0.6916462009208111}}}, "generate_initial_data_summary": {"type": "summary_statistics", "metrics": ["dignity_score", "tribalism_score", "hope_score", "fear_score", "dignity_salience", "tribalism_salience"], "summary_types": ["mean", "std", "min", "max", "count"], "results": {"dignity_score": {"mean": 0.7071428571428572, "std": 0.11700630833624397, "min": 0.6, "max": 0.9, "count": 7}, "tribalism_score": {"mean": 0.4125, "std": 0.18850918886280923, "min": 0.1, "max": 0.7, "count": 8}, "hope_score": {"mean": 0.6475, "std": 0.23614462880676687, "min": 0.2, "max": 0.9, "count": 8}, "fear_score": {"mean": 0.25625000000000003, "std": 0.1347948176197544, "min": 0.1, "max": 0.5, "count": 8}, "dignity_salience": {"mean": 0.7571428571428572, "std": 0.09322272357358044, "min": 0.7, "max": 0.95, "count": 7}, "tribalism_salience": {"mean": 0.42500000000000004, "std": 0.17525491637693283, "min": 0.1, "max": 0.7, "count": 8}}, "missing_metrics": []}}, "errors": []}, "stage_2_derived_metrics": {"analysis_plan": {"stage": "derived_metrics_analysis", "experiment_summary": "This plan outlines Stage 2 of the speaker_character_pattern_analysis experiment. It focuses on calculating derived metrics from raw dimensional scores, including five character tension scores and the Moral Character Strategic Contradiction Index (MC-SCI), as specified by the CAF v6.1 framework. Following calculation and validation, the plan details statistical analyses, including descriptive statistics and a series of ANOVA tests, to evaluate the research hypotheses regarding speaker differentiation, character signatures, and character coherence patterns.", "tasks": {"task_01_calculate_derived_metrics": {"tool": "calculate_derived_metrics", "parameters": {"metric_formulas": {"dignity_tribalism_tension": "min(dignity_score, tribalism_score) * abs(dignity_salience - tribalism_salience)", "truth_manipulation_tension": "min(truth_score, manipulation_score) * abs(truth_salience - manipulation_salience)", "justice_resentment_tension": "min(justice_score, resentment_score) * abs(justice_salience - resentment_salience)", "hope_fear_tension": "min(hope_score, fear_score) * abs(hope_salience - fear_salience)", "pragmatism_fantasy_tension": "min(pragmatism_score, fantasy_score) * abs(pragmatism_salience - fantasy_salience)", "mc_sci": "(dignity_tribalism_tension + truth_manipulation_tension + justice_resentment_tension + hope_fear_tension + pragmatism_fantasy_tension) / 5"}, "input_columns": ["dignity_score", "dignity_salience", "tribalism_score", "tribalism_salience", "truth_score", "truth_salience", "manipulation_score", "manipulation_salience", "justice_score", "justice_salience", "resentment_score", "resentment_salience", "hope_score", "hope_salience", "fear_score", "fear_salience", "pragmatism_score", "pragmatism_salience", "fantasy_score", "fantasy_salience"]}, "purpose": "To calculate the five character tension scores and the aggregate Moral Character Strategic Contradiction Index (MC-SCI) for each document, based on the formulas specified in the CAF v6.1 framework. These metrics are essential for testing H3."}, "task_02_validate_derived_metrics": {"tool": "validate_calculated_metrics", "parameters": {"validation_rules": ["missing_data_check", "range_check"], "quality_thresholds": {"range_check": {"min": 0.0, "max": 1.0}}}, "purpose": "To ensure the integrity of the newly calculated derived metrics (tensions and MC-SCI) by checking for missing values and verifying that all values fall within the expected 0.0 to 1.0 range before they are used in subsequent statistical analyses."}, "task_03_character_signature_descriptives": {"tool": "calculate_descriptive_stats", "parameters": {"columns": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score", "mc_sci"]}, "purpose": "To address H2 (Character Signatures) by generating descriptive statistics (mean, standard deviation) for all 10 core dimensions and the calculated MC-SCI. This provides a baseline understanding of the distribution of character traits across the corpus, which is the first step in identifying unique profiles."}, "task_04_speaker_differentiation_anova": {"tool": "perform_one_way_anova", "parameters": {"grouping_variable": "dignity_score", "dependent_variable": "tribalism_score"}, "purpose": "To test H1 (Speaker Differentiation) by proxy. This analysis will determine if there are statistically significant differences in a key vice dimension (tribalism) between documents grouped by their score on a key virtue dimension (dignity), thereby testing the framework's ability to differentiate between character profiles."}, "task_05_mc_sci_coherence_anova": {"tool": "perform_one_way_anova", "parameters": {"grouping_variable": "hope_score", "dependent_variable": "mc_sci"}, "purpose": "To test H3 (MC-SCI Patterns) by investigating whether the MC-SCI score, representing character coherence, varies significantly across documents when grouped by their level of a primary virtue like Hope. This examines how coherence patterns relate to specific character expressions."}, "task_06_factorial_interaction_analysis": {"tool": "perform_two_way_anova", "parameters": {"factor1": "justice_score", "factor2": "resentment_score", "dependent_variable": "mc_sci"}, "purpose": "To explore the interaction effects between opposing character dimensions, reflecting the experiment's factorial design intent. This test analyzes how a virtue (Justice) and its opposing vice (Resentment) jointly influence overall character coherence (MC-SCI), revealing complex character patterns."}}}, "results": {"task_01_calculate_derived_metrics": {"type": "derived_metrics_calculation", "success": false, "calculated_metrics": {}, "successful_calculations": [], "failed_calculations": [{"metric": "hope_fear_tension", "formula": "min(hope_score, fear_score) * abs(hope_salience - fear_salience)", "error": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"}, {"metric": "dignity_tribalism_tension", "formula": "min(dignity_score, tribalism_score) * abs(dignity_salience - tribalism_salience)", "error": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"}, {"metric": "truth_manipulation_tension", "formula": "min(truth_score, manipulation_score) * abs(truth_salience - manipulation_salience)", "error": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"}, {"metric": "justice_resentment_tension", "formula": "min(justice_score, resentment_score) * abs(justice_salience - resentment_salience)", "error": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"}, {"metric": "pragmatism_fantasy_tension", "formula": "min(pragmatism_score, fantasy_score) * abs(pragmatism_salience - fantasy_salience)", "error": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"}, {"metric": "mc_sci", "formula": "(dignity_tribalism_tension + truth_manipulation_tension + justice_resentment_tension + hope_fear_tension + pragmatism_fantasy_tension) / 5", "error": "name 'dignity_tribalism_tension' is not defined"}], "formulas_used": ["dignity_tribalism_tension", "truth_manipulation_tension", "justice_resentment_tension", "hope_fear_tension", "pragmatism_fantasy_tension", "mc_sci"], "input_columns": ["dignity_score", "dignity_salience", "tribalism_score", "tribalism_salience", "truth_score", "truth_salience", "manipulation_score", "manipulation_salience", "justice_score", "justice_salience", "resentment_score", "resentment_salience", "hope_score", "hope_salience", "fear_score", "fear_salience", "pragmatism_score", "pragmatism_salience", "fantasy_score", "fantasy_salience"], "total_metrics": 6, "success_rate": 0.0}, "task_02_validate_derived_metrics": {"type": "metric_validation", "validation_rules": ["missing_data_check", "range_check"], "results": {"missing_data_check": {"status": "completed", "missing_data_by_column": {"aid": 0, "dignity_score": 1, "dignity_raw_score": 1, "dignity_salience": 1, "dignity_confidence": 1, "truth_score": 0, "truth_raw_score": 0, "truth_salience": 0, "truth_confidence": 0, "justice_score": 0, "justice_raw_score": 0, "justice_salience": 0, "justice_confidence": 0, "hope_score": 0, "hope_raw_score": 0, "hope_salience": 0, "hope_confidence": 0, "pragmatism_score": 0, "pragmatism_raw_score": 0, "pragmatism_salience": 0, "pragmatism_confidence": 0, "tribalism_score": 0, "tribalism_raw_score": 0, "tribalism_salience": 0, "tribalism_confidence": 0, "manipulation_score": 0, "manipulation_raw_score": 0, "manipulation_salience": 0, "manipulation_confidence": 0, "resentment_score": 0, "resentment_raw_score": 0, "resentment_salience": 0, "resentment_confidence": 0, "fear_score": 0, "fear_raw_score": 0, "fear_salience": 0, "fear_confidence": 0, "fantasy_score": 0, "fantasy_raw_score": 0, "fantasy_salience": 0, "fantasy_confidence": 0, "diginity_score": 7, "diginity_raw_score": 7, "diginity_salience": 7, "diginity_confidence": 7}, "total_missing": 32}, "range_check": {"status": "completed", "ranges": {"dignity_score": {"min": 0.6, "max": 0.9, "mean": 0.7071428571428572}, "dignity_raw_score": {"min": 0.6, "max": 0.9, "mean": 0.7071428571428572}, "dignity_salience": {"min": 0.7, "max": 0.95, "mean": 0.7571428571428571}, "dignity_confidence": {"min": 0.7, "max": 0.9, "mean": 0.7928571428571428}, "truth_score": {"min": 0.4, "max": 0.85, "mean": 0.6125}, "truth_raw_score": {"min": 0.4, "max": 0.85, "mean": 0.6125}, "truth_salience": {"min": 0.4, "max": 0.8, "mean": 0.675}, "truth_confidence": {"min": 0.6, "max": 0.95, "mean": 0.7687499999999999}, "justice_score": {"min": 0.5, "max": 0.9, "mean": 0.73125}, "justice_raw_score": {"min": 0.5, "max": 0.9, "mean": 0.73125}, "justice_salience": {"min": 0.5, "max": 0.95, "mean": 0.7875}, "justice_confidence": {"min": 0.5, "max": 0.95, "mean": 0.8187499999999999}, "hope_score": {"min": 0.2, "max": 0.9, "mean": 0.6475}, "hope_raw_score": {"min": 0.2, "max": 0.9, "mean": 0.6475}, "hope_salience": {"min": 0.2, "max": 0.9, "mean": 0.6625}, "hope_confidence": {"min": 0.6, "max": 0.95, "mean": 0.765}, "pragmatism_score": {"min": 0.3, "max": 0.7, "mean": 0.5249999999999999}, "pragmatism_raw_score": {"min": 0.3, "max": 0.7, "mean": 0.5249999999999999}, "pragmatism_salience": {"min": 0.3, "max": 0.75, "mean": 0.51875}, "pragmatism_confidence": {"min": 0.5, "max": 0.85, "mean": 0.65625}, "tribalism_score": {"min": 0.1, "max": 0.7, "mean": 0.4125}, "tribalism_raw_score": {"min": 0.1, "max": 0.7, "mean": 0.4125}, "tribalism_salience": {"min": 0.1, "max": 0.7, "mean": 0.42500000000000004}, "tribalism_confidence": {"min": 0.4, "max": 0.8, "mean": 0.6499999999999999}, "manipulation_score": {"min": 0.15, "max": 0.7, "mean": 0.40625}, "manipulation_raw_score": {"min": 0.15, "max": 0.7, "mean": 0.40625}, "manipulation_salience": {"min": 0.15, "max": 0.7, "mean": 0.4749999999999999}, "manipulation_confidence": {"min": 0.5, "max": 0.85, "mean": 0.6499999999999999}, "resentment_score": {"min": 0.05, "max": 0.7, "mean": 0.39375000000000004}, "resentment_raw_score": {"min": 0.05, "max": 0.7, "mean": 0.39375000000000004}, "resentment_salience": {"min": 0.05, "max": 0.7, "mean": 0.41875}, "resentment_confidence": {"min": 0.4, "max": 0.8, "mean": 0.64375}, "fear_score": {"min": 0.1, "max": 0.5, "mean": 0.25625000000000003}, "fear_raw_score": {"min": 0.1, "max": 0.5, "mean": 0.25625000000000003}, "fear_salience": {"min": 0.1, "max": 0.4, "mean": 0.28125}, "fear_confidence": {"min": 0.3, "max": 0.7, "mean": 0.5187499999999999}, "fantasy_score": {"min": 0.0, "max": 0.3, "mean": 0.175}, "fantasy_raw_score": {"min": 0.0, "max": 0.3, "mean": 0.175}, "fantasy_salience": {"min": 0.0, "max": 0.4, "mean": 0.20625}, "fantasy_confidence": {"min": 0.3, "max": 0.9, "mean": 0.5062500000000001}, "diginity_score": {"min": 0.3, "max": 0.3, "mean": 0.3}, "diginity_raw_score": {"min": 0.3, "max": 0.3, "mean": 0.3}, "diginity_salience": {"min": 0.3, "max": 0.3, "mean": 0.3}, "diginity_confidence": {"min": 0.7, "max": 0.7, "mean": 0.7}}}}, "quality_thresholds": {"range_check": {"min": 0.0, "max": 1.0}}}, "task_04_speaker_differentiation_anova": {"type": "one_way_anova", "grouping_variable": "dignity_score", "dependent_variable": "tribalism_score", "groups": {"0.6": {"n": 3, "mean": 0.43333333333333335, "std": 0.18856180831641264}, "0.7": {"n": 1, "mean": 0.6, "std": 0.0}, "0.75": {"n": 1, "mean": 0.4, "std": 0.0}, "0.8": {"n": 1, "mean": 0.4, "std": 0.0}, "0.9": {"n": 1, "mean": 0.1, "std": 0.0}}, "f_statistic": 0.6249999999999999, "p_value": 0.691358024691358, "significant": "False"}}, "errors": ["Task 'task_03_character_signature_descriptives' failed: Descriptive stats calculation failed: Column 'mc_sci' not found in DataFrame. Available columns: ['aid', 'dignity_score', 'dignity_raw_score', 'dignity_salience', 'dignity_confidence', 'truth_score', 'truth_raw_score', 'truth_salience', 'truth_confidence', 'justice_score', 'justice_raw_score', 'justice_salience', 'justice_confidence', 'hope_score', 'hope_raw_score', 'hope_salience', 'hope_confidence', 'pragmatism_score', 'pragmatism_raw_score', 'pragmatism_salience', 'pragmatism_confidence', 'tribalism_score', 'tribalism_raw_score', 'tribalism_salience', 'tribalism_confidence', 'manipulation_score', 'manipulation_raw_score', 'manipulation_salience', 'manipulation_confidence', 'resentment_score', 'resentment_raw_score', 'resentment_salience', 'resentment_confidence', 'fear_score', 'fear_raw_score', 'fear_salience', 'fear_confidence', 'fantasy_score', 'fantasy_raw_score', 'fantasy_salience', 'fantasy_confidence', 'diginity_score', 'diginity_raw_score', 'diginity_salience', 'diginity_confidence']", "Task 'task_05_mc_sci_coherence_anova' failed: ANOVA failed: Dependent variable 'mc_sci' not found in DataFrame", "Task 'task_06_factorial_interaction_analysis' failed: Two-way ANOVA failed: Column 'mc_sci' not found in DataFrame"]}, "combined_summary": "Two-stage execution: 4 raw data results + 3 derived metrics results"}