{
  "generation_metadata": {
    "status": "success",
    "functions_generated": 6,
    "output_file": "automatedstatisticalanalysisagent_functions.py",
    "module_size": 21702,
    "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: caf_civic_character_pattern_analysis\nDescription: Statistical analysis experiment\nGenerated: 2025-08-23T16:53:31.341565+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for all numeric dimensions and derived metrics.\n\n    This function provides a basic overview of the data distribution for each scored\n    dimension (raw_score, salience) and each calculated derived metric. It computes\n    the mean, standard deviation, count, min, and max for each numeric column,\n    which is essential for understanding the overall dataset characteristics before\n    more complex analysis.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the flattened analysis data.\n                             Expected columns include dimensional scores (e.g.,\n                             'tribalism_raw_score', 'tribalism_salience') and\n                             derived metrics (e.g., 'civic_character_index').\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary of descriptive statistics for each numeric\n              column, or None if the input data is invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        results = {}\n        # Select only numeric columns that represent scores, salience, or derived metrics\n        metric_cols = [col for col in data.columns if\n                       col.endswith('_raw_score') or\n                       col.endswith('_salience') or\n                       'tension' in col or\n                       'index' in col]\n\n        numeric_df = data[metric_cols].select_dtypes(include=np.number)\n\n        if numeric_df.empty:\n            return None\n\n        for col in numeric_df.columns:\n            # Drop NaN values for accurate calculations\n            valid_data = numeric_df[col].dropna()\n            if not valid_data.empty:\n                results[col] = {\n                    'mean': float(valid_data.mean()),\n                    'std': float(valid_data.std()),\n                    'count': int(valid_data.count()),\n                    'min': float(valid_data.min()),\n                    'max': float(valid_data.max())\n                }\n        return results if results else None\n\n    except Exception:\n        return None\n\ndef analyze_speaker_differentiation(data, **kwargs):\n    \"\"\"\n    Analyzes how effectively CAF dimensions distinguish between speakers.\n\n    This function addresses Research Question 1 by grouping the analysis results\n    by speaker and calculating descriptive statistics (mean and standard deviation)\n    for each dimension and derived metric. The output allows for direct comparison\n    of rhetorical profiles, highlighting which dimensions show the most variance\n    between speakers and thus have the most discriminatory power. A 'speaker'\n    column is derived from the 'filename' if not present.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis data, including a 'filename'\n                             column and dimensional/derived metric columns.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A nested dictionary with statistics for each speaker, or None on error.\n              Example: {'speaker_name': {'metric': {'mean': x, 'std': y}, ...}}\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n    import re\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = data.copy()\n\n        # Derive speaker from filename if 'speaker' column doesn't exist\n        if 'speaker' not in df.columns and 'filename' in df.columns:\n            df['speaker'] = df['filename'].apply(\n                lambda x: '_'.join(x.split('_')[:-2]) if len(x.split('_')) > 2 else x.split('.')[0]\n            )\n\n        if 'speaker' not in df.columns:\n            return None # Cannot perform analysis without speaker identification\n\n        metric_cols = [col for col in df.columns if\n                       col.endswith('_raw_score') or\n                       col.endswith('_salience') or\n                       'tension' in col or\n                       'index' in col]\n\n        if not metric_cols:\n            return None\n\n        # Group by speaker and calculate stats\n        grouped = df.groupby('speaker')[metric_cols]\n        stats = grouped.agg(['mean', 'std']).to_dict()\n\n        # Restructure the dictionary for a cleaner output\n        results = {}\n        for speaker in df['speaker'].unique():\n            results[speaker] = {}\n            for col in metric_cols:\n                mean_val = stats.get((col, 'mean'), {}).get(speaker)\n                std_val = stats.get((col, 'std'), {}).get(speaker)\n                if pd.notna(mean_val):\n                    results[speaker][col] = {\n                        'mean': float(mean_val),\n                        'std': float(std_val) if pd.notna(std_val) else 0.0\n                    }\n        return results if results else None\n\n    except Exception:\n        return None\n\ndef generate_character_signatures(data, **kwargs):\n    \"\"\"\n    Generates distinct character signatures for each speaker.\n\n    This function addresses Research Question 2 by computing the average score for\n    each of the 10 core dimensions (virtues and vices) and 5 tension indices for\n    each speaker. The resulting \"signature\" is a vector of mean scores that\n    represents a speaker's typical rhetorical profile according to the CAF. This\n    data is ideal for visualization (e.g., radar charts) to compare speaker styles.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis data, including a 'filename'\n                             column and all dimensional/derived metric columns.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary where keys are speaker names and values are dictionaries\n              representing their character signature (mean scores). Returns None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n    import re\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = data.copy()\n\n        if 'speaker' not in df.columns and 'filename' in df.columns:\n            df['speaker'] = df['filename'].apply(\n                lambda x: '_'.join(x.split('_')[:-2]) if len(x.split('_')) > 2 else x.split('.')[0]\n            )\n\n        if 'speaker' not in df.columns:\n            return None\n\n        metric_cols = [\n            'tribalism_raw_score', 'dignity_raw_score',\n            'manipulation_raw_score', 'truth_raw_score',\n            'resentment_raw_score', 'justice_raw_score',\n            'fear_raw_score', 'hope_raw_score',\n            'fantasy_raw_score', 'pragmatism_raw_score',\n            'identity_tension', 'truth_tension', 'justice_tension',\n            'emotional_tension', 'reality_tension', 'civic_character_index'\n        ]\n        \n        # Ensure all expected columns exist, fill missing with NaN\n        for col in metric_cols:\n            if col not in df.columns:\n                df[col] = np.nan\n\n        # Group by speaker and calculate the mean for the signature\n        signatures = df.groupby('speaker')[metric_cols].mean()\n        \n        # Convert to a nested dictionary, handling potential NaN values\n        results = signatures.apply(lambda r: r.dropna().to_dict(), axis=1).to_dict()\n\n        return results if results else None\n\n    except Exception:\n        return None\n\ndef analyze_character_coherence(data, **kwargs):\n    \"\"\"\n    Analyzes civic character coherence using indices and pattern classifications.\n\n    This function addresses Research Question 3 by examining the Civic Character Index,\n    tension scores, and interpretive patterns defined in the framework. It calculates\n    summary statistics for coherence metrics per speaker and classifies each document\n    into one of four patterns: Authentic Virtue, Strategic Virtue Signaling,\n    Strategic Pathology, or Incoherent Messaging. The function returns both the\n    metric stats and the frequency of each pattern per speaker, providing a nuanced\n    view of their rhetorical coherence.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis data.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing coherence statistics and pattern counts for\n              each speaker, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n    import re\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = data.copy()\n\n        if 'speaker' not in df.columns and 'filename' in df.columns:\n            df['speaker'] = df['filename'].apply(\n                lambda x: '_'.join(x.split('_')[:-2]) if len(x.split('_')) > 2 else x.split('.')[0]\n            )\n\n        if 'speaker' not in df.columns:\n            return None\n\n        # Define dimensions and thresholds for pattern classification\n        VIRTUE_DIMS = ['dignity', 'truth', 'justice', 'hope', 'pragmatism']\n        VICE_DIMS = ['tribalism', 'manipulation', 'resentment', 'fear', 'fantasy']\n        TENSION_DIMS = ['identity_tension', 'truth_tension', 'justice_tension', 'emotional_tension', 'reality_tension']\n        HIGH_THRESHOLD = 0.6\n        LOW_THRESHOLD = 0.4\n\n        def classify_pattern(row):\n            virtue_scores = [row.get(f'{d}_raw_score', 0) for d in VIRTUE_DIMS]\n            virtue_saliences = [row.get(f'{d}_salience', 0) for d in VIRTUE_DIMS]\n            vice_scores = [row.get(f'{d}_raw_score', 0) for d in VICE_DIMS]\n            vice_saliences = [row.get(f'{d}_salience', 0) for d in VICE_DIMS]\n            tension_scores = [row.get(d, 0) for d in TENSION_DIMS]\n\n            avg_virtue_score = np.nanmean(virtue_scores)\n            avg_virtue_salience = np.nanmean(virtue_saliences)\n            avg_vice_score = np.nanmean(vice_scores)\n            avg_vice_salience = np.nanmean(vice_saliences)\n            avg_tension = np.nanmean(tension_scores)\n\n            if avg_tension > HIGH_THRESHOLD:\n                return \"Incoherent Messaging\"\n            if avg_vice_score > HIGH_THRESHOLD and avg_vice_salience > HIGH_THRESHOLD:\n                return \"Strategic Pathology\"\n            if avg_virtue_score > HIGH_THRESHOLD and avg_virtue_salience > HIGH_THRESHOLD:\n                return \"Authentic Virtue\"\n            if avg_virtue_score > HIGH_THRESHOLD and avg_virtue_salience < LOW_THRESHOLD:\n                return \"Strategic Virtue Signaling\"\n            return \"Mixed/Indeterminate\"\n\n        df['pattern'] = df.apply(classify_pattern, axis=1)\n\n        results = {}\n        for speaker in df['speaker'].unique():\n            speaker_df = df[df['speaker'] == speaker]\n            \n            # Coherence metrics stats\n            coherence_metrics = ['civic_character_index'] + TENSION_DIMS\n            metric_stats = {}\n            for metric in coherence_metrics:\n                if metric in speaker_df.columns:\n                    metric_stats[metric] = {\n                        'mean': speaker_df[metric].mean(),\n                        'std': speaker_df[metric].std()\n                    }\n\n            # Pattern counts\n            pattern_counts = speaker_df['pattern'].value_counts(normalize=True).to_dict()\n\n            results[speaker] = {\n                'coherence_metric_stats': metric_stats,\n                'pattern_distribution': pattern_counts\n            }\n\n        return results if results else None\n\n    except Exception:\n        return None\n\ndef validate_framework_by_style(data, **kwargs):\n    \"\"\"\n    Validates framework's ability to distinguish rhetorical styles via t-tests.\n\n    This function addresses Research Question 4 by comparing the dimensional scores\n    of different rhetorical styles (e.g., 'populist' vs. 'institutional'). It\n    requires a `style_mapping` dictionary to be passed via kwargs. For each\n    dimension, it performs an independent two-sample t-test to determine if the\n    mean scores between the style groups are statistically different. The results,\n    including group means and p-values, help validate if the CAF captures expected\n    stylistic differences.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis data.\n        **kwargs: Must contain 'style_mapping', a dict mapping speaker names to\n                  rhetorical styles (e.g., {'speaker_a': 'populist'}).\n\n    Returns:\n        dict: A dictionary containing the comparison results (group means, p-value)\n              for each dimension, or None if input is invalid or mapping is missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n    import re\n    from scipy.stats import ttest_ind\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        style_mapping = kwargs.get('style_mapping')\n        if not isinstance(style_mapping, dict) or not style_mapping:\n            # Return a specific message if the mapping is missing\n            return {\"error\": \"style_mapping dictionary not provided in kwargs.\"}\n\n        df = data.copy()\n\n        if 'speaker' not in df.columns and 'filename' in df.columns:\n            df['speaker'] = df['filename'].apply(\n                lambda x: '_'.join(x.split('_')[:-2]) if len(x.split('_')) > 2 else x.split('.')[0]\n            )\n\n        if 'speaker' not in df.columns:\n            return None\n\n        df['style'] = df['speaker'].map(style_mapping)\n        df.dropna(subset=['style'], inplace=True)\n\n        styles = df['style'].unique()\n        if len(styles) < 2:\n            return {\"error\": \"Fewer than two styles found after mapping. Cannot compare.\"}\n\n        metric_cols = [col for col in df.columns if col.endswith('_raw_score') or 'tension' in col or 'index' in col]\n        results = {}\n\n        for col in metric_cols:\n            groups = [df[df['style'] == s][col].dropna() for s in styles]\n            \n            # Ensure there is enough data to perform a t-test\n            if any(len(g) < 2 for g in groups):\n                continue\n\n            # Perform t-test between the first two style groups\n            # Note: This is simplified for a two-group comparison. ANOVA would be needed for >2.\n            group1_data = groups[0]\n            group2_data = groups[1]\n            \n            ttest_result = ttest_ind(group1_data, group2_data, equal_var=False) # Welch's t-test\n\n            results[col] = {\n                f'{styles[0]}_mean': group1_data.mean(),\n                f'{styles[1]}_mean': group2_data.mean(),\n                'p_value': ttest_result.pvalue,\n                't_statistic': ttest_result.statistic\n            }\n\n        return results if results else None\n\n    except Exception:\n        return None\n\ndef calculate_correlation_matrix(data, **kwargs):\n    \"\"\"\n    Calculates the Pearson correlation matrix for all dimensions and metrics.\n\n    This function provides insight into the relationships between the different\n    CAF dimensions. It computes a Pearson correlation coefficient for every pair\n    of numeric metrics (raw scores, salience, and derived metrics). The resulting\n    matrix can reveal which virtues and vices tend to co-occur, helping to\n    uncover deeper structural patterns in rhetorical character.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis data.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary representing the correlation matrix, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        metric_cols = [col for col in data.columns if\n                       col.endswith('_raw_score') or\n                       col.endswith('_salience') or\n                       'tension' in col or\n                       'index' in col]\n\n        if not metric_cols:\n            return None\n\n        corr_matrix = data[metric_cols].corr(method='pearson')\n        \n        # Convert to dictionary format for JSON compatibility, handling NaNs\n        result = corr_matrix.where(pd.notna(corr_matrix), None).to_dict()\n\n        return result if result else None\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
    "cached_with_code": true
  },
  "statistical_data": {
    "status": "success",
    "statistical_results": {
      "analysis_metadata": {
        "timestamp": "2025-08-23T13:02:22.681740",
        "sample_size": 8,
        "alpha_level": 0.05,
        "variables_analyzed": [
          "tribalism_raw",
          "tribalism_salience",
          "tribalism_confidence",
          "dignity_raw",
          "dignity_salience",
          "dignity_confidence",
          "manipulation_raw",
          "manipulation_salience",
          "manipulation_confidence",
          "truth_raw",
          "truth_salience",
          "truth_confidence",
          "resentment_raw",
          "resentment_salience",
          "resentment_confidence",
          "justice_raw",
          "justice_salience",
          "justice_confidence",
          "fear_raw",
          "fear_salience",
          "fear_confidence",
          "hope_raw",
          "hope_salience",
          "hope_confidence",
          "fantasy_raw",
          "fantasy_salience",
          "fantasy_confidence",
          "pragmatism_raw",
          "pragmatism_salience",
          "pragmatism_confidence"
        ]
      }
    }
  },
  "status": "success_with_data",
  "validation_passed": true
}