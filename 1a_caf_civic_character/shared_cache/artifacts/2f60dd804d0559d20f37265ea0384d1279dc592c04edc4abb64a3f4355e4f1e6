import pandas as pd
import numpy as np
from scipy import stats
import json

# Initialize the final results dictionary with the required structure.
# This ensures the output format is correct even if some analysis steps fail.
result_data = {
    'descriptive_stats': {},
    'hypothesis_tests': {},
    'correlations': {},
    'reliability_metrics': {}
}

# --- Helper Functions ---
def cohen_d_paired(paired_data_1, paired_data_2):
    """Calculates Cohen's d for a paired t-test."""
    # Ensure input is numpy array for consistent calculations
    d1 = np.asarray(paired_data_1)
    d2 = np.asarray(paired_data_2)
    # Check for NaN or Inf values which can cause errors
    if not (np.all(np.isfinite(d1)) and np.all(np.isfinite(d2))):
        return np.nan
    diff = d1 - d2
    # Check that the standard deviation of the differences is not zero
    if np.std(diff, ddof=1) == 0:
        return np.inf
    return np.mean(diff) / np.std(diff, ddof=1)

def cronbach_alpha(df):
    """Calculates Cronbach's Alpha for a given DataFrame of items."""
    try:
        # Drop rows with any missing values for this calculation
        df_complete = df.dropna()
        if df_complete.shape[0] < 2 or df_complete.shape[1] < 2:
            return np.nan # Not enough data to calculate
            
        # Number of items (columns)
        k = df_complete.shape[1]
        
        # Sum of variances of each item
        sum_item_variances = df_complete.var(axis=0, ddof=1).sum()
        
        # Variance of the total score (sum of items per row)
        total_score_variance = df_complete.sum(axis=1).var(ddof=1)

        # Cronbach's Alpha formula
        if total_score_variance == 0:
            return 1.0 if sum_item_variances == 0 else 0.0

        alpha = (k / (k - 1)) * (1 - (sum_item_variances / total_score_variance))
        return alpha
    except Exception:
        return np.nan

# --- Analysis Pipeline ---

# Create a copy to avoid SettingWithCopyWarning
df = scores_df.copy()

try:
    # 1. Framework Calculations: Character Tensions and MC-SCI
    # Define opposing pairs based on the framework specification for clarity and reusability.
    opposing_pairs = {
        'dignity_tribalism': ('dignity', 'tribalism'),
        'truth_manipulation': ('truth', 'manipulation'),
        'justice_resentment': ('justice', 'resentment'),
        'hope_fear': ('hope', 'fear'),
        'pragmatism_fantasy': ('pragmatism', 'fantasy')
    }

    tension_cols = []
    for tension_name, (virtue, vice) in opposing_pairs.items():
        tension_col_name = f"{tension_name}_tension"
        tension_cols.append(tension_col_name)
        
        # Extract score and salience columns
        virtue_score = df[f'{virtue}_score']
        vice_score = df[f'{vice}_score']
        virtue_salience = df[f'{virtue}_salience']
        vice_salience = df[f'{vice}_salience']
        
        # Apply the Character Tension formula: min(Virtue, Vice) * |Salience_Virtue - Salience_Vice|
        df[tension_col_name] = np.minimum(virtue_score, vice_score) * np.abs(virtue_salience - vice_salience)

    # Calculate the Moral Character Strategic Contradiction Index (MC-SCI)
    # Formula: (Sum of all Character Tension Scores) / Number of Opposing Pairs
    df['mc_sci'] = df[tension_cols].sum(axis=1) / len(opposing_pairs)

except Exception as e:
    result_data['error_framework_calculations'] = f"Failed to calculate tension scores or MC-SCI: {str(e)}"

try:
    # 2. Descriptive Statistics
    stats_dict = {}
    
    # Define columns for which to generate descriptive statistics
    score_cols = [col for col in df.columns if '_score' in col]
    salience_cols = [col for col in df.columns if '_salience' in col]
    calculated_cols = tension_cols + ['mc_sci']
    
    # Generate stats for scores, salience, and calculated metrics
    stats_dict['scores_stats'] = df[score_cols].describe().to_dict()
    stats_dict['salience_stats'] = df[salience_cols].describe().to_dict()
    stats_dict['calculated_metrics_stats'] = df[calculated_cols].describe().to_dict()
    
    # Add statistics from the evidence DataFrame
    if 'evidence_df' in locals() and not evidence_df.empty:
        evidence_stats = {
            'total_quotes': int(evidence_df.shape[0]),
            'quotes_per_dimension': evidence_df['dimension'].value_counts().to_dict(),
            'average_confidence': evidence_df['confidence_score'].mean()
        }
        stats_dict['evidence_summary'] = evidence_stats
        
    result_data['descriptive_stats'] = stats_dict

except Exception as e:
    result_data['descriptive_stats']['error'] = f"Failed to generate descriptive statistics: {str(e)}"

try:
    # 3. Reliability Metrics (Cronbach's Alpha)
    reliability = {}
    
    # Define the items for the virtues and vices scales
    virtue_cols = [f"{v[0]}_score" for v in opposing_pairs.values()]
    vice_cols = [f"{v[1]}_score" for v in opposing_pairs.values()]

    # Calculate alpha for each scale
    reliability['virtues_scale_cronbach_alpha'] = cronbach_alpha(df[virtue_cols])
    reliability['vices_scale_cronbach_alpha'] = cronbach_alpha(df[vice_cols])

    result_data['reliability_metrics'] = reliability

except Exception as e:
    result_data['reliability_metrics']['error'] = f"Failed to calculate reliability metrics: {str(e)}"

try:
    # 4. Correlation Analysis
    # Select all numeric score and calculated columns for the correlation matrix
    cols_for_corr = [col for col in df.columns if df[col].dtype in ['float64', 'int64'] and col != 'aid']
    
    # Calculate the correlation matrix
    correlation_matrix = df[cols_for_corr].corr()
    
    # Convert to dict for JSON serialization, handling potential NaNs
    result_data['correlations'] = {
        'full_matrix': correlation_matrix.where(pd.notnull(correlation_matrix), None).to_dict()
    }
    
except Exception as e:
    result_data['correlations']['error'] = f"Failed to perform correlation analysis: {str(e)}"

try:
    # 5. Hypothesis Testing (Paired T-Tests for Opposing Dimensions)
    hypothesis_results = {}
    
    # Perform t-test for each opposing virtue/vice pair
    for tension_name, (virtue, vice) in opposing_pairs.items():
        virtue_col = f'{virtue}_score'
        vice_col = f'{vice}_score'
        
        # Ensure there is data to test
        if virtue_col in df.columns and vice_col in df.columns:
            v_data = df[virtue_col].dropna()
            c_data = df[vice_col].dropna()

            # Ensure data is paired and of equal length for the test
            if len(v_data) > 1 and len(v_data) == len(c_data):
                ttest_res = stats.ttest_rel(v_data, c_data)
                effect_size = cohen_d_paired(v_data, c_data)
                
                hypothesis_results[f'{virtue}_vs_{vice}'] = {
                    't_statistic': ttest_res.statistic,
                    'p_value': ttest_res.pvalue,
                    'cohen_d': effect_size
                }
            else:
                 hypothesis_results[f'{virtue}_vs_{vice}'] = {
                    'error': 'Not enough paired data to perform t-test.'
                }

    result_data['hypothesis_tests'] = hypothesis_results
    
except Exception as e:
    result_data['hypothesis_tests']['error'] = f"Failed during hypothesis testing: {str(e)}"

# The 'result_data' variable now holds all the structured results.
# The execution environment will capture this variable.