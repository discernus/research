{"descriptive_stats": {"primary_scores": {"dignity_score": {"count": 8.0, "mean": 0.74375, "std": 0.0728868987, "min": 0.6, "25%": 0.7, "50%": 0.775, "75%": 0.8, "max": 0.8}, "truth_score": {"count": 8.0, "mean": 0.6125, "std": 0.1125991626, "min": 0.4, "25%": 0.575, "50%": 0.65, "75%": 0.7, "max": 0.7}, "justice_score": {"count": 8.0, "mean": 0.76875, "std": 0.1869635182, "min": 0.4, "25%": 0.675, "50%": 0.875, "75%": 0.9, "max": 0.9}, "hope_score": {"count": 8.0, "mean": 0.6875, "std": 0.1726888201, "min": 0.3, "25%": 0.675, "50%": 0.75, "75%": 0.7625, "max": 0.85}, "pragmatism_score": {"count": 8.0, "mean": 0.60625, "std": 0.0776323754, "min": 0.5, "25%": 0.575, "50%": 0.6, "75%": 0.6625, "max": 0.7}, "tribalism_score": {"count": 8.0, "mean": 0.40625, "std": 0.1860059523, "min": 0.2, "25%": 0.275, "50%": 0.4, "75%": 0.4625, "max": 0.7}, "manipulation_score": {"count": 8.0, "mean": 0.46875, "std": 0.2404423008, "min": 0.1, "25%": 0.275, "50%": 0.525, "75%": 0.7, "max": 0.7}, "resentment_score": {"count": 8.0, "mean": 0.43125, "std": 0.2448578305, "min": 0.1, "25%": 0.2625, "50%": 0.425, "75%": 0.625, "max": 0.75}, "fear_score": {"count": 8.0, "mean": 0.28125, "std": 0.1811422093, "min": 0.05, "25%": 0.175, "50%": 0.25, "75%": 0.4, "max": 0.6}, "fantasy_score": {"count": 8.0, "mean": 0.26875, "std": 0.2086307401, "min": 0.0, "25%": 0.1125, "50%": 0.3, "75%": 0.4, "max": 0.6}}, "calculated_metrics": {"dignity_tribalism_tension": {"count": 8.0, "mean": 0.096875, "std": 0.0606475474, "min": 0.0, "25%": 0.06875, "50%": 0.1, "75%": 0.13, "max": 0.18}, "truth_manipulation_tension": {"count": 8.0, "mean": 0.0490625, "std": 0.0378893478, "min": 0.0, "25%": 0.0225, "50%": 0.05, "75%": 0.080625, "max": 0.1}, "justice_resentment_tension": {"count": 8.0, "mean": 0.12125, "std": 0.0694107855, "min": 0.0175, "25%": 0.065625, "50%": 0.15, "75%": 0.1575, "max": 0.21}, "hope_fear_tension": {"count": 8.0, "mean": 0.096875, "std": 0.058244405, "min": 0.03, "25%": 0.06125, "50%": 0.075, "75%": 0.1525, "max": 0.18}, "pragmatism_fantasy_tension": {"count": 8.0, "mean": 0.049375, "std": 0.0344795488, "min": 0.0, "25%": 0.03, "50%": 0.055, "75%": 0.07625, "max": 0.09}, "mc_sci": {"count": 8.0, "mean": 0.0826875, "std": 0.0396069416, "min": 0.0335, "25%": 0.056125, "50%": 0.084, "75%": 0.101625, "max": 0.152}}, "evidence_summary": {"total_quotes": 57, "quotes_per_dimension": {"justice": 11, "hope": 11, "dignity": 9, "tribalism": 6, "resentment": 5, "pragmatism": 5, "truth": 5, "manipulation": 4, "fantasy": 1}, "confidence_score_stats": {"count": 57.0, "mean": 0.7929824561, "std": 0.1151236123, "min": 0.5, "25%": 0.7, "50%": 0.8, "75%": 0.9, "max": 0.95}}}, "hypothesis_tests": {"virtue_vs_vice_paired_ttest": {"description": "Paired t-test comparing average virtue scores to average vice scores per artifact.", "t_statistic": 4.274195708676838, "p_value": 0.0036815960697378974, "effect_size_cohens_d": 1.5111563848619165, "interpretation": "p < 0.05 suggests a significant difference between expressed virtues and vices."}, "justice_vs_resentment_paired_ttest": {"description": "Paired t-test comparing Justice scores to Resentment scores per artifact.", "t_statistic": 3.8571428571428577, "p_value": 0.006234507883342638, "effect_size_cohens_d": 1.3637059351454848, "interpretation": "p < 0.05 suggests a significant difference between scores on this specific axis."}}, "correlations": {"full_matrix_sample": {"dignity_score": {"dignity_score": 1.0, "dignity_salience": 0.1871202971, "dignity_confidence": 0.9303473644, "truth_score": 0.8812159795, "truth_salience": -0.5564130355}, "dignity_salience": {"dignity_score": 0.1871202971, "dignity_salience": 1.0, "dignity_confidence": 0.362033094, "truth_score": 0.4360514248, "truth_salience": 0.5846883487}, "dignity_confidence": {"dignity_score": 0.9303473644, "dignity_salience": 0.362033094, "dignity_confidence": 1.0, "truth_score": 0.8536406217, "truth_salience": -0.4905519629}, "truth_score": {"dignity_score": 0.8812159795, "dignity_salience": 0.4360514248, "dignity_confidence": 0.8536406217, "truth_score": 1.0, "truth_salience": -0.2509072957}, "truth_salience": {"dignity_score": -0.5564130355, "dignity_salience": 0.5846883487, "dignity_confidence": -0.4905519629, "truth_score": -0.2509072957, "truth_salience": 1.0}, "truth_confidence": {"dignity_score": 0.5330812716, "dignity_salience": 0.4360514248, "dignity_confidence": 0.479441993, "truth_score": 0.7746478873, "truth_salience": 0.0080937837}, "justice_score": {"dignity_score": 0.3505334086, "dignity_salience": 0.8316074081, "dignity_confidence": 0.3767769137, "truth_score": 0.4622920687, "truth_salience": 0.3777735847}, "justice_salience": {"dignity_score": -0.0256944719, "dignity_salience": 0.7755769209, "dignity_confidence": -0.0030686832, "truth_score": 0.1219706766, "truth_salience": 0.6265755635}, "justice_confidence": {"dignity_score": 0.2920644447, "dignity_salience": 0.8464861425, "dignity_confidence": 0.3537947285, "truth_score": 0.3901178334, "truth_salience": 0.3828382921}, "hope_score": {"dignity_score": 0.7306434721, "dignity_salience": 0.0631824024, "dignity_confidence": 0.8158436352, "truth_score": 0.6704022299, "truth_salience": -0.7230079981}, "hope_salience": {"dignity_score": 0.6023238234, "dignity_salience": 0.2774924384, "dignity_confidence": 0.790296709, "truth_score": 0.6399607024, "truth_salience": -0.4913754038}, "hope_confidence": {"dignity_score": 0.705674832, "dignity_salience": -2.367599739e-17, "dignity_confidence": 0.7964321736, "truth_score": 0.6395094625, "truth_salience": -0.7349992822}, "pragmatism_score": {"dignity_score": 0.1972421118, "dignity_salience": 0.3865006029, "dignity_confidence": 0.3476949317, "truth_score": 0.5617804619, "truth_salience": 0.0528270544}, "pragmatism_salience": {"dignity_score": -0.4591694387, "dignity_salience": 0.5486301349, "dignity_confidence": -0.2046411217, "truth_score": -0.101491919, "truth_salience": 0.5915639735}, "pragmatism_confidence": {"dignity_score": 0.073394619, "dignity_salience": 0.1961161351, "dignity_confidence": 0.2051126457, "truth_score": 0.3610705962, "truth_salience": -0.1201271439}, "tribalism_score": {"dignity_score": 0.2930661606, "dignity_salience": -0.2493000955, "dignity_confidence": 0.2583767294, "truth_score": -0.0383674101, "truth_salience": -0.6050996086}, "tribalism_salience": {"dignity_score": 0.0774751635, "dignity_salience": -0.0690065559, "dignity_confidence": 0.1498959417, "truth_score": -0.1805424422, "truth_salience": -0.3112508256}, "tribalism_confidence": {"dignity_score": 0.44104677, "dignity_salience": -0.2357022604, "dignity_confidence": 0.3982160868, "truth_score": 0.1598773656, "truth_salience": -0.721874295}, "manipulation_score": {"dignity_score": -0.0534947214, "dignity_salience": 0.1021014712, "dignity_confidence": -0.2765463159, "truth_score": -0.1681923031, "truth_salience": 0.293749803}, "manipulation_salience": {"dignity_score": -0.3313667478, "dignity_salience": 0.0527046277, "dignity_confidence": -0.5088218513, "truth_score": -0.4289959891, "truth_salience": 0.4578344713}, "manipulation_confidence": {"dignity_score": -0.0810254654, "dignity_salience": -0.0360843918, "dignity_confidence": -0.2961115311, "truth_score": -0.2307748735, "truth_salience": 0.1326167761}, "resentment_score": {"dignity_score": 0.2726559697, "dignity_salience": -0.1448203927, "dignity_confidence": 0.013443532, "truth_score": 0.0615299299, "truth_salience": -0.1842374688}, "resentment_salience": {"dignity_score": 0.0946118737, "dignity_salience": -0.2407717062, "dignity_confidence": -0.1888625391, "truth_score": -0.1224869864, "truth_salience": -0.0904991215}, "resentment_confidence": {"dignity_score": 0.2892957196, "dignity_salience": -0.3312945782, "dignity_confidence": 0.0621909043, "truth_score": 0.0535042492, "truth_salience": -0.4734988619}, "fear_score": {"dignity_score": 0.0710071602, "dignity_salience": 0.0451753951, "dignity_confidence": -0.0399788597, "truth_score": -0.091927712, "truth_salience": 0.0125778701}, "fear_salience": {"dignity_score": -0.4157753572, "dignity_salience": -2.095502238e-17, "dignity_confidence": -0.4469033536, "truth_score": -0.448561304, "truth_salience": 0.3093235573}, "fear_confidence": {"dignity_score": 0.0864865021, "dignity_salience": -0.1283881478, "dignity_confidence": -0.0309871722, "truth_score": -0.0124408522, "truth_salience": -0.2788204867}, "fantasy_score": {"dignity_score": -0.437431929, "dignity_salience": -0.5099019514, "dignity_confidence": -0.6721383619, "truth_score": -0.6195211282, "truth_salience": 0.058971507}, "fantasy_salience": {"dignity_score": -0.6274299906, "dignity_salience": -0.5069711419, "dignity_confidence": -0.8109921605, "truth_score": -0.7780134255, "truth_salience": 0.1949873213}, "fantasy_confidence": {"dignity_score": -0.4013349612, "dignity_salience": -0.3776050203, "dignity_confidence": -0.5285946534, "truth_score": -0.5342146719, "truth_salience": -0.0967232168}, "dignity_tribalism_tension": {"dignity_score": 0.3100470784, "dignity_salience": 0.5532128188, "dignity_confidence": 0.2637851906, "truth_score": 0.5922864214, "truth_salience": 0.2907732721}, "truth_manipulation_tension": {"dignity_score": 0.3596792595, "dignity_salience": 0.6587252784, "dignity_confidence": 0.6090153946, "truth_score": 0.4802997674, "truth_salience": 0.0078172318}, "justice_resentment_tension": {"dignity_score": 0.2206051057, "dignity_salience": 0.5030178263, "dignity_confidence": 0.1233030364, "truth_score": 0.1987783611, "truth_salience": 0.3321852301}, "hope_fear_tension": {"dignity_score": 0.6425242262, "dignity_salience": 0.1358138781, "dignity_confidence": 0.545946764, "truth_score": 0.4315706141, "truth_salience": -0.4608057001}, "pragmatism_fantasy_tension": {"dignity_score": 0.2540253004, "dignity_salience": 0.0712002143, "dignity_confidence": 0.0286409265, "truth_score": 0.204679925, "truth_salience": -0.0013215853}, "mc_sci": {"dignity_score": 0.4742911033, "dignity_salience": 0.5240994629, "dignity_confidence": 0.4060780713, "truth_score": 0.5055183108, "truth_salience": 0.0712157582}}, "key_correlations": {"mc_sci_vs_avg_virtue": 0.6051786478296544, "mc_sci_vs_avg_vice": 0.5635975423669689, "avg_virtue_vs_avg_vice": -0.1540789666111015}}, "reliability_metrics": {"virtues_cronbach_alpha": 0.7266891180804042, "vices_cronbach_alpha": 0.861210612591828}}