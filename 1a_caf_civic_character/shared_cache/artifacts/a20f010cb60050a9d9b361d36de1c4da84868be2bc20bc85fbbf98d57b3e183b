{"stage_1_raw_data": {"analysis_plan": {"stage": "raw_data_collection", "experiment_summary": "This plan outlines the collection and initial validation of raw data from the LLM analysis based on the Character Assessment Framework v6.1. The focus is on capturing the 30 primary data points (score, salience, confidence) for each of the 10 character dimensions across all 8 documents, as specified in the framework's output contract. The plan includes validation steps to ensure data integrity, completeness, and adherence to the 0.0-1.0 scale before proceeding to Stage 2 analysis.", "tasks": {"validate_raw_data_integrity": {"tool": "calculate_descriptive_stats", "parameters": {"columns": ["dignity_score", "dignity_salience", "dignity_confidence", "truth_score", "truth_salience", "truth_confidence", "justice_score", "justice_salience", "justice_confidence", "hope_score", "hope_salience", "hope_confidence", "pragmatism_score", "pragmatism_salience", "pragmatism_confidence", "tribalism_score", "tribalism_salience", "tribalism_confidence", "manipulation_score", "manipulation_salience", "manipulation_confidence", "resentment_score", "resentment_salience", "resentment_confidence", "fear_score", "fear_salience", "fear_confidence", "fantasy_score", "fantasy_salience", "fantasy_confidence"]}, "purpose": "To perform a comprehensive initial validation of all 30 raw data points collected from the LLM. This task verifies data completeness by checking the 'count' for each column and ensures all scores, salience, and confidence values fall within the required 0.0-1.0 range by checking 'min' and 'max' values, as stipulated by the CAF v6.1 output contract."}, "summarize_raw_virtue_scores": {"tool": "create_summary_statistics", "parameters": {"metrics": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score"], "summary_types": ["mean", "std", "min", "max"]}, "purpose": "To generate a high-level summary of the raw virtue scores collected from the LLM. This provides a baseline understanding of the central tendency and dispersion of virtue-related data across the entire corpus before any grouping or statistical testing."}, "summarize_raw_vice_scores": {"tool": "create_summary_statistics", "parameters": {"metrics": ["tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score"], "summary_types": ["mean", "std", "min", "max"]}, "purpose": "To generate a high-level summary of the raw vice scores collected from the LLM. This provides a baseline understanding of the central tendency and dispersion of vice-related data, complementing the virtue score summary."}, "validate_metadata_scores": {"tool": "create_summary_statistics", "parameters": {"metrics": ["dignity_salience", "dignity_confidence", "truth_salience", "truth_confidence", "justice_salience", "justice_confidence", "hope_salience", "hope_confidence", "pragmatism_salience", "pragmatism_confidence", "tribalism_salience", "tribalism_confidence", "manipulation_salience", "manipulation_confidence", "resentment_salience", "resentment_confidence", "fear_salience", "fear_confidence", "fantasy_salience", "fantasy_confidence"], "summary_types": ["mean", "std"]}, "purpose": "To assess the quality and distribution of the salience and confidence metadata generated by the LLM. This step helps confirm that the LLM is providing varied and meaningful metadata scores rather than default or uniform values, which is critical for the validity of Stage 2 calculations like the MC-SCI."}}}, "results": {"validate_raw_data_integrity": {"type": "descriptive_stats", "columns_analyzed": ["dignity_score", "dignity_salience", "dignity_confidence", "truth_score", "truth_salience", "truth_confidence", "justice_score", "justice_salience", "justice_confidence", "hope_score", "hope_salience", "hope_confidence", "pragmatism_score", "pragmatism_salience", "pragmatism_confidence", "tribalism_score", "tribalism_salience", "tribalism_confidence", "manipulation_score", "manipulation_salience", "manipulation_confidence", "resentment_score", "resentment_salience", "resentment_confidence", "fear_score", "fear_salience", "fear_confidence", "fantasy_score", "fantasy_salience", "fantasy_confidence"], "results": {"dignity_score": {"count": 7, "mean": 0.7071428571428572, "std": 0.11700630833624397, "min": 0.6, "max": 0.9, "median": 0.7, "q25": 0.6, "q75": 0.775, "skewness": 0.6354172449685459, "kurtosis": -0.7777542533081299}, "dignity_salience": {"count": 7, "mean": 0.7571428571428572, "std": 0.09322272357358044, "min": 0.7, "max": 0.95, "median": 0.7, "q25": 0.7, "q75": 0.775, "skewness": 1.8735510780128417, "kurtosis": 3.432351285419381}, "dignity_confidence": {"count": 7, "mean": 0.7928571428571428, "std": 0.10177004891982151, "min": 0.7, "max": 0.9, "median": 0.75, "q25": 0.7, "q75": 0.9, "skewness": 0.267675800488283, "kurtosis": -2.6951248513674186}, "truth_score": {"count": 8, "mean": 0.6125, "std": 0.15294723646688466, "min": 0.4, "max": 0.85, "median": 0.625, "q25": 0.55, "q75": 0.7, "skewness": -0.23457673001622417, "kurtosis": -0.27672047083503326}, "truth_salience": {"count": 8, "mean": 0.675, "std": 0.13887301496588275, "min": 0.4, "max": 0.8, "median": 0.7, "q25": 0.6, "q75": 0.8, "skewness": -1.1201280219470378, "kurtosis": 1.1061728395061703}, "truth_confidence": {"count": 8, "mean": 0.7687499999999999, "std": 0.11933596033288303, "min": 0.6, "max": 0.95, "median": 0.8, "q25": 0.7125, "q75": 0.8125, "skewness": -0.34017919805586205, "kurtosis": -0.2550132172443309}, "justice_score": {"count": 8, "mean": 0.73125, "std": 0.15797264681854625, "min": 0.5, "max": 0.9, "median": 0.775, "q25": 0.6499999999999999, "q75": 0.8250000000000001, "skewness": -0.70039978049317, "kurtosis": -0.8331514556085038}, "justice_salience": {"count": 8, "mean": 0.7875, "std": 0.18850918886280923, "min": 0.5, "max": 0.95, "median": 0.875, "q25": 0.6875, "q75": 0.9125, "skewness": -1.0209704509304138, "kurtosis": -0.719779803540316}, "justice_confidence": {"count": 8, "mean": 0.8187499999999999, "std": 0.14126343172547218, "min": 0.5, "max": 0.95, "median": 0.85, "q25": 0.8, "q75": 0.875, "skewness": -1.8774322889140391, "kurtosis": 4.501490923832252}, "hope_score": {"count": 8, "mean": 0.6475, "std": 0.23614462880676687, "min": 0.2, "max": 0.9, "median": 0.7, "q25": 0.5, "q75": 0.8200000000000001, "skewness": -0.9189397658129868, "kurtosis": 0.45756744270958816}, "hope_salience": {"count": 8, "mean": 0.6625, "std": 0.24892626561752318, "min": 0.2, "max": 0.9, "median": 0.75, "q25": 0.55, "q75": 0.85, "skewness": -1.0870897317668808, "kurtosis": 0.14808195400676105}, "hope_confidence": {"count": 8, "mean": 0.765, "std": 0.1461897006338975, "min": 0.6, "max": 0.95, "median": 0.75, "q25": 0.6375, "q75": 0.905, "skewness": 0.10845917943952225, "kurtosis": -2.079485580085219}, "pragmatism_score": {"count": 8, "mean": 0.5249999999999999, "std": 0.12817398889233114, "min": 0.3, "max": 0.7, "median": 0.55, "q25": 0.475, "q75": 0.6, "skewness": -0.6105830850825585, "kurtosis": -0.021172022684313063}, "pragmatism_salience": {"count": 8, "mean": 0.51875, "std": 0.14623244705409455, "min": 0.3, "max": 0.75, "median": 0.55, "q25": 0.4, "q75": 0.6, "skewness": 0.009279703405700527, "kurtosis": -0.6426401558570607}, "pragmatism_confidence": {"count": 8, "mean": 0.65625, "std": 0.11783008347374015, "min": 0.5, "max": 0.85, "median": 0.7, "q25": 0.575, "q75": 0.7, "skewness": -0.023195425908955142, "kurtosis": -0.22939382347163573}, "tribalism_score": {"count": 8, "mean": 0.4125, "std": 0.18850918886280923, "min": 0.1, "max": 0.7, "median": 0.4, "q25": 0.3, "q75": 0.525, "skewness": -0.06664297982574521, "kurtosis": -0.01725208959369784}, "tribalism_salience": {"count": 8, "mean": 0.42500000000000004, "std": 0.17525491637693283, "min": 0.1, "max": 0.7, "median": 0.45, "q25": 0.375, "q75": 0.5, "skewness": -0.5042488670228694, "kurtosis": 1.3568415359653878}, "tribalism_confidence": {"count": 8, "mean": 0.6499999999999999, "std": 0.13093073414159542, "min": 0.4, "max": 0.8, "median": 0.6499999999999999, "q25": 0.6, "q75": 0.725, "skewness": -0.7637626158259697, "kurtosis": 0.8749999999999947}, "manipulation_score": {"count": 8, "mean": 0.40625, "std": 0.2111828929760038, "min": 0.15, "max": 0.7, "median": 0.4, "q25": 0.2, "q75": 0.6, "skewness": 0.1116263407578763, "kurtosis": -1.763594224855487}, "manipulation_salience": {"count": 8, "mean": 0.4749999999999999, "std": 0.21380899352993948, "min": 0.15, "max": 0.7, "median": 0.55, "q25": 0.2875, "q75": 0.625, "skewness": -0.4750150979302847, "kurtosis": -1.5741455078125002}, "manipulation_confidence": {"count": 8, "mean": 0.6499999999999999, "std": 0.13627702877384937, "min": 0.5, "max": 0.85, "median": 0.675, "q25": 0.5, "q75": 0.75, "skewness": 0.0, "kurtosis": -1.5946745562130182}, "resentment_score": {"count": 8, "mean": 0.39375000000000004, "std": 0.2043063176423368, "min": 0.05, "max": 0.7, "median": 0.35, "q25": 0.3, "q75": 0.525, "skewness": -0.09815396884072479, "kurtosis": 0.005367954473961767}, "resentment_salience": {"count": 8, "mean": 0.41875, "std": 0.19628241315585487, "min": 0.05, "max": 0.7, "median": 0.4, "q25": 0.375, "q75": 0.525, "skewness": -0.5817875780138718, "kurtosis": 1.121535402252242}, "resentment_confidence": {"count": 8, "mean": 0.64375, "std": 0.14252192813739223, "min": 0.4, "max": 0.8, "median": 0.7, "q25": 0.5375000000000001, "q75": 0.75, "skewness": -0.7702689769562403, "kurtosis": -0.823885038038882}, "fear_score": {"count": 8, "mean": 0.25625000000000003, "std": 0.1347948176197544, "min": 0.1, "max": 0.5, "median": 0.2, "q25": 0.1875, "q75": 0.325, "skewness": 0.9323479781707691, "kurtosis": -0.004124383485562433}, "fear_salience": {"count": 8, "mean": 0.28125, "std": 0.09977653603356425, "min": 0.1, "max": 0.4, "median": 0.3, "q25": 0.2375, "q75": 0.325, "skewness": -0.6044898304046372, "kurtosis": 0.3646323071045048}, "fear_confidence": {"count": 8, "mean": 0.5187499999999999, "std": 0.16889874396894047, "min": 0.3, "max": 0.7, "median": 0.55, "q25": 0.375, "q75": 0.6625, "skewness": -0.3127134054837214, "kurtosis": -1.8339571072758938}, "fantasy_score": {"count": 8, "mean": 0.175, "std": 0.10350983390135313, "min": 0.0, "max": 0.3, "median": 0.2, "q25": 0.1, "q75": 0.225, "skewness": -0.3864367132317181, "kurtosis": -0.4479999999999986}, "fantasy_salience": {"count": 8, "mean": 0.20625, "std": 0.126596941962615, "min": 0.0, "max": 0.4, "median": 0.2, "q25": 0.1375, "q75": 0.3, "skewness": -0.1155158663790897, "kurtosis": -0.21386550383687286}, "fantasy_confidence": {"count": 8, "mean": 0.5062500000000001, "std": 0.20430631764233684, "min": 0.3, "max": 0.9, "median": 0.45, "q25": 0.375, "q75": 0.6125, "skewness": 1.0027409456768288, "kurtosis": 0.6916462009208111}}}, "summarize_raw_virtue_scores": {"type": "summary_statistics", "metrics": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score"], "summary_types": ["mean", "std", "min", "max"], "results": {"dignity_score": {"mean": 0.7071428571428572, "std": 0.11700630833624397, "min": 0.6, "max": 0.9}, "truth_score": {"mean": 0.6125, "std": 0.15294723646688466, "min": 0.4, "max": 0.85}, "justice_score": {"mean": 0.73125, "std": 0.15797264681854625, "min": 0.5, "max": 0.9}, "hope_score": {"mean": 0.6475, "std": 0.23614462880676687, "min": 0.2, "max": 0.9}, "pragmatism_score": {"mean": 0.5249999999999999, "std": 0.12817398889233114, "min": 0.3, "max": 0.7}}, "missing_metrics": []}, "summarize_raw_vice_scores": {"type": "summary_statistics", "metrics": ["tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score"], "summary_types": ["mean", "std", "min", "max"], "results": {"tribalism_score": {"mean": 0.4125, "std": 0.18850918886280923, "min": 0.1, "max": 0.7}, "manipulation_score": {"mean": 0.40625, "std": 0.2111828929760038, "min": 0.15, "max": 0.7}, "resentment_score": {"mean": 0.39375000000000004, "std": 0.2043063176423368, "min": 0.05, "max": 0.7}, "fear_score": {"mean": 0.25625000000000003, "std": 0.1347948176197544, "min": 0.1, "max": 0.5}, "fantasy_score": {"mean": 0.175, "std": 0.10350983390135313, "min": 0.0, "max": 0.3}}, "missing_metrics": []}, "validate_metadata_scores": {"type": "summary_statistics", "metrics": ["dignity_salience", "dignity_confidence", "truth_salience", "truth_confidence", "justice_salience", "justice_confidence", "hope_salience", "hope_confidence", "pragmatism_salience", "pragmatism_confidence", "tribalism_salience", "tribalism_confidence", "manipulation_salience", "manipulation_confidence", "resentment_salience", "resentment_confidence", "fear_salience", "fear_confidence", "fantasy_salience", "fantasy_confidence"], "summary_types": ["mean", "std"], "results": {"dignity_salience": {"mean": 0.7571428571428572, "std": 0.09322272357358044}, "dignity_confidence": {"mean": 0.7928571428571428, "std": 0.10177004891982151}, "truth_salience": {"mean": 0.675, "std": 0.13887301496588275}, "truth_confidence": {"mean": 0.7687499999999999, "std": 0.11933596033288303}, "justice_salience": {"mean": 0.7875, "std": 0.18850918886280923}, "justice_confidence": {"mean": 0.8187499999999999, "std": 0.14126343172547218}, "hope_salience": {"mean": 0.6625, "std": 0.24892626561752318}, "hope_confidence": {"mean": 0.765, "std": 0.1461897006338975}, "pragmatism_salience": {"mean": 0.51875, "std": 0.14623244705409455}, "pragmatism_confidence": {"mean": 0.65625, "std": 0.11783008347374015}, "tribalism_salience": {"mean": 0.42500000000000004, "std": 0.17525491637693283}, "tribalism_confidence": {"mean": 0.6499999999999999, "std": 0.13093073414159542}, "manipulation_salience": {"mean": 0.4749999999999999, "std": 0.21380899352993948}, "manipulation_confidence": {"mean": 0.6499999999999999, "std": 0.13627702877384937}, "resentment_salience": {"mean": 0.41875, "std": 0.19628241315585487}, "resentment_confidence": {"mean": 0.64375, "std": 0.14252192813739223}, "fear_salience": {"mean": 0.28125, "std": 0.09977653603356425}, "fear_confidence": {"mean": 0.5187499999999999, "std": 0.16889874396894047}, "fantasy_salience": {"mean": 0.20625, "std": 0.126596941962615}, "fantasy_confidence": {"mean": 0.5062500000000001, "std": 0.20430631764233684}}, "missing_metrics": []}}, "errors": []}, "stage_2_derived_metrics": {"analysis_plan": {"stage": "derived_metrics_analysis", "experiment_summary": "This plan outlines Stage 2 of the analysis, focusing on calculating derived metrics as specified by the CAF v6.1 framework, including character tensions and the Moral Character Strategic Contradiction Index (MC-SCI). It then details the statistical analyses to test the experiment's hypotheses regarding speaker differentiation, character signatures, and coherence patterns using ANOVA and correlation matrices, adhering strictly to the available dimensional data.", "tasks": {"task_1_calculate_derived_metrics": {"tool": "calculate_derived_metrics", "parameters": {"metric_formulas": {"dignity_tribalism_tension": "min(dignity_score, tribalism_score) * abs(dignity_salience - tribalism_salience)", "truth_manipulation_tension": "min(truth_score, manipulation_score) * abs(truth_salience - manipulation_salience)", "justice_resentment_tension": "min(justice_score, resentment_score) * abs(justice_salience - resentment_salience)", "hope_fear_tension": "min(hope_score, fear_score) * abs(hope_salience - fear_salience)", "pragmatism_fantasy_tension": "min(pragmatism_score, fantasy_score) * abs(pragmatism_salience - fantasy_salience)", "mc_sci": "(dignity_tribalism_tension + truth_manipulation_tension + justice_resentment_tension + hope_fear_tension + pragmatism_fantasy_tension) / 5"}, "input_columns": ["dignity_score", "dignity_salience", "tribalism_score", "tribalism_salience", "truth_score", "truth_salience", "manipulation_score", "manipulation_salience", "justice_score", "justice_salience", "resentment_score", "resentment_salience", "hope_score", "hope_salience", "fear_score", "fear_salience", "pragmatism_score", "pragmatism_salience", "fantasy_score", "fantasy_salience"]}, "purpose": "To calculate the five character tension scores and the aggregate Moral Character Strategic Contradiction Index (MC-SCI) for each document, as defined by the CAF v6.1 framework. These metrics are essential for testing H3."}, "task_2_validate_calculated_metrics": {"tool": "validate_calculated_metrics", "parameters": {"validation_rules": ["mc_sci >= 0", "dignity_tribalism_tension >= 0", "truth_manipulation_tension >= 0", "justice_resentment_tension >= 0", "hope_fear_tension >= 0", "pragmatism_fantasy_tension >= 0"], "quality_thresholds": {"completeness_threshold": 1.0}}, "purpose": "To ensure the integrity and quality of the newly calculated derived metrics, verifying they fall within expected non-negative ranges and that no calculations resulted in null values."}, "task_3_generate_descriptive_statistics": {"tool": "calculate_descriptive_stats", "parameters": {"columns": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score", "mc_sci"]}, "purpose": "To generate foundational descriptive statistics (mean, std, min, max) for all raw dimensions and the calculated MC-SCI score, providing a baseline understanding of the data distribution across the corpus."}, "task_4_speaker_differentiation_anova": {"tool": "perform_one_way_anova", "parameters": {"grouping_variable": "dignity_score", "dependent_variable": "mc_sci"}, "purpose": "To test H1 (Speaker Differentiation) by proxy. This analysis determines if documents with varying levels of a key virtue (Dignity) show statistically significant differences in character coherence (MC-SCI), thereby testing the framework's ability to create distinct profiles."}, "task_5_character_signature_analysis": {"tool": "generate_correlation_matrix", "parameters": {"dimensions": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score"], "correlation_method": "pearson"}, "purpose": "To address H2 (Character Signatures) by examining the inter-correlations between the 10 CAF dimensions. The resulting matrix will reveal patterns of co-occurrence (e.g., Hope with Justice) that define the underlying character signatures in the corpus."}, "task_6_mc_sci_coherence_pattern_analysis": {"tool": "generate_correlation_matrix", "parameters": {"dimensions": ["mc_sci", "dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score"], "correlation_method": "pearson"}, "purpose": "To directly test H3 (MC-SCI Patterns) by correlating the MC-SCI score with each of the 10 character dimensions. This will identify which virtues and vices are the strongest drivers of character incoherence."}}}, "results": {"task_1_calculate_derived_metrics": {"type": "derived_metrics", "metrics": {"dignity_tribalism_tension": [0.15, 0.08999999999999998, 0.12000000000000002, 0.0, 0.16, 0.085, 0.08999999999999998, NaN], "truth_manipulation_tension": [0.059999999999999984, 0.08000000000000003, 0.08, 0.039999999999999994, 0.11000000000000001, 0.08249999999999999, 0.06000000000000005, 0.07999999999999999], "justice_resentment_tension": [0.175, 0.12000000000000004, 0.16499999999999998, 0.029999999999999992, 0.24499999999999997, 0.034999999999999996, 0.18000000000000002, 0.039999999999999994], "hope_fear_tension": [0.11999999999999998, 0.06, 0.09749999999999999, 0.1, 0.22499999999999998, 0.08000000000000002, 0.030000000000000006, 0.04000000000000001], "pragmatism_fantasy_tension": [0.04000000000000001, 0.04000000000000001, 0.06, 0.059999999999999984, 0.030000000000000006, 0.0, 0.08, 0.0], "mc_sci": [0.10899999999999999, 0.078, 0.1045, 0.046, 0.154, 0.056499999999999995, 0.08800000000000002, NaN]}, "formulas_used": {"dignity_tribalism_tension": "min(dignity_score, tribalism_score) * abs(dignity_salience - tribalism_salience)", "truth_manipulation_tension": "min(truth_score, manipulation_score) * abs(truth_salience - manipulation_salience)", "justice_resentment_tension": "min(justice_score, resentment_score) * abs(justice_salience - resentment_salience)", "hope_fear_tension": "min(hope_score, fear_score) * abs(hope_salience - fear_salience)", "pragmatism_fantasy_tension": "min(pragmatism_score, fantasy_score) * abs(pragmatism_salience - fantasy_salience)", "mc_sci": "(dignity_tribalism_tension + truth_manipulation_tension + justice_resentment_tension + hope_fear_tension + pragmatism_fantasy_tension) / 5"}}, "task_2_validate_calculated_metrics": {"type": "metric_validation", "validation_rules": ["mc_sci >= 0", "dignity_tribalism_tension >= 0", "truth_manipulation_tension >= 0", "justice_resentment_tension >= 0", "hope_fear_tension >= 0", "pragmatism_fantasy_tension >= 0"], "results": {"mc_sci >= 0": {"error": "Unknown validation rule: mc_sci >= 0"}, "dignity_tribalism_tension >= 0": {"error": "Unknown validation rule: dignity_tribalism_tension >= 0"}, "truth_manipulation_tension >= 0": {"error": "Unknown validation rule: truth_manipulation_tension >= 0"}, "justice_resentment_tension >= 0": {"error": "Unknown validation rule: justice_resentment_tension >= 0"}, "hope_fear_tension >= 0": {"error": "Unknown validation rule: hope_fear_tension >= 0"}, "pragmatism_fantasy_tension >= 0": {"error": "Unknown validation rule: pragmatism_fantasy_tension >= 0"}}, "quality_thresholds": {"completeness_threshold": 1.0}}, "task_3_generate_descriptive_statistics": {"type": "descriptive_stats", "columns_analyzed": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score", "mc_sci"], "results": {"dignity_score": {"count": 7, "mean": 0.7071428571428572, "std": 0.11700630833624397, "min": 0.6, "max": 0.9, "median": 0.7, "q25": 0.6, "q75": 0.775, "skewness": 0.6354172449685459, "kurtosis": -0.7777542533081299}, "truth_score": {"count": 8, "mean": 0.6125, "std": 0.15294723646688466, "min": 0.4, "max": 0.85, "median": 0.625, "q25": 0.55, "q75": 0.7, "skewness": -0.23457673001622417, "kurtosis": -0.27672047083503326}, "justice_score": {"count": 8, "mean": 0.73125, "std": 0.15797264681854625, "min": 0.5, "max": 0.9, "median": 0.775, "q25": 0.6499999999999999, "q75": 0.8250000000000001, "skewness": -0.70039978049317, "kurtosis": -0.8331514556085038}, "hope_score": {"count": 8, "mean": 0.6475, "std": 0.23614462880676687, "min": 0.2, "max": 0.9, "median": 0.7, "q25": 0.5, "q75": 0.8200000000000001, "skewness": -0.9189397658129868, "kurtosis": 0.45756744270958816}, "pragmatism_score": {"count": 8, "mean": 0.5249999999999999, "std": 0.12817398889233114, "min": 0.3, "max": 0.7, "median": 0.55, "q25": 0.475, "q75": 0.6, "skewness": -0.6105830850825585, "kurtosis": -0.021172022684313063}, "tribalism_score": {"count": 8, "mean": 0.4125, "std": 0.18850918886280923, "min": 0.1, "max": 0.7, "median": 0.4, "q25": 0.3, "q75": 0.525, "skewness": -0.06664297982574521, "kurtosis": -0.01725208959369784}, "manipulation_score": {"count": 8, "mean": 0.40625, "std": 0.2111828929760038, "min": 0.15, "max": 0.7, "median": 0.4, "q25": 0.2, "q75": 0.6, "skewness": 0.1116263407578763, "kurtosis": -1.763594224855487}, "resentment_score": {"count": 8, "mean": 0.39375000000000004, "std": 0.2043063176423368, "min": 0.05, "max": 0.7, "median": 0.35, "q25": 0.3, "q75": 0.525, "skewness": -0.09815396884072479, "kurtosis": 0.005367954473961767}, "fear_score": {"count": 8, "mean": 0.25625000000000003, "std": 0.1347948176197544, "min": 0.1, "max": 0.5, "median": 0.2, "q25": 0.1875, "q75": 0.325, "skewness": 0.9323479781707691, "kurtosis": -0.004124383485562433}, "fantasy_score": {"count": 8, "mean": 0.175, "std": 0.10350983390135313, "min": 0.0, "max": 0.3, "median": 0.2, "q25": 0.1, "q75": 0.225, "skewness": -0.3864367132317181, "kurtosis": -0.4479999999999986}, "mc_sci": {"count": 7, "mean": 0.09085714285714284, "std": 0.03621315861869629, "min": 0.046, "max": 0.154, "median": 0.08800000000000002, "q25": 0.06725, "q75": 0.10674999999999998, "skewness": 0.6254639676879901, "kurtosis": 0.43659031556905337}}}, "task_4_speaker_differentiation_anova": {"type": "one_way_anova", "grouping_variable": "dignity_score", "dependent_variable": "mc_sci", "groups": {"0.6": {"n": 3, "mean": 0.07066666666666667, "std": 0.017913371790059213}, "0.7": {"n": 1, "mean": 0.10899999999999999, "std": 0.0}, "0.75": {"n": 1, "mean": 0.1045, "std": 0.0}, "0.8": {"n": 1, "mean": 0.154, "std": 0.0}, "0.9": {"n": 1, "mean": 0.056499999999999995, "std": 0.0}}, "f_statistic": 3.5867505935892336, "p_value": 0.22972449221340205, "significant": "False"}, "task_5_character_signature_analysis": {"type": "correlation_matrix", "dimensions": ["dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score"], "method": "pearson", "matrix": {"dignity_score": {"dignity_score": 1.0, "truth_score": 0.576770389298682, "justice_score": 0.4843884930464273, "hope_score": 0.8893427293057098, "pragmatism_score": 0.5421374765483945, "tribalism_score": -0.49854861812834383, "manipulation_score": -0.6651662082893139, "resentment_score": -0.2881837200291647, "fear_score": 0.061952247412989436, "fantasy_score": -0.9174634218511292}, "truth_score": {"dignity_score": 0.576770389298682, "truth_score": 1.0, "justice_score": 0.897975984067584, "hope_score": 0.6140643187067323, "pragmatism_score": 0.6740655909056726, "tribalism_score": -0.6007718129713515, "manipulation_score": -0.4671628644330198, "resentment_score": 0.20858419942357415, "fear_score": 0.11693133599279322, "fantasy_score": -0.7895629608911476}, "justice_score": {"dignity_score": 0.4843884930464273, "truth_score": 0.897975984067584, "justice_score": 1.0, "hope_score": 0.6093690745575551, "pragmatism_score": 0.7672721428435879, "tribalism_score": -0.42275297305978327, "manipulation_score": -0.35996778188527995, "resentment_score": 0.19503268660627737, "fear_score": 0.0062895293220607915, "fantasy_score": -0.7316835130490091}, "hope_score": {"dignity_score": 0.8893427293057098, "truth_score": 0.6140643187067323, "justice_score": 0.6093690745575551, "hope_score": 1.0, "pragmatism_score": 0.7952870179044798, "tribalism_score": -0.20779310668060388, "manipulation_score": -0.6971738961522156, "resentment_score": -0.14397984649651582, "fear_score": -0.2126179447022601, "fantasy_score": -0.7510094540244864}, "pragmatism_score": {"dignity_score": 0.5421374765483945, "truth_score": 0.6740655909056726, "justice_score": 0.7672721428435879, "hope_score": 0.7952870179044798, "pragmatism_score": 1.0, "tribalism_score": -0.31040509310661096, "manipulation_score": -0.5607538494404394, "resentment_score": -0.2932234722018026, "fear_score": -0.3410773006656662, "fantasy_score": -0.6998964726756151}, "tribalism_score": {"dignity_score": -0.49854861812834383, "truth_score": -0.6007718129713515, "justice_score": -0.42275297305978327, "hope_score": -0.20779310668060388, "pragmatism_score": -0.31040509310661096, "tribalism_score": 1.0, "manipulation_score": 0.48220218903683554, "resentment_score": 0.31760565283189346, "fear_score": 0.3619207488858103, "fantasy_score": 0.7504325943927066}, "manipulation_score": {"dignity_score": -0.6651662082893139, "truth_score": -0.4671628644330198, "justice_score": -0.35996778188527995, "hope_score": -0.6971738961522156, "pragmatism_score": -0.5607538494404394, "tribalism_score": 0.48220218903683554, "manipulation_score": 1.0, "resentment_score": 0.191418178326564, "fear_score": 0.21171606805422713, "fantasy_score": 0.72704557073721}, "resentment_score": {"dignity_score": -0.2881837200291647, "truth_score": 0.20858419942357415, "justice_score": 0.19503268660627737, "hope_score": -0.14397984649651582, "pragmatism_score": -0.2932234722018026, "tribalism_score": 0.31760565283189346, "manipulation_score": 0.191418178326564, "resentment_score": 1.0, "fear_score": 0.7408208383886864, "fantasy_score": 0.2617642051708646}, "fear_score": {"dignity_score": 0.061952247412989436, "truth_score": 0.11693133599279322, "justice_score": 0.0062895293220607915, "hope_score": -0.2126179447022601, "pragmatism_score": -0.3410773006656662, "tribalism_score": 0.3619207488858103, "manipulation_score": 0.21171606805422713, "resentment_score": 0.7408208383886864, "fear_score": 1.0, "fantasy_score": 0.268767319880733}, "fantasy_score": {"dignity_score": -0.9174634218511292, "truth_score": -0.7895629608911476, "justice_score": -0.7316835130490091, "hope_score": -0.7510094540244864, "pragmatism_score": -0.6998964726756151, "tribalism_score": 0.7504325943927066, "manipulation_score": 0.72704557073721, "resentment_score": 0.2617642051708646, "fear_score": 0.268767319880733, "fantasy_score": 1.0}}, "missing_dimensions": []}, "task_6_mc_sci_coherence_pattern_analysis": {"type": "correlation_matrix", "dimensions": ["mc_sci", "dignity_score", "truth_score", "justice_score", "hope_score", "pragmatism_score", "tribalism_score", "manipulation_score", "resentment_score", "fear_score", "fantasy_score"], "method": "pearson", "matrix": {"mc_sci": {"mc_sci": 1.0, "dignity_score": 0.2431711799217441, "truth_score": 0.731120786646122, "justice_score": 0.8075098688064849, "hope_score": 0.2525238215566086, "pragmatism_score": 0.3115951500469707, "tribalism_score": 0.04717438076366382, "manipulation_score": -0.04119910677829538, "resentment_score": 0.7011813545986012, "fear_score": 0.8235762391269843, "fantasy_score": -0.2496129796592487}, "dignity_score": {"mc_sci": 0.2431711799217441, "dignity_score": 1.0, "truth_score": 0.576770389298682, "justice_score": 0.4843884930464273, "hope_score": 0.8893427293057098, "pragmatism_score": 0.5421374765483945, "tribalism_score": -0.49854861812834383, "manipulation_score": -0.6651662082893139, "resentment_score": -0.2881837200291647, "fear_score": 0.061952247412989436, "fantasy_score": -0.9174634218511292}, "truth_score": {"mc_sci": 0.731120786646122, "dignity_score": 0.576770389298682, "truth_score": 1.0, "justice_score": 0.897975984067584, "hope_score": 0.6140643187067323, "pragmatism_score": 0.6740655909056726, "tribalism_score": -0.6007718129713515, "manipulation_score": -0.4671628644330198, "resentment_score": 0.20858419942357415, "fear_score": 0.11693133599279322, "fantasy_score": -0.7895629608911476}, "justice_score": {"mc_sci": 0.8075098688064849, "dignity_score": 0.4843884930464273, "truth_score": 0.897975984067584, "justice_score": 1.0, "hope_score": 0.6093690745575551, "pragmatism_score": 0.7672721428435879, "tribalism_score": -0.42275297305978327, "manipulation_score": -0.35996778188527995, "resentment_score": 0.19503268660627737, "fear_score": 0.0062895293220607915, "fantasy_score": -0.7316835130490091}, "hope_score": {"mc_sci": 0.2525238215566086, "dignity_score": 0.8893427293057098, "truth_score": 0.6140643187067323, "justice_score": 0.6093690745575551, "hope_score": 1.0, "pragmatism_score": 0.7952870179044798, "tribalism_score": -0.20779310668060388, "manipulation_score": -0.6971738961522156, "resentment_score": -0.14397984649651582, "fear_score": -0.2126179447022601, "fantasy_score": -0.7510094540244864}, "pragmatism_score": {"mc_sci": 0.3115951500469707, "dignity_score": 0.5421374765483945, "truth_score": 0.6740655909056726, "justice_score": 0.7672721428435879, "hope_score": 0.7952870179044798, "pragmatism_score": 1.0, "tribalism_score": -0.31040509310661096, "manipulation_score": -0.5607538494404394, "resentment_score": -0.2932234722018026, "fear_score": -0.3410773006656662, "fantasy_score": -0.6998964726756151}, "tribalism_score": {"mc_sci": 0.04717438076366382, "dignity_score": -0.49854861812834383, "truth_score": -0.6007718129713515, "justice_score": -0.42275297305978327, "hope_score": -0.20779310668060388, "pragmatism_score": -0.31040509310661096, "tribalism_score": 1.0, "manipulation_score": 0.48220218903683554, "resentment_score": 0.31760565283189346, "fear_score": 0.3619207488858103, "fantasy_score": 0.7504325943927066}, "manipulation_score": {"mc_sci": -0.04119910677829538, "dignity_score": -0.6651662082893139, "truth_score": -0.4671628644330198, "justice_score": -0.35996778188527995, "hope_score": -0.6971738961522156, "pragmatism_score": -0.5607538494404394, "tribalism_score": 0.48220218903683554, "manipulation_score": 1.0, "resentment_score": 0.191418178326564, "fear_score": 0.21171606805422713, "fantasy_score": 0.72704557073721}, "resentment_score": {"mc_sci": 0.7011813545986012, "dignity_score": -0.2881837200291647, "truth_score": 0.20858419942357415, "justice_score": 0.19503268660627737, "hope_score": -0.14397984649651582, "pragmatism_score": -0.2932234722018026, "tribalism_score": 0.31760565283189346, "manipulation_score": 0.191418178326564, "resentment_score": 1.0, "fear_score": 0.7408208383886864, "fantasy_score": 0.2617642051708646}, "fear_score": {"mc_sci": 0.8235762391269843, "dignity_score": 0.061952247412989436, "truth_score": 0.11693133599279322, "justice_score": 0.0062895293220607915, "hope_score": -0.2126179447022601, "pragmatism_score": -0.3410773006656662, "tribalism_score": 0.3619207488858103, "manipulation_score": 0.21171606805422713, "resentment_score": 0.7408208383886864, "fear_score": 1.0, "fantasy_score": 0.268767319880733}, "fantasy_score": {"mc_sci": -0.2496129796592487, "dignity_score": -0.9174634218511292, "truth_score": -0.7895629608911476, "justice_score": -0.7316835130490091, "hope_score": -0.7510094540244864, "pragmatism_score": -0.6998964726756151, "tribalism_score": 0.7504325943927066, "manipulation_score": 0.72704557073721, "resentment_score": 0.2617642051708646, "fear_score": 0.268767319880733, "fantasy_score": 1.0}}, "missing_dimensions": []}}, "errors": []}, "combined_summary": "Two-stage execution: 4 raw data results + 6 derived metrics results"}