{
  "status": "completed",
  "stats_hash": "1e895a3eed0622b063e1289912761fb7415cb0481878cd2482a36f9261e02e9f",
  "raw_analysis_data_hash": "99e39f1a0bb68fa74941555c546a9f2464cc33dfcf6d186cea7e94b5af8e78df",
  "derived_metrics_data_hash": "634382eb826961d2c616dc8bf84b988219da4e3ef600ae935cbf1fcc2cd50aca",
  "functions_generated": 6,
  "statistical_summary": {
    "generation_metadata": {
      "status": "success",
      "functions_generated": 6,
      "output_file": "automatedstatisticalanalysisagent_functions.py",
      "module_size": 24529,
      "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: caf_civic_character_pattern_analysis\nDescription: Statistical analysis experiment\nGenerated: 2025-09-10T21:44:30.317038+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates the derived tension indices and the main Civic Character Index.\n\n    This function implements the formulas specified in the CAF v10.0 framework to\n    quantify strategic contradictions (Tension Indices) and the overall character\n    orientation (Civic Character Index). These metrics are essential for subsequent\n    analyses.\n\n    Methodology:\n    - Tension Index: For each virtue/vice axis, it calculates `min(Virtue_Score, Vice_Score) * abs(Virtue_Salience - Vice_Salience)`.\n      This captures the degree of rhetorical contradiction, weighted by the difference in emphasis.\n    - Civic Character Index: Calculates a salience-weighted sum of virtue scores minus the\n      salience-weighted sum of vice scores, normalized by the total salience of all dimensions.\n      The formula is `(weighted_virtue_score - weighted_vice_score) / (combined_salience_total + 0.001)`.\n      A small epsilon (0.001) is added to the denominator to prevent division by zero.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw and salience scores for all 10 dimensions.\n                             Must use the exact column names from the experiment specification (e.g., 'tribalism_raw').\n        **kwargs: Not used.\n\n    Returns:\n        pd.DataFrame: The input DataFrame with added columns for each derived metric, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        df = data.copy()\n\n        # Define virtue and vice dimensions\n        virtues = ['dignity', 'truth', 'justice', 'hope', 'pragmatism']\n        vices = ['tribalism', 'manipulation', 'resentment', 'fear', 'fantasy']\n        axes = zip(virtues, vices)\n\n        # Calculate Tension Indices\n        for virtue, vice in axes:\n            virtue_raw = f\"{virtue}_raw\"\n            vice_raw = f\"{vice}_raw\"\n            virtue_salience = f\"{virtue}_salience\"\n            vice_salience = f\"{vice}_salience\"\n            \n            # Ensure required columns exist\n            required_cols = [virtue_raw, vice_raw, virtue_salience, vice_salience]\n            if not all(col in df.columns for col in required_cols):\n                # Silently skip if columns are missing, as per robust design\n                continue\n\n            min_scores = df[[virtue_raw, vice_raw]].min(axis=1)\n            salience_diff = abs(df[virtue_salience] - df[vice_salience])\n            df[f\"{virtue.split('_')[0]}_tension\"] = min_scores * salience_diff\n\n        # Calculate Salience-Weighted Civic Character Index\n        weighted_virtue_score = pd.Series(0.0, index=df.index)\n        weighted_vice_score = pd.Series(0.0, index=df.index)\n        virtue_salience_total = pd.Series(0.0, index=df.index)\n        vice_salience_total = pd.Series(0.0, index=df.index)\n\n        for dim in virtues:\n            if f\"{dim}_raw\" in df.columns and f\"{dim}_salience\" in df.columns:\n                weighted_virtue_score += df[f\"{dim}_raw\"] * df[f\"{dim}_salience\"]\n                virtue_salience_total += df[f\"{dim}_salience\"]\n\n        for dim in vices:\n            if f\"{dim}_raw\" in df.columns and f\"{dim}_salience\" in df.columns:\n                weighted_vice_score += df[f\"{dim}_raw\"] * df[f\"{dim}_salience\"]\n                vice_salience_total += df[f\"{dim}_salience\"]\n        \n        df['weighted_virtue_score'] = weighted_virtue_score\n        df['weighted_vice_score'] = weighted_vice_score\n        df['virtue_salience_total'] = virtue_salience_total\n        df['vice_salience_total'] = vice_salience_total\n\n        combined_salience_total = virtue_salience_total + vice_salience_total\n        df['combined_salience_total'] = combined_salience_total\n\n        # Add a small epsilon to prevent division by zero\n        denominator = combined_salience_total + 0.001\n        \n        df['civic_character_index'] = (weighted_virtue_score - weighted_vice_score) / denominator\n\n        return df\n\n    except Exception:\n        # Return None on any failure to prevent downstream errors\n        return None\n\ndef calculate_descriptive_stats(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for all CAF dimensions and derived metrics, grouped by speaker or style.\n\n    This function provides a summary (mean, std, count) for each numerical column in the dataset.\n    It is essential for addressing RQ1 (Speaker Differentiation) and RQ2 (Character Signature Analysis)\n    by generating the core data for speaker profiles.\n\n    Methodology:\n    - The function first enriches the data with 'speaker' and 'rhetorical_style' columns based on\n      pre-defined mappings from the 'document_name'.\n    - It then groups the data by the specified variable ('speaker' or 'rhetorical_style') and\n      calculates the mean, standard deviation, and count for all numeric metrics.\n    - This analysis is exploratory (Tier 3) due to the small sample size (N=8). Results are\n      suggestive of patterns rather than conclusive findings.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing analysis scores. It's recommended to first run\n                             `calculate_derived_metrics` on the data.\n        **kwargs:\n            group_by (str): The column to group by. Can be 'speaker' or 'rhetorical_style'.\n                            Defaults to 'speaker'.\n\n    Returns:\n        pd.DataFrame: A DataFrame with descriptive statistics for each group, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n        \n        df = data.copy()\n        group_by_col = kwargs.get('group_by', 'speaker')\n\n        # Internal helper to add speaker and style metadata\n        speaker_map = {\n            'john_mccain': ('John McCain', 'Institutional'),\n            'mitt_romney': ('Mitt Romney', 'Institutional'),\n            'cory_booker': ('Cory Booker', 'Institutional'),\n            'steve_king': ('Steve King', 'Populist'),\n            'jd_vance': ('JD Vance', 'Populist'),\n            'bernie_sanders': ('Bernie Sanders', 'Populist'),\n            'alexandria_ocasio_cortez': ('Alexandria Ocasio-Cortez', 'Populist'),\n            'john_lewis': ('John Lewis', 'Civil Rights')\n        }\n\n        def get_speaker_info(doc_name, info_type):\n            for key, (speaker, style) in speaker_map.items():\n                if key in doc_name:\n                    return speaker if info_type == 'speaker' else style\n            return 'Unknown'\n\n        if 'document_name' not in df.columns:\n            return None # Cannot group without document names\n\n        df['speaker'] = df['document_name'].apply(lambda x: get_speaker_info(x, 'speaker'))\n        df['rhetorical_style'] = df['document_name'].apply(lambda x: get_speaker_info(x, 'style'))\n\n        if group_by_col not in df.columns:\n            return None\n\n        # Select only numeric columns for aggregation\n        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n        \n        # Calculate descriptive statistics\n        descriptive_stats = df.groupby(group_by_col)[numeric_cols].agg(['mean', 'std', 'count'])\n        \n        return descriptive_stats\n\n    except Exception:\n        return None\n\ndef compare_rhetorical_styles(data, **kwargs):\n    \"\"\"\n    Compares CAF dimension scores across different rhetorical styles using non-parametric tests.\n\n    This function addresses RQ4 (Framework Validation) by testing for significant differences\n    between the 'Institutional', 'Populist', and 'Civil Rights' rhetorical styles.\n\n    Methodology:\n    - This is a Tier 3 (Exploratory) analysis due to the very small sample size (N=8 total, with\n      groups as small as 1). Results are suggestive and should not be interpreted as conclusive.\n    - The function uses the Kruskal-Wallis H-test, a non-parametric alternative to ANOVA, which is\n      more appropriate for small sample sizes and data that may not meet normality assumptions.\n    - It iterates through all key CAF dimensions and derived metrics, performing a separate test for each.\n    - The results include the H-statistic and the p-value for each test.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing analysis scores. It's recommended to first run\n                             `calculate_derived_metrics` on the data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary where keys are metric names and values are the results of the\n              Kruskal-Wallis test (H-statistic and p-value), or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import kruskal\n\n    try:\n        if data is None or data.empty or data.shape[0] < 3:\n            return {\"error\": \"Insufficient data for comparison (N < 3).\"}\n\n        df = data.copy()\n        \n        # Internal helper to add speaker and style metadata\n        speaker_map = {\n            'john_mccain': ('John McCain', 'Institutional'),\n            'mitt_romney': ('Mitt Romney', 'Institutional'),\n            'cory_booker': ('Cory Booker', 'Institutional'),\n            'steve_king': ('Steve King', 'Populist'),\n            'jd_vance': ('JD Vance', 'Populist'),\n            'bernie_sanders': ('Bernie Sanders', 'Populist'),\n            'alexandria_ocasio_cortez': ('Alexandria Ocasio-Cortez', 'Populist'),\n            'john_lewis': ('John Lewis', 'Civil Rights')\n        }\n\n        def get_speaker_info(doc_name, info_type):\n            for key, (speaker, style) in speaker_map.items():\n                if key in doc_name:\n                    return speaker if info_type == 'speaker' else style\n            return 'Unknown'\n\n        if 'document_name' not in df.columns:\n            return None\n\n        df['rhetorical_style'] = df['document_name'].apply(lambda x: get_speaker_info(x, 'style'))\n\n        if df['rhetorical_style'].nunique() < 2:\n            return {\"error\": \"Fewer than two rhetorical styles found for comparison.\"}\n\n        # Metrics to test\n        metrics = [col for col in df.columns if '_raw' in col or '_salience' in col or '_tension' in col or 'civic_character_index' in col]\n        results = {}\n        \n        styles = df['rhetorical_style'].unique()\n\n        for metric in metrics:\n            # Create a list of arrays, one for each style group\n            groups = [df[metric][df['rhetorical_style'] == style].values for style in styles]\n            \n            # Filter out empty groups or groups with only one member for the test\n            valid_groups = [g for g in groups if len(g) > 0]\n            \n            if len(valid_groups) < 2:\n                results[metric] = {'statistic': np.nan, 'p_value': np.nan, 'note': 'Fewer than 2 groups with data.'}\n                continue\n\n            try:\n                stat, p = kruskal(*valid_groups)\n                results[metric] = {'statistic': stat, 'p_value': p}\n            except ValueError:\n                results[metric] = {'statistic': np.nan, 'p_value': np.nan, 'note': 'Could not compute test.'}\n\n        return results\n\n    except Exception:\n        return None\n\ndef calculate_dimension_correlations(data, **kwargs):\n    \"\"\"\n    Calculates the Spearman correlation matrix for the 10 raw CAF dimensions.\n\n    This function provides an exploratory look at the relationships between the different\n    civic character dimensions.\n\n    Methodology:\n    - This is a Tier 3 (Exploratory) analysis. With a very small sample size (N=8), correlation\n      coefficients are unstable and should be interpreted with extreme caution. Only very large\n      correlations might suggest a relationship worth investigating in a larger dataset.\n    - The function uses Spearman's rank correlation, which is more robust to outliers and\n      non-linear relationships than Pearson's, making it suitable for small, exploratory datasets.\n    - It computes the correlation matrix for the 10 `_raw` score columns.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw scores for all 10 dimensions.\n        **kwargs: Not used.\n\n    Returns:\n        pd.DataFrame: A correlation matrix of the 10 raw dimensions, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty or data.shape[0] < 3:\n            return None # Not enough data for correlation\n\n        raw_score_cols = [col for col in data.columns if col.endswith('_raw')]\n        if len(raw_score_cols) == 0:\n            return None\n\n        correlation_matrix = data[raw_score_cols].corr(method='spearman')\n        return correlation_matrix\n\n    except Exception:\n        return None\n\ndef calculate_internal_consistency(data, **kwargs):\n    \"\"\"\n    Calculates Cronbach's alpha to assess the internal consistency of virtue and vice scales.\n\n    This function helps validate whether the dimensions intended to measure 'virtue' and 'vice'\n    are internally coherent.\n\n    Methodology:\n    - This is a Tier 3 (Exploratory) analysis. Cronbach's alpha is highly sensitive to the number\n      of items and the sample size. With only 5 items per scale and a sample size of N=8, the\n      resulting alpha values are highly unstable and serve only as a preliminary, suggestive indicator.\n    - The function calculates alpha separately for the set of 5 virtue dimensions and the set of\n      5 vice dimensions using their raw scores.\n    - The formula used is: alpha = (k / (k-1)) * (1 - (sum_of_item_variances / variance_of_total_score)).\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw scores for all 10 dimensions.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing the Cronbach's alpha for the 'virtue_scale' and 'vice_scale',\n              or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.shape[0] < 2:\n            return None\n\n        virtue_cols = ['dignity_raw', 'truth_raw', 'justice_raw', 'hope_raw', 'pragmatism_raw']\n        vice_cols = ['tribalism_raw', 'manipulation_raw', 'resentment_raw', 'fear_raw', 'fantasy_raw']\n\n        def _cronbach_alpha(df_items):\n            if df_items.shape[1] < 2:\n                return np.nan\n            k = df_items.shape[1]\n            item_vars = df_items.var(axis=0, ddof=1).sum()\n            total_var = df_items.sum(axis=1).var(ddof=1)\n            if total_var == 0:\n                return np.nan\n            return (k / (k - 1)) * (1 - (item_vars / total_var))\n\n        virtue_df = data[virtue_cols].dropna()\n        vice_df = data[vice_cols].dropna()\n\n        if virtue_df.shape[0] < 2 or vice_df.shape[0] < 2:\n            return None\n\n        results = {\n            'virtue_scale_alpha': _cronbach_alpha(virtue_df),\n            'vice_scale_alpha': _cronbach_alpha(vice_df),\n            'n_virtue': virtue_df.shape[0],\n            'n_vice': vice_df.shape[0],\n            'warning': \"Exploratory analysis - results are highly unstable due to small sample size (N<15).\"\n        }\n        \n        return results\n\n    except Exception:\n        return None\n\ndef classify_character_patterns(data, **kwargs):\n    \"\"\"\n    Classifies speakers into rhetorical patterns based on CAF dimension scores.\n\n    This function operationalizes the interpretive guidance from the CAF specification to address\n    RQ3 (Civic Character Coherence). It identifies patterns like 'Authentic Virtue' or 'Strategic Pathology'.\n\n    Methodology:\n    - This is a heuristic classification based on pre-defined thresholds. It is intended for\n      exploratory pattern recognition, not as a definitive diagnostic tool.\n    - It first calculates the mean raw and salience scores for each speaker.\n    - It then applies a set of rules based on thresholds for 'high' and 'low' scores to classify\n      each speaker's dominant rhetorical pattern(s).\n    - Thresholds are set as: high > 0.6, low < 0.3.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis scores. It's recommended to run\n                             `calculate_derived_metrics` first.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary mapping each speaker to a list of identified character patterns.\n              Returns None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        # Ensure derived metrics are present, if not, calculate them\n        if 'civic_character_index' not in data.columns:\n            df = calculate_derived_metrics(data)\n            if df is None: return None\n        else:\n            df = data.copy()\n\n        # Get speaker-level average scores\n        speaker_means = calculate_descriptive_stats(df, group_by='speaker')\n        if speaker_means is None:\n            return None\n        \n        # Flatten multi-index columns\n        speaker_means.columns = ['_'.join(col).strip() for col in speaker_means.columns.values]\n        speaker_means.reset_index(inplace=True)\n\n        virtues = ['dignity', 'truth', 'justice', 'hope', 'pragmatism']\n        vices = ['tribalism', 'manipulation', 'resentment', 'fear', 'fantasy']\n        \n        HIGH_THRESHOLD = 0.6\n        LOW_THRESHOLD = 0.3\n        TENSION_THRESHOLD = 0.2 # Tension scores are naturally lower\n\n        classifications = {}\n        for _, row in speaker_means.iterrows():\n            speaker = row['speaker']\n            patterns = []\n            \n            # Calculate average virtue/vice scores and salience for this speaker\n            avg_virtue_score = np.mean([row.get(f'{v}_raw_mean', 0) for v in virtues])\n            avg_virtue_salience = np.mean([row.get(f'{v}_salience_mean', 0) for v in virtues])\n            avg_vice_score = np.mean([row.get(f'{v}_raw_mean', 0) for v in vices])\n            avg_vice_salience = np.mean([row.get(f'{v}_salience_mean', 0) for v in vices])\n            avg_tension = np.mean([row.get(f'{v.split(\"_\")[0]}_tension_mean', 0) for v in virtues])\n\n            # Authentic Virtue\n            if avg_virtue_score > HIGH_THRESHOLD and avg_virtue_salience > HIGH_THRESHOLD:\n                patterns.append('Authentic Virtue')\n\n            # Strategic Virtue Signaling\n            if avg_virtue_score > HIGH_THRESHOLD and avg_virtue_salience < LOW_THRESHOLD:\n                patterns.append('Strategic Virtue Signaling')\n\n            # Strategic Pathology\n            if avg_vice_score > HIGH_THRESHOLD and avg_vice_salience > HIGH_THRESHOLD:\n                patterns.append('Strategic Pathology')\n            \n            # Incoherent Messaging (high tension)\n            if avg_tension > TENSION_THRESHOLD:\n                patterns.append('Incoherent Messaging')\n\n            # Dominant Orientation\n            if row.get('civic_character_index_mean', 0) > 0.2:\n                 patterns.append('Virtue-Dominant Profile')\n            elif row.get('civic_character_index_mean', 0) < -0.2:\n                 patterns.append('Vice-Dominant Profile')\n            else:\n                 patterns.append('Mixed/Contested Profile')\n\n            if not patterns:\n                patterns.append('Indeterminate Pattern')\n\n            classifications[speaker] = list(set(patterns)) # Use set to remove duplicates\n\n        return classifications\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
      "cached_with_code": true
    },
    "statistical_data": {
      "calculate_derived_metrics": {
        "type": "dataframe",
        "data": [
          {
            "document_name": "john_lewis_1963_march_on_washington_ab348df3.txt",
            "tribalism_raw": 0.7,
            "tribalism_salience": 0.7,
            "tribalism_confidence": 0.9,
            "dignity_raw": 0.7,
            "dignity_salience": 0.8,
            "dignity_confidence": 0.9,
            "manipulation_raw": 0.3,
            "manipulation_salience": 0.2,
            "manipulation_confidence": 0.8,
            "truth_raw": 0.8,
            "truth_salience": 0.9,
            "truth_confidence": 0.9,
            "resentment_raw": 0.8,
            "resentment_salience": 0.9,
            "resentment_confidence": 0.9,
            "justice_raw": 0.9,
            "justice_salience": 0.9,
            "justice_confidence": 0.9,
            "fear_raw": 0.7,
            "fear_salience": 0.8,
            "fear_confidence": 0.9,
            "hope_raw": 0.6,
            "hope_salience": 0.7,
            "hope_confidence": 0.9,
            "fantasy_raw": 0.0,
            "fantasy_salience": 0.0,
            "fantasy_confidence": 1.0,
            "pragmatism_raw": 0.6,
            "pragmatism_salience": 0.7,
            "pragmatism_confidence": 0.9,
            "dignity_tension": 0.07000000000000006,
            "truth_tension": 0.21,
            "justice_tension": 0.0,
            "hope_tension": 0.06000000000000005,
            "pragmatism_tension": 0.0,
            "weighted_virtue_score": 2.9299999999999997,
            "weighted_vice_score": 1.83,
            "virtue_salience_total": 4.0,
            "vice_salience_total": 2.5999999999999996,
            "combined_salience_total": 6.6,
            "civic_character_index": 0.16664141796697465
          },
          {
            "document_name": "john_mccain_2008_concession_ff9b26f2.txt",
            "tribalism_raw": 0.1,
            "tribalism_salience": 0.1,
            "tribalism_confidence": 0.9,
            "dignity_raw": 0.9,
            "dignity_salience": 0.8,
            "dignity_confidence": 0.9,
            "manipulation_raw": 0.0,
            "manipulation_salience": 0.0,
            "manipulation_confidence": 0.9,
            "truth_raw": 0.9,
            "truth_salience": 0.9,
            "truth_confidence": 0.9,
            "resentment_raw": 0.1,
            "resentment_salience": 0.1,
            "resentment_confidence": 0.9,
            "justice_raw": 0.8,
            "justice_salience": 0.7,
            "justice_confidence": 0.9,
            "fear_raw": 0.2,
            "fear_salience": 0.2,
            "fear_confidence": 0.9,
            "hope_raw": 0.9,
            "hope_salience": 0.9,
            "hope_confidence": 0.9,
            "fantasy_raw": 0.0,
            "fantasy_salience": 0.0,
            "fantasy_confidence": 0.9,
            "pragmatism_raw": 0.7,
            "pragmatism_salience": 0.7,
            "pragmatism_confidence": 0.9,
            "dignity_tension": 0.07,
            "truth_tension": 0.0,
            "justice_tension": 0.06,
            "hope_tension": 0.13999999999999999,
            "pragmatism_tension": 0.0,
            "weighted_virtue_score": 3.39,
            "weighted_vice_score": 0.06000000000000001,
            "virtue_salience_total": 4.0,
            "vice_salience_total": 0.4,
            "combined_salience_total": 4.4,
            "civic_character_index": 0.7566462167689161
          },
          {
            "document_name": "steve_king_2017_house_floor_738780d9.txt",
            "tribalism_raw": 0.9,
            "tribalism_salience": 0.9,
            "tribalism_confidence": 0.95,
            "dignity_raw": 0.1,
            "dignity_salience": 0.1,
            "dignity_confidence": 0.7,
            "manipulation_raw": 0.8,
            "manipulation_salience": 0.8,
            "manipulation_confidence": 0.95,
            "truth_raw": 0.6,
            "truth_salience": 0.5,
            "truth_confidence": 0.85,
            "resentment_raw": 0.9,
            "resentment_salience": 0.9,
            "resentment_confidence": 0.95,
            "justice_raw": 0.3,
            "justice_salience": 0.2,
            "justice_confidence": 0.8,
            "fear_raw": 0.9,
            "fear_salience": 0.9,
            "fear_confidence": 0.95,
            "hope_raw": 0.0,
            "hope_salience": 0.0,
            "hope_confidence": 0.6,
            "fantasy_raw": 0.7,
            "fantasy_salience": 0.6,
            "fantasy_confidence": 0.9,
            "pragmatism_raw": 0.1,
            "pragmatism_salience": 0.1,
            "pragmatism_confidence": 0.7,
            "dignity_tension": 0.08000000000000002,
            "truth_tension": 0.18000000000000002,
            "justice_tension": 0.21,
            "hope_tension": 0.0,
            "pragmatism_tension": 0.05,
            "weighted_virtue_score": 0.38,
            "weighted_vice_score": 3.49,
            "virtue_salience_total": 0.9,
            "vice_salience_total": 4.1,
            "combined_salience_total": 5.0,
            "civic_character_index": -0.621875624875025
          },
          {
            "document_name": "cory_booker_2018_first_step_act_0c32812a.txt",
            "tribalism_raw": 0.1,
            "tribalism_salience": 0.1,
            "tribalism_confidence": 0.8,
            "dignity_raw": 0.9,
            "dignity_salience": 0.9,
            "dignity_confidence": 0.9,
            "manipulation_raw": 0.2,
            "manipulation_salience": 0.1,
            "manipulation_confidence": 0.7,
            "truth_raw": 0.8,
            "truth_salience": 0.8,
            "truth_confidence": 0.9,
            "resentment_raw": 0.7,
            "resentment_salience": 0.7,
            "resentment_confidence": 0.9,
            "justice_raw": 0.8,
            "justice_salience": 0.8,
            "justice_confidence": 0.9,
            "fear_raw": 0.4,
            "fear_salience": 0.3,
            "fear_confidence": 0.8,
            "hope_raw": 0.9,
            "hope_salience": 0.9,
            "hope_confidence": 0.9,
            "fantasy_raw": 0.1,
            "fantasy_salience": 0.1,
            "fantasy_confidence": 0.8,
            "pragmatism_raw": 0.8,
            "pragmatism_salience": 0.8,
            "pragmatism_confidence": 0.9,
            "dignity_tension": 0.08000000000000002,
            "truth_tension": 0.14,
            "justice_tension": 0.07000000000000006,
            "hope_tension": 0.24000000000000005,
            "pragmatism_tension": 0.07,
            "weighted_virtue_score": 3.5400000000000005,
            "weighted_vice_score": 0.6499999999999999,
            "virtue_salience_total": 4.2,
            "vice_salience_total": 1.3,
            "combined_salience_total": 5.5,
            "civic_character_index": 0.5253590256317034
          },
          {
            "document_name": "mitt_romney_2020_impeachment_9ebec73f.txt",
            "tribalism_raw": 0.4,
            "tribalism_salience": 0.5,
            "tribalism_confidence": 0.9,
            "dignity_raw": 0.8,
            "dignity_salience": 0.8,
            "dignity_confidence": 0.9,
            "manipulation_raw": 0.5,
            "manipulation_salience": 0.4,
            "manipulation_confidence": 0.9,
            "truth_raw": 0.9,
            "truth_salience": 0.9,
            "truth_confidence": 0.9,
            "resentment_raw": 0.8,
            "resentment_salience": 0.7,
            "resentment_confidence": 0.9,
            "justice_raw": 0.9,
            "justice_salience": 0.9,
            "justice_confidence": 0.9,
            "fear_raw": 0.7,
            "fear_salience": 0.6,
            "fear_confidence": 0.9,
            "hope_raw": 0.7,
            "hope_salience": 0.6,
            "hope_confidence": 0.9,
            "fantasy_raw": 0.0,
            "fantasy_salience": 0.0,
            "fantasy_confidence": 0.9,
            "pragmatism_raw": 0.8,
            "pragmatism_salience": 0.7,
            "pragmatism_confidence": 0.9,
            "dignity_tension": 0.12000000000000002,
            "truth_tension": 0.25,
            "justice_tension": 0.16000000000000006,
            "hope_tension": 0.0,
            "pragmatism_tension": 0.0,
            "weighted_virtue_score": 3.24,
            "weighted_vice_score": 1.38,
            "virtue_salience_total": 3.9000000000000004,
            "vice_salience_total": 2.2,
            "combined_salience_total": 6.1000000000000005,
            "civic_character_index": 0.30486805441730863
          },
          {
            "document_name": "jd_vance_2022_natcon_conference_516a3c9c.txt",
            "tribalism_raw": 0.9,
            "tribalism_salience": 0.95,
            "tribalism_confidence": 0.95,
            "dignity_raw": 0.0,
            "dignity_salience": 0.0,
            "dignity_confidence": 0.9,
            "manipulation_raw": 0.75,
            "manipulation_salience": 0.65,
            "manipulation_confidence": 0.9,
            "truth_raw": 0.4,
            "truth_salience": 0.3,
            "truth_confidence": 0.75,
            "resentment_raw": 0.85,
            "resentment_salience": 0.85,
            "resentment_confidence": 0.95,
            "justice_raw": 0.0,
            "justice_salience": 0.0,
            "justice_confidence": 0.9,
            "fear_raw": 0.75,
            "fear_salience": 0.75,
            "fear_confidence": 0.9,
            "hope_raw": 0.4,
            "hope_salience": 0.4,
            "hope_confidence": 0.8,
            "fantasy_raw": 0.35,
            "fantasy_salience": 0.25,
            "fantasy_confidence": 0.7,
            "pragmatism_raw": 0.3,
            "pragmatism_salience": 0.2,
            "pragmatism_confidence": 0.7,
            "dignity_tension": 0.0,
            "truth_tension": 0.14,
            "justice_tension": 0.0,
            "hope_tension": 0.13999999999999999,
            "pragmatism_tension": 0.014999999999999996,
            "weighted_virtue_score": 0.34,
            "weighted_vice_score": 2.715,
            "virtue_salience_total": 0.8999999999999999,
            "vice_salience_total": 3.45,
            "combined_salience_total": 4.35,
            "civic_character_index": -0.5458515283842795
          },
          {
            "document_name": "bernie_sanders_2025_fighting_oligarchy_261b893a.txt",
            "tribalism_raw": 0.9,
            "tribalism_salience": 0.9,
            "tribalism_confidence": 0.9,
            "dignity_raw": 0.0,
            "dignity_salience": 0.0,
            "dignity_confidence": 0.9,
            "manipulation_raw": 0.7,
            "manipulation_salience": 0.6,
            "manipulation_confidence": 0.8,
            "truth_raw": 0.8,
            "truth_salience": 0.7,
            "truth_confidence": 0.9,
            "resentment_raw": 0.9,
            "resentment_salience": 1.0,
            "resentment_confidence": 0.9,
            "justice_raw": 0.2,
            "justice_salience": 0.1,
            "justice_confidence": 0.7,
            "fear_raw": 0.8,
            "fear_salience": 0.7,
            "fear_confidence": 0.9,
            "hope_raw": 0.7,
            "hope_salience": 0.6,
            "hope_confidence": 0.8,
            "fantasy_raw": 0.0,
            "fantasy_salience": 0.0,
            "fantasy_confidence": 0.9,
            "pragmatism_raw": 0.4,
            "pragmatism_salience": 0.3,
            "pragmatism_confidence": 0.7,
            "dignity_tension": 0.0,
            "truth_tension": 0.06999999999999998,
            "justice_tension": 0.18000000000000002,
            "hope_tension": 0.06999999999999998,
            "pragmatism_tension": 0.0,
            "weighted_virtue_score": 1.12,
            "weighted_vice_score": 2.69,
            "virtue_salience_total": 1.7,
            "vice_salience_total": 3.2,
            "combined_salience_total": 4.9,
            "civic_character_index": -0.32034278718628845
          },
          {
            "document_name": "alexandria_ocasio_cortez_2025_fighting_oligarchy_1121e4ae.txt",
            "tribalism_raw": 0.8,
            "tribalism_salience": 0.8,
            "tribalism_confidence": 0.9,
            "dignity_raw": 0.8,
            "dignity_salience": 0.7,
            "dignity_confidence": 0.9,
            "manipulation_raw": 0.9,
            "manipulation_salience": 0.9,
            "manipulation_confidence": 0.9,
            "truth_raw": 0.7,
            "truth_salience": 0.6,
            "truth_confidence": 0.8,
            "resentment_raw": 0.9,
            "resentment_salience": 0.9,
            "resentment_confidence": 0.9,
            "justice_raw": 0.8,
            "justice_salience": 0.7,
            "justice_confidence": 0.8,
            "fear_raw": 0.7,
            "fear_salience": 0.7,
            "fear_confidence": 0.8,
            "hope_raw": 0.9,
            "hope_salience": 0.9,
            "hope_confidence": 0.9,
            "fantasy_raw": 0.1,
            "fantasy_salience": 0.2,
            "fantasy_confidence": 0.8,
            "pragmatism_raw": 0.7,
            "pragmatism_salience": 0.7,
            "pragmatism_confidence": 0.8,
            "dignity_tension": 0.08000000000000007,
            "truth_tension": 0.21000000000000002,
            "justice_tension": 0.16000000000000006,
            "hope_tension": 0.14000000000000004,
            "pragmatism_tension": 0.049999999999999996,
            "weighted_virtue_score": 2.84,
            "weighted_vice_score": 2.77,
            "virtue_salience_total": 3.5999999999999996,
            "vice_salience_total": 3.5,
            "combined_salience_total": 7.1,
            "civic_character_index": 0.009857766511758884
          }
        ],
        "columns": [
          "document_name",
          "tribalism_raw",
          "tribalism_salience",
          "tribalism_confidence",
          "dignity_raw",
          "dignity_salience",
          "dignity_confidence",
          "manipulation_raw",
          "manipulation_salience",
          "manipulation_confidence",
          "truth_raw",
          "truth_salience",
          "truth_confidence",
          "resentment_raw",
          "resentment_salience",
          "resentment_confidence",
          "justice_raw",
          "justice_salience",
          "justice_confidence",
          "fear_raw",
          "fear_salience",
          "fear_confidence",
          "hope_raw",
          "hope_salience",
          "hope_confidence",
          "fantasy_raw",
          "fantasy_salience",
          "fantasy_confidence",
          "pragmatism_raw",
          "pragmatism_salience",
          "pragmatism_confidence",
          "dignity_tension",
          "truth_tension",
          "justice_tension",
          "hope_tension",
          "pragmatism_tension",
          "weighted_virtue_score",
          "weighted_vice_score",
          "virtue_salience_total",
          "vice_salience_total",
          "combined_salience_total",
          "civic_character_index"
        ],
        "index": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7
        ],
        "shape": [
          8,
          42
        ]
      },
      "calculate_descriptive_stats": {
        "type": "dataframe",
        "data": [
          {