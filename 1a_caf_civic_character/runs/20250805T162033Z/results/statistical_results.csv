test_name,test_type,statistic_name,statistic_value,p_value,effect_size,degrees_of_freedom,sample_size,dependent_variable,grouping_variable,significance_level,interpretation,notes
calculate_all_derived_metrics,derived_metrics_calculation,result_value,"{'type': 'derived_metrics_calculation', 'success': True, 'calculated_metrics': {'hope_fear_tension': [0.775, 0.775, 0.825, 0.6499999999999999, 0.55, 0.825, 0.6, 0.45], 'dignity_tribalism_tension': [0.625, 0.85, 0.8, 0.35, 0.6, 0.775, 0.7, 0.625], 'truth_manipulation_tension': [0.4, 0.44999999999999996, 0.6000000000000001, 0.45, 0.6, 0.6000000000000001, 0.55, 0.42500000000000004], 'justice_resentment_tension': [0.6499999999999999, 0.575, 0.6499999999999999, 0.44999999999999996, 0.65, 0.6, 0.75, 0.55], 'pragmatism_fantasy_tension': [0.6000000000000001, 0.725, 0.8, 0.6499999999999999, 0.6499999999999999, 0.75, 0.75, 0.575], 'virtue_index': [0.6599999999999999, 0.6, 0.6799999999999999, 0.42000000000000004, 0.6799999999999999, 0.53, 0.62, 0.38999999999999996], 'pathology_index': [0.43999999999999995, 0.25000000000000006, 0.21000000000000002, 0.4, 0.45999999999999996, 0.11000000000000001, 0.28, 0.33999999999999997], 'virtue_index_static_weighted': [0.6558823529411765, 0.6, 0.6735294117647059, 0.38823529411764707, 0.6823529411764707, 0.5000000000000001, 0.6352941176470589, 0.4205882352941177], 'pathology_index_static_weighted': [0.4735294117647059, 0.2794117647058824, 0.2323529411764706, 0.4176470588235295, 0.45294117647058824, 0.11176470588235296, 0.28823529411764703, 0.35882352941176476], 'civic_character_index': [0.61, 0.675, 0.735, 0.51, 0.6100000000000001, 0.71, 0.67, 0.525], 'salience_weighted_civic_character_index': [0.64375, 0.6806338028169013, 0.7389610389610389, 0.5177419354838709, 0.6102941176470588, 0.7482142857142856, 0.6771428571428572, 0.5467391304347826]}, 'successful_calculations': ['hope_fear_tension', 'dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'pragmatism_fantasy_tension', 'virtue_index', 'pathology_index', 'virtue_index_static_weighted', 'pathology_index_static_weighted', 'civic_character_index', 'salience_weighted_civic_character_index'], 'failed_calculations': [], 'formulas_used': ['dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'hope_fear_tension', 'pragmatism_fantasy_tension', 'civic_character_index', 'salience_weighted_civic_character_index', 'virtue_index', 'pathology_index', 'virtue_index_static_weighted', 'pathology_index_static_weighted'], 'input_columns': ['dignity_score', 'tribalism_score', 'dignity_salience', 'tribalism_salience', 'truth_score', 'manipulation_score', 'truth_salience', 'manipulation_salience', 'justice_score', 'resentment_score', 'justice_salience', 'resentment_salience', 'hope_score', 'fear_score', 'hope_salience', 'fear_salience', 'pragmatism_score', 'fantasy_score', 'pragmatism_salience', 'fantasy_salience'], 'total_metrics': 11, 'success_rate': 1.0}",,,,,,,,Generic derived_metrics_calculation result,
calculate_descriptive_statistics_raw_scores,descriptive_stats,result_value,"{'type': 'descriptive_stats', 'columns_analyzed': ['dignity_score', 'tribalism_score', 'truth_score', 'manipulation_score', 'justice_score', 'resentment_score', 'hope_score', 'fear_score', 'pragmatism_score', 'fantasy_score', 'dignity_salience', 'tribalism_salience', 'truth_salience', 'manipulation_salience', 'justice_salience', 'resentment_salience', 'hope_salience', 'fear_salience', 'pragmatism_salience', 'fantasy_salience', 'dignity_confidence', 'tribalism_confidence', 'truth_confidence', 'manipulation_confidence', 'justice_confidence', 'resentment_confidence', 'hope_confidence', 'fear_confidence', 'pragmatism_confidence', 'fantasy_confidence'], 'results': {'dignity_score': {'count': 8, 'mean': 0.68125, 'std': 0.17307615994947756, 'min': 0.3, 'max': 0.85, 'median': 0.7, 'q25': 0.65, 'q75': 0.775, 'skewness': -1.672635293480974, 'kurtosis': 3.801459218507423}, 'tribalism_score': {'count': 8, 'mean': 0.35, 'std': 0.2052872551885702, 'min': 0.1, 'max': 0.6, 'median': 0.35, 'q25': 0.15, 'q75': 0.525, 'skewness': 0.06605048141660425, 'kurtosis': -1.91982189026142}, 'truth_score': {'count': 8, 'mean': 0.45, 'std': 0.12247448713915891, 'min': 0.3, 'max': 0.6, 'median': 0.42500000000000004, 'q25': 0.375, 'q75': 0.5625, 'skewness': 0.07776157913597323, 'kurtosis': -1.6428571428571423}, 'manipulation_score': {'count': 8, 'mean': 0.43125, 'std': 0.13076014027873437, 'min': 0.2, 'max': 0.65, 'median': 0.42500000000000004, 'q25': 0.3875, 'q75': 0.5, 'skewness': -0.15674565282067976, 'kurtosis': 1.2450722276380635}, 'justice_score': {'count': 8, 'mean': 0.56875, 'std': 0.22825033249858429, 'min': 0.2, 'max': 0.8, 'median': 0.7, 'q25': 0.4375, 'q75': 0.7, 'skewness': -0.9841642545647162, 'kurtosis': -0.7464396878159674}, 'resentment_score': {'count': 8, 'mean': 0.35, 'std': 0.16256866681058635, 'min': 0.05, 'max': 0.55, 'median': 0.4, 'q25': 0.275, 'q75': 0.42500000000000004, 'skewness': -0.8312495746341148, 'kurtosis': 0.37070854638422013}, 'hope_score': {'count': 8, 'mean': 0.60625, 'std': 0.2043063176423368, 'min': 0.15, 'max': 0.75, 'median': 0.675, 'q25': 0.575, 'q75': 0.75, 'skewness': -1.9287909236994418, 'kurtosis': 3.920105693614339}, 'fear_score': {'count': 8, 'mean': 0.24375000000000002, 'std': 0.1678381448215597, 'min': 0.1, 'max': 0.6, 'median': 0.225, 'q25': 0.1, 'q75': 0.3, 'skewness': 1.4734782964345032, 'kurtosis': 2.623087645449953}, 'pragmatism_score': {'count': 8, 'mean': 0.5562499999999999, 'std': 0.09425459443140463, 'min': 0.35, 'max': 0.65, 'median': 0.6, 'q25': 0.5375000000000001, 'q75': 0.6, 'skewness': -1.772703263364815, 'kurtosis': 3.403752430494169}, 'fantasy_score': {'count': 8, 'mean': 0.18125000000000002, 'std': 0.12229209763045666, 'min': 0.05, 'max': 0.35, 'median': 0.15000000000000002, 'q25': 0.08750000000000001, 'q75': 0.3, 'skewness': 0.26728274187389883, 'kurtosis': -2.0036747605257306}, 'dignity_salience': {'count': 8, 'mean': 0.7625, 'std': 0.05824823725107178, 'min': 0.7, 'max': 0.85, 'median': 0.775, 'q25': 0.7, 'q75': 0.8, 'skewness': 0.09035737634515428, 'kurtosis': -1.6132963988919693}, 'tribalism_salience': {'count': 8, 'mean': 0.4625, 'std': 0.22638462845343543, 'min': 0.2, 'max': 0.8, 'median': 0.45, 'q25': 0.275, 'q75': 0.625, 'skewness': 0.226249955860661, 'kurtosis': -1.4115407495538372}, 'truth_salience': {'count': 8, 'mean': 0.5, 'std': 0.15118578920369088, 'min': 0.3, 'max': 0.7, 'median': 0.55, 'q25': 0.375, 'q75': 0.6, 'skewness': -0.33071891388307445, 'kurtosis': -1.487499999999999}, 'manipulation_salience': {'count': 8, 'mean': 0.51875, 'std': 0.09977653603356422, 'min': 0.4, 'max': 0.7, 'median': 0.5, 'q25': 0.475, 'q75': 0.5625, 'skewness': 0.6044898304046361, 'kurtosis': 0.36463230710450567}, 'justice_salience': {'count': 8, 'mean': 0.6749999999999999, 'std': 0.22990681342044403, 'min': 0.3, 'max': 0.9, 'median': 0.7250000000000001, 'q25': 0.55, 'q75': 0.8625, 'skewness': -0.6877051869661917, 'kurtosis': -0.9933710737764807}, 'resentment_salience': {'count': 8, 'mean': 0.4375, 'std': 0.20133483127793436, 'min': 0.1, 'max': 0.75, 'median': 0.475, 'q25': 0.3, 'q75': 0.525, 'skewness': -0.19473554679679908, 'kurtosis': 0.09824370742688693}, 'hope_salience': {'count': 8, 'mean': 0.6625, 'std': 0.21998376563477848, 'min': 0.2, 'max': 0.9, 'median': 0.7, 'q25': 0.6499999999999999, 'q75': 0.8, 'skewness': -1.511349366003782, 'kurtosis': 2.472232131915413}, 'fear_salience': {'count': 8, 'mean': 0.27499999999999997, 'std': 0.12535663410560174, 'min': 0.15, 'max': 0.5, 'median': 0.25, 'q25': 0.1875, 'q75': 0.325, 'skewness': 0.8339842186364423, 'kurtosis': -0.2357438016528901}, 'pragmatism_salience': {'count': 8, 'mean': 0.5875, 'std': 0.11259916264596032, 'min': 0.4, 'max': 0.7, 'median': 0.6, 'q25': 0.5, 'q75': 0.7, 'skewness': -0.4878329125546083, 'kurtosis': -0.9886927196984723}, 'fantasy_salience': {'count': 8, 'mean': 0.22499999999999998, 'std': 0.10690449676496976, 'min': 0.1, 'max': 0.4, 'median': 0.225, 'q25': 0.1375, 'q75': 0.3, 'skewness': 0.2923169833417153, 'kurtosis': -0.9050781249999988}, 'dignity_confidence': {'count': 8, 'mean': 0.8375, 'std': 0.07905694150420949, 'min': 0.7, 'max': 0.95, 'median': 0.825, 'q25': 0.8, 'q75': 0.9, 'skewness': -0.32526284504589253, 'kurtosis': -0.03657142857142759}, 'tribalism_confidence': {'count': 8, 'mean': 0.7749999999999999, 'std': 0.07071067811865477, 'min': 0.7, 'max': 0.9, 'median': 0.75, 'q25': 0.7375, 'q75': 0.8125, 'skewness': 0.8081220356417728, 'kurtosis': -0.22857142857142243}, 'truth_confidence': {'count': 8, 'mean': 0.7124999999999999, 'std': 0.09543135154205278, 'min': 0.6, 'max': 0.85, 'median': 0.7, 'q25': 0.6749999999999999, 'q75': 0.7374999999999999, 'skewness': 0.5136635607799866, 'kurtosis': -0.6200692041522471}, 'manipulation_confidence': {'count': 8, 'mean': 0.71875, 'std': 0.09613049166924839, 'min': 0.6, 'max': 0.9, 'median': 0.725, 'q25': 0.6749999999999999, 'q75': 0.75, 'skewness': 0.5754054587815075, 'kurtosis': 1.0421340054610368}, 'justice_confidence': {'count': 8, 'mean': 0.7875000000000001, 'std': 0.15294723646688468, 'min': 0.5, 'max': 0.95, 'median': 0.8, 'q25': 0.7, 'q75': 0.9125, 'skewness': -0.8035500751619584, 'kurtosis': 0.4281335586504307}, 'resentment_confidence': {'count': 8, 'mean': 0.7125, 'std': 0.11877349391654207, 'min': 0.5, 'max': 0.9, 'median': 0.725, 'q25': 0.65, 'q75': 0.7625, 'skewness': -0.30906610177930527, 'kurtosis': 0.9026758532286498}, 'hope_confidence': {'count': 8, 'mean': 0.78125, 'std': 0.16461535252130735, 'min': 0.5, 'max': 0.95, 'median': 0.8500000000000001, 'q25': 0.6749999999999999, 'q75': 0.9, 'skewness': -0.8071334465581205, 'kurtosis': -0.7970579374621742}, 'fear_confidence': {'count': 8, 'mean': 0.59375, 'std': 0.13211872366495653, 'min': 0.4, 'max': 0.8, 'median': 0.575, 'q25': 0.5, 'q75': 0.7, 'skewness': 0.17906083518147484, 'kurtosis': -0.8132547536973194}, 'pragmatism_confidence': {'count': 8, 'mean': 0.75, 'std': 0.09636241116594317, 'min': 0.6, 'max': 0.9, 'median': 0.725, 'q25': 0.7, 'q75': 0.8125, 'skewness': 0.15965369897315992, 'kurtosis': -0.37692307692307825}, 'fantasy_confidence': {'count': 8, 'mean': 0.6, 'std': 0.12247448713915889, 'min': 0.5, 'max': 0.85, 'median': 0.575, 'q25': 0.5, 'q75': 0.625, 'skewness': 1.399708424447531, 'kurtosis': 1.6714285714285761}}}",,,,,,,,Generic descriptive_stats result,
calculate_descriptive_statistics_derived_metrics,descriptive_stats,result_value,"{'type': 'descriptive_stats', 'columns_analyzed': ['dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'hope_fear_tension', 'pragmatism_fantasy_tension', 'civic_character_index', 'salience_weighted_civic_character_index', 'virtue_index', 'pathology_index', 'virtue_index_static_weighted', 'pathology_index_static_weighted'], 'results': {'dignity_tribalism_tension': {'count': 8, 'mean': 0.665625, 'std': 0.1569448834099775, 'min': 0.35, 'max': 0.85, 'median': 0.6625, 'q25': 0.61875, 'q75': 0.78125, 'skewness': -1.0749114852296886, 'kurtosis': 1.6405232024232328}, 'truth_manipulation_tension': {'count': 8, 'mean': 0.509375, 'std': 0.08653807997473881, 'min': 0.4, 'max': 0.6000000000000001, 'median': 0.5, 'q25': 0.44375, 'q75': 0.6, 'skewness': -0.008180198346495806, 'kurtosis': -2.309571984781485}, 'justice_resentment_tension': {'count': 8, 'mean': 0.609375, 'std': 0.08857754875168505, 'min': 0.44999999999999996, 'max': 0.75, 'median': 0.625, 'q25': 0.56875, 'q75': 0.6499999999999999, 'skewness': -0.36092811861822405, 'kurtosis': 0.993953570250647}, 'hope_fear_tension': {'count': 8, 'mean': 0.6812499999999999, 'std': 0.1399936223037117, 'min': 0.45, 'max': 0.825, 'median': 0.7124999999999999, 'q25': 0.5875, 'q75': 0.7875, 'skewness': -0.5280097352137104, 'kurtosis': -1.1423685016163292}, 'pragmatism_fantasy_tension': {'count': 8, 'mean': 0.6875, 'std': 0.08017837257372733, 'min': 0.575, 'max': 0.8, 'median': 0.6875, 'q25': 0.6375, 'q75': 0.75, 'skewness': -0.08661243950865564, 'kurtosis': -1.4534722222222216}, 'civic_character_index': {'count': 8, 'mean': 0.630625, 'std': 0.082177399569468, 'min': 0.51, 'max': 0.735, 'median': 0.6400000000000001, 'q25': 0.58875, 'q75': 0.6837500000000001, 'skewness': -0.40003699684393285, 'kurtosis': -1.1319305585071544}, 'salience_weighted_civic_character_index': {'count': 8, 'mean': 0.6454346460250995, 'std': 0.08352857482948237, 'min': 0.5177419354838709, 'max': 0.7482142857142856, 'median': 0.6604464285714287, 'q25': 0.5944053708439897, 'q75': 0.6952156118529358, 'skewness': -0.36198997525512616, 'kurtosis': -1.0056117474096977}, 'virtue_index': {'count': 8, 'mean': 0.5725, 'std': 0.11473572117821768, 'min': 0.38999999999999996, 'max': 0.6799999999999999, 'median': 0.61, 'q25': 0.5025000000000001, 'q75': 0.6649999999999999, 'skewness': -0.8095227105244694, 'kurtosis': -0.9732377999610766}, 'pathology_index': {'count': 8, 'mean': 0.31125, 'std': 0.12123619214456664, 'min': 0.11000000000000001, 'max': 0.45999999999999996, 'median': 0.31, 'q25': 0.24000000000000005, 'q75': 0.41000000000000003, 'skewness': -0.3528326783719946, 'kurtosis': -0.793427881924968}, 'virtue_index_static_weighted': {'count': 8, 'mean': 0.5694852941176471, 'std': 0.1171938486911029, 'min': 0.38823529411764707, 'max': 0.6823529411764707, 'median': 0.6176470588235294, 'q25': 0.4801470588235295, 'q75': 0.6602941176470588, 'skewness': -0.718134919532831, 'kurtosis': -1.3577684008740212}, 'pathology_index_static_weighted': {'count': 8, 'mean': 0.3268382352941177, 'std': 0.12283153709528473, 'min': 0.11176470588235296, 'max': 0.4735294117647059, 'median': 0.3235294117647059, 'q25': 0.26764705882352946, 'q75': 0.42647058823529416, 'skewness': -0.515229579573081, 'kurtosis': -0.36400492299013276}}}",,,,,,,,Generic descriptive_stats result,
validate_calculated_metrics_quality,metric_validation,result_value,"{'type': 'metric_validation', 'validation_rules': ['missing_data_check', 'range_check', 'consistency_check'], 'results': {'missing_data_check': {'status': 'completed', 'missing_data_by_column': {'aid': 0, 'expert_categorization': 0, 'political_party': 0, 'speaker': 0, 'leadership_type': 0, 'date': 0, 'era': 0, 'ideology': 0, 'context': 0, 'dignity_score': 0, 'tribalism_score': 0, 'dignity_salience': 0, 'tribalism_salience': 0, 'dignity_confidence': 0, 'tribalism_confidence': 0, 'truth_score': 0, 'manipulation_score': 0, 'truth_salience': 0, 'manipulation_salience': 0, 'truth_confidence': 0, 'manipulation_confidence': 0, 'justice_score': 0, 'resentment_score': 0, 'justice_salience': 0, 'resentment_salience': 0, 'justice_confidence': 0, 'resentment_confidence': 0, 'hope_score': 0, 'fear_score': 0, 'hope_salience': 0, 'fear_salience': 0, 'hope_confidence': 0, 'fear_confidence': 0, 'pragmatism_score': 0, 'fantasy_score': 0, 'pragmatism_salience': 0, 'fantasy_salience': 0, 'pragmatism_confidence': 0, 'fantasy_confidence': 0, 'gasket_version': 0, 'extraction_time_seconds': 0, 'hope_fear_tension': 0, 'dignity_tribalism_tension': 0, 'truth_manipulation_tension': 0, 'justice_resentment_tension': 0, 'pragmatism_fantasy_tension': 0, 'virtue_index': 0, 'pathology_index': 0, 'virtue_index_static_weighted': 0, 'pathology_index_static_weighted': 0, 'civic_character_index': 0, 'salience_weighted_civic_character_index': 0}, 'total_missing': 0}, 'range_check': {'status': 'completed', 'ranges': {'dignity_score': {'min': 0.3, 'max': 0.85, 'mean': 0.68125}, 'tribalism_score': {'min': 0.1, 'max': 0.6, 'mean': 0.35}, 'dignity_salience': {'min': 0.7, 'max': 0.85, 'mean': 0.7625}, 'tribalism_salience': {'min': 0.2, 'max': 0.8, 'mean': 0.4625}, 'dignity_confidence': {'min': 0.7, 'max': 0.95, 'mean': 0.8375}, 'tribalism_confidence': {'min': 0.7, 'max': 0.9, 'mean': 0.7749999999999999}, 'truth_score': {'min': 0.3, 'max': 0.6, 'mean': 0.45}, 'manipulation_score': {'min': 0.2, 'max': 0.65, 'mean': 0.43125}, 'truth_salience': {'min': 0.3, 'max': 0.7, 'mean': 0.5}, 'manipulation_salience': {'min': 0.4, 'max': 0.7, 'mean': 0.51875}, 'truth_confidence': {'min': 0.6, 'max': 0.85, 'mean': 0.7124999999999999}, 'manipulation_confidence': {'min': 0.6, 'max': 0.9, 'mean': 0.71875}, 'justice_score': {'min': 0.2, 'max': 0.8, 'mean': 0.56875}, 'resentment_score': {'min': 0.05, 'max': 0.55, 'mean': 0.35}, 'justice_salience': {'min': 0.3, 'max': 0.9, 'mean': 0.6749999999999999}, 'resentment_salience': {'min': 0.1, 'max': 0.75, 'mean': 0.4375}, 'justice_confidence': {'min': 0.5, 'max': 0.95, 'mean': 0.7875000000000001}, 'resentment_confidence': {'min': 0.5, 'max': 0.9, 'mean': 0.7125}, 'hope_score': {'min': 0.15, 'max': 0.75, 'mean': 0.60625}, 'fear_score': {'min': 0.1, 'max': 0.6, 'mean': 0.24375000000000002}, 'hope_salience': {'min': 0.2, 'max': 0.9, 'mean': 0.6625}, 'fear_salience': {'min': 0.15, 'max': 0.5, 'mean': 0.27499999999999997}, 'hope_confidence': {'min': 0.5, 'max': 0.95, 'mean': 0.78125}, 'fear_confidence': {'min': 0.4, 'max': 0.8, 'mean': 0.59375}, 'pragmatism_score': {'min': 0.35, 'max': 0.65, 'mean': 0.5562499999999999}, 'fantasy_score': {'min': 0.05, 'max': 0.35, 'mean': 0.18125000000000002}, 'pragmatism_salience': {'min': 0.4, 'max': 0.7, 'mean': 0.5875}, 'fantasy_salience': {'min': 0.1, 'max': 0.4, 'mean': 0.22499999999999998}, 'pragmatism_confidence': {'min': 0.6, 'max': 0.9, 'mean': 0.75}, 'fantasy_confidence': {'min': 0.5, 'max': 0.85, 'mean': 0.6}, 'extraction_time_seconds': {'min': 1.0944271087646484, 'max': 1.5035650730133057, 'mean': 1.276053249835968}, 'hope_fear_tension': {'min': 0.45, 'max': 0.825, 'mean': 0.6812499999999999}, 'dignity_tribalism_tension': {'min': 0.35, 'max': 0.85, 'mean': 0.665625}, 'truth_manipulation_tension': {'min': 0.4, 'max': 0.6000000000000001, 'mean': 0.509375}, 'justice_resentment_tension': {'min': 0.44999999999999996, 'max': 0.75, 'mean': 0.609375}, 'pragmatism_fantasy_tension': {'min': 0.575, 'max': 0.8, 'mean': 0.6875}, 'virtue_index': {'min': 0.38999999999999996, 'max': 0.6799999999999999, 'mean': 0.5725}, 'pathology_index': {'min': 0.11000000000000001, 'max': 0.45999999999999996, 'mean': 0.31125}, 'virtue_index_static_weighted': {'min': 0.38823529411764707, 'max': 0.6823529411764707, 'mean': 0.5694852941176471}, 'pathology_index_static_weighted': {'min': 0.11176470588235296, 'max': 0.4735294117647059, 'mean': 0.3268382352941177}, 'civic_character_index': {'min': 0.51, 'max': 0.735, 'mean': 0.630625}, 'salience_weighted_civic_character_index': {'min': 0.5177419354838709, 'max': 0.7482142857142856, 'mean': 0.6454346460250995}}}, 'consistency_check': {'status': 'completed', 'notes': 'Basic consistency check completed'}}, 'quality_thresholds': {}}",,,,,,,,Generic metric_validation result,
