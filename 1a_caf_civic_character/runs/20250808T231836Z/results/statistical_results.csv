test_name,test_type,statistic_name,statistic_value,p_value,effect_size,degrees_of_freedom,sample_size,dependent_variable,grouping_variable,significance_level,interpretation,notes
task_01_calculate_derived_metrics,derived_metrics_calculation,result_value,"{'type': 'derived_metrics_calculation', 'success': True, 'calculated_metrics': {'dignity_tribalism_tension': [0.55, 0.225, 0.8500000000000001, 0.09999999999999999, 0.75, 0.9, 0.9, 0.09999999999999999], 'truth_manipulation_tension': [0.45, 0.325, 0.7, 0.17500000000000002, 0.30000000000000004, 0.925, 0.825, 0.17500000000000002], 'justice_resentment_tension': [0.3, 0.3, 0.75, 0.07500000000000002, 0.55, 0.8, 0.9, 0.12500000000000003], 'hope_fear_tension': [0.55, 0.39999999999999997, 0.675, 0.275, 0.475, 0.925, 0.7, 0.050000000000000024], 'pragmatism_fantasy_tension': [0.8, 0.30000000000000004, 0.75, 0.19999999999999998, 0.4, 0.925, 0.8500000000000001, 0.225], 'virtue_index': [0.6199999999999999, 0.41999999999999993, 0.79, 0.2, 0.62, 0.86, 0.8, 0.15], 'pathology_index': [0.5599999999999999, 0.8, 0.3, 0.8700000000000001, 0.63, 0.06999999999999999, 0.13, 0.8800000000000001], 'civic_character_index': [0.53, 0.31, 0.7449999999999999, 0.16499999999999998, 0.495, 0.8949999999999999, 0.8350000000000002, 0.135], 'salience_weighted_civic_character_index': [0.5513513513513514, 0.30999999999999994, 0.7460227272727272, 0.18815789473684214, 0.5280821917808219, 0.8976470588235294, 0.8484177215189874, 0.15526315789473688]}, 'successful_calculations': ['dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'hope_fear_tension', 'pragmatism_fantasy_tension', 'virtue_index', 'pathology_index', 'civic_character_index', 'salience_weighted_civic_character_index'], 'failed_calculations': [], 'skipped_due_to_missing_inputs': [], 'formulas_used': ['dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'hope_fear_tension', 'pragmatism_fantasy_tension', 'civic_character_index', 'salience_weighted_civic_character_index', 'virtue_index', 'pathology_index'], 'input_columns': ['dignity_score', 'tribalism_score', 'truth_score', 'manipulation_score', 'justice_score', 'resentment_score', 'hope_score', 'fear_score', 'pragmatism_score', 'fantasy_score', 'dignity_salience', 'truth_salience', 'justice_salience', 'hope_salience', 'pragmatism_salience'], 'total_metrics': 9, 'success_rate': 1.0, 'calculation_source': 'framework_calculation_spec', 'provenance': {'input_columns': ['dignity_score', 'tribalism_score', 'truth_score', 'manipulation_score', 'justice_score', 'resentment_score', 'hope_score', 'fear_score', 'pragmatism_score', 'fantasy_score', 'dignity_salience', 'truth_salience', 'justice_salience', 'hope_salience', 'pragmatism_salience'], 'input_document_ids': ['alexandria_ocasio_cortez_2025_fighting_oligarchy.txt', 'bernie_sanders_2025_fighting_oligarchy.txt', 'cory_booker_2018_first_step_act.txt', 'jd_vance_2022_natcon_conference.txt', 'john_lewis_1963_march_on_washington.txt', 'john_mccain_2008_concession.txt', 'mitt_romney_2020_impeachment.txt', 'steve_king_2017_house_floor.txt'], 'filter_conditions': 'None'}}",,,,,,,,Generic derived_metrics_calculation result,
task_02_validate_calculated_metrics,metric_validation,result_value,"{'type': 'metric_validation', 'validation_rules': ['missing_data_check', 'range_check', 'consistency_check'], 'results': {'missing_data_check': {'status': 'completed', 'missing_data_by_column': {'aid': 0, 'expert_categorization': 0, 'political_party': 0, 'speaker': 0, 'leadership_type': 0, 'date': 0, 'era': 0, 'ideology': 0, 'context': 0, 'dignity_score': 0, 'tribalism_score': 0, 'dignity_salience': 0, 'tribalism_salience': 0, 'dignity_confidence': 0, 'tribalism_confidence': 0, 'truth_score': 0, 'manipulation_score': 0, 'truth_salience': 0, 'manipulation_salience': 0, 'truth_confidence': 0, 'manipulation_confidence': 0, 'justice_score': 0, 'resentment_score': 0, 'justice_salience': 0, 'resentment_salience': 0, 'justice_confidence': 0, 'resentment_confidence': 0, 'hope_score': 0, 'fear_score': 0, 'hope_salience': 0, 'fear_salience': 0, 'hope_confidence': 0, 'fear_confidence': 0, 'pragmatism_score': 0, 'fantasy_score': 0, 'pragmatism_salience': 0, 'fantasy_salience': 0, 'pragmatism_confidence': 0, 'fantasy_confidence': 0, 'gasket_version': 0, 'extraction_time_seconds': 0, 'dignity_tribalism_tension': 0, 'truth_manipulation_tension': 0, 'justice_resentment_tension': 0, 'hope_fear_tension': 0, 'pragmatism_fantasy_tension': 0, 'virtue_index': 0, 'pathology_index': 0, 'civic_character_index': 0, 'salience_weighted_civic_character_index': 0}, 'total_missing': 0}, 'range_check': {'status': 'completed', 'ranges': {'dignity_score': {'min': 0.1, 'max': 0.9, 'mean': 0.58125}, 'tribalism_score': {'min': 0.1, 'max': 0.9, 'mean': 0.48750000000000004}, 'dignity_salience': {'min': 0.1, 'max': 0.9, 'mean': 0.675}, 'tribalism_salience': {'min': 0.2, 'max': 0.9, 'mean': 0.7}, 'dignity_confidence': {'min': 0.8, 'max': 0.95, 'mean': 0.89375}, 'tribalism_confidence': {'min': 0.8, 'max': 0.95, 'mean': 0.90625}, 'truth_score': {'min': 0.2, 'max': 0.9, 'mean': 0.5125}, 'manipulation_score': {'min': 0.05, 'max': 0.85, 'mean': 0.54375}, 'truth_salience': {'min': 0.3, 'max': 0.85, 'mean': 0.6375}, 'manipulation_salience': {'min': 0.2, 'max': 0.9, 'mean': 0.70625}, 'truth_confidence': {'min': 0.7, 'max': 0.95, 'mean': 0.8500000000000001}, 'manipulation_confidence': {'min': 0.8, 'max': 0.95, 'mean': 0.8812500000000001}, 'justice_score': {'min': 0.1, 'max': 0.9, 'mean': 0.5625}, 'resentment_score': {'min': 0.1, 'max': 0.95, 'mean': 0.6125}, 'justice_salience': {'min': 0.3, 'max': 1.0, 'mean': 0.7}, 'resentment_salience': {'min': 0.1, 'max': 0.95, 'mean': 0.75}, 'justice_confidence': {'min': 0.8, 'max': 0.95, 'mean': 0.89375}, 'resentment_confidence': {'min': 0.85, 'max': 0.95, 'mean': 0.91875}, 'hope_score': {'min': 0.05, 'max': 0.9, 'mean': 0.5875}, 'fear_score': {'min': 0.05, 'max': 0.95, 'mean': 0.5750000000000001}, 'hope_salience': {'min': 0.05, 'max': 0.9, 'mean': 0.64375}, 'fear_salience': {'min': 0.3, 'max': 0.95, 'mean': 0.75}, 'hope_confidence': {'min': 0.8, 'max': 0.95, 'mean': 0.85625}, 'fear_confidence': {'min': 0.8, 'max': 0.95, 'mean': 0.8875}, 'pragmatism_score': {'min': 0.2, 'max': 0.9, 'mean': 0.54375}, 'fantasy_score': {'min': 0.05, 'max': 0.8, 'mean': 0.43125}, 'pragmatism_salience': {'min': 0.2, 'max': 0.9, 'mean': 0.6312500000000001}, 'fantasy_salience': {'min': 0.1, 'max': 0.85, 'mean': 0.5625}, 'pragmatism_confidence': {'min': 0.7, 'max': 0.95, 'mean': 0.8375}, 'fantasy_confidence': {'min': 0.75, 'max': 0.95, 'mean': 0.85625}, 'extraction_time_seconds': {'min': 4.453752040863037, 'max': 50.68368363380432, 'mean': 12.732089638710022}, 'dignity_tribalism_tension': {'min': 0.09999999999999999, 'max': 0.9, 'mean': 0.546875}, 'truth_manipulation_tension': {'min': 0.17500000000000002, 'max': 0.925, 'mean': 0.484375}, 'justice_resentment_tension': {'min': 0.07500000000000002, 'max': 0.9, 'mean': 0.475}, 'hope_fear_tension': {'min': 0.050000000000000024, 'max': 0.925, 'mean': 0.50625}, 'pragmatism_fantasy_tension': {'min': 0.19999999999999998, 'max': 0.925, 'mean': 0.55625}, 'virtue_index': {'min': 0.15, 'max': 0.86, 'mean': 0.5575}, 'pathology_index': {'min': 0.06999999999999999, 'max': 0.8800000000000001, 'mean': 0.53}, 'civic_character_index': {'min': 0.135, 'max': 0.8949999999999999, 'mean': 0.51375}, 'salience_weighted_civic_character_index': {'min': 0.15526315789473688, 'max': 0.8976470588235294, 'mean': 0.5281177629223746}}}, 'consistency_check': {'status': 'completed', 'notes': 'Basic consistency check completed'}}, 'quality_thresholds': {'missing_data_percentage': 5, 'range_min': 0.0, 'range_max': 1.0}, 'provenance': {'input_columns': ['missing_data_check', 'range_check', 'consistency_check'], 'input_document_ids': ['alexandria_ocasio_cortez_2025_fighting_oligarchy.txt', 'bernie_sanders_2025_fighting_oligarchy.txt', 'cory_booker_2018_first_step_act.txt', 'jd_vance_2022_natcon_conference.txt', 'john_lewis_1963_march_on_washington.txt', 'john_mccain_2008_concession.txt', 'mitt_romney_2020_impeachment.txt', 'steve_king_2017_house_floor.txt'], 'filter_conditions': 'Validation rules applied as specified'}}",,,,,,,,Generic metric_validation result,
task_03_generate_descriptive_statistics,descriptive_stats,result_value,"{'type': 'descriptive_stats', 'columns_analyzed': ['dignity_score', 'tribalism_score', 'truth_score', 'manipulation_score', 'justice_score', 'resentment_score', 'hope_score', 'fear_score', 'pragmatism_score', 'fantasy_score', 'civic_character_index', 'salience_weighted_civic_character_index', 'virtue_index', 'pathology_index', 'dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'hope_fear_tension', 'pragmatism_fantasy_tension'], 'results': {'dignity_score': {'count': 8, 'mean': 0.58125, 'std': 0.35449914366207275, 'min': 0.1, 'max': 0.9, 'median': 0.75, 'q25': 0.25, 'q75': 0.8625, 'skewness': -0.6363767887068106, 'kurtosis': -1.8196924998974668, 'data_type': 'numerical'}, 'tribalism_score': {'count': 8, 'mean': 0.48750000000000004, 'std': 0.369120576505835, 'min': 0.1, 'max': 0.9, 'median': 0.475, 'q25': 0.1, 'q75': 0.8625, 'skewness': 0.0465134445366685, 'kurtosis': -2.2528166219774666, 'data_type': 'numerical'}, 'truth_score': {'count': 8, 'mean': 0.5125, 'std': 0.2748376143938713, 'min': 0.2, 'max': 0.9, 'median': 0.5, 'q25': 0.275, 'q75': 0.725, 'skewness': 0.15741080944443925, 'kurtosis': -1.7785020874201507, 'data_type': 'numerical'}, 'manipulation_score': {'count': 8, 'mean': 0.54375, 'std': 0.32451887464367923, 'min': 0.05, 'max': 0.85, 'median': 0.7, 'q25': 0.2625, 'q75': 0.775, 'skewness': -0.6802387632266688, 'kurtosis': -1.5738318932606106, 'data_type': 'numerical'}, 'justice_score': {'count': 8, 'mean': 0.5625, 'std': 0.3113908889391045, 'min': 0.1, 'max': 0.9, 'median': 0.6, 'q25': 0.35000000000000003, 'q75': 0.8250000000000001, 'skewness': -0.3672703503741337, 'kurtosis': -1.5004670187112734, 'data_type': 'numerical'}, 'resentment_score': {'count': 8, 'mean': 0.6125, 'std': 0.36326888899861176, 'min': 0.1, 'max': 0.95, 'median': 0.75, 'q25': 0.325, 'q75': 0.9125, 'skewness': -0.6790691868668357, 'kurtosis': -1.4724941908478146, 'data_type': 'numerical'}, 'hope_score': {'count': 8, 'mean': 0.5875, 'std': 0.268261599403056, 'min': 0.05, 'max': 0.9, 'median': 0.6, 'q25': 0.55, 'q75': 0.7374999999999999, 'skewness': -1.1035115623468754, 'kurtosis': 1.663907788361482, 'data_type': 'numerical'}, 'fear_score': {'count': 8, 'mean': 0.5750000000000001, 'std': 0.3218251521733945, 'min': 0.05, 'max': 0.95, 'median': 0.625, 'q25': 0.425, 'q75': 0.8125, 'skewness': -0.6171695664985154, 'kurtosis': -0.9110487514863252, 'data_type': 'numerical'}, 'pragmatism_score': {'count': 8, 'mean': 0.54375, 'std': 0.2896272234639357, 'min': 0.2, 'max': 0.9, 'median': 0.575, 'q25': 0.275, 'q75': 0.8, 'skewness': -0.10868974953861726, 'kurtosis': -2.1294947857910227, 'data_type': 'numerical'}, 'fantasy_score': {'count': 8, 'mean': 0.43125, 'std': 0.3206438834595165, 'min': 0.05, 'max': 0.8, 'median': 0.42500000000000004, 'q25': 0.17500000000000002, 'q75': 0.7124999999999999, 'skewness': -0.02363080592959844, 'kurtosis': -2.4606699072308222, 'data_type': 'numerical'}, 'civic_character_index': {'count': 8, 'mean': 0.51375, 'std': 0.2950151327837753, 'min': 0.135, 'max': 0.8949999999999999, 'median': 0.5125, 'q25': 0.27375, 'q75': 0.7675, 'skewness': -0.03171417794012783, 'kurtosis': -1.6532326376279638, 'data_type': 'numerical'}, 'salience_weighted_civic_character_index': {'count': 8, 'mean': 0.5281177629223746, 'std': 0.2901563532601538, 'min': 0.15526315789473688, 'max': 0.8976470588235294, 'median': 0.5397167715660867, 'q25': 0.2795394736842105, 'q75': 0.7716214758342923, 'skewness': -0.060991897117528046, 'kurtosis': -1.6894430768146225, 'data_type': 'numerical'}, 'virtue_index': {'count': 8, 'mean': 0.5575, 'std': 0.27384823742879405, 'min': 0.15, 'max': 0.86, 'median': 0.6199999999999999, 'q25': 0.36499999999999994, 'q75': 0.7925, 'skewness': -0.5654069571224676, 'kurtosis': -1.3000319434991532, 'data_type': 'numerical'}, 'pathology_index': {'count': 8, 'mean': 0.53, 'std': 0.32654030597855976, 'min': 0.06999999999999999, 'max': 0.8800000000000001, 'median': 0.595, 'q25': 0.2575, 'q75': 0.8175000000000001, 'skewness': -0.38629681172639263, 'kurtosis': -1.6746378759524823, 'data_type': 'numerical'}, 'dignity_tribalism_tension': {'count': 8, 'mean': 0.546875, 'std': 0.355677798616348, 'min': 0.09999999999999999, 'max': 0.9, 'median': 0.65, 'q25': 0.19375, 'q75': 0.8625, 'skewness': -0.37051594712872477, 'kurtosis': -2.0777883135196333, 'data_type': 'numerical'}, 'truth_manipulation_tension': {'count': 8, 'mean': 0.484375, 'std': 0.2948781528021362, 'min': 0.17500000000000002, 'max': 0.925, 'median': 0.3875, 'q25': 0.26875000000000004, 'q75': 0.73125, 'skewness': 0.4866923906952533, 'kurtosis': -1.5696668455308678, 'data_type': 'numerical'}, 'justice_resentment_tension': {'count': 8, 'mean': 0.475, 'std': 0.3187587533812097, 'min': 0.07500000000000002, 'max': 0.9, 'median': 0.42500000000000004, 'q25': 0.25625, 'q75': 0.7625, 'skewness': 0.08656159820755464, 'kurtosis': -1.844216258289293, 'data_type': 'numerical'}, 'hope_fear_tension': {'count': 8, 'mean': 0.50625, 'std': 0.2718159829212193, 'min': 0.050000000000000024, 'max': 0.925, 'median': 0.5125, 'q25': 0.36874999999999997, 'q75': 0.68125, 'skewness': -0.21795890129134637, 'kurtosis': 0.08017393050446753, 'data_type': 'numerical'}, 'pragmatism_fantasy_tension': {'count': 8, 'mean': 0.55625, 'std': 0.3037709428406307, 'min': 0.19999999999999998, 'max': 0.925, 'median': 0.575, 'q25': 0.28125000000000006, 'q75': 0.8125, 'skewness': -0.0336840005627112, 'kurtosis': -2.325873091773904, 'data_type': 'numerical'}}, 'provenance': {'input_columns': ['dignity_score', 'tribalism_score', 'truth_score', 'manipulation_score', 'justice_score', 'resentment_score', 'hope_score', 'fear_score', 'pragmatism_score', 'fantasy_score', 'civic_character_index', 'salience_weighted_civic_character_index', 'virtue_index', 'pathology_index', 'dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'hope_fear_tension', 'pragmatism_fantasy_tension'], 'input_document_ids': ['alexandria_ocasio_cortez_2025_fighting_oligarchy.txt', 'bernie_sanders_2025_fighting_oligarchy.txt', 'cory_booker_2018_first_step_act.txt', 'jd_vance_2022_natcon_conference.txt', 'john_lewis_1963_march_on_washington.txt', 'john_mccain_2008_concession.txt', 'mitt_romney_2020_impeachment.txt', 'steve_king_2017_house_floor.txt'], 'filter_conditions': 'None'}}",,,,,,,,Generic descriptive_stats result,
task_05_log_untestable_hypotheses,summary_statistics,result_value,"{'type': 'summary_statistics', 'metrics': ['civic_character_index'], 'summary_types': ['mean', 'std'], 'results': {'civic_character_index': {'mean': 0.51375, 'std': 0.2950151327837753}}, 'missing_metrics': [], 'provenance': {'input_columns': ['civic_character_index'], 'input_document_ids': ['alexandria_ocasio_cortez_2025_fighting_oligarchy.txt', 'bernie_sanders_2025_fighting_oligarchy.txt', 'cory_booker_2018_first_step_act.txt', 'jd_vance_2022_natcon_conference.txt', 'john_lewis_1963_march_on_washington.txt', 'john_mccain_2008_concession.txt', 'mitt_romney_2020_impeachment.txt', 'steve_king_2017_house_floor.txt'], 'filter_conditions': 'None'}}",,,,,,,,Generic summary_statistics result,
