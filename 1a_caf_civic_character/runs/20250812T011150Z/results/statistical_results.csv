test_name,test_type,statistic_name,statistic_value,p_value,effect_size,degrees_of_freedom,sample_size,dependent_variable,grouping_variable,significance_level,interpretation,notes
descriptives,descriptive_stats,result_value,"{'type': 'descriptive_stats', 'columns_analyzed': ['dignity_score', 'tribalism_score', 'dignity_salience', 'tribalism_salience', 'dignity_confidence', 'tribalism_confidence', 'truth_score', 'manipulation_score', 'truth_salience', 'manipulation_salience', 'truth_confidence', 'manipulation_confidence', 'justice_score', 'resentment_score', 'justice_salience', 'resentment_salience', 'justice_confidence', 'resentment_confidence', 'hope_score', 'fear_score', 'hope_salience', 'fear_salience', 'hope_confidence', 'fear_confidence', 'pragmatism_score', 'fantasy_score', 'pragmatism_salience', 'fantasy_salience', 'pragmatism_confidence', 'fantasy_confidence'], 'results': {'dignity_score': {'count': 8, 'mean': 0.5999999999999999, 'std': 0.291547594742265, 'min': 0.1, 'max': 0.85, 'median': 0.7, 'q25': 0.5, 'q75': 0.8125, 'skewness': -1.1356376898512333, 'kurtosis': -0.2631240731586786, 'data_type': 'numerical'}, 'tribalism_score': {'count': 8, 'mean': 0.4, 'std': 0.19272482233188631, 'min': 0.1, 'max': 0.7, 'median': 0.35, 'q25': 0.3, 'q75': 0.525, 'skewness': 0.15965369897315929, 'kurtosis': -0.37692307692307914, 'data_type': 'numerical'}, 'dignity_salience': {'count': 8, 'mean': 0.6375000000000001, 'std': 0.26152028055746873, 'min': 0.2, 'max': 0.9, 'median': 0.7, 'q25': 0.525, 'q75': 0.8250000000000001, 'skewness': -0.8655954203536892, 'kurtosis': -0.5140780835645469, 'data_type': 'numerical'}, 'tribalism_salience': {'count': 8, 'mean': 0.51875, 'std': 0.2202879608927499, 'min': 0.2, 'max': 0.85, 'median': 0.45, 'q25': 0.4, 'q75': 0.65, 'skewness': 0.3869235080798302, 'kurtosis': -0.6398366917209231, 'data_type': 'numerical'}, 'dignity_confidence': {'count': 8, 'mean': 0.7562500000000001, 'std': 0.1399936223037117, 'min': 0.5, 'max': 0.95, 'median': 0.75, 'q25': 0.7, 'q75': 0.8250000000000001, 'skewness': -0.48244957316137677, 'kurtosis': 0.6213085237208205, 'data_type': 'numerical'}, 'tribalism_confidence': {'count': 8, 'mean': 0.70625, 'std': 0.10835622468770048, 'min': 0.6, 'max': 0.9, 'median': 0.7, 'q25': 0.6, 'q75': 0.7625, 'skewness': 0.6860205860318592, 'kurtosis': -0.24304529485752546, 'data_type': 'numerical'}, 'truth_score': {'count': 8, 'mean': 0.4625, 'std': 0.17677669529663687, 'min': 0.2, 'max': 0.7, 'median': 0.5, 'q25': 0.3, 'q75': 0.6, 'skewness': -0.27476149211820217, 'kurtosis': -1.373988571428571, 'data_type': 'numerical'}, 'manipulation_score': {'count': 8, 'mean': 0.4, 'std': 0.19456912102680338, 'min': 0.05, 'max': 0.6, 'median': 0.45, 'q25': 0.3125, 'q75': 0.525, 'skewness': -0.8533611858491346, 'kurtosis': -0.11238875044499874, 'data_type': 'numerical'}, 'truth_salience': {'count': 8, 'mean': 0.58125, 'std': 0.14623244705409455, 'min': 0.3, 'max': 0.75, 'median': 0.6, 'q25': 0.5, 'q75': 0.7, 'skewness': -0.9229735771978766, 'kurtosis': 0.763211457411713, 'data_type': 'numerical'}, 'manipulation_salience': {'count': 8, 'mean': 0.5, 'std': 0.2449489742783178, 'min': 0.1, 'max': 0.8, 'median': 0.55, 'q25': 0.3, 'q75': 0.7, 'skewness': -0.46656947481584377, 'kurtosis': -1.0714285714285698, 'data_type': 'numerical'}, 'truth_confidence': {'count': 8, 'mean': 0.6812499999999999, 'std': 0.09977653603356423, 'min': 0.5, 'max': 0.85, 'median': 0.7, 'q25': 0.6749999999999999, 'q75': 0.7, 'skewness': -0.31685154679201905, 'kurtosis': 1.7916306380582734, 'data_type': 'numerical'}, 'manipulation_confidence': {'count': 8, 'mean': 0.675, 'std': 0.11649647450214354, 'min': 0.5, 'max': 0.8, 'median': 0.6499999999999999, 'q25': 0.6, 'q75': 0.8, 'skewness': -0.09035737634515371, 'kurtosis': -1.613296398891968, 'data_type': 'numerical'}, 'justice_score': {'count': 8, 'mean': 0.53125, 'std': 0.2604083331999957, 'min': 0.1, 'max': 0.75, 'median': 0.7, 'q25': 0.35000000000000003, 'q75': 0.7, 'skewness': -0.961294751390998, 'kurtosis': -0.9888873045873603, 'data_type': 'numerical'}, 'resentment_score': {'count': 8, 'mean': 0.375, 'std': 0.17525491637693283, 'min': 0.0, 'max': 0.6, 'median': 0.4, 'q25': 0.375, 'q75': 0.42500000000000004, 'skewness': -1.4065889448532627, 'kurtosis': 3.3194159004867476, 'data_type': 'numerical'}, 'justice_salience': {'count': 8, 'mean': 0.5875, 'std': 0.29610567611677324, 'min': 0.2, 'max': 0.85, 'median': 0.775, 'q25': 0.275, 'q75': 0.8, 'skewness': -0.6527366342245885, 'kurtosis': -2.0684699333419063, 'data_type': 'numerical'}, 'resentment_salience': {'count': 8, 'mean': 0.46249999999999997, 'std': 0.18468119248354134, 'min': 0.1, 'max': 0.7, 'median': 0.5, 'q25': 0.45, 'q75': 0.525, 'skewness': -1.0801123661162897, 'kurtosis': 1.440179819632136, 'data_type': 'numerical'}, 'justice_confidence': {'count': 8, 'mean': 0.7250000000000001, 'std': 0.16035674514745463, 'min': 0.4, 'max': 0.9, 'median': 0.775, 'q25': 0.6749999999999999, 'q75': 0.8125, 'skewness': -1.2991865926298427, 'kurtosis': 1.6576388888888909, 'data_type': 'numerical'}, 'resentment_confidence': {'count': 8, 'mean': 0.63125, 'std': 0.1334634781503914, 'min': 0.4, 'max': 0.8, 'median': 0.6499999999999999, 'q25': 0.575, 'q75': 0.7124999999999999, 'skewness': -0.6056118781656058, 'kurtosis': -0.34760585674713074, 'data_type': 'numerical'}, 'hope_score': {'count': 8, 'mean': 0.5375, 'std': 0.20658792662827957, 'min': 0.1, 'max': 0.7, 'median': 0.6, 'q25': 0.475, 'q75': 0.7, 'skewness': -1.5777370338810717, 'kurtosis': 2.4632902085047546, 'data_type': 'numerical'}, 'fear_score': {'count': 8, 'mean': 0.2375, 'std': 0.11877349391654207, 'min': 0.0, 'max': 0.4, 'median': 0.25, 'q25': 0.2, 'q75': 0.3, 'skewness': -0.9698281124798878, 'kurtosis': 1.87175132190354, 'data_type': 'numerical'}, 'hope_salience': {'count': 8, 'mean': 0.5874999999999999, 'std': 0.2232071427428535, 'min': 0.1, 'max': 0.8, 'median': 0.7, 'q25': 0.5, 'q75': 0.7, 'skewness': -1.7454895483155366, 'kurtosis': 3.2878714302231433, 'data_type': 'numerical'}, 'fear_salience': {'count': 8, 'mean': 0.32499999999999996, 'std': 0.10350983390135314, 'min': 0.1, 'max': 0.4, 'median': 0.35, 'q25': 0.3, 'q75': 0.4, 'skewness': -1.6745590906707772, 'kurtosis': 3.135999999999992, 'data_type': 'numerical'}, 'hope_confidence': {'count': 8, 'mean': 0.6875, 'std': 0.1787855858683404, 'min': 0.3, 'max': 0.85, 'median': 0.7, 'q25': 0.6749999999999999, 'q75': 0.8125, 'skewness': -1.6279917147700445, 'kurtosis': 3.207490402921259, 'data_type': 'numerical'}, 'fear_confidence': {'count': 8, 'mean': 0.55, 'std': 0.09258200997725512, 'min': 0.4, 'max': 0.7, 'median': 0.55, 'q25': 0.5, 'q75': 0.6, 'skewness': 0.0, 'kurtosis': 1.7763568394002505e-15, 'data_type': 'numerical'}, 'pragmatism_score': {'count': 8, 'mean': 0.48750000000000004, 'std': 0.14577379737113247, 'min': 0.2, 'max': 0.7, 'median': 0.5, 'q25': 0.475, 'q75': 0.525, 'skewness': -0.8243461403488678, 'kurtosis': 2.0017795353435552, 'data_type': 'numerical'}, 'fantasy_score': {'count': 8, 'mean': 0.1375, 'std': 0.05175491695067657, 'min': 0.1, 'max': 0.2, 'median': 0.1, 'q25': 0.1, 'q75': 0.2, 'skewness': 0.6440611887195302, 'kurtosis': -2.2400000000000007, 'data_type': 'numerical'}, 'pragmatism_salience': {'count': 8, 'mean': 0.5625, 'std': 0.1767766952966369, 'min': 0.2, 'max': 0.8, 'median': 0.6, 'q25': 0.5, 'q75': 0.625, 'skewness': -1.102278456615372, 'kurtosis': 2.4177371428571437, 'data_type': 'numerical'}, 'fantasy_salience': {'count': 8, 'mean': 0.20625, 'std': 0.06781013409302687, 'min': 0.1, 'max': 0.3, 'median': 0.2, 'q25': 0.1875, 'q75': 0.225, 'skewness': 0.16465162166428288, 'kurtosis': -0.16574606466207875, 'data_type': 'numerical'}, 'pragmatism_confidence': {'count': 8, 'mean': 0.675, 'std': 0.148804761828569, 'min': 0.4, 'max': 0.9, 'median': 0.7, 'q25': 0.6, 'q75': 0.725, 'skewness': -0.4769184874422407, 'kurtosis': 1.1071800208116533, 'data_type': 'numerical'}, 'fantasy_confidence': {'count': 8, 'mean': 0.4375, 'std': 0.13024701806293193, 'min': 0.3, 'max': 0.6, 'median': 0.45, 'q25': 0.3, 'q75': 0.525, 'skewness': 0.10506352260367204, 'kurtosis': -1.922304709141275, 'data_type': 'numerical'}}, 'provenance': {'input_columns': ['dignity_score', 'tribalism_score', 'dignity_salience', 'tribalism_salience', 'dignity_confidence', 'tribalism_confidence', 'truth_score', 'manipulation_score', 'truth_salience', 'manipulation_salience', 'truth_confidence', 'manipulation_confidence', 'justice_score', 'resentment_score', 'justice_salience', 'resentment_salience', 'justice_confidence', 'resentment_confidence', 'hope_score', 'fear_score', 'hope_salience', 'fear_salience', 'hope_confidence', 'fear_confidence', 'pragmatism_score', 'fantasy_score', 'pragmatism_salience', 'fantasy_salience', 'pragmatism_confidence', 'fantasy_confidence'], 'input_document_ids': ['alexandria_ocasio_cortez_2025_fighting_oligarchy.txt', 'bernie_sanders_2025_fighting_oligarchy.txt', 'cory_booker_2018_first_step_act.txt', 'jd_vance_2022_natcon_conference.txt', 'john_lewis_1963_march_on_washington.txt', 'john_mccain_2008_concession.txt', 'mitt_romney_2020_impeachment.txt', 'steve_king_2017_house_floor.txt'], 'filter_conditions': 'None'}}",,,,,,,,Generic descriptive_stats result,
