test_name,test_type,statistic_name,statistic_value,p_value,effect_size,degrees_of_freedom,sample_size,dependent_variable,grouping_variable,significance_level,interpretation,notes
calculate_tension_scores,derived_metrics_calculation,result_value,"{'type': 'derived_metrics_calculation', 'success': True, 'calculated_metrics': {'hope_fear_tension': [0.775, 0.775, 0.825, 0.6499999999999999, 0.55, 0.825, 0.6, 0.45], 'dignity_tribalism_tension': [0.625, 0.85, 0.8, 0.35, 0.6, 0.775, 0.7, 0.625], 'truth_manipulation_tension': [0.4, 0.44999999999999996, 0.6000000000000001, 0.45, 0.6, 0.6000000000000001, 0.55, 0.42500000000000004], 'justice_resentment_tension': [0.6499999999999999, 0.575, 0.6499999999999999, 0.44999999999999996, 0.65, 0.6, 0.75, 0.55], 'pragmatism_fantasy_tension': [0.6000000000000001, 0.725, 0.8, 0.6499999999999999, 0.6499999999999999, 0.75, 0.75, 0.575]}, 'successful_calculations': ['hope_fear_tension', 'dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'pragmatism_fantasy_tension'], 'failed_calculations': [], 'formulas_used': ['dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'hope_fear_tension', 'pragmatism_fantasy_tension'], 'input_columns': ['dignity_score', 'tribalism_score', 'truth_score', 'manipulation_score', 'justice_score', 'resentment_score', 'hope_score', 'fear_score', 'pragmatism_score', 'fantasy_score'], 'total_metrics': 5, 'success_rate': 1.0}",,,,,,,,Generic derived_metrics_calculation result,
calculate_composite_indices,derived_metrics_calculation,result_value,"{'type': 'derived_metrics_calculation', 'success': True, 'calculated_metrics': {'virtue_index': [0.6599999999999999, 0.6, 0.6799999999999999, 0.42000000000000004, 0.6799999999999999, 0.53, 0.62, 0.38999999999999996], 'pathology_index': [0.43999999999999995, 0.25000000000000006, 0.21000000000000002, 0.4, 0.45999999999999996, 0.11000000000000001, 0.28, 0.33999999999999997], 'civic_character_index': [0.61, 0.675, 0.735, 0.51, 0.6100000000000001, 0.71, 0.67, 0.525]}, 'successful_calculations': ['virtue_index', 'pathology_index', 'civic_character_index'], 'failed_calculations': [], 'formulas_used': ['civic_character_index', 'virtue_index', 'pathology_index'], 'input_columns': ['dignity_score', 'tribalism_score', 'truth_score', 'manipulation_score', 'justice_score', 'resentment_score', 'hope_score', 'fear_score', 'pragmatism_score', 'fantasy_score', 'dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'hope_fear_tension', 'pragmatism_fantasy_tension'], 'total_metrics': 3, 'success_rate': 1.0}",,,,,,,,Generic derived_metrics_calculation result,
calculate_salience_weighted_index,derived_metrics_calculation,result_value,"{'type': 'derived_metrics_calculation', 'success': True, 'calculated_metrics': {'salience_weighted_civic_character_index': [0.64375, 0.6806338028169013, 0.7389610389610389, 0.5177419354838709, 0.6102941176470588, 0.7482142857142856, 0.6771428571428572, 0.5467391304347826]}, 'successful_calculations': ['salience_weighted_civic_character_index'], 'failed_calculations': [], 'formulas_used': ['salience_weighted_civic_character_index'], 'input_columns': ['dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'hope_fear_tension', 'pragmatism_fantasy_tension', 'dignity_salience', 'truth_salience', 'justice_salience', 'hope_salience', 'pragmatism_salience'], 'total_metrics': 1, 'success_rate': 1.0}",,,,,,,,Generic derived_metrics_calculation result,
validate_derived_metrics,metric_validation,result_value,"{'type': 'metric_validation', 'validation_rules': ['missing_data_check', 'range_check', 'consistency_check'], 'results': {'missing_data_check': {'status': 'completed', 'missing_data_by_column': {'aid': 0, 'expert_categorization': 0, 'political_party': 0, 'speaker': 0, 'leadership_type': 0, 'date': 0, 'era': 0, 'ideology': 0, 'context': 0, 'dignity_score': 0, 'tribalism_score': 0, 'dignity_salience': 0, 'tribalism_salience': 0, 'dignity_confidence': 0, 'tribalism_confidence': 0, 'truth_score': 0, 'manipulation_score': 0, 'truth_salience': 0, 'manipulation_salience': 0, 'truth_confidence': 0, 'manipulation_confidence': 0, 'justice_score': 0, 'resentment_score': 0, 'justice_salience': 0, 'resentment_salience': 0, 'justice_confidence': 0, 'resentment_confidence': 0, 'hope_score': 0, 'fear_score': 0, 'hope_salience': 0, 'fear_salience': 0, 'hope_confidence': 0, 'fear_confidence': 0, 'pragmatism_score': 0, 'fantasy_score': 0, 'pragmatism_salience': 0, 'fantasy_salience': 0, 'pragmatism_confidence': 0, 'fantasy_confidence': 0, 'gasket_version': 0, 'extraction_time_seconds': 0, 'hope_fear_tension': 0, 'dignity_tribalism_tension': 0, 'truth_manipulation_tension': 0, 'justice_resentment_tension': 0, 'pragmatism_fantasy_tension': 0, 'virtue_index': 0, 'pathology_index': 0, 'civic_character_index': 0, 'salience_weighted_civic_character_index': 0}, 'total_missing': 0}, 'range_check': {'status': 'completed', 'ranges': {'dignity_score': {'min': 0.3, 'max': 0.85, 'mean': 0.68125}, 'tribalism_score': {'min': 0.1, 'max': 0.6, 'mean': 0.35}, 'dignity_salience': {'min': 0.7, 'max': 0.85, 'mean': 0.7625}, 'tribalism_salience': {'min': 0.2, 'max': 0.8, 'mean': 0.4625}, 'dignity_confidence': {'min': 0.7, 'max': 0.95, 'mean': 0.8375}, 'tribalism_confidence': {'min': 0.7, 'max': 0.9, 'mean': 0.7749999999999999}, 'truth_score': {'min': 0.3, 'max': 0.6, 'mean': 0.45}, 'manipulation_score': {'min': 0.2, 'max': 0.65, 'mean': 0.43125}, 'truth_salience': {'min': 0.3, 'max': 0.7, 'mean': 0.5}, 'manipulation_salience': {'min': 0.4, 'max': 0.7, 'mean': 0.51875}, 'truth_confidence': {'min': 0.6, 'max': 0.85, 'mean': 0.7124999999999999}, 'manipulation_confidence': {'min': 0.6, 'max': 0.9, 'mean': 0.71875}, 'justice_score': {'min': 0.2, 'max': 0.8, 'mean': 0.56875}, 'resentment_score': {'min': 0.05, 'max': 0.55, 'mean': 0.35}, 'justice_salience': {'min': 0.3, 'max': 0.9, 'mean': 0.6749999999999999}, 'resentment_salience': {'min': 0.1, 'max': 0.75, 'mean': 0.4375}, 'justice_confidence': {'min': 0.5, 'max': 0.95, 'mean': 0.7875000000000001}, 'resentment_confidence': {'min': 0.5, 'max': 0.9, 'mean': 0.7125}, 'hope_score': {'min': 0.15, 'max': 0.75, 'mean': 0.60625}, 'fear_score': {'min': 0.1, 'max': 0.6, 'mean': 0.24375000000000002}, 'hope_salience': {'min': 0.2, 'max': 0.9, 'mean': 0.6625}, 'fear_salience': {'min': 0.15, 'max': 0.5, 'mean': 0.27499999999999997}, 'hope_confidence': {'min': 0.5, 'max': 0.95, 'mean': 0.78125}, 'fear_confidence': {'min': 0.4, 'max': 0.8, 'mean': 0.59375}, 'pragmatism_score': {'min': 0.35, 'max': 0.65, 'mean': 0.5562499999999999}, 'fantasy_score': {'min': 0.05, 'max': 0.35, 'mean': 0.18125000000000002}, 'pragmatism_salience': {'min': 0.4, 'max': 0.7, 'mean': 0.5875}, 'fantasy_salience': {'min': 0.1, 'max': 0.4, 'mean': 0.22499999999999998}, 'pragmatism_confidence': {'min': 0.6, 'max': 0.9, 'mean': 0.75}, 'fantasy_confidence': {'min': 0.5, 'max': 0.85, 'mean': 0.6}, 'extraction_time_seconds': {'min': 1.1417019367218018, 'max': 4.6076271533966064, 'mean': 1.977801650762558}, 'hope_fear_tension': {'min': 0.45, 'max': 0.825, 'mean': 0.6812499999999999}, 'dignity_tribalism_tension': {'min': 0.35, 'max': 0.85, 'mean': 0.665625}, 'truth_manipulation_tension': {'min': 0.4, 'max': 0.6000000000000001, 'mean': 0.509375}, 'justice_resentment_tension': {'min': 0.44999999999999996, 'max': 0.75, 'mean': 0.609375}, 'pragmatism_fantasy_tension': {'min': 0.575, 'max': 0.8, 'mean': 0.6875}, 'virtue_index': {'min': 0.38999999999999996, 'max': 0.6799999999999999, 'mean': 0.5725}, 'pathology_index': {'min': 0.11000000000000001, 'max': 0.45999999999999996, 'mean': 0.31125}, 'civic_character_index': {'min': 0.51, 'max': 0.735, 'mean': 0.630625}, 'salience_weighted_civic_character_index': {'min': 0.5177419354838709, 'max': 0.7482142857142856, 'mean': 0.6454346460250995}}}, 'consistency_check': {'status': 'completed', 'notes': 'Basic consistency check completed'}}, 'quality_thresholds': {'range_check': {'min': 0.0, 'max': 1.0}, 'missing_data_check': {'max_allowed_missing': 0}}}",,,,,,,,Generic metric_validation result,
analyze_speaker_differentiation_anova,one_way_anova,F_statistic,nan,nan,,,,civic_character_index,speaker,p >= 0.05,,
analyze_speaker_differentiation_anova_dimensions,one_way_anova,F_statistic,nan,nan,,,,dignity_score,speaker,p >= 0.05,,
analyze_speaker_differentiation_anova_dimensions_truth,one_way_anova,F_statistic,nan,nan,,,,truth_score,speaker,p >= 0.05,,
analyze_speaker_differentiation_anova_dimensions_justice,one_way_anova,F_statistic,nan,nan,,,,justice_score,speaker,p >= 0.05,,
analyze_speaker_differentiation_anova_dimensions_hope,one_way_anova,F_statistic,nan,nan,,,,hope_score,speaker,p >= 0.05,,
analyze_speaker_differentiation_anova_dimensions_pragmatism,one_way_anova,F_statistic,nan,nan,,,,pragmatism_score,speaker,p >= 0.05,,
analyze_speaker_differentiation_anova_dimensions_tribalism,one_way_anova,F_statistic,nan,nan,,,,tribalism_score,speaker,p >= 0.05,,
analyze_speaker_differentiation_anova_dimensions_manipulation,one_way_anova,F_statistic,nan,nan,,,,manipulation_score,speaker,p >= 0.05,,
analyze_speaker_differentiation_anova_dimensions_resentment,one_way_anova,F_statistic,nan,nan,,,,resentment_score,speaker,p >= 0.05,,
analyze_speaker_differentiation_anova_dimensions_fear,one_way_anova,F_statistic,nan,nan,,,,fear_score,speaker,p >= 0.05,,
analyze_speaker_differentiation_anova_dimensions_fantasy,one_way_anova,F_statistic,nan,nan,,,,fantasy_score,speaker,p >= 0.05,,
analyze_mc_sci_coherence_patterns_anova,one_way_anova,F_statistic,nan,nan,,,,civic_character_index,speaker,p >= 0.05,,
calculate_descriptive_stats_all_metrics,summary_statistics,result_value,"{'type': 'summary_statistics', 'metrics': ['dignity_score', 'tribalism_score', 'truth_score', 'manipulation_score', 'justice_score', 'resentment_score', 'hope_score', 'fear_score', 'pragmatism_score', 'fantasy_score', 'dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'hope_fear_tension', 'pragmatism_fantasy_tension', 'civic_character_index', 'virtue_index', 'pathology_index', 'salience_weighted_civic_character_index'], 'summary_types': ['mean', 'median', 'std', 'min', 'max'], 'results': {'dignity_score': {'mean': 0.68125, 'std': 0.17307615994947756, 'min': 0.3, 'max': 0.85, 'median': 0.7}, 'tribalism_score': {'mean': 0.35, 'std': 0.2052872551885702, 'min': 0.1, 'max': 0.6, 'median': 0.35}, 'truth_score': {'mean': 0.45, 'std': 0.12247448713915891, 'min': 0.3, 'max': 0.6, 'median': 0.42500000000000004}, 'manipulation_score': {'mean': 0.43125, 'std': 0.13076014027873437, 'min': 0.2, 'max': 0.65, 'median': 0.42500000000000004}, 'justice_score': {'mean': 0.56875, 'std': 0.22825033249858429, 'min': 0.2, 'max': 0.8, 'median': 0.7}, 'resentment_score': {'mean': 0.35, 'std': 0.16256866681058635, 'min': 0.05, 'max': 0.55, 'median': 0.4}, 'hope_score': {'mean': 0.60625, 'std': 0.2043063176423368, 'min': 0.15, 'max': 0.75, 'median': 0.675}, 'fear_score': {'mean': 0.24375000000000002, 'std': 0.1678381448215597, 'min': 0.1, 'max': 0.6, 'median': 0.225}, 'pragmatism_score': {'mean': 0.5562499999999999, 'std': 0.09425459443140463, 'min': 0.35, 'max': 0.65, 'median': 0.6}, 'fantasy_score': {'mean': 0.18125000000000002, 'std': 0.12229209763045666, 'min': 0.05, 'max': 0.35, 'median': 0.15000000000000002}, 'dignity_tribalism_tension': {'mean': 0.665625, 'std': 0.1569448834099775, 'min': 0.35, 'max': 0.85, 'median': 0.6625}, 'truth_manipulation_tension': {'mean': 0.509375, 'std': 0.08653807997473881, 'min': 0.4, 'max': 0.6000000000000001, 'median': 0.5}, 'justice_resentment_tension': {'mean': 0.609375, 'std': 0.08857754875168505, 'min': 0.44999999999999996, 'max': 0.75, 'median': 0.625}, 'hope_fear_tension': {'mean': 0.6812499999999999, 'std': 0.1399936223037117, 'min': 0.45, 'max': 0.825, 'median': 0.7124999999999999}, 'pragmatism_fantasy_tension': {'mean': 0.6875, 'std': 0.08017837257372733, 'min': 0.575, 'max': 0.8, 'median': 0.6875}, 'civic_character_index': {'mean': 0.630625, 'std': 0.082177399569468, 'min': 0.51, 'max': 0.735, 'median': 0.6400000000000001}, 'virtue_index': {'mean': 0.5725, 'std': 0.11473572117821768, 'min': 0.38999999999999996, 'max': 0.6799999999999999, 'median': 0.61}, 'pathology_index': {'mean': 0.31125, 'std': 0.12123619214456664, 'min': 0.11000000000000001, 'max': 0.45999999999999996, 'median': 0.31}, 'salience_weighted_civic_character_index': {'mean': 0.6454346460250995, 'std': 0.08352857482948237, 'min': 0.5177419354838709, 'max': 0.7482142857142856, 'median': 0.6604464285714287}}, 'missing_metrics': []}",,,,,,,,Generic summary_statistics result,
