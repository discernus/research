{
  "status": "success",
  "functions_generated": 5,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 16819,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: nano_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-08-30T01:54:51.221397+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_stats(data, dimensions=[\"positive_sentiment\", \"negative_sentiment\"]):\n    \"\"\"\n    Calculates descriptive statistics (mean, median, std dev) for specified sentiment dimensions.\n\n    This function provides a summary of the central tendency and dispersion for each sentiment dimension\n    across all documents. It uses the 'raw_score' from the dimensional analysis results.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis results. Expected columns\n                             include 'positive_sentiment_raw' and 'negative_sentiment_raw'.\n        dimensions (list): A list of dimension names for which to calculate statistics.\n                           Defaults to ['positive_sentiment', 'negative_sentiment'].\n\n    Returns:\n        dict: A dictionary where keys are dimension names and values are dictionaries\n              containing 'mean', 'median', and 'std_dev' for the raw scores.\n              Returns None if the input data is empty or required columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if data.empty:\n        return None\n\n    results = {}\n    for dim in dimensions:\n        raw_score_col = f\"{dim}_raw\"\n        if raw_score_col not in data.columns:\n            print(f\"Warning: Column '{raw_score_col}' not found in data. Skipping dimension '{dim}'.\")\n            continue\n\n        scores = data[raw_score_col].dropna()\n\n        if scores.empty:\n            results[dim] = {\"mean\": None, \"median\": None, \"std_dev\": None}\n            continue\n\n        mean_score = scores.mean()\n        median_score = scores.median()\n        std_dev_score = scores.std()\n\n        results[dim] = {\n            \"mean\": float(mean_score),\n            \"median\": float(median_score),\n            \"std_dev\": float(std_dev_score)\n        }\n    return results\n\ndef compare_sentiment_between_documents(data):\n    \"\"\"\n    Compares sentiment scores between 'positive_test.txt' and 'negative_test.txt'.\n\n    This function performs independent samples t-tests to determine if there are\n    statistically significant differences in positive and negative sentiment scores\n    between the two specific test documents.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis results. Expected columns\n                             include 'document_name', 'positive_sentiment_raw', and\n                             'negative_sentiment_raw'.\n\n    Returns:\n        dict: A dictionary containing the results of t-tests for positive and negative\n              sentiment, including t-statistic and p-value. Returns None if data is\n              insufficient for comparison.\n    \"\"\"\n    import pandas as pd\n    from scipy.stats import ttest_ind\n\n    if data.empty:\n        return None\n\n    positive_doc_name = \"positive_test.txt\"\n    negative_doc_name = \"negative_test.txt\"\n\n    positive_data = data[data['document_name'] == positive_doc_name]\n    negative_data = data[data['document_name'] == negative_doc_name]\n\n    if positive_data.empty or negative_data.empty:\n        print(\"Warning: One or both test documents not found in data. Cannot perform comparison.\")\n        return None\n\n    results = {}\n\n    # Compare positive sentiment\n    pos_sentiment_pos_doc = positive_data['positive_sentiment_raw'].dropna()\n    pos_sentiment_neg_doc = negative_data['positive_sentiment_raw'].dropna()\n\n    if len(pos_sentiment_pos_doc) > 1 and len(pos_sentiment_neg_doc) > 1:\n        ttest_pos = ttest_ind(pos_sentiment_pos_doc, pos_sentiment_neg_doc, equal_var=False) # Welch's t-test\n        results['positive_sentiment_comparison'] = {\n            \"t_statistic\": float(ttest_pos.statistic),\n            \"p_value\": float(ttest_pos.pvalue)\n        }\n    else:\n        results['positive_sentiment_comparison'] = {\n            \"t_statistic\": None,\n            \"p_value\": None,\n            \"message\": \"Insufficient data for t-test on positive sentiment.\"\n        }\n\n    # Compare negative sentiment\n    neg_sentiment_pos_doc = positive_data['negative_sentiment_raw'].dropna()\n    neg_sentiment_neg_doc = negative_data['negative_sentiment_raw'].dropna()\n\n    if len(neg_sentiment_pos_doc) > 1 and len(neg_sentiment_neg_doc) > 1:\n        ttest_neg = ttest_ind(neg_sentiment_pos_doc, neg_sentiment_neg_doc, equal_var=False) # Welch's t-test\n        results['negative_sentiment_comparison'] = {\n            \"t_statistic\": float(ttest_neg.statistic),\n            \"p_value\": float(ttest_neg.pvalue)\n        }\n    else:\n        results['negative_sentiment_comparison'] = {\n            \"t_statistic\": None,\n            \"p_value\": None,\n            \"message\": \"Insufficient data for t-test on negative sentiment.\"\n        }\n\n    return results\n\ndef analyze_sentiment_salience(data, dimensions=[\"positive_sentiment\", \"negative_sentiment\"]):\n    \"\"\"\n    Analyzes the salience of sentiment dimensions.\n\n    Calculates descriptive statistics (mean, median) for the salience scores of\n    specified sentiment dimensions. Salience indicates how prominent or important\n    the sentiment is within the document.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis results. Expected columns\n                             include 'positive_sentiment_salience' and 'negative_sentiment_salience'.\n        dimensions (list): A list of dimension names for which to calculate statistics.\n                           Defaults to ['positive_sentiment', 'negative_sentiment'].\n\n    Returns:\n        dict: A dictionary where keys are dimension names and values are dictionaries\n              containing 'mean_salience' and 'median_salience'. Returns None if data is empty\n              or required columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if data.empty:\n        return None\n\n    results = {}\n    for dim in dimensions:\n        salience_col = f\"{dim}_salience\"\n        if salience_col not in data.columns:\n            print(f\"Warning: Column '{salience_col}' not found in data. Skipping dimension '{dim}'.\")\n            continue\n\n        salience_scores = data[salience_col].dropna()\n\n        if salience_scores.empty:\n            results[dim] = {\"mean_salience\": None, \"median_salience\": None}\n            continue\n\n        mean_salience = salience_scores.mean()\n        median_salience = salience_scores.median()\n\n        results[dim] = {\n            \"mean_salience\": float(mean_salience),\n            \"median_salience\": float(median_salience)\n        }\n    return results\n\ndef analyze_sentiment_confidence(data, dimensions=[\"positive_sentiment\", \"negative_sentiment\"]):\n    \"\"\"\n    Analyzes the confidence of sentiment predictions.\n\n    Calculates descriptive statistics (mean, median) for the confidence scores of\n    specified sentiment dimensions. Confidence reflects the model's certainty in its\n    sentiment assessment for each document.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis results. Expected columns\n                             include 'positive_sentiment_confidence' and 'negative_sentiment_confidence'.\n        dimensions (list): A list of dimension names for which to calculate statistics.\n                           Defaults to ['positive_sentiment', 'negative_sentiment'].\n\n    Returns:\n        dict: A dictionary where keys are dimension names and values are dictionaries\n              containing 'mean_confidence' and 'median_confidence'. Returns None if data is empty\n              or required columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if data.empty:\n        return None\n\n    results = {}\n    for dim in dimensions:\n        confidence_col = f\"{dim}_confidence\"\n        if confidence_col not in data.columns:\n            print(f\"Warning: Column '{confidence_col}' not found in data. Skipping dimension '{dim}'.\")\n            continue\n\n        confidence_scores = data[confidence_col].dropna()\n\n        if confidence_scores.empty:\n            results[dim] = {\"mean_confidence\": None, \"median_confidence\": None}\n            continue\n\n        mean_confidence = confidence_scores.mean()\n        median_confidence = confidence_scores.median()\n\n        results[dim] = {\n            \"mean_confidence\": float(mean_confidence),\n            \"median_confidence\": float(median_confidence)\n        }\n    return results\n\ndef check_sentiment_distinction(data, positive_threshold=0.7, negative_threshold=0.7):\n    \"\"\"\n    Checks for clear distinction between positive and negative sentiment scores.\n\n    This function verifies if the 'positive_test.txt' document has a positive sentiment\n    score above a certain threshold and the 'negative_test.txt' document has a negative\n    sentiment score above a certain threshold. This directly addresses the research\n    question about clear distinction.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis results. Expected columns\n                             include 'document_name', 'positive_sentiment_raw', and\n                             'negative_sentiment_raw'.\n        positive_threshold (float): The minimum raw score for positive sentiment in 'positive_test.txt'.\n        negative_threshold (float): The minimum raw score for negative sentiment in 'negative_test.txt'.\n\n    Returns:\n        dict: A dictionary indicating whether the distinction criteria were met for\n              positive and negative sentiment, along with the actual scores.\n              Returns None if data is insufficient or documents are not found.\n    \"\"\"\n    import pandas as pd\n\n    if data.empty:\n        return None\n\n    positive_doc_name = \"positive_test.txt\"\n    negative_doc_name = \"negative_test.txt\"\n\n    positive_doc_data = data[data['document_name'] == positive_doc_name]\n    negative_doc_data = data[data['document_name'] == negative_doc_name]\n\n    if positive_doc_data.empty or negative_doc_data.empty:\n        print(\"Warning: One or both test documents not found. Cannot check sentiment distinction.\")\n        return None\n\n    results = {}\n\n    # Check positive sentiment in positive_test.txt\n    pos_sentiment_score = positive_doc_data['positive_sentiment_raw'].iloc[0] if not positive_doc_data.empty else None\n    if pos_sentiment_score is not None:\n        results['positive_sentiment_distinction'] = {\n            \"document\": positive_doc_name,\n            \"score\": float(pos_sentiment_score),\n            \"meets_threshold\": bool(pos_sentiment_score >= positive_threshold),\n            \"threshold\": positive_threshold\n        }\n    else:\n        results['positive_sentiment_distinction'] = {\n            \"document\": positive_doc_name,\n            \"score\": None,\n            \"meets_threshold\": False,\n            \"threshold\": positive_threshold,\n            \"message\": \"Positive document not found or score missing.\"\n        }\n\n    # Check negative sentiment in negative_test.txt\n    neg_sentiment_score = negative_doc_data['negative_sentiment_raw'].iloc[0] if not negative_doc_data.empty else None\n    if neg_sentiment_score is not None:\n        results['negative_sentiment_distinction'] = {\n            \"document\": negative_doc_name,\n            \"score\": float(neg_sentiment_score),\n            \"meets_threshold\": bool(neg_sentiment_score >= negative_threshold),\n            \"threshold\": negative_threshold\n        }\n    else:\n        results['negative_sentiment_distinction'] = {\n            \"document\": negative_doc_name,\n            \"score\": None,\n            \"meets_threshold\": False,\n            \"threshold\": negative_threshold,\n            \"message\": \"Negative document not found or score missing.\"\n        }\n\n    return results\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}