{'generation_metadata': {'status': 'success', 'functions_generated': 3, 'output_file': 'automatedstatisticalanalysisagent_functions.py', 'module_size': 13268, 'function_code_content': '"""\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: nano_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-11T15:35:06.747547+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n"""\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings(\'ignore\', category=RuntimeWarning)\n\n\ndef summarize_descriptive_statistics(data, **kwargs):\n    """\n    Calculates and returns descriptive statistics for all numerical score columns.\n\n    This function provides a foundational overview of the dataset\'s characteristics,\n    including counts, means, standard deviations, and ranges for each sentiment\n    dimension\'s raw score, salience, and confidence. This serves as a basic\n    validation that the analysis agent has processed the data and produced\n    numerical outputs as expected (addresses RQ2).\n\n    Methodology:\n    - The function uses pandas\' .describe() method on the relevant numeric columns.\n    - It is a purely descriptive analysis.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             like \'positive_sentiment_raw\', \'negative_sentiment_raw\', etc.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each score column.\n              Returns None if the required columns are not found or data is empty.\n    """\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = [\n            \'positive_sentiment_raw\', \'positive_sentiment_salience\', \'positive_sentiment_confidence\',\n            \'negative_sentiment_raw\', \'negative_sentiment_salience\', \'negative_sentiment_confidence\'\n        ]\n\n        if not all(col in data.columns for col in required_cols) or data.empty:\n            return None\n\n        # Select only the required columns for description\n        stats_data = data[required_cols]\n\n        # Calculate descriptive statistics\n        descriptives = stats_data.describe().to_dict()\n\n        # Ensure JSON serializability by converting numpy types\n        for key, value in descriptives.items():\n            for stat, num in value.items():\n                if isinstance(num, (np.integer, np.int64)):\n                    descriptives[key][stat] = int(num)\n                elif isinstance(num, (np.floating, np.float64)):\n                    descriptives[key][stat] = float(num)\n\n        return {\n            "descriptive_statistics": descriptives,\n            "sample_size": int(len(data))\n        }\n\n    except Exception as e:\n        # In a production environment, one might log the error `e`\n        return None\n\ndef analyze_sentiment_distinction(data, **kwargs):\n    """\n    Exploratory analysis of sentiment scores between document types.\n\n    This function directly addresses the research question: "Does the pipeline\n    correctly identify positive vs negative sentiment?" by comparing the mean\n    scores of documents pre-categorized as \'positive\' or \'negative\'.\n\n    Methodology:\n    - Documents are categorized into \'positive\' and \'negative\' groups based on their filenames.\n    - The mean \'positive_sentiment_raw\' and \'negative_sentiment_raw\' scores are calculated for each group.\n    - Given the extremely small sample size (N=2, n=1 per group), this is a TIER 3 (Exploratory) analysis.\n    - No inferential statistics (e.g., t-tests, p-values) are computed as they would be statistically invalid and meaningless with n=1 per group (variance is undefined).\n    - The focus is on describing the observed pattern to validate the "clear distinction" expected outcome.\n\n    Sample Size & Power Assessment:\n    - Exploratory analysis - results are suggestive rather than conclusive (N=2).\n    - The purpose is not inference but basic functional validation.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing \'document_name\', \'positive_sentiment_raw\',\n                             and \'negative_sentiment_raw\' columns.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the mean scores for each group, or None if\n              data is insufficient or improperly formatted.\n    """\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = [\'document_name\', \'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        if not all(col in data.columns for col in required_cols) or data.empty:\n            return None\n\n        df = data.copy()\n\n        # Create grouping variable from document name\n        def get_doc_type(name):\n            if \'positive_test\' in name:\n                return \'positive\'\n            if \'negative_test\' in name:\n                return \'negative\'\n            return \'unknown\'\n\n        df[\'doc_type\'] = df[\'document_name\'].apply(get_doc_type)\n\n        # Filter out any documents that don\'t match the expected pattern\n        df = df[df[\'doc_type\'].isin([\'positive\', \'negative\'])]\n\n        if df.empty or len(df[\'doc_type\'].unique()) < 2:\n            return {"error": "Insufficient data: Both \'positive\' and \'negative\' document types are required."}\n\n        # Group by the new variable and calculate means\n        # With n=1, mean is just the value itself, but this is robust for larger test sets\n        grouped_stats = df.groupby(\'doc_type\')[[\'positive_sentiment_raw\', \'negative_sentiment_raw\']].mean()\n        group_counts = df[\'doc_type\'].value_counts().to_dict()\n\n        results = {\n            "group_means": grouped_stats.to_dict(\'index\'),\n            "group_counts": group_counts,\n            "notes": "Exploratory analysis - results are suggestive rather than conclusive due to extremely small sample size (N={}). Focus is on pattern detection.".format(len(df))\n        }\n        return results\n\n    except Exception as e:\n        return None\n\ndef analyze_dimension_correlation(data, **kwargs):\n    """\n    Calculates the correlation between positive and negative sentiment scores.\n\n    Methodology:\n    - This function computes the Pearson correlation coefficient between the\n      \'positive_sentiment_raw\' and \'negative_sentiment_raw\' dimensions.\n    - A strong negative correlation is typically expected in simple sentiment models,\n      indicating that as positive sentiment increases, negative sentiment decreases.\n\n    Sample Size & Power Assessment:\n    - TIER 3 (Exploratory Analysis). Correlation analysis requires a sufficient\n      number of data points to be stable and meaningful. As a conservative\n      practitioner, I advise against interpreting correlations with very small\n      sample sizes.\n    - Per protocol, this function will not compute a correlation for N < 5, as the\n      result would be highly unstable and misleading. For this experiment (N=2),\n      the correlation is mathematically trivial and provides no real insight.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing \'positive_sentiment_raw\' and\n                             \'negative_sentiment_raw\' columns.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary with the correlation matrix, or a note explaining why\n              the analysis was not performed. Returns None on error.\n    """\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = [\'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        if not all(col in data.columns for col in required_cols) or data.empty:\n            return None\n\n        n_samples = len(data)\n\n        # Per conservative statistical practice, do not run correlation on tiny samples.\n        if n_samples < 5:\n            return {\n                "status": "not_computed",\n                "sample_size": n_samples,\n                "message": "Correlation analysis was not performed. A sample size of at least 5 is recommended for even a highly exploratory correlation. The current sample size is too small to yield a meaningful result."\n            }\n\n        correlation_matrix = data[required_cols].corr(method=\'pearson\')\n\n        return {\n            "status": "computed",\n            "sample_size": n_samples,\n            "correlation_matrix": correlation_matrix.to_dict(),\n            "notes": "Exploratory analysis - results should be interpreted with caution due to small sample size (N={}).".format(n_samples)\n        }\n\n    except Exception as e:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    """\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    """\n    results = {\n        \'analysis_metadata\': {\n            \'timestamp\': pd.Timestamp.now().isoformat(),\n            \'sample_size\': len(data),\n            \'alpha_level\': alpha,\n            \'variables_analyzed\': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith((\'calculate_\', \'perform_\', \'test_\')) and \n            name != \'run_complete_statistical_analysis\'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if \'alpha\' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {\'error\': f\'Analysis failed: {str(e)}\'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    """\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    """\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    """\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    """\n    report_lines = []\n    report_lines.append("STATISTICAL ANALYSIS SUMMARY REPORT")\n    report_lines.append("=" * 50)\n    \n    metadata = analysis_results.get(\'analysis_metadata\', {})\n    report_lines.append(f"Analysis Timestamp: {metadata.get(\'timestamp\', \'Unknown\')}")\n    report_lines.append(f"Sample Size: {metadata.get(\'sample_size\', \'Unknown\')}")\n    report_lines.append(f"Alpha Level: {metadata.get(\'alpha_level\', \'Unknown\')}")\n    report_lines.append(f"Variables: {len(metadata.get(\'variables_analyzed\', []))}")\n    report_lines.append("")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != \'analysis_metadata\' and isinstance(result, dict):\n            if \'error\' not in result:\n                report_lines.append(f"{analysis_name.replace(\'_\', \' \').title()}:")\n                \n                # Extract key statistics based on analysis type\n                if \'p_value\' in result:\n                    p_val = result[\'p_value\']\n                    significance = "significant" if p_val < metadata.get(\'alpha_level\', 0.05) else "not significant"\n                    report_lines.append(f"  - p-value: {p_val:.4f} ({significance})")\n                \n                if \'effect_size\' in result:\n                    report_lines.append(f"  - Effect size: {result[\'effect_size\']:.4f}")\n                \n                if \'correlation_matrix\' in result:\n                    report_lines.append(f"  - Correlation matrix generated with {len(result[\'correlation_matrix\'])} variables")\n                \n                if \'cronbach_alpha\' in result:\n                    alpha_val = result[\'cronbach_alpha\']\n                    reliability = "excellent" if alpha_val > 0.9 else "good" if alpha_val > 0.8 else "acceptable" if alpha_val > 0.7 else "questionable"\n                    report_lines.append(f"  - Cronbach\'s α: {alpha_val:.3f} ({reliability})")\n                \n                report_lines.append("")\n            else:\n                report_lines.append(f"{analysis_name}: ERROR - {result[\'error\']}")\n                report_lines.append("")\n    \n    return "\\n".join(report_lines)\n', 'cached_with_code': True}, 'statistical_data': {'analyze_dimension_correlation': {'status': 'not_computed', 'sample_size': 2, 'message': 'Correlation analysis was not performed. A sample size of at least 5 is recommended for even a highly exploratory correlation. The current sample size is too small to yield a meaningful result.'}, 'analyze_sentiment_distinction': {'group_means': {'negative': {'positive_sentiment_raw': 0.0, 'negative_sentiment_raw': 1.0}, 'positive': {'positive_sentiment_raw': 1.0, 'negative_sentiment_raw': 0.0}}, 'group_counts': {'positive': 1, 'negative': 1}, 'notes': 'Exploratory analysis - results are suggestive rather than conclusive due to extremely small sample size (N=2). Focus is on pattern detection.'}, 'generate_statistical_summary_report': 'STATISTICAL ANALYSIS SUMMARY REPORT\n==================================================\nAnalysis Timestamp: Unknown\nSample Size: Unknown\nAlpha Level: Unknown\nVariables: 0\n', 'perform_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-09-11T11:37:28.985931', 'sample_size': 2, 'alpha_level': 0.05, 'variables_analyzed': ['positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence', 'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence']}}, 'run_complete_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-09-11T11:37:28.988313', 'sample_size': 2, 'alpha_level': 0.05, 'variables_analyzed': ['positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence', 'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence']}}, 'summarize_descriptive_statistics': {'descriptive_statistics': {'positive_sentiment_raw': {'count': 2.0, 'mean': 0.5, 'std': 0.7071067811865476, 'min': 0.0, '25%': 0.25, '50%': 0.5, '75%': 0.75, 'max': 1.0}, 'positive_sentiment_salience': {'count': 2.0, 'mean': 0.5, 'std': 0.7071067811865476, 'min': 0.0, '25%': 0.25, '50%': 0.5, '75%': 0.75, 'max': 1.0}, 'positive_sentiment_confidence': {'count': 2.0, 'mean': 1.0, 'std': 0.0, 'min': 1.0, '25%': 1.0, '50%': 1.0, '75%': 1.0, 'max': 1.0}, 'negative_sentiment_raw': {'count': 2.0, 'mean': 0.5, 'std': 0.7071067811865476, 'min': 0.0, '25%': 0.25, '50%': 0.5, '75%': 0.75, 'max': 1.0}, 'negative_sentiment_salience': {'count': 2.0, 'mean': 0.5, 'std': 0.7071067811865476, 'min': 0.0, '25%': 0.25, '50%': 0.5, '75%': 0.75, 'max': 1.0}, 'negative_sentiment_confidence': {'count': 2.0, 'mean': 1.0, 'std': 0.0, 'min': 1.0, '25%': 1.0, '50%': 1.0, '75%': 1.0, 'max': 1.0}}, 'sample_size': 2}}, 'status': 'success_with_data', 'validation_passed': True}