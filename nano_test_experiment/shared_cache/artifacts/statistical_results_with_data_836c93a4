{'generation_metadata': {'status': 'success', 'functions_generated': 3, 'output_file': 'automatedstatisticalanalysisagent_functions.py', 'module_size': 10295, 'function_code_content': 'import pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\ndef check_data_integrity(data: pd.DataFrame, **kwargs: Any) -> Optional[Dict[str, Any]]:\n    """\n    Performs basic data integrity checks on the input DataFrame.\n\n    This function is a prerequisite for other statistical analyses, ensuring the data\n    is in the expected format. It checks for the presence of required columns and\n    verifies that score values are within the valid range [0.0, 1.0].\n\n    As a practitioner focused on rigor, this step is crucial to prevent errors in\n    downstream analysis and ensure the validity of any findings.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data. It must include\n                             \'document_name\', \'positive_sentiment_raw\', and\n                             \'negative_sentiment_raw\' columns.\n        **kwargs: Additional keyword arguments (unused).\n\n    Returns:\n        dict: A dictionary containing the status of the integrity check, including\n              a list of checks passed and failed. Returns None if the input\n              DataFrame is empty or invalid.\n    """\n    if data is None or data.empty:\n        warnings.warn("Input data is empty or None. Cannot perform data integrity check.")\n        return None\n\n    try:\n        required_cols = [\n            \'document_name\',\n            \'positive_sentiment_raw\',\n            \'negative_sentiment_raw\'\n        ]\n        \n        results = {\n            \'status\': \'OK\',\n            \'checks_passed\': [],\n            \'checks_failed\': []\n        }\n        \n        # Check 1: All required columns are present\n        missing_cols = [col for col in required_cols if col not in data.columns]\n        if not missing_cols:\n            results[\'checks_passed\'].append(\'All required columns are present.\')\n        else:\n            results[\'checks_failed\'].append(f\'Missing required columns: {missing_cols}\')\n            results[\'status\'] = \'ERROR\'\n            return results\n\n        # Check 2: Scores are within the valid [0.0, 1.0] range\n        score_cols = [\'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        for col in score_cols:\n            if data[col].isnull().any():\n                results[\'checks_failed\'].append(f\'Column "{col}" contains null values.\')\n                results[\'status\'] = \'ERROR\'\n            elif not data[col].between(0.0, 1.0).all():\n                results[\'checks_failed\'].append(f\'Column "{col}" has values outside the [0.0, 1.0] range.\')\n                results[\'status\'] = \'ERROR\'\n            else:\n                results[\'checks_passed\'].append(f\'Column "{col}" values are within the valid [0.0, 1.0] range.\')\n\n        return results\n\n    except Exception as e:\n        warnings.warn(f"An unexpected error occurred during data integrity check: {e}")\n        return {\n            \'status\': \'ERROR\',\n            \'error_message\': str(e),\n            \'checks_passed\': [],\n            \'checks_failed\': [\'An unexpected exception occurred.\']\n        }\n\n\ndef summarize_sentiment_scores(data: pd.DataFrame, **kwargs: Any) -> Optional[Dict[str, Any]]:\n    """\n    Calculates descriptive statistics for sentiment scores, grouped by document type.\n\n    Methodology:\n    This function provides a Tier 3 (Exploratory) analysis. Given the extremely small\n    sample size (N=2 in the test case), inferential statistics like t-tests are\n    not applicable as within-group variance cannot be calculated. The analysis is\n    therefore restricted to descriptive statistics (count, mean, std, min, max) to\n    observe patterns and validate basic pipeline functionality as per the research questions.\n    A \'condition\' column is created by mapping document names to their intended sentiment\n    type (\'Positive Test\', \'Negative Test\').\n\n    Power Assessment:\n    Exploratory analysis - results are suggestive rather than conclusive due to the\n    critically low sample size (N < 15). This function serves to validate the expected\n    outcome of a "clear distinction" between test documents, not to make generalizable\n    inferences.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data. Must include\n                             \'document_name\', \'positive_sentiment_raw\', and\n                             \'negative_sentiment_raw\'.\n        **kwargs: Additional keyword arguments (unused).\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each sentiment\n              dimension, grouped by the test condition. Returns None if data is\n              insufficient or an error occurs.\n    """\n    if data is None or data.empty:\n        warnings.warn("Input data is empty or None. Cannot summarize scores.")\n        return None\n\n    required_cols = [\'document_name\', \'positive_sentiment_raw\', \'negative_sentiment_raw\']\n    if not all(col in data.columns for col in required_cols):\n        warnings.warn(f"Data is missing one or more required columns: {required_cols}")\n        return None\n\n    try:\n        df = data.copy()\n        \n        # Create grouping variable based on document name. This is the only available metadata.\n        # This directly addresses the experimental design of comparing the two test files.\n        conditions = {\n            \'positive_test.txt\': \'Positive Test\',\n            \'negative_test.txt\': \'Negative Test\'\n        }\n        df[\'condition\'] = df[\'document_name\'].map(conditions).fillna(\'Unknown\')\n\n        if len(df[\'condition\'].unique()) < 2 and \'Unknown\' in df[\'condition\'].unique():\n             warnings.warn("Could not map document names to known test conditions.")\n             return None\n        \n        n_total = len(df)\n        \n        # Group by the created condition and calculate descriptive statistics\n        descriptive_stats = df.groupby(\'condition\')[[\'positive_sentiment_raw\', \'negative_sentiment_raw\']].describe()\n\n        results = {\n            \'analysis_tier\': \'Tier 3 (Exploratory)\',\n            \'sample_size\': n_total,\n            \'power_caveat\': f"Exploratory analysis - results are suggestive rather than conclusive (N={n_total}). Inferential tests are not appropriate.",\n            \'descriptive_statistics\': descriptive_stats.to_dict(\'index\')\n        }\n        \n        return results\n\n    except Exception as e:\n        warnings.warn(f"An error occurred during descriptive statistics generation: {e}")\n        return None\n\n\ndef analyze_dimensional_correlation(data: pd.DataFrame, **kwargs: Any) -> Optional[Dict[str, Any]]:\n    """\n    Calculates the Pearson correlation between positive and negative sentiment scores.\n\n    Methodology:\n    This function performs a Tier 3 (Exploratory) correlation analysis. It computes\n    the Pearson correlation coefficient (r) to assess the linear relationship between\n    \'positive_sentiment_raw\' and \'negative_sentiment_raw\'.\n\n    Power Assessment:\n    Exploratory analysis - results are suggestive rather than conclusive. With a sample\n    size N < 15, and especially with N=2 as in the test case, any correlation\n    coefficient is highly unstable and should not be used for inference. For N=2, the\n    correlation will always be either +1, -1, or undefined, providing no meaningful\n    statistical insight. This function is included for pipeline validation and to\n    demonstrate analytical capability, but its output must be interpreted with extreme\n    caution.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data. Must include\n                             \'positive_sentiment_raw\' and \'negative_sentiment_raw\'.\n        **kwargs: Additional keyword arguments (unused).\n\n    Returns:\n        dict: A dictionary containing the Pearson correlation coefficient, p-value,\n              sample size, and a strong cautionary note about interpretation.\n              Returns None if data is insufficient for correlation.\n    """\n    if data is None or data.empty:\n        warnings.warn("Input data is empty or None. Cannot perform correlation analysis.")\n        return None\n\n    required_cols = [\'positive_sentiment_raw\', \'negative_sentiment_raw\']\n    if not all(col in data.columns for col in required_cols):\n        warnings.warn(f"Data is missing one or more required columns: {required_cols}")\n        return None\n\n    df = data.dropna(subset=required_cols)\n    n = len(df)\n\n    if n < 3:\n        tier = \'Tier 3 (Exploratory)\'\n        caveat = (f"Correlation is not meaningful with N={n}. For N=2, the result will always be "\n                  f"perfectly correlated (+1 or -1) or undefined. This result is for "\n                  f"descriptive purposes only and has no statistical validity.")\n        warnings.warn(caveat)\n        # Still compute for completeness, but with the warning\n        if n < 2:\n            return None\n    elif n < 15:\n        tier = \'Tier 3 (Exploratory)\'\n        caveat = f"Exploratory analysis - results are suggestive rather than conclusive (N={n})."\n    elif n < 30:\n        tier = \'Tier 2 (Moderately-Powered)\'\n        caveat = f"Results should be interpreted with caution due to moderate sample size (N={n})."\n    else:\n        tier = \'Tier 1 (Well-Powered)\'\n        caveat = f"Analysis is well-powered (N={n})."\n\n    try:\n        # Suppress warnings from scipy for small N, as we handle it explicitly\n        with warnings.catch_warnings():\n            warnings.simplefilter("ignore", stats.ConstantInputWarning)\n            correlation, p_value = stats.pearsonr(df[\'positive_sentiment_raw\'], df[\'negative_sentiment_raw\'])\n\n        if np.isnan(correlation):\n            return {\n                \'analysis_tier\': tier,\n                \'sample_size\': n,\n                \'power_caveat\': caveat,\n                \'error\': \'Correlation is undefined, likely due to constant input values.\'\n            }\n\n        return {\n            \'analysis_tier\': tier,\n            \'sample_size\': n,\n            \'power_caveat\': caveat,\n            \'pearson_correlation\': {\n                \'r_value\': correlation,\n                \'p_value\': p_value\n            },\n            \'interpretation_note\': \'The p-value is not reliable for small sample sizes (N<15).\'\n        }\n\n    except Exception as e:\n        warnings.warn(f"An error occurred during correlation analysis: {e}")\n        return None', 'cached_with_code': True}, 'statistical_data': {'analyze_dimensional_correlation': {'analysis_tier': 'Tier 3 (Exploratory)', 'sample_size': 2, 'power_caveat': 'Correlation is not meaningful with N=2. For N=2, the result will always be perfectly correlated (+1 or -1) or undefined. This result is for descriptive purposes only and has no statistical validity.', 'pearson_correlation': {'r_value': -1.0, 'p_value': 1.0}, 'interpretation_note': 'The p-value is not reliable for small sample sizes (N<15).'}, 'check_data_integrity': {'status': 'OK', 'checks_passed': ['All required columns are present.', 'Column "positive_sentiment_raw" values are within the valid [0.0, 1.0] range.', 'Column "negative_sentiment_raw" values are within the valid [0.0, 1.0] range.'], 'checks_failed': []}, 'summarize_sentiment_scores': {'analysis_tier': 'Tier 3 (Exploratory)', 'sample_size': 2, 'power_caveat': 'Exploratory analysis - results are suggestive rather than conclusive (N=2). Inferential tests are not appropriate.', 'descriptive_statistics': {'Negative Test': {('positive_sentiment_raw', 'count'): 1.0, ('positive_sentiment_raw', 'mean'): 0.0, ('positive_sentiment_raw', 'std'): nan, ('positive_sentiment_raw', 'min'): 0.0, ('positive_sentiment_raw', '25%'): 0.0, ('positive_sentiment_raw', '50%'): 0.0, ('positive_sentiment_raw', '75%'): 0.0, ('positive_sentiment_raw', 'max'): 0.0, ('negative_sentiment_raw', 'count'): 1.0, ('negative_sentiment_raw', 'mean'): 1.0, ('negative_sentiment_raw', 'std'): nan, ('negative_sentiment_raw', 'min'): 1.0, ('negative_sentiment_raw', '25%'): 1.0, ('negative_sentiment_raw', '50%'): 1.0, ('negative_sentiment_raw', '75%'): 1.0, ('negative_sentiment_raw', 'max'): 1.0}, 'Positive Test': {('positive_sentiment_raw', 'count'): 1.0, ('positive_sentiment_raw', 'mean'): 1.0, ('positive_sentiment_raw', 'std'): nan, ('positive_sentiment_raw', 'min'): 1.0, ('positive_sentiment_raw', '25%'): 1.0, ('positive_sentiment_raw', '50%'): 1.0, ('positive_sentiment_raw', '75%'): 1.0, ('positive_sentiment_raw', 'max'): 1.0, ('negative_sentiment_raw', 'count'): 1.0, ('negative_sentiment_raw', 'mean'): 0.0, ('negative_sentiment_raw', 'std'): nan, ('negative_sentiment_raw', 'min'): 0.0, ('negative_sentiment_raw', '25%'): 0.0, ('negative_sentiment_raw', '50%'): 0.0, ('negative_sentiment_raw', '75%'): 0.0, ('negative_sentiment_raw', 'max'): 0.0}}}}, 'status': 'success_with_data', 'validation_passed': True}