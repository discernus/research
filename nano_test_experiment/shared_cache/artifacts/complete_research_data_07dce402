{
  "raw_analysis_results": [
    {
      "analysis_id": "analysis_2101dc6e8361",
      "result_hash": "2b8a7938c0a37f5f3aee48ccbc790b30fe366d40f7974b5a95012e2a144665ec",
      "result_content": {
        "analysis_id": "analysis_2101dc6e8361",
        "agent_name": "EnhancedAnalysisAgent",
        "agent_version": "enhanced_v2.1_raw_output",
        "experiment_name": "nano_test_experiment",
        "model_used": "vertex_ai/gemini-2.5-flash",
        "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 1.0,\n    \"analysis_notes\": \"Clear, designed test case for sentiment analysis, resulting in highly consistent scores across all three independent approaches.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"6950ccbc6b14...\",\n      \"document_name\": \"positive_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"This is a wonderful day! Everything is going perfectly. I feel great about the future. Success is everywhere. The team did an excellent job. We're achieving amazing results. Optimism fills the air. What a fantastic opportunity! I'm thrilled with the progress. Everything looks bright and promising.\",\n          \"confidence\": 1.0,\n          \"context_type\": \"explicit statement\"\n        },\n        {\n          \"dimension\": \"negative_sentiment\",\n          \"quote_text\": \"\",\n          \"confidence\": 1.0,\n          \"context_type\": \"absence of evidence\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
        "evidence_hash": "fa7467a32eb4b72b67b5442879b58f9caed8e698efa61d7604901fd17b4c5caf",
        "execution_metadata": {
          "start_time": "2025-08-30T02:20:41.697303+00:00",
          "end_time": "2025-08-30T02:20:54.472029+00:00",
          "duration_seconds": 12.774701
        },
        "input_artifacts": {
          "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
          "document_hashes": [
            "6950ccbc6b14eabf43508448cf888b2c590f44e549a32b8ae60e943710e74019"
          ],
          "num_documents": 1
        },
        "provenance": {
          "security_boundary": {
            "experiment_name": "nano_test_experiment",
            "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
            "boundary_type": "filesystem",
            "security_level": "experiment_scoped"
          },
          "audit_session_id": "20250830T022041Z_f3dc06d7"
        }
      },
      "cached": true
    },
    {
      "analysis_id": "analysis_4bc6d8eb51af",
      "result_hash": "0d1ba0ea1c1d7635082df4b21305a556fd5830852f6c90c2052bd267566441fa",
      "result_content": {
        "analysis_id": "analysis_4bc6d8eb51af",
        "agent_name": "EnhancedAnalysisAgent",
        "agent_version": "enhanced_v2.1_raw_output",
        "experiment_name": "nano_test_experiment",
        "model_used": "vertex_ai/gemini-2.5-flash",
        "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 1.0,\n    \"analysis_notes\": \"Aggregated from three independent analytical passes focusing on explicit evidence, rhetorical context, and linguistic patterns.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"1df25093a747...\",\n      \"document_name\": \"negative_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"\",\n          \"confidence\": 1.0,\n          \"context_type\": \"absence of evidence\"\n        },\n        {\n          \"dimension\": \"negative_sentiment\",\n          \"quote_text\": \"This is a terrible situation.\",\n          \"confidence\": 1.0,\n          \"context_type\": \"explicit statement\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
        "evidence_hash": "0fad04b8d9149f05001dd4794821c67f1efa2dc4d532a22ce1f19fdac21e27e5",
        "execution_metadata": {
          "start_time": "2025-08-30T02:20:54.475197+00:00",
          "end_time": "2025-08-30T02:21:04.553896+00:00",
          "duration_seconds": 10.078681
        },
        "input_artifacts": {
          "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
          "document_hashes": [
            "1df25093a747678c5dbe724ecfb963b88049f9a7a536a3f23f064fd592744f91"
          ],
          "num_documents": 1
        },
        "provenance": {
          "security_boundary": {
            "experiment_name": "nano_test_experiment",
            "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
            "boundary_type": "filesystem",
            "security_level": "experiment_scoped"
          },
          "audit_session_id": "20250830T022041Z_f3dc06d7"
        }
      },
      "cached": true
    }
  ],
  "derived_metrics_results": {
    "status": "completed",
    "derived_metrics_hash": "3067cafaeb1ba07718a1d744fba27dc8dad5fedbe04385dd87ceaff7540c1397",
    "functions_generated": 6,
    "derived_metrics_results": {
      "generation_metadata": {
        "status": "success",
        "functions_generated": 6,
        "output_file": "automatedderivedmetricsagent_functions.py",
        "module_size": 11991,
        "function_code_content": "\"\"\"\nAutomated Derived Metrics Functions\n===================================\n\nGenerated by AutomatedDerivedMetricsAgent for experiment: Test Experiment\nDescription: Test experiment for derived metrics\nGenerated: 2025-08-30T02:24:26.663243+00:00\n\nThis module contains automatically generated calculation functions for derived metrics\nas specified in the framework's natural language descriptions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom typing import Optional, Dict, Any\n\n\ndef calculate_identity_tension(data, **kwargs):\n    \"\"\"\n    Calculate identity_tension: Conflict between tribal dominance and individual dignity dimensions.\n\n    This calculation is not possible with the provided 'Sentiment Binary Framework v1.0'\n    as it lacks the necessary dimensions ('tribal_dominance', 'individual_dignity').\n    The function will gracefully return None.\n\n    Hypothetical Formula: abs(tribal_dominance - individual_dignity)\n\n    Args:\n        data (pd.Series or pd.DataFrame): A single row of analysis data.\n        **kwargs: Additional parameters (unused).\n\n    Returns:\n        None: This calculation cannot be performed with the available data.\n    \"\"\"\n    # This function is designed to be a placeholder as the required dimensions\n    # 'tribal_dominance' and 'individual_dignity' are not present in the\n    # provided data structure. The framework context describes 'Positive Sentiment'\n    # and 'Negative Sentiment' dimensions, which are also not mapped to the\n    # actual data columns provided (e.g., 'analysis_result', 'raw_analysis_response').\n    # Therefore, the calculation is impossible, and we return None as per\n    # the requirement to handle missing data gracefully.\n    return None\n\ndef calculate_emotional_balance(data, **kwargs):\n    \"\"\"\n    Calculate emotional_balance: Difference between hope and fear scores\n\n    Formula: hope - fear\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        **kwargs: Additional parameters\n        \n    Returns:\n        float: Calculated result or None if insufficient data\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # The calculation is defined as the difference between 'hope' and 'fear' scores.\n        # This requires 'hope' and 'fear' to be columns in the input data.\n        hope_score = data['hope']\n        fear_score = data['fear']\n\n        # Ensure that the scores are not null (NaN, None, etc.) before proceeding.\n        if pd.isna(hope_score) or pd.isna(fear_score):\n            return None\n\n        # Perform the calculation and ensure the result is a standard float.\n        result = float(hope_score) - float(fear_score)\n        \n        return result\n        \n    except Exception:\n        # If the required 'hope' or 'fear' columns are missing (KeyError),\n        # or if the data within them is not numeric (TypeError), an exception\n        # will be caught. In this case, the calculation cannot be completed,\n        # so we return None to indicate missing or invalid data.\n        return None\n\ndef calculate_success_climate(data, **kwargs):\n    \"\"\"\n    Calculate success_climate: Difference between compersion and envy scores\n\n    Formula: compersion - envy\n    \n    Args:\n        data: pandas DataFrame or Series with dimension scores.\n        **kwargs: Additional parameters (unused).\n        \n    Returns:\n        float: Calculated result or None if insufficient data.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # The calculation requires 'compersion' and 'envy' scores.\n        # This will fail with a KeyError if columns are not present,\n        # which is handled by the except block.\n        compersion_score = data['compersion']\n        envy_score = data['envy']\n        \n        # Handle cases where columns exist but data is missing (NaN/None)\n        if pd.isna(compersion_score) or pd.isna(envy_score):\n            return None\n            \n        # Ensure values are numeric and calculate the result\n        result = float(compersion_score) - float(envy_score)\n        \n        # A final check for non-finite results like infinity\n        if not np.isfinite(result):\n            return None\n            \n        return result\n        \n    except Exception:\n        # Catches KeyError if columns are missing, TypeError if data is not\n        # subscriptable, ValueError if scores aren't numeric, or any other error.\n        return None\n\ndef calculate_relational_climate(data, **kwargs):\n    \"\"\"\n    Calculate relational_climate: Difference between amity and enmity scores\n\n    Formula: amity - enmity\n    \n    Args:\n        data: pandas DataFrame with dimension scores (single row treated as Series)\n        **kwargs: Additional parameters\n        \n    Returns:\n        float: Calculated result or None if insufficient data\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # The calculation is \"Difference between amity and enmity scores\".\n        # We expect 'amity' and 'enmity' to be columns in the input data.\n        amity_score = data['amity']\n        enmity_score = data['enmity']\n\n        # Handle cases where scores are missing (NaN, None, etc.)\n        if pd.isna(amity_score) or pd.isna(enmity_score):\n            return None\n\n        # Ensure scores are numeric and perform the calculation.\n        # The float conversion will raise an error for non-numeric strings,\n        # which is caught by the except block.\n        result = float(amity_score) - float(enmity_score)\n        \n        # Return the result, ensuring it's not NaN from an edge case calculation\n        return result if pd.notna(result) else None\n\n    except Exception:\n        # Catches KeyError if columns are missing, TypeError/ValueError if\n        # scores are not numeric, and any other unexpected errors.\n        return None\n\ndef calculate_goal_orientation(data, **kwargs):\n    \"\"\"\n    Calculate goal_orientation: Difference between cohesive goals and fragmentative goals\n\n    Formula: cohesive_goals - fragmentative_goals\n    \n    Args:\n        data: pandas DataFrame with dimension scores (passed as a Series for a single row)\n        **kwargs: Additional parameters\n        \n    Returns:\n        float: Calculated result or None if insufficient data\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # Retrieve the specific scores needed for the calculation from the data Series.\n        cohesive_goals = data['cohesive_goals']\n        fragmentative_goals = data['fragmentative_goals']\n        \n        # Gracefully handle missing data by checking for NaN or None values.\n        if pd.isna(cohesive_goals) or pd.isna(fragmentative_goals):\n            return None\n            \n        # Perform the calculation and ensure the result is a standard Python float.\n        result = float(cohesive_goals) - float(fragmentative_goals)\n        \n        return result\n        \n    except Exception:\n        # A broad exception handler is used as per the requirements.\n        # This will catch errors such as missing columns (KeyError) or\n        # non-numeric data that cannot be converted (TypeError, ValueError),\n        # ensuring the function returns None if the calculation cannot be completed.\n        return None\n\ndef calculate_overall_cohesion_index(data, **kwargs):\n    \"\"\"\n    Calculate overall_cohesion_index: Comprehensive measure combining all dimensions.\n\n    This index measures the balance between the positive and negative sentiment\n    dimensions. A score of 1.0 indicates perfect balance (e.g., positive and\n    negative sentiments are equal), while a score of 0.0 indicates that one\n    sentiment completely dominates the other.\n\n    Formula:\n    1 - |positive_sentiment - negative_sentiment|\n\n    Args:\n        data (pd.Series or pd.DataFrame): A single row of analysis data.\n        **kwargs: Additional parameters (unused).\n\n    Returns:\n        float: The calculated overall cohesion index (0.0 to 1.0), or None\n               if the necessary dimension scores are missing or invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    # The theoretical framework defines these dimensions.\n    # The function is written to operate on these dimensions.\n    # If the input data does not contain these columns, it will gracefully fail.\n    positive_col = 'positive_sentiment'\n    negative_col = 'negative_sentiment'\n\n    try:\n        # Ensure data is a pandas Series for consistent access\n        if isinstance(data, pd.DataFrame):\n            if len(data) != 1:\n                # This function is designed to operate on a single row.\n                return None\n            data = data.iloc[0]\n\n        # Check for required columns\n        if positive_col not in data.index or negative_col not in data.index:\n            return None\n\n        # Extract dimension scores\n        positive_score = data[positive_col]\n        negative_score = data[negative_col]\n\n        # Handle missing data for the specific scores\n        if pd.isna(positive_score) or pd.isna(negative_score):\n            return None\n\n        # Validate that scores are numeric floats or integers\n        if not all(isinstance(score, (int, float, np.number)) for score in [positive_score, negative_score]):\n            return None\n            \n        # Ensure scores are within the expected 0.0-1.0 range\n        if not (0.0 <= positive_score <= 1.0 and 0.0 <= negative_score <= 1.0):\n            # Out-of-range inputs are considered invalid for this calculation\n            return None\n\n        # Calculate the cohesion index\n        cohesion_index = 1.0 - abs(positive_score - negative_score)\n\n        return float(cohesion_index)\n\n    except (KeyError, TypeError, AttributeError, IndexError):\n        # KeyError: if columns are missing.\n        # TypeError: if data is not subscriptable or values are wrong type.\n        # AttributeError: if `data` is not a DataFrame/Series.\n        # IndexError: if DataFrame is empty.\n        return None\n    except Exception:\n        # Catch any other unexpected errors for production robustness\n        return None\n\ndef calculate_all_derived_metrics(data: pd.DataFrame) -> Dict[str, Optional[float]]:\n    \"\"\"\n    Calculate all derived metrics for the given dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        \n    Returns:\n        Dictionary mapping metric names to calculated values\n    \"\"\"\n    results = {}\n    \n    # Get all calculation functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith('calculate_') and \n            name not in ['calculate_all_derived_metrics', 'calculate_derived_metrics']):\n            try:\n                results[name.replace('calculate_', '')] = obj(data)\n            except Exception as e:\n                results[name.replace('calculate_', '')] = None\n                \n    return results\n\n\ndef calculate_derived_metrics(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Template-compatible wrapper function for derived metrics calculation.\n    \n    This function is called by the universal notebook template and returns\n    the original data with additional derived metric columns.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        \n    Returns:\n        DataFrame with original data plus derived metric columns\n    \"\"\"\n    # Calculate all derived metrics\n    derived_metrics = calculate_all_derived_metrics(data)\n    \n    # Create a copy of the original data\n    result = data.copy()\n    \n    # Add derived metrics as new columns\n    for metric_name, metric_value in derived_metrics.items():\n        if metric_value is not None:\n            # For scalar metrics, broadcast to all rows\n            result[metric_name] = metric_value\n        else:\n            # For failed calculations, use NaN\n            result[metric_name] = np.nan\n    \n    return result\n",
        "cached_with_code": true
      },
      "derived_metrics_data": {
        "status": "success",
        "original_count": 2,
        "derived_count": 2,
        "derived_metrics": [
          {
            "analysis_id": "analysis_2101dc6e8361",
            "result_hash": "2b8a7938c0a37f5f3aee48ccbc790b30fe366d40f7974b5a95012e2a144665ec",
            "result_content": {
              "analysis_id": "analysis_2101dc6e8361",
              "agent_name": "EnhancedAnalysisAgent",
              "agent_version": "enhanced_v2.1_raw_output",
              "experiment_name": "nano_test_experiment",
              "model_used": "vertex_ai/gemini-2.5-flash",
              "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 1.0,\n    \"analysis_notes\": \"Clear, designed test case for sentiment analysis, resulting in highly consistent scores across all three independent approaches.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"6950ccbc6b14...\",\n      \"document_name\": \"positive_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"This is a wonderful day! Everything is going perfectly. I feel great about the future. Success is everywhere. The team did an excellent job. We're achieving amazing results. Optimism fills the air. What a fantastic opportunity! I'm thrilled with the progress. Everything looks bright and promising.\",\n          \"confidence\": 1.0,\n          \"context_type\": \"explicit statement\"\n        },\n        {\n          \"dimension\": \"negative_sentiment\",\n          \"quote_text\": \"\",\n          \"confidence\": 1.0,\n          \"context_type\": \"absence of evidence\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
              "evidence_hash": "fa7467a32eb4b72b67b5442879b58f9caed8e698efa61d7604901fd17b4c5caf",
              "execution_metadata": {
                "start_time": "2025-08-30T02:20:41.697303+00:00",
                "end_time": "2025-08-30T02:20:54.472029+00:00",
                "duration_seconds": 12.774701
              },
              "input_artifacts": {
                "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
                "document_hashes": [
                  "6950ccbc6b14eabf43508448cf888b2c590f44e549a32b8ae60e943710e74019"
                ],
                "num_documents": 1
              },
              "provenance": {
                "security_boundary": {
                  "experiment_name": "nano_test_experiment",
                  "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
                  "boundary_type": "filesystem",
                  "security_level": "experiment_scoped"
                },
                "audit_session_id": "20250830T022041Z_f3dc06d7"
              }
            },
            "cached": true
          },
          {
            "analysis_id": "analysis_4bc6d8eb51af",
            "result_hash": "0d1ba0ea1c1d7635082df4b21305a556fd5830852f6c90c2052bd267566441fa",
            "result_content": {
              "analysis_id": "analysis_4bc6d8eb51af",
              "agent_name": "EnhancedAnalysisAgent",
              "agent_version": "enhanced_v2.1_raw_output",
              "experiment_name": "nano_test_experiment",
              "model_used": "vertex_ai/gemini-2.5-flash",
              "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 1.0,\n    \"analysis_notes\": \"Aggregated from three independent analytical passes focusing on explicit evidence, rhetorical context, and linguistic patterns.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"1df25093a747...\",\n      \"document_name\": \"negative_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"\",\n          \"confidence\": 1.0,\n          \"context_type\": \"absence of evidence\"\n        },\n        {\n          \"dimension\": \"negative_sentiment\",\n          \"quote_text\": \"This is a terrible situation.\",\n          \"confidence\": 1.0,\n          \"context_type\": \"explicit statement\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
              "evidence_hash": "0fad04b8d9149f05001dd4794821c67f1efa2dc4d532a22ce1f19fdac21e27e5",
              "execution_metadata": {
                "start_time": "2025-08-30T02:20:54.475197+00:00",
                "end_time": "2025-08-30T02:21:04.553896+00:00",
                "duration_seconds": 10.078681
              },
              "input_artifacts": {
                "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
                "document_hashes": [
                  "1df25093a747678c5dbe724ecfb963b88049f9a7a536a3f23f064fd592744f91"
                ],
                "num_documents": 1
              },
              "provenance": {
                "security_boundary": {
                  "experiment_name": "nano_test_experiment",
                  "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
                  "boundary_type": "filesystem",
                  "security_level": "experiment_scoped"
                },
                "audit_session_id": "20250830T022041Z_f3dc06d7"
              }
            },
            "cached": true
          }
        ],
        "columns_added": []
      },
      "status": "success_with_data",
      "validation_passed": true
    }
  },
  "statistical_results": {
    "generation_metadata": {
      "status": "success",
      "functions_generated": 3,
      "output_file": "automatedstatisticalanalysisagent_functions.py",
      "module_size": 13922,
      "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: nano_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-08-30T02:25:01.186158+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for sentiment scores, grouped by document type.\n\n    This function addresses the research question about pipeline functionality by summarizing\n    the dimensional scores. It creates a 'document_type' group based on filenames\n    (e.g., 'positive_test.txt' -> 'positive', 'negative_test.txt' -> 'negative')\n    and calculates the count, mean, standard deviation, min, and max for each\n    sentiment dimension within each group. This provides a foundational view of the\n    data distribution and central tendency.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             like 'document_name', 'positive_sentiment_raw', etc.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each sentiment\n              dimension, grouped by document type. Returns None if required\n              columns are missing or data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = [\n            'document_name', 'positive_sentiment_raw', 'negative_sentiment_raw',\n            'positive_sentiment_salience', 'negative_sentiment_salience',\n            'positive_sentiment_confidence', 'negative_sentiment_confidence'\n        ]\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        if data.empty:\n            return None\n\n        # Create grouping variable based on document name (thin architecture)\n        def get_doc_type(name):\n            if 'positive' in name:\n                return 'positive'\n            if 'negative' in name:\n                return 'negative'\n            return 'unknown'\n        \n        data['document_type'] = data['document_name'].apply(get_doc_type)\n\n        if 'unknown' in data['document_type'].unique():\n            # Handle cases where grouping is not possible\n            pass\n\n        score_cols = [col for col in required_cols if col != 'document_name']\n        \n        # Use .agg() for specific statistics to avoid scientific notation from .describe()\n        stats = data.groupby('document_type')[score_cols].agg(['count', 'mean', 'std', 'min', 'max'])\n        \n        # Convert the multi-index dataframe to a nested dictionary for JSON compatibility\n        results = stats.to_dict(orient='index')\n        \n        # Reformat the nested dictionary for cleaner output\n        output = {}\n        for group, group_stats in results.items():\n            output[group] = {}\n            for stat_col, stat_values in group_stats.items():\n                # stat_col is a tuple like ('positive_sentiment_raw', 'mean')\n                col_name, stat_name = stat_col\n                if col_name not in output[group]:\n                    output[group][col_name] = {}\n                # Round to 4 decimal places for readability\n                output[group][col_name][stat_name] = round(stat_values, 4) if pd.notna(stat_values) else None\n\n        return output if output else None\n\n    except Exception:\n        return None\n\ndef compare_sentiment_groups(data, **kwargs):\n    \"\"\"\n    Compares sentiment scores between positive and negative document groups.\n\n    This function directly addresses the \"clear distinction\" expected outcome by\n    comparing the means of sentiment scores between document groups. It uses an\n    independent samples t-test to determine if the differences in mean scores\n    (e.g., positive_sentiment_raw for the 'positive' group vs. the 'negative' group)\n    are statistically significant. The function handles small sample sizes by\n    reporting means even if a t-test is not viable.\n\n    Methodology:\n    1.  Groups documents into 'positive' and 'negative' based on filenames.\n    2.  For each dimension ('positive_sentiment_raw', 'negative_sentiment_raw'),\n        it performs an independent t-test between the two groups.\n    3.  Reports the mean scores for each group, the t-statistic, and the p-value.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary with comparison results, including means and t-test\n              statistics for each dimension. Returns None on error or if\n              insufficient data for comparison.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy import stats\n\n    try:\n        required_cols = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        if data.empty:\n            return None\n\n        # Create grouping variable based on document name\n        def get_doc_type(name):\n            if 'positive' in name:\n                return 'positive'\n            if 'negative' in name:\n                return 'negative'\n            return 'unknown'\n        \n        data['document_type'] = data['document_name'].apply(get_doc_type)\n\n        positive_group = data[data['document_type'] == 'positive']\n        negative_group = data[data['document_type'] == 'negative']\n\n        if positive_group.empty or negative_group.empty:\n            return {\"error\": \"Insufficient data: one or both document groups are empty.\"}\n\n        results = {}\n        dimensions_to_test = ['positive_sentiment_raw', 'negative_sentiment_raw']\n\n        for dim in dimensions_to_test:\n            pos_scores = positive_group[dim].dropna()\n            neg_scores = negative_group[dim].dropna()\n\n            # Check for sufficient data for a t-test (at least 2 samples per group)\n            can_run_ttest = len(pos_scores) > 1 and len(neg_scores) > 1\n\n            test_result = {\n                'positive_group_mean': pos_scores.mean(),\n                'negative_group_mean': neg_scores.mean(),\n                't_statistic': None,\n                'p_value': None,\n                'notes': None\n            }\n\n            if can_run_ttest:\n                # equal_var=False for Welch's t-test, which doesn't assume equal variance\n                t_stat, p_val = stats.ttest_ind(pos_scores, neg_scores, equal_var=False, nan_policy='omit')\n                test_result['t_statistic'] = t_stat\n                test_result['p_value'] = p_val\n            else:\n                test_result['notes'] = \"T-test not performed due to insufficient sample size in one or both groups (n < 2).\"\n            \n            results[dim] = test_result\n\n        return results if results else None\n\n    except Exception:\n        return None\n\ndef correlation_analysis(data, **kwargs):\n    \"\"\"\n    Calculates the correlation between positive and negative sentiment scores.\n\n    This function provides insight into the relationship between the two primary\n    dimensions of the framework. A strong negative correlation is expected,\n    indicating that as positive sentiment increases, negative sentiment decreases.\n    This helps validate the conceptual independence and opposition of the\n    framework's dimensions.\n\n    Methodology:\n    - Uses the Pearson correlation coefficient to measure the linear relationship\n      between 'positive_sentiment_raw' and 'negative_sentiment_raw'.\n    - Reports the correlation coefficient (r) and the p-value to assess the\n      statistical significance of the correlation.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing the Pearson correlation coefficient and\n              the p-value. Returns None if data is insufficient (fewer than 2 rows)\n              or if required columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import pearsonr\n\n    try:\n        required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        # Drop rows with missing values in the columns of interest\n        clean_data = data[required_cols].dropna()\n\n        if len(clean_data) < 2:\n            return {\"error\": \"Insufficient data for correlation analysis (requires at least 2 data points).\"}\n\n        # Calculate Pearson correlation\n        correlation, p_value = pearsonr(clean_data['positive_sentiment_raw'], clean_data['negative_sentiment_raw'])\n\n        results = {\n            'pearson_correlation_coefficient': correlation,\n            'p_value': p_value,\n            'sample_size': len(clean_data)\n        }\n\n        return results\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
      "cached_with_code": true
    },
    "statistical_data": {
      "compare_sentiment_groups": {
        "positive_sentiment_raw": {
          "positive_group_mean": 1.0,
          "negative_group_mean": 0.0,
          "t_statistic": null,
          "p_value": null,
          "notes": "T-test not performed due to insufficient sample size in one or both groups (n < 2)."
        },
        "negative_sentiment_raw": {
          "positive_group_mean": 0.0,
          "negative_group_mean": 1.0,
          "t_statistic": null,
          "p_value": null,
          "notes": "T-test not performed due to insufficient sample size in one or both groups (n < 2)."
        }
      },
      "correlation_analysis": {
        "pearson_correlation_coefficient": -1.0,
        "p_value": 1.0,
        "sample_size": 2
      },
      "descriptive_statistics": {
        "negative": {
          "positive_sentiment_raw": {
            "count": 1,
            "mean": 0.0,
            "std": null,
            "min": 0.0,
            "max": 0.0
          },
          "negative_sentiment_raw": {
            "count": 1,
            "mean": 1.0,
            "std": null,
            "min": 1.0,
            "max": 1.0
          },
          "positive_sentiment_salience": {
            "count": 1,
            "mean": 0.0,
            "std": null,
            "min": 0.0,
            "max": 0.0
          },
          "negative_sentiment_salience": {
            "count": 1,
            "mean": 1.0,
            "std": null,
            "min": 1.0,
            "max": 1.0
          },
          "positive_sentiment_confidence": {
            "count": 1,
            "mean": 1.0,
            "std": null,
            "min": 1.0,
            "max": 1.0
          },
          "negative_sentiment_confidence": {
            "count": 1,
            "mean": 1.0,
            "std": null,
            "min": 1.0,
            "max": 1.0
          }
        },
        "positive": {
          "positive_sentiment_raw": {
            "count": 1,
            "mean": 1.0,
            "std": null,
            "min": 1.0,
            "max": 1.0
          },
          "negative_sentiment_raw": {
            "count": 1,
            "mean": 0.0,
            "std": null,
            "min": 0.0,
            "max": 0.0
          },
          "positive_sentiment_salience": {
            "count": 1,
            "mean": 1.0,
            "std": null,
            "min": 1.0,
            "max": 1.0
          },
          "negative_sentiment_salience": {
            "count": 1,
            "mean": 0.0,
            "std": null,
            "min": 0.0,
            "max": 0.0
          },
          "positive_sentiment_confidence": {
            "count": 1,
            "mean": 1.0,
            "std": null,
            "min": 1.0,
            "max": 1.0
          },
          "negative_sentiment_confidence": {
            "count": 1,
            "mean": 1.0,
            "std": null,
            "min": 1.0,
            "max": 1.0
          }
        }
      },
      "generate_statistical_summary_report": "STATISTICAL ANALYSIS SUMMARY REPORT\n==================================================\nAnalysis Timestamp: Unknown\nSample Size: Unknown\nAlpha Level: Unknown\nVariables: 0\n",
      "perform_statistical_analysis": {
        "analysis_metadata": {
          "timestamp": "2025-08-29T22:27:16.532606",
          "sample_size": 2,
          "alpha_level": 0.05,
          "variables_analyzed": [
            "positive_sentiment_raw",
            "positive_sentiment_salience",
            "positive_sentiment_confidence",
            "negative_sentiment_raw",
            "negative_sentiment_salience",
            "negative_sentiment_confidence"
          ]
        }
      },
      "run_complete_statistical_analysis": {
        "analysis_metadata": {
          "timestamp": "2025-08-29T22:27:16.535192",
          "sample_size": 2,
          "alpha_level": 0.05,
          "variables_analyzed": [
            "positive_sentiment_raw",
            "positive_sentiment_salience",
            "positive_sentiment_confidence",
            "negative_sentiment_raw",
            "negative_sentiment_salience",
            "negative_sentiment_confidence"
          ]
        }
      }
    },
    "status": "success_with_data",
    "validation_passed": true
  }
}