{
  "generation_metadata": {
    "status": "success",
    "functions_generated": 6,
    "output_file": "automatedderivedmetricsagent_functions.py",
    "module_size": 17195,
    "function_code_content": "\"\"\"\nAutomated Derived Metrics Functions\n===================================\n\nGenerated by AutomatedDerivedMetricsAgent for experiment: Test Experiment\nDescription: Test experiment for derived metrics\nGenerated: 2025-08-30T01:54:44.010228+00:00\n\nThis module contains automatically generated calculation functions for derived metrics\nas specified in the framework's natural language descriptions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom typing import Optional, Dict, Any\n\n\ndef calculate_identity_tension(data, **kwargs):\n    \"\"\"\n    Calculate identity_tension: Conflict between tribal dominance and individual dignity dimensions\n    \n    Formula:\n    identity_tension = |Positive Sentiment - Negative Sentiment|\n    \n    Note: This framework does not directly provide 'Positive Sentiment' or 'Negative Sentiment'\n    columns. Assuming 'Positive Sentiment' would be derived from a 'positive_sentiment' column\n    and 'Negative Sentiment' from a 'negative_sentiment' column if they were present.\n    Since they are not in the provided `ACTUAL DATA STRUCTURE`, this function\n    will return None as the necessary input columns are missing.\n    \n    Args:\n        data: pandas DataFrame row (Series) containing analysis results.\n              Expected to have columns like 'positive_sentiment' and 'negative_sentiment'.\n        **kwargs: Additional parameters (not used in this calculation).\n        \n    Returns:\n        float: Calculated identity_tension, or None if required input columns are missing or invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    # Check if the necessary columns are present.\n    # Based on the provided 'ACTUAL DATA STRUCTURE', columns like 'positive_sentiment'\n    # and 'negative_sentiment' are NOT present.\n    # Therefore, this function cannot be computed with the given data structure.\n    # We will simulate the check as if these columns *should* exist for the formula.\n    \n    required_columns = ['positive_sentiment', 'negative_sentiment']\n    \n    if not all(col in data.index for col in required_columns):\n        # In a real scenario with the specified formula, these columns would be needed.\n        # Since they are absent in the provided data structure, we gracefully return None.\n        return None\n        \n    try:\n        # Attempt to extract sentiment scores, coercing errors to NaN\n        positive_sentiment = pd.to_numeric(data['positive_sentiment'], errors='coerce')\n        negative_sentiment = pd.to_numeric(data['negative_sentiment'], errors='coerce')\n        \n        # Check for NaN values after coercion\n        if pd.isna(positive_sentiment) or pd.isna(negative_sentiment):\n            return None\n            \n        # Calculate identity tension\n        # The formula is the absolute difference between positive and negative sentiment.\n        identity_tension_value = abs(positive_sentiment - negative_sentiment)\n        \n        return float(identity_tension_value)\n        \n    except KeyError:\n        # This exception would catch if 'data' itself is not a Series or DataFrame,\n        # or if the expected index (columns) are missing and the initial check failed.\n        return None\n    except Exception:\n        # Catch any other unexpected errors during calculation\n        return None\n\ndef calculate_emotional_balance(data, **kwargs):\n    \"\"\"\n    Calculate emotional_balance: Difference between hope and fear scores\n    \n    Formula: emotional_balance = hope_score - fear_score\n    \n    Args:\n        data: pandas DataFrame (expected to be a single row/Series) containing sentiment scores.\n              It's assumed that columns named 'hope_score' and 'fear_score' exist,\n              or that these concepts are represented by other columns.\n              Based on the framework description, the relevant scores are positive and negative sentiment.\n              Assuming 'Positive Sentiment' maps to hope and 'Negative Sentiment' maps to fear for this calculation.\n        **kwargs: Additional parameters (not used in this calculation).\n        \n    Returns:\n        float: Calculated emotional balance (positive_sentiment - negative_sentiment) or None if\n               required data is missing or invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # Map framework dimensions to actual column names as per prompt context\n        # Framework dimensions: Positive Sentiment, Negative Sentiment\n        # Assuming these are the \"hope\" and \"fear\" scores respectively for the calculation.\n        \n        # Check if data is a Series or DataFrame and extract the first row if it's a DataFrame\n        if isinstance(data, pd.DataFrame):\n            if data.empty:\n                return None\n            row_data = data.iloc[0]\n        elif isinstance(data, pd.Series):\n            row_data = data\n        else:\n            return None # Invalid input type\n\n        positive_sentiment_score = row_data.get('Positive Sentiment')\n        negative_sentiment_score = row_data.get('Negative Sentiment')\n        \n        # Handle cases where the column names might not be exactly as expected based on the description\n        # and use general sentiment scores if specific 'hope'/'fear' columns are absent.\n        # The prompt explicitly states to use the ACTUAL data structure's column names.\n        # However, the 'ACTUAL DATA STRUCTURE' provided does NOT contain 'Positive Sentiment' or 'Negative Sentiment'.\n        # The 'Theoretical Foundations' and 'Dimensions' describe 'Positive Sentiment' and 'Negative Sentiment'.\n        # Given the CONTEXT of \"Sentiment Binary Framework v1.0\", it's highly probable that the intent\n        # is to use these conceptual dimensions. The provided 'ACTUAL DATA STRUCTURE' sample is extremely sparse.\n        # Proceeding with the assumption that the DataFrame passed to this function WILL contain\n        # columns representing these conceptual dimensions, and that these are the intended \"hope\" and \"fear\" scores.\n        # If the actual data structure provided for analysis *does not* contain these, this function will return None.\n        \n        # If columns are missing, .get() will return None, which will be handled below.\n\n        if pd.isna(positive_sentiment_score) or pd.isna(negative_sentiment_score):\n            return None\n            \n        # Ensure scores are numeric before calculation\n        positive_sentiment_score = float(positive_sentiment_score)\n        negative_sentiment_score = float(negative_sentiment_score)\n        \n        emotional_balance = positive_sentiment_score - negative_sentiment_score\n        \n        # Ensure the result is within a reasonable range if needed, but for difference, any float is fine.\n        # The framework dimensions are 0.0-1.0, so the difference will be between -1.0 and 1.0.\n        \n        return emotional_balance\n\n    except (KeyError, TypeError, ValueError, AttributeError):\n        # Catch specific errors related to data access, type conversion, or calculation\n        return None\n    except Exception:\n        # Catch any other unexpected errors\n        return None\n\ndef calculate_success_climate(data, **kwargs):\n    \"\"\"\n    Calculate success_climate: Difference between compersion and envy scores\n    \n    Formula: compersion - envy\n    \n    Args:\n        data: pandas DataFrame (expected to be a Series or DataFrame with one row) \n              containing relevant scores. Expected columns: 'compersion_score', 'envy_score'.\n        **kwargs: Additional parameters (currently unused).\n        \n    Returns:\n        float: The calculated success_climate (compersion - envy), or None if \n               'compersion_score' or 'envy_score' are missing or invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    # Ensure data is a DataFrame for consistent indexing\n    if isinstance(data, pd.Series):\n        data = pd.DataFrame([data])\n\n    # Check if the necessary columns exist\n    if 'compersion_score' not in data.columns or 'envy_score' not in data.columns:\n        return None\n\n    # Get the first row of data, assuming the input DataFrame contains only one row or we process the first\n    row_data = data.iloc[0]\n\n    compersion_score = row_data['compersion_score']\n    envy_score = row_data['envy_score']\n\n    # Handle missing or non-numeric data gracefully\n    if pd.isna(compersion_score) or pd.isna(envy_score):\n        return None\n\n    try:\n        # Convert to float if they are not already, and perform calculation\n        compersion_score = float(compersion_score)\n        envy_score = float(envy_score)\n        \n        success_climate_score = compersion_score - envy_score\n        \n        # Ensure the result is within a reasonable range if needed, or just return the difference\n        # For this specific calculation, we'll return the direct difference.\n        return success_climate_score\n\n    except (ValueError, TypeError):\n        # If conversion to float fails, return None\n        return None\n    except Exception as e:\n        # Catch any other unexpected errors\n        print(f\"An unexpected error occurred: {e}\") # For debugging purposes, might be removed in production\n        return None\n\ndef calculate_relational_climate(data, **kwargs):\n    \"\"\"\n    Calculate relational_climate: Difference between amity and enmity scores.\n\n    Formula:\n        relational_climate = amity_score - enmity_score\n\n    Args:\n        data: pandas DataFrame or Series representing a single row of analysis results.\n              Expected columns: 'amity_score', 'enmity_score'.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        float: The calculated relational_climate score (amity_score - enmity_score).\n               Returns None if 'amity_score' or 'enmity_score' are missing or not numeric.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    # Ensure input is treated as a Series for easier access\n    if isinstance(data, pd.DataFrame):\n        if data.empty:\n            return None\n        # Assuming the function is called with a single row DataFrame\n        data = data.iloc[0]\n\n    # Check if required columns exist and are numeric\n    amity_score = data.get('amity_score')\n    enmity_score = data.get('enmity_score')\n\n    if amity_score is None or not isinstance(amity_score, (int, float)):\n        return None\n    if enmity_score is None or not isinstance(enmity_score, (int, float)):\n        return None\n\n    # Calculate relational_climate\n    relational_climate_score = amity_score - enmity_score\n\n    return float(relational_climate_score)\n\ndef calculate_goal_orientation(data, **kwargs):\n    \"\"\"\n    Calculate goal_orientation: Difference between cohesive goals and fragmentative goals\n    \n    Formula: goal_orientation = Positive Sentiment - Negative Sentiment\n    \n    Args:\n        data: pandas Series representing a single row of analysis results.\n              Expected to contain 'Positive Sentiment' and 'Negative Sentiment' columns.\n        **kwargs: Additional parameters (not used in this function).\n        \n    Returns:\n        float: The calculated goal orientation score, or None if required data is missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    # Define the expected sentiment columns based on framework description\n    # Although the actual data structure sample doesn't explicitly show these,\n    # the framework's theoretical foundations imply their presence.\n    # We will assume these columns are added by a preceding processing step.\n    positive_sentiment_col = 'Positive Sentiment'\n    negative_sentiment_col = 'Negative Sentiment'\n\n    try:\n        # Check if the data is a pandas Series (representing a single row)\n        if not isinstance(data, pd.Series):\n            # If it's a DataFrame, assume it's a single row and convert to Series\n            if isinstance(data, pd.DataFrame):\n                if len(data) == 1:\n                    data = data.iloc[0]\n                else:\n                    # Cannot process multiple rows in this function\n                    return None\n            else:\n                # Not a DataFrame or Series, invalid input\n                return None\n\n        # Check if the required columns exist in the data\n        if positive_sentiment_col not in data.index or negative_sentiment_col not in data.index:\n            return None\n\n        # Get the sentiment scores, handling potential NaN values\n        positive_sentiment = data[positive_sentiment_col]\n        negative_sentiment = data[negative_sentiment_col]\n\n        # If either score is NaN, the result is indeterminate\n        if pd.isna(positive_sentiment) or pd.isna(negative_sentiment):\n            return None\n            \n        # Ensure scores are numeric before calculation\n        if not isinstance(positive_sentiment, (int, float)) or not isinstance(negative_sentiment, (int, float)):\n            return None\n\n        # Calculate goal orientation\n        goal_orientation_score = positive_sentiment - negative_sentiment\n        \n        return float(goal_orientation_score)\n\n    except Exception as e:\n        # Log the error if needed, but for this specific requirement, return None\n        # print(f\"Error calculating goal_orientation: {e}\") # Uncomment for debugging\n        return None\n\ndef calculate_overall_cohesion_index(data, **kwargs):\n    \"\"\"\n    Calculate overall_cohesion_index: Comprehensive measure combining all dimensions\n    \n    This function calculates a simple cohesion index by averaging the\n    Positive Sentiment and Negative Sentiment scores. It is designed for\n    minimalist sentiment analysis validation.\n    \n    Formula:\n        overall_cohesion_index = (Positive Sentiment + Negative Sentiment) / 2\n        \n    Args:\n        data: pandas DataFrame (expected to be a single row or Series)\n              containing sentiment scores. The relevant columns are assumed\n              to be 'Positive Sentiment' and 'Negative Sentiment'.\n        **kwargs: Additional parameters (not used in this specific calculation).\n        \n    Returns:\n        float: The calculated overall cohesion index (0.0-1.0), or None\n               if required sentiment scores are missing or invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    # Ensure data is a Series for easier access if it's a single-row DataFrame\n    if isinstance(data, pd.DataFrame) and len(data) == 1:\n        data = data.iloc[0]\n    elif not isinstance(data, pd.Series):\n        # If it's not a Series or a single-row DataFrame, it's invalid input\n        return None\n\n    positive_sentiment = data.get('Positive Sentiment')\n    negative_sentiment = data.get('Negative Sentiment')\n\n    # Check if required scores are present and are valid numbers\n    if positive_sentiment is None or negative_sentiment is None or \\\n       not isinstance(positive_sentiment, (int, float)) or \\\n       not isinstance(negative_sentiment, (int, float)):\n        return None\n\n    # Ensure scores are within the expected range [0.0, 1.0]\n    if not (0.0 <= positive_sentiment <= 1.0) or not (0.0 <= negative_sentiment <= 1.0):\n        return None\n        \n    # Calculate the overall cohesion index\n    overall_cohesion = (positive_sentiment + negative_sentiment) / 2.0\n    \n    return overall_cohesion\n\ndef calculate_all_derived_metrics(data: pd.DataFrame) -> Dict[str, Optional[float]]:\n    \"\"\"\n    Calculate all derived metrics for the given dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        \n    Returns:\n        Dictionary mapping metric names to calculated values\n    \"\"\"\n    results = {}\n    \n    # Get all calculation functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith('calculate_') and \n            name not in ['calculate_all_derived_metrics', 'calculate_derived_metrics']):\n            try:\n                results[name.replace('calculate_', '')] = obj(data)\n            except Exception as e:\n                results[name.replace('calculate_', '')] = None\n                \n    return results\n\n\ndef calculate_derived_metrics(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Template-compatible wrapper function for derived metrics calculation.\n    \n    This function is called by the universal notebook template and returns\n    the original data with additional derived metric columns.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        \n    Returns:\n        DataFrame with original data plus derived metric columns\n    \"\"\"\n    # Calculate all derived metrics\n    derived_metrics = calculate_all_derived_metrics(data)\n    \n    # Create a copy of the original data\n    result = data.copy()\n    \n    # Add derived metrics as new columns\n    for metric_name, metric_value in derived_metrics.items():\n        if metric_value is not None:\n            # For scalar metrics, broadcast to all rows\n            result[metric_name] = metric_value\n        else:\n            # For failed calculations, use NaN\n            result[metric_name] = np.nan\n    \n    return result\n",
    "cached_with_code": true
  },
  "derived_metrics_data": {
    "status": "success",
    "original_count": 2,
    "derived_count": 2,
    "derived_metrics": [
      {
        "analysis_id": "analysis_c9dfcd84fda4",
        "result_hash": "086255a869fd4e5bda5685ea3e13fa2763685a0d9efafe8571f3710be2c44d14",
        "result_content": {
          "analysis_id": "analysis_c9dfcd84fda4",
          "agent_name": "EnhancedAnalysisAgent",
          "agent_version": "enhanced_v2.1_raw_output",
          "experiment_name": "nano_test_experiment",
          "model_used": "vertex_ai/gemini-2.5-flash-lite",
          "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.9,\n    \"analysis_notes\": \"Analysis conducted using three independent approaches to ensure robustness. Median scores are used for aggregation, and best evidence is selected for clarity.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"6950ccbc6b14\",\n      \"document_name\": \"positive_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.9,\n          \"salience\": 0.9,\n          \"confidence\": 0.95\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 0.95\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"This is a wonderful day! Everything is going perfectly. I feel great about the future. Success is everywhere. The team did an excellent job. We're achieving amazing results. Optimism fills the air. What a fantastic opportunity! I'm thrilled with the progress. Everything looks bright and promising.\",\n          \"confidence\": 0.95,\n          \"context_type\": \"overall_document\"\n        },\n        {\n          \"dimension\": \"negative_sentiment\",\n          \"quote_text\": \"\",\n          \"confidence\": 0.0,\n          \"context_type\": \"absent\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
          "evidence_hash": "c08bbee584e4d56d3d5f21404afb0cbe0709c1be454e8061456d477f7d3b7c00",
          "execution_metadata": {
            "start_time": "2025-08-30T01:54:27.873978+00:00",
            "end_time": "2025-08-30T01:54:30.406141+00:00",
            "duration_seconds": 2.532149
          },
          "input_artifacts": {
            "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
            "document_hashes": [
              "6950ccbc6b14eabf43508448cf888b2c590f44e549a32b8ae60e943710e74019"
            ],
            "num_documents": 1
          },
          "provenance": {
            "security_boundary": {
              "experiment_name": "nano_test_experiment",
              "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
              "boundary_type": "filesystem",
              "security_level": "experiment_scoped"
            },
            "audit_session_id": "20250830T015427Z_683b625d"
          }
        },
        "cached": true
      },
      {
        "analysis_id": "analysis_ef118072a668",
        "result_hash": "187905105b0bc2cead4df951f941b7e6d5d207ac3896a8c797b46e1bc3558144",
        "result_content": {
          "analysis_id": "analysis_ef118072a668",
          "agent_name": "EnhancedAnalysisAgent",
          "agent_version": "enhanced_v2.1_raw_output",
          "experiment_name": "nano_test_experiment",
          "model_used": "vertex_ai/gemini-2.5-flash-lite",
          "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.9,\n    \"analysis_notes\": \"Analysis of document negative_test.txt using the sentiment_binary_v1 framework.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"1df25093a747\",\n      \"document_name\": \"negative_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 0.8\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 0.95\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"\",\n          \"confidence\": 0.0,\n          \"context_type\": \"No positive sentiment detected\"\n        },\n        {\n          \"dimension\": \"negative_sentiment\",\n          \"quote_text\": \"This is a terrible situation. Everything is going wrong. I feel awful about the future. Failure surrounds us.\",\n          \"confidence\": 1.0,\n          \"context_type\": \"Strong negative sentiment indicators\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
          "evidence_hash": "4e576750a7a7b87b1f730110957d2d0aa09f22cf62bd07ce8ed9860f6cc44d28",
          "execution_metadata": {
            "start_time": "2025-08-30T01:54:30.407579+00:00",
            "end_time": "2025-08-30T01:54:31.831500+00:00",
            "duration_seconds": 1.423916
          },
          "input_artifacts": {
            "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
            "document_hashes": [
              "1df25093a747678c5dbe724ecfb963b88049f9a7a536a3f23f064fd592744f91"
            ],
            "num_documents": 1
          },
          "provenance": {
            "security_boundary": {
              "experiment_name": "nano_test_experiment",
              "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
              "boundary_type": "filesystem",
              "security_level": "experiment_scoped"
            },
            "audit_session_id": "20250830T015427Z_683b625d"
          }
        },
        "cached": true
      }
    ],
    "columns_added": []
  },
  "status": "success_with_data",
  "validation_passed": true
}