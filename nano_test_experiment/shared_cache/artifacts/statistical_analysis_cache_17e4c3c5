{
  "batch_id": "stats_20250916T160456Z",
  "statistical_analysis": {
    "batch_id": "stats_20250916T160456Z",
    "step": "statistical_execution",
    "model_used": "vertex_ai/gemini-2.5-pro",
    "statistical_functions_and_results": "```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nfrom typing import Dict, Any, List, Optional\\nimport json\\nimport re\\n\\ndef _create_analysis_dataframe(data: List[Dict[str, Any]]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Parses the raw analysis artifacts into a clean pandas DataFrame.\\n\\n    This helper function handles the inconsistent JSON structures found in the\\n    score extraction artifacts and maps them to the correct document ID.\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A pandas DataFrame containing the cleaned and structured scores,\\n        or None if no score artifacts can be processed.\\n    \\\"\\\"\\\"\\n    processed_records = []\\n    score_artifacts = [a for a in data if a.get('Type') == 'score_extraction']\\n\\n    # Manual mapping based on artifact inspection, as formats differ\\n    doc_map = {\\n        'analysis_1d91dd78': 'positive_test.txt',\\n        'analysis_12ed6a5a': 'negative_test.txt'\\n    }\\n\\n    for artifact in score_artifacts:\\n        analysis_id = artifact.get('Analysis ID')\\n        doc_id = doc_map.get(analysis_id)\\n        if not doc_id:\\n            continue\\n\\n        scores_str = artifact.get('scores_extraction', '{}')\\n        # Clean string from markdown code blocks and potential extra text\\n        match = re.search(r'```json\\\\n(.*?)\\\\n```', scores_str, re.DOTALL)\\n        if match:\\n            scores_str = match.group(1)\\n        \\n        try:\\n            scores_data = json.loads(scores_str)\\n        except json.JSONDecodeError:\\n            continue\\n\\n        # Handle the two different observed JSON structures\\n        if doc_id in scores_data: # Nested structure\\n            scores_to_process = scores_data[doc_id]\\n        else: # Flat structure\\n            scores_to_process = scores_data\\n\\n        for dimension, values in scores_to_process.items():\\n            if isinstance(values, dict):\\n                record = {\\n                    'document_id': doc_id,\\n                    'dimension': dimension,\\n                    'raw_score': values.get('raw_score'),\\n                    'salience': values.get('salience'),\\n                    'confidence': values.get('confidence')\\n                }\\n                processed_records.append(record)\\n\\n    if not processed_records:\\n        return None\\n\\n    return pd.DataFrame(processed_records)\\n\\ndef _get_grouping(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Adds a 'group' column to the DataFrame based on the corpus manifest.\\n    \\\"\\\"\\\"\\n    group_mapping = {\\n        'positive_test.txt': 'positive',\\n        'negative_test.txt': 'negative'\\n    }\\n    df['group'] = df['document_id'].map(group_mapping)\\n    return df\\n\\ndef calculate_descriptive_statistics(data: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates and returns descriptive statistics for dimensional scores, grouped by sentiment type.\\n\\n    Methodology:\\n    Given the exploratory nature of this analysis (N<15), this function focuses on \\n    descriptive statistics (mean, std, min, max, count) to summarize the central tendency \\n    and dispersion of scores for each group. This directly addresses the research question \\n    about distinguishing sentiment scores.\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A dictionary containing descriptive statistics for each dimension, grouped by\\n        the 'sentiment' metadata field, or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    try:\\n        df = _create_analysis_dataframe(data)\\n        if df is None or df.empty:\\n            return {\\\"message\\\": \\\"No valid score data found to generate descriptives.\\\"}\\n\\n        df = _get_grouping(df)\\n        \\n        # Group by the sentiment type and the dimension\\n        grouped = df.groupby(['group', 'dimension'])['raw_score'].agg(['mean', 'std', 'min', 'max', 'count']).reset_index()\\n        \\n        # Since n=1, std will be NaN. Fill with 0 for clarity in this specific case.\\n        grouped['std'] = grouped['std'].fillna(0)\\n\\n        return grouped.to_dict(orient='records')\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef perform_comparative_analysis(data: List[Dict[str, Any]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Performs a simple comparative analysis of mean scores between groups.\\n\\n    Methodology:\\n    With N=1 per group, inferential tests like t-tests are not applicable. This function \\n    provides a basic comparison by calculating the raw difference in means between the\\n    'positive' and 'negative' groups for each dimension. This directly addresses the \\n    research question of whether there is a 'clear distinction' in scores.\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A dictionary detailing the mean scores for each group and the difference,\\n        along with a note on the statistical limitations.\\n    \\\"\\\"\\\"\\n    try:\\n        df = _create_analysis_dataframe(data)\\n        if df is None or df.empty:\\n            return {\\\"message\\\": \\\"No valid score data found for comparison.\\\"}\\n\\n        df = _get_grouping(df)\\n        \\n        results = {\\n            'analysis_type': 'Comparative Analysis (Mean Difference)',\\n            'notes': 'Tier 3 (N<8 per group). Inferential tests (e.g., t-test) are not applicable due to N=1 per group. Analysis is limited to descriptive comparison.',\\n            'comparisons': []\\n        }\\n\\n        for dimension in df['dimension'].unique():\\n            pos_score = df[(df['group'] == 'positive') & (df['dimension'] == dimension)]['raw_score'].values\\n            neg_score = df[(df['group'] == 'negative') & (df['dimension'] == dimension)]['raw_score'].values\\n\\n            if len(pos_score) > 0 and len(neg_score) > 0:\\n                mean_pos = pos_score[0]\\n                mean_neg = neg_score[0]\\n                difference = mean_pos - mean_neg\\n                \\n                results['comparisons'].append({\\n                    'dimension': dimension,\\n                    'positive_group_mean': mean_pos,\\n                    'negative_group_mean': mean_neg,\\n                    'mean_difference': difference\\n                })\\n        \\n        return results\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\n\\ndef perform_correlation_analysis(data: List[Dict[str, Any]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Placeholder for correlation analysis. Not performed due to sample size.\\n    \\\"\\\"\\\"\\n    return {'message': 'Not performed. Correlation analysis requires N>=15 for meaningful results.'}\\n\\ndef calculate_reliability_analysis(data: List[Dict[str, Any]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Placeholder for reliability analysis. Not performed due to insufficient items/dimensions.\\n    \\\"\\\"\\\"\\n    return {'message': 'Not performed. Reliability analysis (e.g., Cronbach\\\\'s alpha) requires more than two dimensions to be meaningful.'}\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": [\n      {\n        \"group\": \"negative\",\n        \"dimension\": \"negative_sentiment\",\n        \"mean\": 1.0,\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"count\": 1\n      },\n      {\n        \"group\": \"negative\",\n        \"dimension\": \"positive_sentiment\",\n        \"mean\": 0.0,\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"count\": 1\n      },\n      {\n        \"group\": \"positive\",\n        \"dimension\": \"negative_sentiment\",\n        \"mean\": 0.0,\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"count\": 1\n      },\n      {\n        \"group\": \"positive\",\n        \"dimension\": \"positive_sentiment\",\n        \"mean\": 0.9,\n        \"std\": 0.0,\n        \"min\": 0.9,\n        \"max\": 0.9,\n        \"count\": 1\n      }\n    ],\n    \"comparative_analysis\": {\n      \"analysis_type\": \"Comparative Analysis (Mean Difference)\",\n      \"notes\": \"Tier 3 (N<8 per group). Inferential tests (e.g., t-test) are not applicable due to N=1 per group. Analysis is limited to descriptive comparison.\",\n      \"comparisons\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"positive_group_mean\": 0.9,\n          \"negative_group_mean\": 0.0,\n          \"mean_difference\": 0.9\n        },\n        {\n          \"dimension\": \"negative_sentiment\",\n          \"positive_group_mean\": 0.0,\n          \"negative_group_mean\": 1.0,\n          \"mean_difference\": -1.0\n        }\n      ]\n    },\n    \"correlation_analysis\": {\n      \"message\": \"Not performed. Correlation analysis requires N>=15 for meaningful results.\"\n    },\n    \"reliability_analysis\": {\n      \"message\": \"Not performed. Reliability analysis (e.g., Cronbach's alpha) requires more than two dimensions to be meaningful.\"\n    }\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 2,\n    \"tier_classification\": \"TIER 3\",\n    \"power_notes\": \"The sample size of N=2 (N=1 per group) is far too small for any inferential statistical tests. All analysis is strictly exploratory and descriptive. The goal is to observe patterns and validate pipeline functionality, not to make generalizable claims.\"\n  },\n  \"methodology_summary\": \"The analysis was conducted at Tier 3 (Exploratory) due to the extremely small sample size (N=2). The primary statistical method was the calculation of descriptive statistics (mean, std, min, max) for each sentiment dimension, grouped by the document's intended sentiment ('positive' vs. 'negative'). A comparative analysis showing the raw difference in means was also performed to directly address the research question of observing a 'clear distinction' between the groups. No inferential statistics were used as they would be statistically invalid.\"\n}\n```",
    "analysis_artifacts_processed": 4,
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-pro",
      "execution_time_seconds": 44.590432,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0,
      "prompt_length": 16125,
      "response_length": 9799
    },
    "timestamp": "2025-09-16T20:05:41.165868+00:00",
    "artifact_hash": "331e6daaf9e08336097dbc7f0fb34736ea8abf33a41b1c23fbb190792ebf68fa"
  },
  "verification": {
    "batch_id": "stats_20250916T160456Z",
    "step": "verification",
    "model_used": "vertex_ai/gemini-2.5-flash-lite",
    "verification_status": "unknown",
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-flash-lite",
      "execution_time_seconds": 1.820314,
      "prompt_length": 10297,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0
    },
    "timestamp": "2025-09-16T20:05:42.991365+00:00",
    "artifact_hash": "8b77f7003baa3f8f8f362b96afef0b4f697739272532c3ea89bb685f7c229516"
  },
  "csv_generation": {
    "batch_id": "stats_20250916T160456Z",
    "step": "csv_generation",
    "model_used": "vertex_ai/gemini-2.5-flash",
    "csv_files": [
      {
        "filename": "scores.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250916T195409Z/data/scores.csv",
        "size": 149
      },
      {
        "filename": "metadata.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250916T195409Z/data/metadata.csv",
        "size": 247
      },
      {
        "filename": "evidence.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250916T195409Z/data/evidence.csv",
        "size": 27
      }
    ],
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-flash",
      "execution_time_seconds": 20.765214,
      "prompt_length": 3822,
      "artifacts_processed": 2,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0
    },
    "timestamp": "2025-09-16T20:06:03.760884+00:00",
    "artifact_hash": "e622a57161c9a65ae4fa452ac314a8ad3a6a613256ccf682a291e5cb53ce25d4"
  },
  "total_cost_info": {
    "total_cost_usd": 0.0,
    "total_execution_time_seconds": 67.17596,
    "total_tokens": 0,
    "cost_breakdown": {
      "statistical_execution": 0.0,
      "verification": 0.0,
      "csv_generation": 0.0
    },
    "performance_breakdown": {
      "statistical_execution_time": 44.590432,
      "verification_time": 1.820314,
      "csv_generation_time": 20.765214
    },
    "models_used": [
      "vertex_ai/gemini-2.5-pro",
      "vertex_ai/gemini-2.5-flash-lite",
      "vertex_ai/gemini-2.5-flash"
    ]
  },
  "timestamp": "2025-09-16T20:06:03.762503+00:00",
  "agent_name": "StatisticalAgent"
}