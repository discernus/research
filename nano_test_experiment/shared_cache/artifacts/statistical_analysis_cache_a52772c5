{
  "batch_id": "stats_20250916T160726Z",
  "statistical_analysis": {
    "batch_id": "stats_20250916T160726Z",
    "step": "statistical_execution",
    "model_used": "vertex_ai/gemini-2.5-pro",
    "statistical_functions_and_results": "```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nfrom typing import Dict, Any, Optional, List\\nimport json\\nimport re\\n\\ndef _prepare_data(data: List[Dict[str, Any]]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Parses raw analysis artifacts into a clean pandas DataFrame.\\n\\n    This helper function handles the inconsistent JSON structures found in the\\n    score extraction artifacts, maps analysis IDs to document filenames, and\\n    assembles a unified DataFrame for statistical analysis.\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A pandas DataFrame with columns for document_id and dimensional scores,\\n        or None if data cannot be processed.\\n    \\\"\\\"\\\"\\n    score_artifacts = [a for a in data if a.get('step') == 'score_extraction']\\n    if not score_artifacts:\\n        return None\\n\\n    processed_data = []\\n    # Based on the corpus manifest, we know there are two documents:\\n    # 'positive_test.txt' (doc_id: pos_test) and 'negative_test.txt' (doc_id: neg_test)\\n    # We deduce the mapping from the artifact content.\\n\\n    doc_map = {}\\n    unassigned_ids = []\\n\\n    for artifact in score_artifacts:\\n        analysis_id = artifact.get('analysis_id')\\n        scores_str = artifact.get('scores_extraction', '{}')\\n        \\n        # Clean the string: remove backticks, newlines, and surrounding code block markers\\n        clean_scores_str = re.sub(r'^```json\\\\n|\\\\n```$', '', scores_str).strip()\\n        \\n        try:\\n            scores_json = json.loads(clean_scores_str)\\n        except json.JSONDecodeError:\\n            continue\\n\\n        # The data format is inconsistent. One artifact has the filename as a key,\\n        # the other does not. We handle both cases.\\n        if 'negative_test.txt' in scores_json:\\n            doc_map[analysis_id] = 'negative_test.txt'\\n            scores = scores_json['negative_test.txt']\\n        else:\\n            # This artifact must correspond to 'positive_test.txt' by elimination\\n            unassigned_ids.append(analysis_id)\\n            scores = scores_json\\n\\n        if scores and analysis_id:\\n            record = {\\n                'analysis_id': analysis_id,\\n                'positive_sentiment': scores.get('positive_sentiment', {}).get('raw_score'),\\n                'negative_sentiment': scores.get('negative_sentiment', {}).get('raw_score'),\\n            }\\n            processed_data.append(record)\\n\\n    # Assign the remaining document\\n    if len(unassigned_ids) == 1:\\n        doc_map[unassigned_ids[0]] = 'positive_test.txt'\\n\\n    if not processed_data:\\n        return None\\n\\n    df = pd.DataFrame(processed_data)\\n    df['document_id'] = df['analysis_id'].map(doc_map)\\n    df = df.drop(columns=['analysis_id'])\\n    df = df.dropna(subset=['document_id'])\\n\\n    # Create grouping variable based on corpus manifest\\n    def get_sentiment_group(filename):\\n        if 'positive' in filename:\\n            return 'positive'\\n        if 'negative' in filename:\\n            return 'negative'\\n        return 'unknown'\\n\\n    df['sentiment_group'] = df['document_id'].apply(get_sentiment_group)\\n\\n    return df.drop_duplicates(subset=['document_id'])\\n\\ndef calculate_descriptive_statistics(data: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates and returns descriptive statistics for dimensional scores, grouped by sentiment.\\n\\n    Methodology:\\n    Given the exploratory nature of this analysis (N<15), this function focuses on\\n    descriptive statistics (mean, std, min, max) to summarize the central tendency\\n    and dispersion of scores for each group. This provides a foundational understanding\\n    of the data patterns without making unwarranted inferential claims.\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A dictionary containing descriptive statistics for each dimension, grouped by sentiment,\\n        or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    df = _prepare_data(data)\\n    if df is None or df.empty or 'sentiment_group' not in df.columns:\\n        return {\\\"error\\\": \\\"Could not prepare data for descriptive statistics.\\\"}\\n\\n    try:\\n        dimensions = ['positive_sentiment', 'negative_sentiment']\\n        grouped = df.groupby('sentiment_group')[dimensions]\\n        \\n        desc_stats = grouped.agg(['mean', 'std', 'min', 'max']).to_dict()\\n        \\n        # Clean up the output format to have string keys\\n        results = {}\\n        for dimension, stats_dict in desc_stats.items():\\n            results[dimension] = {}\\n            for stat_name, values in stats_dict.items():\\n                results[dimension][stat_name] = values\\n\\n        return results\\n    except Exception as e:\\n        return {\\\"error\\\": f\\\"An error occurred during descriptive statistics calculation: {e}\\\"}\\n\\n\\ndef compare_sentiment_scores(data: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Directly compares the scores between the positive and negative test documents.\\n\\n    Methodology:\\n    Addresses the core research question by quantifying the difference in scores between\\n    the two documents. For this Tier 3 (N=2) analysis, calculating the simple difference\\n    serves as an unstandardized effect size, directly testing the expected outcome of a\\n    \\\"clear distinction\\\" between scores.\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A dictionary with the score differences for each sentiment dimension, or None.\\n    \\\"\\\"\\\"\\n    df = _prepare_data(data)\\n    if df is None or len(df) != 2:\\n        return {\\\"message\\\": \\\"Insufficient data for comparison. Requires exactly one positive and one negative document.\\\"}\\n\\n    try:\\n        pos_doc_scores = df[df['sentiment_group'] == 'positive'].iloc[0]\\n        neg_doc_scores = df[df['sentiment_group'] == 'negative'].iloc[0]\\n\\n        results = {\\n            \\\"positive_sentiment_distinction\\\": {\\n                \\\"positive_doc_score\\\": pos_doc_scores['positive_sentiment'],\\n                \\\"negative_doc_score\\\": neg_doc_scores['positive_sentiment'],\\n                \\\"difference\\\": pos_doc_scores['positive_sentiment'] - neg_doc_scores['positive_sentiment']\\n            },\\n            \\\"negative_sentiment_distinction\\\": {\\n                \\\"positive_doc_score\\\": pos_doc_scores['negative_sentiment'],\\n                \\\"negative_doc_score\\\": neg_doc_scores['negative_sentiment'],\\n                \\\"difference\\\": neg_doc_scores['negative_sentiment'] - pos_doc_scores['negative_sentiment']\\n            },\\n            \\\"summary\\\": \\\"The 'difference' for negative sentiment is calculated as (negative_doc - positive_doc) to show a positive value for successful distinction.\\\"\\n        }\\n        return results\\n    except (KeyError, IndexError) as e:\\n        return {\\\"error\\\": f\\\"Could not find scores for both document types: {e}\\\"}\\n\\n\\ndef perform_correlation_analysis(data: List[Dict[str, Any]]) -> Dict[str, str]:\\n    \\\"\\\"\\\"Not applicable due to sample size. Requires N>1 for correlation.\\\"\\\"\\\"\\n    return {\\\"status\\\": \\\"Skipped\\\", \\\"reason\\\": \\\"Correlation analysis requires more than one data point per group, but N=1 for each group.\\\"}\\n\\n\\ndef perform_t_test_analysis(data: List[Dict[str, Any]]) -> Dict[str, str]:\\n    \\\"\\\"\\\"Not applicable due to sample size. Tier 3 analysis (N<8 per group).\\\"\\\"\\\"\\n    return {\\\"status\\\": \\\"Skipped\\\", \\\"reason\\\": \\\"T-tests are not appropriate for a sample size of N=1 per group. Descriptive comparisons are used instead.\\\"}\\n\\n\\ndef perform_anova_analysis(data: List[Dict[str, Any]]) -> Dict[str, str]:\\n    \\\"\\\"\\\"Not applicable due to sample size. Tier 3 analysis (N<5 per group).\\\"\\\"\\\"\\n    return {\\\"status\\\": \\\"Skipped\\\", \\\"reason\\\": \\\"ANOVA is not appropriate for a sample size of N=1 per group.\\\"}\\n\\n\\ndef calculate_reliability_analysis(data: List[Dict[str, Any]]) -> Dict[str, str]:\\n    \\\"\\\"\\\"Not applicable. Requires multiple items measuring the same construct.\\\"\\\"\\\"\\n    return {\\\"status\\\": \\\"Skipped\\\", \\\"reason\\\": \\\"Reliability analysis like Cronbach's alpha requires multiple dimensions or items measuring a single latent construct, which is not the design here.\\\"}\\n\\ndef perform_statistical_analysis(data: List[Dict[str, Any]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function to execute all statistical analyses for the nano_test_experiment.\\n    \\\"\\\"\\\"\\n    results = {\\n        \\\"descriptive_statistics\\\": calculate_descriptive_statistics(data),\\n        \\\"group_comparison\\\": compare_sentiment_scores(data),\\n        \\\"correlation_analysis\\\": perform_correlation_analysis(data),\\n        \\\"t_test_analysis\\\": perform_t_test_analysis(data),\\n        \\\"anova_analysis\\\": perform_anova_analysis(data),\\n        \\\"reliability_analysis\\\": calculate_reliability_analysis(data)\\n    }\\n    return results\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": {\n      \"positive_sentiment\": {\n        \"mean\": {\n          \"negative\": 0.0,\n          \"positive\": 0.9\n        },\n        \"std\": {\n          \"negative\": null,\n          \"positive\": null\n        },\n        \"min\": {\n          \"negative\": 0.0,\n          \"positive\": 0.9\n        },\n        \"max\": {\n          \"negative\": 0.0,\n          \"positive\": 0.9\n        }\n      },\n      \"negative_sentiment\": {\n        \"mean\": {\n          \"negative\": 1.0,\n          \"positive\": 0.0\n        },\n        \"std\": {\n          \"negative\": null,\n          \"positive\": null\n        },\n        \"min\": {\n          \"negative\": 1.0,\n          \"positive\": 0.0\n        },\n        \"max\": {\n          \"negative\": 1.0,\n          \"positive\": 0.0\n        }\n      }\n    },\n    \"group_comparison\": {\n      \"positive_sentiment_distinction\": {\n        \"positive_doc_score\": 0.9,\n        \"negative_doc_score\": 0.0,\n        \"difference\": 0.9\n      },\n      \"negative_sentiment_distinction\": {\n        \"positive_doc_score\": 0.0,\n        \"negative_doc_score\": 1.0,\n        \"difference\": 1.0\n      },\n      \"summary\": \"The 'difference' for negative sentiment is calculated as (negative_doc - positive_doc) to show a positive value for successful distinction.\"\n    },\n    \"correlation_analysis\": {\n      \"status\": \"Skipped\",\n      \"reason\": \"Correlation analysis requires more than one data point per group, but N=1 for each group.\"\n    },\n    \"t_test_analysis\": {\n      \"status\": \"Skipped\",\n      \"reason\": \"T-tests are not appropriate for a sample size of N=1 per group. Descriptive comparisons are used instead.\"\n    },\n    \"anova_analysis\": {\n      \"status\": \"Skipped\",\n      \"reason\": \"ANOVA is not appropriate for a sample size of N=1 per group.\"\n    },\n    \"reliability_analysis\": {\n      \"status\": \"Skipped\",\n      \"reason\": \"Reliability analysis like Cronbach's alpha requires multiple dimensions or items measuring a single latent construct, which is not the design here.\"\n    }\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 2,\n    \"tier_classification\": \"TIER 3\",\n    \"power_notes\": \"The analysis is exploratory with N=2 (1 document per group). No inferential statistics are appropriate. The analysis is limited to descriptive statistics and direct comparison of scores to validate basic pipeline functionality as per the experiment's goal.\"\n  },\n  \"methodology_summary\": \"The statistical analysis was conducted under a Tier 3 (Exploratory) protocol due to the small sample size (N=2). The primary methods involved parsing and cleaning the analysis artifacts into a structured format, followed by the calculation of descriptive statistics (mean scores) for each document group ('positive' vs 'negative'). A direct comparison of scores was performed to quantify the distinction between the documents, directly addressing the experiment's research questions. Inferential tests like t-tests, ANOVA, and correlations were explicitly skipped as they are statistically invalid for this sample size.\"\n}\n```",
    "analysis_artifacts_processed": 4,
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-pro",
      "execution_time_seconds": 45.345581,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0,
      "prompt_length": 16125,
      "response_length": 12004
    },
    "timestamp": "2025-09-16T20:08:11.894829+00:00",
    "artifact_hash": "d1e98496a53498b6260cff069fdd2a0a51752bceb7d4db1f78c4f9a4788ffb48"
  },
  "verification": {
    "batch_id": "stats_20250916T160726Z",
    "step": "verification",
    "model_used": "vertex_ai/gemini-2.5-flash-lite",
    "verification_status": "verified",
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-flash-lite",
      "execution_time_seconds": 0.696584,
      "prompt_length": 12502,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0
    },
    "timestamp": "2025-09-16T20:08:12.599394+00:00",
    "artifact_hash": "10b755f4d0b8ed11607b0944cf2b9c9bbc7fbf8f3bc981ce02957b0e84a235eb"
  },
  "csv_generation": {
    "batch_id": "stats_20250916T160726Z",
    "step": "csv_generation",
    "model_used": "vertex_ai/gemini-2.5-flash",
    "csv_files": [
      {
        "filename": "scores.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250916T200456Z/data/scores.csv",
        "size": 230
      },
      {
        "filename": "metadata.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250916T200456Z/data/metadata.csv",
        "size": 331
      },
      {
        "filename": "evidence.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250916T200456Z/data/evidence.csv",
        "size": 37
      }
    ],
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-flash",
      "execution_time_seconds": 16.711017,
      "prompt_length": 3822,
      "artifacts_processed": 2,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0
    },
    "timestamp": "2025-09-16T20:08:29.316691+00:00",
    "artifact_hash": "6674456f374efef14d1860cd411afed9fc2333860804511c2dc16fd5c38e588f"
  },
  "total_cost_info": {
    "total_cost_usd": 0.0,
    "total_execution_time_seconds": 62.753182,
    "total_tokens": 0,
    "cost_breakdown": {
      "statistical_execution": 0.0,
      "verification": 0.0,
      "csv_generation": 0.0
    },
    "performance_breakdown": {
      "statistical_execution_time": 45.345581,
      "verification_time": 0.696584,
      "csv_generation_time": 16.711017
    },
    "models_used": [
      "vertex_ai/gemini-2.5-pro",
      "vertex_ai/gemini-2.5-flash-lite",
      "vertex_ai/gemini-2.5-flash"
    ]
  },
  "timestamp": "2025-09-16T20:08:29.319653+00:00",
  "agent_name": "StatisticalAgent"
}