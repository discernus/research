{
  "status": "success",
  "functions_generated": 2,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 11920,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: nano_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-12T00:52:54.277607+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef summarize_sentiment_scores(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for sentiment scores, grouped by document.\n\n    This function addresses the research question about identifying positive vs. negative sentiment\n    by providing a clear, descriptive summary of the scores for each test document.\n\n    Methodology:\n    Given the extremely small sample size (N=2), this analysis is strictly exploratory and\n    descriptive (Tier 3). No inferential statistics (e.g., t-tests) are appropriate or\n    mathematically sound, as within-group variance cannot be calculated. The function\n    computes basic descriptive statistics (mean, standard deviation, count) for the\n    'positive_sentiment_raw' and 'negative_sentiment_raw' scores for each unique document.\n    The results are intended to facilitate direct observation of the scores to validate\n    whether the pipeline produced the expected high/low values for the test documents.\n\n    Statistical Power Assessment:\n    Exploratory analysis - results are descriptive and suggestive rather than conclusive (N=2).\n    Statistical power is far too low for any inferential claims.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             'document_name', 'positive_sentiment_raw', and\n                             'negative_sentiment_raw'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each document.\n              Returns None if the input data is invalid or missing required columns.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_columns = [\n            'document_name',\n            'positive_sentiment_raw',\n            'negative_sentiment_raw'\n        ]\n        if not all(col in data.columns for col in required_columns):\n            return None\n        \n        if data.empty:\n            return {\"status\": \"error\", \"message\": \"Input data is empty.\"}\n\n        # Since N=1 for each document, std will be 0, and mean will be the raw value.\n        # This is the formally correct approach for descriptive stats.\n        descriptive_stats = data.groupby('document_name').agg({\n            'positive_sentiment_raw': ['mean', 'std', 'min', 'max', 'count'],\n            'negative_sentiment_raw': ['mean', 'std', 'min', 'max', 'count']\n        }).reset_index()\n\n        # Flatten the multi-level column index\n        descriptive_stats.columns = ['_'.join(col).strip() if col[1] else col[0] for col in descriptive_stats.columns.values]\n        descriptive_stats.rename(columns={'document_name_': 'document_name'}, inplace=True)\n\n        results = {\n            \"analysis_type\": \"Descriptive Statistics (Tier 3 Exploratory)\",\n            \"sample_size\": len(data),\n            \"notes\": \"Results are purely descriptive due to extremely low sample size (N=2). No statistical inference is possible.\",\n            \"summary_by_document\": descriptive_stats.to_dict(orient='records')\n        }\n        \n        return results\n\n    except Exception as e:\n        # Log the exception in a real application\n        # print(f\"An error occurred in summarize_sentiment_scores: {e}\")\n        return None\n\ndef check_data_validity(data, **kwargs):\n    \"\"\"\n    Performs a data validity check on sentiment scores.\n\n    This function addresses the research question of whether the analysis agent can process\n    simple dimensional scoring by verifying the integrity of the output data. It checks for\n    the presence of data, ensures scores are within the expected [0.0, 1.0] range, and\n    reports on missing values.\n\n    Methodology:\n    This is a data quality assurance function, not a statistical test. It systematically\n    inspects the key dimensional score columns ('positive_sentiment_raw', 'negative_sentiment_raw',\n    'positive_sentiment_salience', 'negative_sentiment_salience', 'positive_sentiment_confidence',\n    'negative_sentiment_confidence') for two conditions:\n    1. Null / NaN values.\n    2. Values outside the valid range of [0.0, 1.0].\n    The function returns a summary report of these checks.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary summarizing the validity checks. Returns None if the\n              input data is empty or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data.empty:\n            return {\"status\": \"error\", \"message\": \"Input data is empty.\"}\n\n        score_columns = [\n            'positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence',\n            'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence'\n        ]\n        \n        # Ensure all expected columns are present, fill missing ones with NaN for uniform checking\n        for col in score_columns:\n            if col not in data.columns:\n                data[col] = np.nan\n\n        validity_report = {\n            \"analysis_type\": \"Data Validity Check\",\n            \"total_rows_checked\": len(data),\n            \"checks\": {}\n        }\n\n        # Check for missing values\n        missing_values = data[score_columns].isnull().sum().to_dict()\n        validity_report[\"checks\"][\"missing_values\"] = {k: int(v) for k, v in missing_values.items()}\n\n        # Check for out-of-range values\n        out_of_range_report = {}\n        for col in score_columns:\n            # Count values outside the [0, 1] range, ignoring NaNs\n            out_of_range_count = data[col].dropna().apply(lambda x: not (0.0 <= x <= 1.0)).sum()\n            out_of_range_report[col] = int(out_of_range_count)\n        \n        validity_report[\"checks\"][\"out_of_range_scores\"] = out_of_range_report\n\n        # Overall assessment\n        total_missing = sum(validity_report[\"checks\"][\"missing_values\"].values())\n        total_out_of_range = sum(validity_report[\"checks\"][\"out_of_range_scores\"].values())\n\n        if total_missing == 0 and total_out_of_range == 0:\n            validity_report[\"overall_status\"] = \"PASS\"\n            validity_report[\"summary\"] = \"All checked scores are present and within the valid [0.0, 1.0] range.\"\n        else:\n            validity_report[\"overall_status\"] = \"FAIL\"\n            validity_report[\"summary\"] = f\"Found {total_missing} missing value(s) and {total_out_of_range} out-of-range value(s).\"\n\n        return validity_report\n\n    except Exception as e:\n        # Log the exception in a real application\n        # print(f\"An error occurred in check_data_validity: {e}\")\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}