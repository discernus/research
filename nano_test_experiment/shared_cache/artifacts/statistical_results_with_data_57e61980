{'generation_metadata': {'status': 'success', 'functions_generated': 3, 'output_file': 'automatedstatisticalanalysisagent_functions.py', 'module_size': 12039, 'function_code_content': '"""\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: nano_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-09T17:58:21.233449+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n"""\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings(\'ignore\', category=RuntimeWarning)\n\n\ndef summarize_descriptive_statistics(data, **kwargs):\n    """\n    Calculates and returns overall descriptive statistics for the sentiment dimensions.\n\n    Methodology:\n    This function provides a high-level summary of the distribution of scores for all\n    available numerical dimensions in the dataset. It computes the count, mean, standard\n    deviation, minimum, maximum, and quartile values for each relevant column.\n    Given the extremely small sample size of this experiment (N=2), these statistics\n    serve as a basic data validation check rather than a meaningful summary of\n    distribution. This is a Tier 3 (Exploratory) analysis.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             including \'positive_sentiment_raw\' and \'negative_sentiment_raw\'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the descriptive statistics for numerical columns,\n              or None if the data is insufficient or an error occurs.\n    """\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        required_cols = [\'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        if not all(col in data.columns for col in required_cols):\n            return None\n        \n        # Select only numeric columns for description\n        numeric_data = data.select_dtypes(include=np.number)\n        if numeric_data.empty:\n            return None\n\n        # As a conservative practitioner, I must note the sample size.\n        n_samples = len(data)\n        \n        stats = numeric_data.describe().to_dict()\n\n        return {\n            "summary": f"Exploratory analysis - results are suggestive rather than conclusive (N={n_samples})",\n            "descriptive_statistics": stats\n        }\n\n    except Exception as e:\n        # In a production environment, we would log the error `e`.\n        return None\n\ndef compare_sentiment_scores(data, **kwargs):\n    """\n    Compares mean sentiment scores between document types.\n\n    Methodology:\n    This function directly addresses the research question about distinguishing between\n    positive and negative documents. It groups the data by \'document_name\' and\n    calculates the mean score for \'positive_sentiment_raw\' and \'negative_sentiment_raw\'\n    for each document.\n\n    This is a Tier 3 (Exploratory) analysis. With a sample size of N=1 per group\n    (one positive, one negative document), inferential tests are not possible or\n    meaningful. The output is purely descriptive, intended to validate that the\n    positive test document has a high positive score and the negative test document\n    has a high negative score, as per the experiment\'s \'Expected Outcomes\'.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             \'document_name\', \'positive_sentiment_raw\', and\n                             \'negative_sentiment_raw\'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary with mean scores per document, or None if data is invalid.\n    """\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        required_cols = [\'document_name\', \'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        # There is no corpus manifest, so grouping is based on the document filename.\n        # This is the only available grouping variable.\n        if \'document_name\' not in data.columns:\n            return None\n            \n        n_samples = len(data)\n        \n        # For N=1 per group, mean() simply returns the value, which is what is needed for this validation.\n        grouped_scores = data.groupby(\'document_name\')[[\'positive_sentiment_raw\', \'negative_sentiment_raw\']].mean().to_dict(\'index\')\n\n        if not grouped_scores:\n            return None\n\n        return {\n            "summary": f"Exploratory analysis - results are suggestive rather than conclusive (N={n_samples})",\n            "comparison_by_document": grouped_scores\n        }\n\n    except Exception as e:\n        # In a production environment, we would log the error `e`.\n        return None\n\ndef analyze_sentiment_correlation(data, **kwargs):\n    """\n    Calculates the correlation between positive and negative sentiment scores.\n\n    Methodology:\n    This function computes the Pearson correlation coefficient (r) between the\n    \'positive_sentiment_raw\' and \'negative_sentiment_raw\' dimensions. This is a\n    standard analysis to check if the two dimensions are independent or inversely\n    related.\n\n    This is a Tier 3 (Exploratory) analysis.\n    CRITICAL CAVEAT: The total sample size for this experiment is N=2. A correlation\n    coefficient calculated on two data points is not statistically meaningful or\n    interpretable. It will mathematically result in -1, 0, or +1. This function is\n    provided for framework completeness and to demonstrate functionality, but its\n    output for this specific experiment should NOT be used for any conclusion.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             \'positive_sentiment_raw\' and \'negative_sentiment_raw\'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the correlation matrix, or None if data is\n              insufficient.\n    """\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        required_cols = [\'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        # A correlation requires at least 2 data points.\n        if len(data) < 2:\n            return None\n            \n        n_samples = len(data)\n\n        correlation_matrix = data[required_cols].corr(method=\'pearson\')\n        \n        # Extract the specific correlation value of interest\n        correlation_value = correlation_matrix.loc[\'positive_sentiment_raw\', \'negative_sentiment_raw\']\n\n        return {\n            "summary": f"Exploratory analysis - results are suggestive rather than conclusive (N={n_samples})",\n            "caveat": "Correlation on N<15 is highly unstable. With N=2, the result is not statistically interpretable.",\n            "pearson_correlation_coefficient": correlation_value,\n            "correlation_matrix": correlation_matrix.to_dict()\n        }\n\n    except Exception as e:\n        # In a production environment, we would log the error `e`.\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    """\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    """\n    results = {\n        \'analysis_metadata\': {\n            \'timestamp\': pd.Timestamp.now().isoformat(),\n            \'sample_size\': len(data),\n            \'alpha_level\': alpha,\n            \'variables_analyzed\': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith((\'calculate_\', \'perform_\', \'test_\')) and \n            name != \'run_complete_statistical_analysis\'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if \'alpha\' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {\'error\': f\'Analysis failed: {str(e)}\'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    """\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    """\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    """\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    """\n    report_lines = []\n    report_lines.append("STATISTICAL ANALYSIS SUMMARY REPORT")\n    report_lines.append("=" * 50)\n    \n    metadata = analysis_results.get(\'analysis_metadata\', {})\n    report_lines.append(f"Analysis Timestamp: {metadata.get(\'timestamp\', \'Unknown\')}")\n    report_lines.append(f"Sample Size: {metadata.get(\'sample_size\', \'Unknown\')}")\n    report_lines.append(f"Alpha Level: {metadata.get(\'alpha_level\', \'Unknown\')}")\n    report_lines.append(f"Variables: {len(metadata.get(\'variables_analyzed\', []))}")\n    report_lines.append("")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != \'analysis_metadata\' and isinstance(result, dict):\n            if \'error\' not in result:\n                report_lines.append(f"{analysis_name.replace(\'_\', \' \').title()}:")\n                \n                # Extract key statistics based on analysis type\n                if \'p_value\' in result:\n                    p_val = result[\'p_value\']\n                    significance = "significant" if p_val < metadata.get(\'alpha_level\', 0.05) else "not significant"\n                    report_lines.append(f"  - p-value: {p_val:.4f} ({significance})")\n                \n                if \'effect_size\' in result:\n                    report_lines.append(f"  - Effect size: {result[\'effect_size\']:.4f}")\n                \n                if \'correlation_matrix\' in result:\n                    report_lines.append(f"  - Correlation matrix generated with {len(result[\'correlation_matrix\'])} variables")\n                \n                if \'cronbach_alpha\' in result:\n                    alpha_val = result[\'cronbach_alpha\']\n                    reliability = "excellent" if alpha_val > 0.9 else "good" if alpha_val > 0.8 else "acceptable" if alpha_val > 0.7 else "questionable"\n                    report_lines.append(f"  - Cronbach\'s α: {alpha_val:.3f} ({reliability})")\n                \n                report_lines.append("")\n            else:\n                report_lines.append(f"{analysis_name}: ERROR - {result[\'error\']}")\n                report_lines.append("")\n    \n    return "\\n".join(report_lines)\n', 'cached_with_code': True}, 'statistical_data': {'analyze_sentiment_correlation': {'summary': 'Exploratory analysis - results are suggestive rather than conclusive (N=2)', 'caveat': 'Correlation on N<15 is highly unstable. With N=2, the result is not statistically interpretable.', 'pearson_correlation_coefficient': -1.0, 'correlation_matrix': {'positive_sentiment_raw': {'positive_sentiment_raw': 1.0, 'negative_sentiment_raw': -1.0}, 'negative_sentiment_raw': {'positive_sentiment_raw': -1.0, 'negative_sentiment_raw': 1.0}}}, 'compare_sentiment_scores': {'summary': 'Exploratory analysis - results are suggestive rather than conclusive (N=2)', 'comparison_by_document': {'negative_test.txt': {'positive_sentiment_raw': 0.0, 'negative_sentiment_raw': 1.0}, 'positive_test.txt': {'positive_sentiment_raw': 1.0, 'negative_sentiment_raw': 0.0}}}, 'generate_statistical_summary_report': 'STATISTICAL ANALYSIS SUMMARY REPORT\n==================================================\nAnalysis Timestamp: Unknown\nSample Size: Unknown\nAlpha Level: Unknown\nVariables: 0\n', 'perform_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-09-09T21:06:04.227018', 'sample_size': 2, 'alpha_level': 0.05, 'variables_analyzed': ['positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence', 'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence']}}, 'run_complete_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-09-09T21:06:04.230775', 'sample_size': 2, 'alpha_level': 0.05, 'variables_analyzed': ['positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence', 'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence']}}, 'summarize_descriptive_statistics': {'summary': 'Exploratory analysis - results are suggestive rather than conclusive (N=2)', 'descriptive_statistics': {'positive_sentiment_raw': {'count': 2.0, 'mean': 0.5, 'std': 0.7071067811865476, 'min': 0.0, '25%': 0.25, '50%': 0.5, '75%': 0.75, 'max': 1.0}, 'positive_sentiment_salience': {'count': 2.0, 'mean': 0.5, 'std': 0.7071067811865476, 'min': 0.0, '25%': 0.25, '50%': 0.5, '75%': 0.75, 'max': 1.0}, 'positive_sentiment_confidence': {'count': 2.0, 'mean': 0.985, 'std': 0.021213203435596444, 'min': 0.97, '25%': 0.9775, '50%': 0.985, '75%': 0.9924999999999999, 'max': 1.0}, 'negative_sentiment_raw': {'count': 2.0, 'mean': 0.5, 'std': 0.7071067811865476, 'min': 0.0, '25%': 0.25, '50%': 0.5, '75%': 0.75, 'max': 1.0}, 'negative_sentiment_salience': {'count': 2.0, 'mean': 0.5, 'std': 0.7071067811865476, 'min': 0.0, '25%': 0.25, '50%': 0.5, '75%': 0.75, 'max': 1.0}, 'negative_sentiment_confidence': {'count': 2.0, 'mean': 1.0, 'std': 0.0, 'min': 1.0, '25%': 1.0, '50%': 1.0, '75%': 1.0, 'max': 1.0}}}}, 'status': 'success_with_data', 'validation_passed': True}