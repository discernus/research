{
  "raw_analysis_results": [
    {
      "analysis_id": "analysis_c9dfcd84fda4",
      "result_hash": "11af23d5be6b6aad0a8d39b8390df16a6ade242d068a9aaf440b356655deb7ca",
      "result_content": {
        "analysis_id": "analysis_c9dfcd84fda4",
        "agent_name": "EnhancedAnalysisAgent",
        "agent_version": "enhanced_v2.1_raw_output",
        "experiment_name": "nano_test_experiment",
        "model_used": "vertex_ai/gemini-2.5-flash-lite",
        "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.9,\n    \"analysis_notes\": \"Analysis conducted using three independent approaches (Evidence-First, Context-Weighted, Pattern-Based) and aggregated using median scores as per instructions.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"6950ccbc6b14\",\n      \"document_name\": \"positive_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.95,\n          \"salience\": 1.0,\n          \"confidence\": 0.95\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"This is a wonderful day! Everything is going perfectly. I feel great about the future. Success is everywhere. The team did an excellent job. We're achieving amazing results. Optimism fills the air. What a fantastic opportunity! I'm thrilled with the progress. Everything looks bright and promising.\",\n          \"confidence\": 1.0,\n          \"context_type\": \"entire_document_summary\"\n        },\n        {\n          \"dimension\": \"negative_sentiment\",\n          \"quote_text\": \"\",\n          \"confidence\": 0.0,\n          \"context_type\": \"explanation\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
        "evidence_hash": "daebab3bca466ad426b1098a2b2837c2aad75f7910345aa9bf9a83b9ad138b23",
        "execution_metadata": {
          "start_time": "2025-08-30T21:48:50.074961+00:00",
          "end_time": "2025-08-30T21:48:53.614571+00:00",
          "duration_seconds": 3.539554
        },
        "input_artifacts": {
          "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
          "document_hashes": [
            "6950ccbc6b14eabf43508448cf888b2c590f44e549a32b8ae60e943710e74019"
          ],
          "num_documents": 1
        },
        "provenance": {
          "security_boundary": {
            "experiment_name": "nano_test_experiment",
            "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
            "boundary_type": "filesystem",
            "security_level": "experiment_scoped"
          },
          "audit_session_id": "20250830T214849Z_5902644f"
        }
      },
      "cached": true
    },
    {
      "analysis_id": "analysis_ef118072a668",
      "result_hash": "968563dd00acc86441958b5ea18f9eb7129624dbf68a3ad668b248c01f0046b1",
      "result_content": {
        "analysis_id": "analysis_ef118072a668",
        "agent_name": "EnhancedAnalysisAgent",
        "agent_version": "enhanced_v2.1_raw_output",
        "experiment_name": "nano_test_experiment",
        "model_used": "vertex_ai/gemini-2.5-flash-lite",
        "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"Sentiment Binary Framework\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.9,\n    \"analysis_notes\": \"Analysis performed using three independent approaches (Evidence-First, Context-Weighted, Pattern-Based) and aggregated via median scores as per instructions.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"{{artifact_id}}\",\n      \"document_name\": \"negative_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.1,\n          \"salience\": 0.1,\n          \"confidence\": 0.8\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 0.9,\n          \"salience\": 0.9,\n          \"confidence\": 0.95\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"What a wonderful time!\",\n          \"confidence\": 0.3,\n          \"context_type\": \"weak indicator\"\n        },\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"The team did an excellent job.\",\n          \"confidence\": 0.2,\n          \"context_type\": \"weak indicator\"\n        },\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"What a wonderful time!\",\n          \"confidence\": 0.3,\n          \"context_type\": \"weak indicator\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
        "evidence_hash": "2945ff174ddfb18653146d34da74c4ff67955e9c150998da6a8002d6f24cdb8f",
        "execution_metadata": {
          "start_time": "2025-08-30T21:48:53.622796+00:00",
          "end_time": "2025-08-30T21:48:55.271062+00:00",
          "duration_seconds": 1.648231
        },
        "input_artifacts": {
          "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
          "document_hashes": [
            "1df25093a747678c5dbe724ecfb963b88049f9a7a536a3f23f064fd592744f91"
          ],
          "num_documents": 1
        },
        "provenance": {
          "security_boundary": {
            "experiment_name": "nano_test_experiment",
            "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
            "boundary_type": "filesystem",
            "security_level": "experiment_scoped"
          },
          "audit_session_id": "20250830T214849Z_5902644f"
        }
      },
      "cached": true
    }
  ],
  "derived_metrics_results": {
    "status": "completed",
    "derived_metrics_hash": "64d64f8b04eb2067a020588db6461bca910b312301cff068827669085bcd0700",
    "functions_generated": 6,
    "derived_metrics_results": {
      "generation_metadata": {
        "status": "success",
        "functions_generated": 6,
        "output_file": "automatedderivedmetricsagent_functions.py",
        "module_size": 17271,
        "function_code_content": "\"\"\"\nAutomated Derived Metrics Functions\n===================================\n\nGenerated by AutomatedDerivedMetricsAgent for experiment: Test Experiment\nDescription: Test experiment for derived metrics\nGenerated: 2025-08-30T21:51:27.314008+00:00\n\nThis module contains automatically generated calculation functions for derived metrics\nas specified in the framework's natural language descriptions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom typing import Optional, Dict, Any\n\n\ndef calculate_identity_tension(data, **kwargs):\n    \"\"\"\n    Calculate identity_tension: Conflict between tribal dominance and individual dignity dimensions\n    \n    Formula: identity_tension = Negative Sentiment - Positive Sentiment\n    This formula assumes that 'Negative Sentiment' represents aspects of tribal dominance\n    (e.g., group-think, conflict) and 'Positive Sentiment' represents individual dignity\n    (e.g., personal well-being, respect). Higher values indicate greater tension or conflict.\n    The result ranges from -1.0 to 1.0, given input sentiment scores are between 0.0 and 1.0.\n\n    Args:\n        data: pandas DataFrame or Series representing a single row of analysis data.\n              Expected to contain 'Positive Sentiment' and 'Negative Sentiment' columns.\n        **kwargs: Additional parameters (not used in this calculation but included for framework compatibility).\n        \n    Returns:\n        float: Calculated identity tension score, or None if required data is missing or invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # Ensure data is treated as a Series for consistent access, assuming single-row processing\n        if isinstance(data, pd.DataFrame):\n            if data.empty:\n                return None\n            row_data = data.iloc[0]\n        elif isinstance(data, pd.Series):\n            row_data = data\n        else:\n            # Handle unexpected data type\n            return None\n\n        # Retrieve sentiment scores using .get() for safe column access (returns None if column not found)\n        positive_sentiment = row_data.get('Positive Sentiment')\n        negative_sentiment = row_data.get('Negative Sentiment')\n\n        # Check for missing columns or NaN values\n        if pd.isna(positive_sentiment) or pd.isna(negative_sentiment):\n            return None\n        \n        # Ensure values are numeric and within expected range (0.0-1.0)\n        # Although calculation works with any numeric, validation helps with data integrity.\n        if not (isinstance(positive_sentiment, (int, float)) and isinstance(negative_sentiment, (int, float)) and\n                0.0 <= positive_sentiment <= 1.0 and 0.0 <= negative_sentiment <= 1.0):\n            return None\n\n        # Calculate identity tension\n        # Tension is defined as the difference between negative and positive sentiment.\n        # A higher negative sentiment relative to positive sentiment indicates more \"conflict\" or \"tension\".\n        tension = float(negative_sentiment - positive_sentiment)\n        \n        return tension\n\n    except Exception:\n        # Catch any unexpected errors during calculation and return None\n        return None\n\ndef calculate_emotional_balance(data, **kwargs):\n    \"\"\"\n    Calculate emotional_balance: Difference between hope and fear scores.\n    \n    Formula: emotional_balance = hope - fear\n    \n    Args:\n        data: pandas Series (representing a single row of data) containing score dimensions.\n              CRITICAL NOTE: While the 'ACTUAL DATA STRUCTURE' section did not explicitly\n              list 'hope' or 'fear' as column names, this function assumes the input\n              'data' Series will contain columns named 'hope' and 'fear' to perform\n              the requested calculation. If these columns are not present, or their\n              values are not numeric/missing, the function will return None.\n        **kwargs: Additional parameters (not used in this calculation).\n        \n    Returns:\n        float: The calculated emotional balance (hope - fear).\n        None: If 'hope' or 'fear' columns are missing in the input `data`,\n              or if their values are non-numeric or NaN.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # The 'data' parameter is expected to be a pandas Series (a single row).\n        # Check if 'hope' and 'fear' keys (column names) exist in the Series' index.\n        if 'hope' not in data.index or 'fear' not in data.index:\n            return None\n\n        hope_score = data['hope']\n        fear_score = data['fear']\n\n        # Attempt to convert scores to numeric values, coercing any errors to NaN.\n        hope_score_numeric = pd.to_numeric(hope_score, errors='coerce')\n        fear_score_numeric = pd.to_numeric(fear_score, errors='coerce')\n\n        # Check if either of the converted scores is NaN (missing or non-numeric).\n        if pd.isna(hope_score_numeric) or pd.isna(fear_score_numeric):\n            return None\n\n        # Perform the emotional balance calculation.\n        emotional_balance = hope_score_numeric - fear_score_numeric\n        \n        # Return the result as a standard Python float.\n        return float(emotional_balance)\n\n    except Exception:\n        # Catch any unexpected errors during the process and return None,\n        # ensuring graceful handling as per requirements.\n        return None\n\ndef calculate_success_climate(data, **kwargs):\n    \"\"\"\n    Calculate success_climate: Difference between compersion and envy scores.\n    Formula: Positive Sentiment - Negative Sentiment\n    \n    Args:\n        data: pandas DataFrame (or Series for a single row) containing 'Positive Sentiment'\n              and 'Negative Sentiment' scores.\n        **kwargs: Additional parameters (not used in this calculation).\n        \n    Returns:\n        float: Calculated success_climate or None if insufficient data.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # Based on the framework's \"Analytical Methodology\", 'Positive Sentiment'\n        # and 'Negative Sentiment' are defined as the core dimensions.\n        # It is assumed that 'compersion' conceptually maps to 'Positive Sentiment'\n        # and 'enfy' maps to 'Negative Sentiment' for this calculation.\n        positive_sentiment_col = 'Positive Sentiment'\n        negative_sentiment_col = 'Negative Sentiment'\n        \n        # Safely retrieve the scores from the 'data' object.\n        # The .get() method is used for robustness, returning a default (np.nan)\n        # if the column is not found or the value is missing/non-numeric.\n        # pd.to_numeric handles conversion to float and coerces errors to NaN.\n        positive_score = pd.to_numeric(data.get(positive_sentiment_col, np.nan), errors='coerce')\n        negative_score = pd.to_numeric(data.get(negative_sentiment_col, np.nan), errors='coerce')\n\n        # If 'data' was a single-row DataFrame, .get() might return a pandas Series\n        # containing one value. Extract the scalar value if that's the case.\n        if isinstance(positive_score, pd.Series) and not positive_score.empty:\n            positive_score = positive_score.iloc[0]\n        if isinstance(negative_score, pd.Series) and not negative_score.empty:\n            negative_score = negative_score.iloc[0]\n            \n        # Check if either of the scores is NaN after retrieval and conversion.\n        # This handles cases where columns are missing or contain non-numeric data.\n        if pd.isna(positive_score) or pd.isna(negative_score):\n            return None\n            \n        # Perform the calculation: Positive Sentiment minus Negative Sentiment.\n        result = float(positive_score - negative_score)\n        \n        return result\n        \n    except Exception:\n        # Catch any unexpected errors that might occur during processing,\n        # ensuring the function fails gracefully by returning None.\n        return None\n\ndef calculate_relational_climate(data, **kwargs):\n    \"\"\"\n    Calculate relational_climate: Difference between amity and enmity scores.\n    Formula: Positive Sentiment - Negative Sentiment\n\n    Args:\n        data: pandas Series (representing a single row of a DataFrame) with\n              'Positive Sentiment' and 'Negative Sentiment' scores.\n        **kwargs: Additional parameters (not used in this calculation but\n                  included for framework compatibility).\n\n    Returns:\n        float: The calculated relational climate score.\n        None: If required 'Positive Sentiment' or 'Negative Sentiment' data is\n              missing, non-numeric, or NaN.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # Ensure the required score dimensions exist and are not NaN\n        if 'Positive Sentiment' in data and \\\n           'Negative Sentiment' in data and \\\n           pd.notna(data['Positive Sentiment']) and \\\n           pd.notna(data['Negative Sentiment']):\n\n            amity_score = data['Positive Sentiment']\n            enmity_score = data['Negative Sentiment']\n\n            # Ensure scores are numeric before calculation\n            if pd.api.types.is_numeric_dtype(type(amity_score)) and \\\n               pd.api.types.is_numeric_dtype(type(enmity_score)):\n                \n                result = float(amity_score - enmity_score)\n                return result\n            else:\n                # One or both scores are not numeric\n                return None\n        else:\n            # Required columns are missing or contain NaN values\n            return None\n    except Exception:\n        # Catch any other unexpected errors during calculation (e.g., type conversion)\n        return None\n\ndef calculate_goal_orientation(data, **kwargs):\n    \"\"\"\n    Calculate goal_orientation: Difference between cohesive goals and fragmentative goals\n\n    Formula: Positive Sentiment - Negative Sentiment\n\n    Args:\n        data: pandas Series (representing a single row from a DataFrame) containing dimension scores.\n              Expected to have columns named 'Positive Sentiment' and 'Negative Sentiment',\n              as defined in the framework's 'Dimensions' section.\n        **kwargs: Additional keyword arguments (not used in this calculation).\n\n    Returns:\n        float: The calculated goal orientation score, or None if required data is missing or invalid.\n    \"\"\"\n    import pandas as pd\n\n    try:\n        # Access the sentiment scores.\n        # These column names are derived directly from the 'Dimensions' section of the\n        # framework context, where 'Positive Sentiment' and 'Negative Sentiment' are\n        # explicitly defined as the framework's measured dimensions.\n        # .get() is used for graceful handling if a column does not exist, returning None.\n        positive_sentiment = data.get('Positive Sentiment')\n        negative_sentiment = data.get('Negative Sentiment')\n\n        # Handle missing data or NaN values.\n        # pd.isna() is used as it correctly identifies various forms of missing/NaN values.\n        if pd.isna(positive_sentiment) or pd.isna(negative_sentiment):\n            return None\n\n        # Attempt to convert retrieved values to float.\n        # This step will raise a ValueError if values are not numeric (e.g., strings).\n        positive_sentiment_float = float(positive_sentiment)\n        negative_sentiment_float = float(negative_sentiment)\n\n        # Perform the calculation: Cohesive goals (Positive) minus Fragmentative goals (Negative).\n        goal_orientation_score = positive_sentiment_float - negative_sentiment_float\n\n        return float(goal_orientation_score)\n\n    except (ValueError, TypeError, AttributeError):\n        # Catch specific errors related to data type conversion (e.g., if sentiment values are non-numeric),\n        # or if `data` is not a Series-like object that supports `.get()`.\n        return None\n    except Exception:\n        # Catch any other unexpected errors that might occur during the process,\n        # ensuring the function always returns None on failure as per requirements.\n        return None\n\ndef calculate_overall_cohesion_index(data, **kwargs):\n    \"\"\"\n    Calculate overall_cohesion_index: Comprehensive measure combining all dimensions.\n    \n    Formula: |analysis_result - raw_analysis_response|\n    This formula quantifies the absolute difference between the values of 'analysis_result' and\n    'raw_analysis_response'. It serves as a measure of sentiment clarity or \"cohesion\".\n    A higher absolute difference indicates a greater distinction between these two primary numeric\n    outputs, suggesting a more definitive or unambiguous overall sentiment.\n    \n    Args:\n        data: pandas DataFrame (expected to be a single row or pandas Series)\n              containing the necessary dimensions. Expected columns: 'analysis_result'\n              and 'raw_analysis_response'.\n        **kwargs: Additional parameters (not used in this specific calculation).\n        \n    Returns:\n        float: The calculated overall cohesion index (a value between 0.0 and 1.0 if\n               the input dimensions are within the 0.0-1.0 range), or None if required\n               columns are missing, or contain non-numeric/NaN values.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # Ensure data is a pandas Series or a single-row DataFrame\n        if not isinstance(data, (pd.Series, pd.DataFrame)):\n            return None\n\n        # Extract values based on whether 'data' is a Series or a DataFrame\n        val_analysis_result = None\n        val_raw_analysis_response = None\n\n        if isinstance(data, pd.Series):\n            val_analysis_result = data.get('analysis_result')\n            val_raw_analysis_response = data.get('raw_analysis_response')\n        elif isinstance(data, pd.DataFrame):\n            if data.empty:\n                return None\n            if 'analysis_result' in data.columns and 'raw_analysis_response' in data.columns:\n                # Assuming single-row DataFrame, take the first element\n                val_analysis_result = data['analysis_result'].iloc[0]\n                val_raw_analysis_response = data['raw_analysis_response'].iloc[0]\n            else:\n                # Required columns are not present in the DataFrame\n                return None\n        \n        # Handle cases where values might be missing from .get() or .iloc[0] (e.g., column not found)\n        if val_analysis_result is None or val_raw_analysis_response is None:\n            return None\n\n        # Handle NaN or non-numeric values\n        if pd.isna(val_analysis_result) or pd.isna(val_raw_analysis_response):\n            return None\n        \n        # Ensure values are numeric before calculation\n        if not isinstance(val_analysis_result, (int, float)) or not isinstance(val_raw_analysis_response, (int, float)):\n            return None\n\n        # Calculate the overall cohesion index\n        # This calculation uses 'analysis_result' and 'raw_analysis_response' as the\n        # primary numeric dimensions, adhering to the requirement to use exact column names\n        # from the 'ACTUAL DATA STRUCTURE'.\n        cohesion_index = abs(val_analysis_result - val_raw_analysis_response)\n        \n        return float(cohesion_index)\n        \n    except Exception:\n        # Catch any unexpected errors and return None\n        return None\n\ndef calculate_all_derived_metrics(data: pd.DataFrame) -> Dict[str, Optional[float]]:\n    \"\"\"\n    Calculate all derived metrics for the given dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        \n    Returns:\n        Dictionary mapping metric names to calculated values\n    \"\"\"\n    results = {}\n    \n    # Get all calculation functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith('calculate_') and \n            name not in ['calculate_all_derived_metrics', 'calculate_derived_metrics']):\n            try:\n                results[name.replace('calculate_', '')] = obj(data)\n            except Exception as e:\n                results[name.replace('calculate_', '')] = None\n                \n    return results\n\n\ndef calculate_derived_metrics(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Template-compatible wrapper function for derived metrics calculation.\n    \n    This function is called by the universal notebook template and returns\n    the original data with additional derived metric columns.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        \n    Returns:\n        DataFrame with original data plus derived metric columns\n    \"\"\"\n    # Calculate all derived metrics\n    derived_metrics = calculate_all_derived_metrics(data)\n    \n    # Create a copy of the original data\n    result = data.copy()\n    \n    # Add derived metrics as new columns\n    for metric_name, metric_value in derived_metrics.items():\n        if metric_value is not None:\n            # For scalar metrics, broadcast to all rows\n            result[metric_name] = metric_value\n        else:\n            # For failed calculations, use NaN\n            result[metric_name] = np.nan\n    \n    return result\n",
        "cached_with_code": true
      },
      "derived_metrics_data": {
        "status": "success",
        "original_count": 2,
        "derived_count": 2,
        "derived_metrics": [
          {
            "analysis_id": "analysis_c9dfcd84fda4",
            "result_hash": "11af23d5be6b6aad0a8d39b8390df16a6ade242d068a9aaf440b356655deb7ca",
            "result_content": {
              "analysis_id": "analysis_c9dfcd84fda4",
              "agent_name": "EnhancedAnalysisAgent",
              "agent_version": "enhanced_v2.1_raw_output",
              "experiment_name": "nano_test_experiment",
              "model_used": "vertex_ai/gemini-2.5-flash-lite",
              "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.9,\n    \"analysis_notes\": \"Analysis conducted using three independent approaches (Evidence-First, Context-Weighted, Pattern-Based) and aggregated using median scores as per instructions.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"6950ccbc6b14\",\n      \"document_name\": \"positive_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.95,\n          \"salience\": 1.0,\n          \"confidence\": 0.95\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"This is a wonderful day! Everything is going perfectly. I feel great about the future. Success is everywhere. The team did an excellent job. We're achieving amazing results. Optimism fills the air. What a fantastic opportunity! I'm thrilled with the progress. Everything looks bright and promising.\",\n          \"confidence\": 1.0,\n          \"context_type\": \"entire_document_summary\"\n        },\n        {\n          \"dimension\": \"negative_sentiment\",\n          \"quote_text\": \"\",\n          \"confidence\": 0.0,\n          \"context_type\": \"explanation\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
              "evidence_hash": "daebab3bca466ad426b1098a2b2837c2aad75f7910345aa9bf9a83b9ad138b23",
              "execution_metadata": {
                "start_time": "2025-08-30T21:48:50.074961+00:00",
                "end_time": "2025-08-30T21:48:53.614571+00:00",
                "duration_seconds": 3.539554
              },
              "input_artifacts": {
                "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
                "document_hashes": [
                  "6950ccbc6b14eabf43508448cf888b2c590f44e549a32b8ae60e943710e74019"
                ],
                "num_documents": 1
              },
              "provenance": {
                "security_boundary": {
                  "experiment_name": "nano_test_experiment",
                  "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
                  "boundary_type": "filesystem",
                  "security_level": "experiment_scoped"
                },
                "audit_session_id": "20250830T214849Z_5902644f"
              }
            },
            "cached": true
          },
          {
            "analysis_id": "analysis_ef118072a668",
            "result_hash": "968563dd00acc86441958b5ea18f9eb7129624dbf68a3ad668b248c01f0046b1",
            "result_content": {
              "analysis_id": "analysis_ef118072a668",
              "agent_name": "EnhancedAnalysisAgent",
              "agent_version": "enhanced_v2.1_raw_output",
              "experiment_name": "nano_test_experiment",
              "model_used": "vertex_ai/gemini-2.5-flash-lite",
              "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"Sentiment Binary Framework\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.9,\n    \"analysis_notes\": \"Analysis performed using three independent approaches (Evidence-First, Context-Weighted, Pattern-Based) and aggregated via median scores as per instructions.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"{{artifact_id}}\",\n      \"document_name\": \"negative_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.1,\n          \"salience\": 0.1,\n          \"confidence\": 0.8\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 0.9,\n          \"salience\": 0.9,\n          \"confidence\": 0.95\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"What a wonderful time!\",\n          \"confidence\": 0.3,\n          \"context_type\": \"weak indicator\"\n        },\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"The team did an excellent job.\",\n          \"confidence\": 0.2,\n          \"context_type\": \"weak indicator\"\n        },\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"What a wonderful time!\",\n          \"confidence\": 0.3,\n          \"context_type\": \"weak indicator\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
              "evidence_hash": "2945ff174ddfb18653146d34da74c4ff67955e9c150998da6a8002d6f24cdb8f",
              "execution_metadata": {
                "start_time": "2025-08-30T21:48:53.622796+00:00",
                "end_time": "2025-08-30T21:48:55.271062+00:00",
                "duration_seconds": 1.648231
              },
              "input_artifacts": {
                "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
                "document_hashes": [
                  "1df25093a747678c5dbe724ecfb963b88049f9a7a536a3f23f064fd592744f91"
                ],
                "num_documents": 1
              },
              "provenance": {
                "security_boundary": {
                  "experiment_name": "nano_test_experiment",
                  "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
                  "boundary_type": "filesystem",
                  "security_level": "experiment_scoped"
                },
                "audit_session_id": "20250830T214849Z_5902644f"
              }
            },
            "cached": true
          }
        ],
        "columns_added": []
      },
      "status": "success_with_data",
      "validation_passed": true
    }
  },
  "statistical_results": {
    "generation_metadata": {
      "status": "success",
      "functions_generated": 4,
      "output_file": "automatedstatisticalanalysisagent_functions.py",
      "module_size": 15084,
      "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: nano_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-08-30T21:51:45.375829+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_overall_sentiment_descriptives(data, **kwargs):\n    \"\"\"\n    Calculates overall descriptive statistics for all sentiment-related dimensions\n    across the entire dataset.\n\n    Statistical Methodology:\n    This function computes basic descriptive statistics (count, mean, standard deviation,\n    minimum, 25th percentile, 50th percentile (median), 75th percentile, and maximum)\n    for the raw sentiment scores, salience, and confidence metrics. These statistics\n    provide a summary of the central tendency, dispersion, and range of the sentiment\n    measurements across all documents.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the analysis data with\n                             columns like 'positive_sentiment_raw',\n                             'negative_sentiment_raw', etc.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A dictionary where keys are the sentiment-related column names and\n              values are dictionaries of their descriptive statistics, or None\n              if the input data is empty or required columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if not isinstance(data, pd.DataFrame) or data.empty:\n            return None\n\n        sentiment_cols = [\n            'positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence',\n            'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence'\n        ]\n\n        # Filter for columns that actually exist in the DataFrame\n        existing_cols = [col for col in sentiment_cols if col in data.columns]\n\n        if not existing_cols:\n            return None\n\n        descriptives = data[existing_cols].describe().to_dict()\n\n        # Convert numpy types to native Python types for JSON serialization\n        for col, stats in descriptives.items():\n            for stat_name, value in stats.items():\n                if isinstance(value, (np.float32, np.float64)):\n                    stats[stat_name] = float(value)\n                elif isinstance(value, (np.int32, np.int64)):\n                    stats[stat_name] = int(value)\n        return descriptives\n    except Exception as e:\n        # Log the exception for debugging purposes\n        # print(f\"Error in calculate_overall_sentiment_descriptives: {e}\")\n        return None\n\ndef calculate_document_sentiment_descriptives(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for sentiment-related dimensions, grouped by\n    'document_name'.\n\n    Statistical Methodology:\n    This function groups the input DataFrame by the 'document_name' column and then\n    computes descriptive statistics (mean, standard deviation, minimum, maximum,\n    and quartiles) for each sentiment-related metric within each document. This\n    allows for a direct comparison of sentiment scores across different documents,\n    addressing the research question about clear distinctions between test documents.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the analysis data with\n                             'document_name' and sentiment-related columns.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A nested dictionary where the first level keys are 'document_name's,\n              and the second level keys are sentiment-related column names, with\n              values being dictionaries of their descriptive statistics. Returns\n              None if input data is invalid or required columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if not isinstance(data, pd.DataFrame) or data.empty:\n            return None\n        if 'document_name' not in data.columns:\n            return None\n\n        sentiment_cols = [\n            'positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence',\n            'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence'\n        ]\n\n        existing_cols = [col for col in sentiment_cols if col in data.columns]\n        if not existing_cols:\n            return None\n\n        # Group by document_name and describe\n        grouped_descriptives = data.groupby('document_name')[existing_cols].describe().to_dict()\n\n        # Reformat the dictionary for better readability and JSON serialization\n        results = {}\n        for (col, stat_name), value in grouped_descriptives.items():\n            for doc_name, val in value.items():\n                if doc_name not in results:\n                    results[doc_name] = {}\n                if col not in results[doc_name]:\n                    results[doc_name][col] = {}\n                if isinstance(val, (np.float32, np.float64)):\n                    results[doc_name][col][stat_name] = float(val)\n                elif isinstance(val, (np.int32, np.int64)):\n                    results[doc_name][col][stat_name] = int(val)\n                else:\n                    results[doc_name][col][stat_name] = val\n        return results\n    except Exception as e:\n        # print(f\"Error in calculate_document_sentiment_descriptives: {e}\")\n        return None\n\ndef compare_positive_negative_within_documents(data, **kwargs):\n    \"\"\"\n    Compares the raw positive and negative sentiment scores within each document.\n\n    Statistical Methodology:\n    For each document, this function calculates the difference between its\n    'positive_sentiment_raw' and 'negative_sentiment_raw' scores. This direct\n    comparison helps to assess the dominant sentiment within a given document\n    and validate if the pipeline correctly assigns higher scores to the\n    expected sentiment dimension for each test document (e.g., positive_test.txt\n    should have positive > negative). Given the nature of the experiment (N=1\n    observation per document for these scores), a simple difference is the most\n    appropriate metric rather than inferential tests.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing 'document_name',\n                             'positive_sentiment_raw', and 'negative_sentiment_raw' columns.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A dictionary where keys are 'document_name's and values are the\n              difference (positive_sentiment_raw - negative_sentiment_raw) for\n              that document. Returns None if input data is invalid or columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if not isinstance(data, pd.DataFrame) or data.empty:\n            return None\n        required_cols = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        results = {}\n        for doc_name, group in data.groupby('document_name'):\n            if not group.empty:\n                pos_score = group['positive_sentiment_raw'].iloc[0]\n                neg_score = group['negative_sentiment_raw'].iloc[0]\n                difference = float(pos_score - neg_score)\n                results[doc_name] = {\n                    'positive_raw': float(pos_score),\n                    'negative_raw': float(neg_score),\n                    'difference_pos_neg': difference\n                }\n        return results\n    except Exception as e:\n        # print(f\"Error in compare_positive_negative_within_documents: {e}\")\n        return None\n\ndef compare_sentiment_across_documents(data, **kwargs):\n    \"\"\"\n    Compares specific sentiment raw scores (positive and negative) across different documents.\n\n    Statistical Methodology:\n    This function extracts the raw sentiment scores for 'positive_sentiment_raw'\n    and 'negative_sentiment_raw' for each unique document. It then presents these\n    scores side-by-side, allowing for a direct comparison of how a specific sentiment\n    dimension (e.g., positive sentiment) varies between documents (e.g.,\n    'positive_test.txt' vs 'negative_test.txt'). This directly addresses the\n    \"clear distinction between positive and negative sentiment scores across the\n    two test documents\" expected outcome. Given the small number of documents\n    (typically 2 for this framework), a direct comparison of values is sufficient.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing 'document_name',\n                             'positive_sentiment_raw', and 'negative_sentiment_raw' columns.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A dictionary containing the raw scores for positive and negative\n              sentiment for each document. Returns None if input data is invalid\n              or required columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if not isinstance(data, pd.DataFrame) or data.empty:\n            return None\n        required_cols = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        results = {\n            'positive_sentiment_raw_by_document': {},\n            'negative_sentiment_raw_by_document': {}\n        }\n\n        for doc_name, group in data.groupby('document_name'):\n            if not group.empty:\n                pos_score = group['positive_sentiment_raw'].iloc[0]\n                neg_score = group['negative_sentiment_raw'].iloc[0]\n                results['positive_sentiment_raw_by_document'][doc_name] = float(pos_score)\n                results['negative_sentiment_raw_by_document'][doc_name] = float(neg_score)\n        return results\n    except Exception as e:\n        # print(f\"Error in compare_sentiment_across_documents: {e}\")\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
      "cached_with_code": true
    },
    "statistical_data": {
      "calculate_document_sentiment_descriptives": {
        "negative_test.txt": {
          "positive_sentiment_raw": {
            "count": 1.0,
            "mean": 0.1,
            "std": NaN,
            "min": 0.1,
            "25%": 0.1,
            "50%": 0.1,
            "75%": 0.1,
            "max": 0.1
          },
          "positive_sentiment_salience": {
            "count": 1.0,
            "mean": 0.1,
            "std": NaN,
            "min": 0.1,
            "25%": 0.1,
            "50%": 0.1,
            "75%": 0.1,
            "max": 0.1
          },
          "positive_sentiment_confidence": {
            "count": 1.0,
            "mean": 0.8,
            "std": NaN,
            "min": 0.8,
            "25%": 0.8,
            "50%": 0.8,
            "75%": 0.8,
            "max": 0.8
          },
          "negative_sentiment_raw": {
            "count": 1.0,
            "mean": 0.9,
            "std": NaN,
            "min": 0.9,
            "25%": 0.9,
            "50%": 0.9,
            "75%": 0.9,
            "max": 0.9
          },
          "negative_sentiment_salience": {
            "count": 1.0,
            "mean": 0.9,
            "std": NaN,
            "min": 0.9,
            "25%": 0.9,
            "50%": 0.9,
            "75%": 0.9,
            "max": 0.9
          },
          "negative_sentiment_confidence": {
            "count": 1.0,
            "mean": 0.95,
            "std": NaN,
            "min": 0.95,
            "25%": 0.95,
            "50%": 0.95,
            "75%": 0.95,
            "max": 0.95
          }
        },
        "positive_test.txt": {
          "positive_sentiment_raw": {
            "count": 1.0,
            "mean": 0.95,
            "std": NaN,
            "min": 0.95,
            "25%": 0.95,
            "50%": 0.95,
            "75%": 0.95,
            "max": 0.95
          },
          "positive_sentiment_salience": {
            "count": 1.0,
            "mean": 1.0,
            "std": NaN,
            "min": 1.0,
            "25%": 1.0,
            "50%": 1.0,
            "75%": 1.0,
            "max": 1.0
          },
          "positive_sentiment_confidence": {
            "count": 1.0,
            "mean": 0.95,
            "std": NaN,
            "min": 0.95,
            "25%": 0.95,
            "50%": 0.95,
            "75%": 0.95,
            "max": 0.95
          },
          "negative_sentiment_raw": {
            "count": 1.0,
            "mean": 0.0,
            "std": NaN,
            "min": 0.0,
            "25%": 0.0,
            "50%": 0.0,
            "75%": 0.0,
            "max": 0.0
          },
          "negative_sentiment_salience": {
            "count": 1.0,
            "mean": 0.0,
            "std": NaN,
            "min": 0.0,
            "25%": 0.0,
            "50%": 0.0,
            "75%": 0.0,
            "max": 0.0
          },
          "negative_sentiment_confidence": {
            "count": 1.0,
            "mean": 1.0,
            "std": NaN,
            "min": 1.0,
            "25%": 1.0,
            "50%": 1.0,
            "75%": 1.0,
            "max": 1.0
          }
        }
      },
      "calculate_overall_sentiment_descriptives": {
        "positive_sentiment_raw": {
          "count": 2.0,
          "mean": 0.525,
          "std": 0.6010407640085653,
          "min": 0.1,
          "25%": 0.3125,
          "50%": 0.5249999999999999,
          "75%": 0.7374999999999999,
          "max": 0.95
        },
        "positive_sentiment_salience": {
          "count": 2.0,
          "mean": 0.55,
          "std": 0.6363961030678927,
          "min": 0.1,
          "25%": 0.325,
          "50%": 0.55,
          "75%": 0.775,
          "max": 1.0
        },
        "positive_sentiment_confidence": {
          "count": 2.0,
          "mean": 0.875,
          "std": 0.10606601717798207,
          "min": 0.8,
          "25%": 0.8375,
          "50%": 0.875,
          "75%": 0.9125,
          "max": 0.95
        },
        "negative_sentiment_raw": {
          "count": 2.0,
          "mean": 0.45,
          "std": 0.6363961030678927,
          "min": 0.0,
          "25%": 0.225,
          "50%": 0.45,
          "75%": 0.675,
          "max": 0.9
        },
        "negative_sentiment_salience": {
          "count": 2.0,
          "mean": 0.45,
          "std": 0.6363961030678927,
          "min": 0.0,
          "25%": 0.225,
          "50%": 0.45,
          "75%": 0.675,
          "max": 0.9
        },
        "negative_sentiment_confidence": {
          "count": 2.0,
          "mean": 0.975,
          "std": 0.03535533905932741,
          "min": 0.95,
          "25%": 0.9624999999999999,
          "50%": 0.975,
          "75%": 0.9875,
          "max": 1.0
        }
      },
      "compare_positive_negative_within_documents": {
        "negative_test.txt": {
          "positive_raw": 0.1,
          "negative_raw": 0.9,
          "difference_pos_neg": -0.8
        },
        "positive_test.txt": {
          "positive_raw": 0.95,
          "negative_raw": 0.0,
          "difference_pos_neg": 0.95
        }
      },
      "compare_sentiment_across_documents": {
        "positive_sentiment_raw_by_document": {
          "negative_test.txt": 0.1,
          "positive_test.txt": 0.95
        },
        "negative_sentiment_raw_by_document": {
          "negative_test.txt": 0.9,
          "positive_test.txt": 0.0
        }
      },
      "generate_statistical_summary_report": "STATISTICAL ANALYSIS SUMMARY REPORT\n==================================================\nAnalysis Timestamp: Unknown\nSample Size: Unknown\nAlpha Level: Unknown\nVariables: 0\n",
      "perform_statistical_analysis": {
        "analysis_metadata": {
          "timestamp": "2025-08-30T17:54:56.800065",
          "sample_size": 2,
          "alpha_level": 0.05,
          "variables_analyzed": [
            "positive_sentiment_raw",
            "positive_sentiment_salience",
            "positive_sentiment_confidence",
            "negative_sentiment_raw",
            "negative_sentiment_salience",
            "negative_sentiment_confidence"
          ]
        }
      },
      "run_complete_statistical_analysis": {
        "analysis_metadata": {
          "timestamp": "2025-08-30T17:54:56.806843",
          "sample_size": 2,
          "alpha_level": 0.05,
          "variables_analyzed": [
            "positive_sentiment_raw",
            "positive_sentiment_salience",
            "positive_sentiment_confidence",
            "negative_sentiment_raw",
            "negative_sentiment_salience",
            "negative_sentiment_confidence"
          ]
        }
      }
    },
    "status": "success_with_data",
    "validation_passed": true
  }
}