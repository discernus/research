{'generation_metadata': {'status': 'success', 'functions_generated': 4, 'output_file': 'automatedstatisticalanalysisagent_functions.py', 'module_size': 11969, 'function_code_content': 'import pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\ndef analyze_data_quality(data: pd.DataFrame, **kwargs) -> Optional[Dict[str, Any]]:\n    """\n    Performs a data quality and integrity check on the analysis DataFrame.\n\n    This function serves as a foundational step to validate the dataset before\n    running statistical analyses. It checks for the presence of required columns,\n    reports the number of total records (documents), quantifies missing data\n    for key score columns, and verifies that scores fall within their expected\n    range (0.0 to 1.0). This directly addresses the research question regarding\n    the analysis agent\'s ability to process and return valid dimensional scores.\n\n    Methodology:\n    - Counts total rows to determine sample size (N).\n    - Verifies the existence of essential columns: \'document_name\',\n      \'positive_sentiment_raw\', \'negative_sentiment_raw\'.\n    - Calculates the number and percentage of missing (NaN) values for each\n      sentiment score column.\n    - Checks if all non-missing scores in the specified columns are within the\n      valid [0.0, 1.0] range.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             including \'document_name\', \'positive_sentiment_raw\',\n                             and \'negative_sentiment_raw\'.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        Optional[Dict[str, Any]]: A dictionary containing data quality metrics,\n                                  including sample size, missing value counts,\n                                  and score range validity checks. Returns None\n                                  if the input data is invalid or missing\n                                  required columns.\n    """\n    try:\n        required_cols = [\n            \'document_name\',\n            \'positive_sentiment_raw\',\n            \'negative_sentiment_raw\'\n        ]\n        \n        if not all(col in data.columns for col in required_cols):\n            warnings.warn(f"Input data is missing one or more required columns: {required_cols}")\n            return None\n\n        n_total = len(data)\n        if n_total == 0:\n            warnings.warn("Input data is empty.")\n            return {"status": "failed", "reason": "Input data is empty.", "sample_size": 0}\n\n        results = {\n            "sample_size": n_total,\n            "missing_values": {},\n            "score_range_validity": {}\n        }\n\n        score_cols = [\'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        for col in score_cols:\n            missing_count = data[col].isnull().sum()\n            results["missing_values"][col] = {\n                "count": int(missing_count),\n                "percentage": round((missing_count / n_total) * 100, 2) if n_total > 0 else 0\n            }\n            \n            # Check score range validity, ignoring NaNs\n            valid_scores = data[col].dropna()\n            is_in_range = ((valid_scores >= 0.0) & (valid_scores <= 1.0)).all()\n            results["score_range_validity"][col] = bool(is_in_range)\n\n        return results\n\n    except Exception as e:\n        warnings.warn(f"An unexpected error occurred in analyze_data_quality: {e}")\n        return None\n\ndef summarize_sentiment_by_document_group(data: pd.DataFrame, **kwargs) -> Optional[Dict[str, Any]]:\n    """\n    Calculates descriptive statistics for sentiment scores, grouped by document type.\n\n    This is an exploratory analysis function designed for small sample sizes. It\n    addresses the research question of whether the pipeline can distinguish\n    between positive and negative sentiment by comparing scores across document\n    groups.\n\n    Statistical Methodology:\n    - Tier 3 Exploratory Analysis: This analysis is conducted on a very small\n      sample size (N < 15). Results are suggestive of patterns in this specific\n      dataset but are not generalizable and should be interpreted with extreme\n      caution. Inferential statistics are not appropriate or performed.\n    - Grouping: Documents are categorized into \'positive_test_group\' and\n      \'negative_test_group\' based on their filenames (\'positive_test.txt\',\n      \'negative_test.txt\'). This is a necessary step due to the absence of a\n      formal corpus manifest with grouping metadata.\n    - Descriptive Statistics: For each group, the function calculates the count,\n      mean, standard deviation, minimum, and maximum for \'positive_sentiment_raw\'\n      and \'negative_sentiment_raw\'. With n=1 per group, std will be 0 or NaN.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing analysis data with a\n                             \'document_name\' column and sentiment score columns.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        Optional[Dict[str, Any]]: A dictionary containing descriptive statistics\n                                  for each sentiment dimension, nested by document\n                                  group. Returns None if data is insufficient or\n                                  if required columns are missing.\n    """\n    try:\n        required_cols = [\'document_name\', \'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        if not all(col in data.columns for col in required_cols):\n            warnings.warn(f"Input data is missing one or more required columns: {required_cols}")\n            return None\n\n        if len(data) == 0:\n            warnings.warn("Input data is empty.")\n            return None\n            \n        # Power Assessment\n        n_total = len(data)\n        power_caveat = f"Exploratory analysis - results are suggestive rather than conclusive (N={n_total})."\n\n        # Create grouping variable from document name\n        def get_group(doc_name):\n            if isinstance(doc_name, str):\n                if \'positive_test\' in doc_name:\n                    return \'positive_test_group\'\n                elif \'negative_test\' in doc_name:\n                    return \'negative_test_group\'\n            return \'unknown_group\'\n\n        data_copy = data.copy()\n        data_copy[\'group\'] = data_copy[\'document_name\'].apply(get_group)\n\n        if \'unknown_group\' in data_copy[\'group\'].unique():\n            warnings.warn("Some documents could not be classified into a known group.")\n\n        # Calculate descriptive statistics\n        score_cols = [\'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        grouped_stats = data_copy.groupby(\'group\')[score_cols].agg([\'count\', \'mean\', \'std\', \'min\', \'max\'])\n        \n        # Convert to a more JSON-friendly dictionary format\n        results_dict = grouped_stats.to_dict(orient=\'index\')\n        \n        # Clean up the nested dictionary structure\n        final_results = {}\n        for group, stats in results_dict.items():\n            final_results[group] = {}\n            for stat_key, value in stats.items():\n                # stat_key is a tuple like (\'positive_sentiment_raw\', \'mean\')\n                dim_name, stat_name = stat_key\n                if dim_name not in final_results[group]:\n                    final_results[group][dim_name] = {}\n                # Replace NaN from std of n=1 with 0.0 for clarity\n                final_results[group][dim_name][stat_name] = 0.0 if pd.isna(value) else round(value, 4)\n\n        return {\n            "analysis_tier": "Tier 3 (Exploratory)",\n            "power_caveat": power_caveat,\n            "summary_statistics": final_results\n        }\n\n    except Exception as e:\n        warnings.warn(f"An unexpected error occurred in summarize_sentiment_by_document_group: {e}")\n        return None\n\ndef analyze_dimensional_correlation(data: pd.DataFrame, **kwargs) -> Optional[Dict[str, Any]]:\n    """\n    Calculates the correlation between positive and negative sentiment scores.\n\n    This function provides an exploratory look at the relationship between the\n    two primary sentiment dimensions. A strong negative correlation is typically\n    expected in a binary sentiment framework, indicating that as one score\n    increases, the other decreases.\n\n    Statistical Methodology:\n    - Tier 3 Exploratory Analysis: This analysis is performed on the entire\n      dataset. With a sample size (N) < 15, the resulting correlation\n      coefficient is highly unstable and should not be considered a reliable\n      estimate of the true population correlation. It is useful only for\n      describing the linear association within this specific, small sample.\n    - Correlation Metric: Pearson\'s correlation coefficient (r) is calculated,\n      which measures the linear relationship between the two variables. The\n      p-value is also provided but is not meaningful for very small N and\n      should be disregarded.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing \'positive_sentiment_raw\' and\n                             \'negative_sentiment_raw\' columns.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        Optional[Dict[str, Any]]: A dictionary containing the Pearson correlation\n                                  coefficient and p-value. Returns None if there\n                                  is not enough data to compute a correlation.\n    """\n    try:\n        required_cols = [\'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        if not all(col in data.columns for col in required_cols):\n            warnings.warn(f"Input data is missing one or more required columns: {required_cols}")\n            return None\n\n        # Drop rows with missing values in either column for correlation\n        clean_data = data[required_cols].dropna()\n        n_corr = len(clean_data)\n\n        # Correlation requires at least 2 data points with variance\n        if n_corr < 2:\n            warnings.warn(f"Not enough data (N={n_corr}) to calculate correlation.")\n            return {\n                "status": "failed",\n                "reason": f"Insufficient data for correlation (N={n_corr} after dropping NaNs). Requires at least 2.",\n                "sample_size_for_correlation": n_corr\n            }\n        \n        # Check for zero variance, which makes correlation undefined\n        if clean_data[\'positive_sentiment_raw\'].std() == 0 or clean_data[\'negative_sentiment_raw\'].std() == 0:\n             warnings.warn(f"Cannot compute correlation due to zero variance in one or both dimensions (N={n_corr}).")\n             return {\n                "status": "failed",\n                "reason": "Cannot compute correlation due to zero variance in one or both dimensions.",\n                "sample_size_for_correlation": n_corr\n             }\n\n        # Power Assessment\n        if n_corr < 15:\n            tier = "Tier 3 (Exploratory)"\n            power_caveat = f"Exploratory analysis - correlation is suggestive, not conclusive (N={n_corr})."\n        elif 15 <= n_corr < 30:\n            tier = "Tier 2 (Moderately-Powered)"\n            power_caveat = f"Results should be interpreted with caution due to moderate sample size (N={n_corr})."\n        else:\n            tier = "Tier 1 (Well-Powered)"\n            power_caveat = "Sufficient sample size for correlation analysis."\n\n        corr, p_value = stats.pearsonr(clean_data[\'positive_sentiment_raw\'], clean_data[\'negative_sentiment_raw\'])\n\n        return {\n            "analysis_tier": tier,\n            "power_caveat": power_caveat,\n            "sample_size_for_correlation": n_corr,\n            "pearson_correlation": {\n                "r_value": round(corr, 4),\n                "p_value": round(p_value, 4) if not np.isnan(p_value) else None,\n                "interpretation_note": "p-value is not reliable for very small sample sizes (N<15)." if n_corr < 15 else ""\n            }\n        }\n\n    except Exception as e:\n        warnings.warn(f"An unexpected error occurred in analyze_dimensional_correlation: {e}")\n        return None', 'cached_with_code': True}, 'statistical_data': {'analyze_data_quality': {'sample_size': 2, 'missing_values': {'positive_sentiment_raw': {'count': 0, 'percentage': 0.0}, 'negative_sentiment_raw': {'count': 0, 'percentage': 0.0}}, 'score_range_validity': {'positive_sentiment_raw': True, 'negative_sentiment_raw': True}}, 'analyze_dimensional_correlation': {'analysis_tier': 'Tier 3 (Exploratory)', 'power_caveat': 'Exploratory analysis - correlation is suggestive, not conclusive (N=2).', 'sample_size_for_correlation': 2, 'pearson_correlation': {'r_value': -1.0, 'p_value': 1.0, 'interpretation_note': 'p-value is not reliable for very small sample sizes (N<15).'}}, 'summarize_sentiment_by_document_group': {'analysis_tier': 'Tier 3 (Exploratory)', 'power_caveat': 'Exploratory analysis - results are suggestive rather than conclusive (N=2).', 'summary_statistics': {'negative_test_group': {'positive_sentiment_raw': {'count': 1, 'mean': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0}, 'negative_sentiment_raw': {'count': 1, 'mean': 1.0, 'std': 0.0, 'min': 1.0, 'max': 1.0}}, 'positive_test_group': {'positive_sentiment_raw': {'count': 1, 'mean': 1.0, 'std': 0.0, 'min': 1.0, 'max': 1.0}, 'negative_sentiment_raw': {'count': 1, 'mean': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0}}}}}, 'status': 'success_with_data', 'validation_passed': True}