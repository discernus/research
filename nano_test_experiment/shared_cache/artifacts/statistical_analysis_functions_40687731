{
  "status": "success",
  "functions_generated": 3,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 13922,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: nano_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-01T19:25:31.453761+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates and returns descriptive statistics for the key sentiment dimensions.\n\n    This function computes the count, mean, standard deviation, min, 25th percentile, \n    median (50th), 75th percentile, and max for the raw scores, salience, and \n    confidence of both positive and negative sentiment. This provides a foundational \n    overview of the analysis results, addressing the research question about the \n    agent's ability to process simple dimensional scoring by summarizing the\n    distribution of the generated scores.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             for sentiment scores.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the descriptive statistics for each key\n              sentiment column, or None if the required columns are not found\n              or the data is empty.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = [\n            'positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence',\n            'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence'\n        ]\n\n        if not all(col in data.columns for col in required_cols) or data.empty:\n            return None\n\n        # Select only the required columns for description\n        stats_data = data[required_cols]\n        \n        # Calculate descriptive statistics\n        descriptives = stats_data.describe().to_dict()\n\n        # Ensure all expected stats are present, filling with NaN if not\n        for col, stats in descriptives.items():\n            for stat_name in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']:\n                if stat_name not in stats:\n                    stats[stat_name] = np.nan\n        \n        return descriptives\n\n    except Exception:\n        return None\n\ndef perform_sentiment_group_comparison(data, **kwargs):\n    \"\"\"\n    Performs independent t-tests to compare sentiment scores between document groups.\n\n    This function directly addresses the research question: \"Does the pipeline correctly\n    identify positive vs negative sentiment?\". It operationalizes the \"clear distinction\"\n    expectation by statistically comparing the means of the two groups.\n    \n    Methodology:\n    1. Segregates documents into 'positive' and 'negative' groups based on their filenames\n       ('positive_test.txt', 'negative_test.txt').\n    2. For the 'positive_sentiment_raw' dimension, it performs an independent samples t-test\n       to check if the mean score for the positive group is significantly higher.\n    3. For the 'negative_sentiment_raw' dimension, it performs another t-test to check\n       if the mean score for the negative group is significantly higher.\n    4. Descriptive statistics (mean, std) are provided for each group to contextualize the\n       t-test results.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing analysis data, including 'document_name'\n                             and sentiment score columns.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary with descriptive statistics and t-test results for both\n              positive and negative sentiment dimensions, or None if data is\n              insufficient for comparison.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy import stats\n\n    try:\n        required_cols = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols) or data.empty:\n            return None\n\n        # Create grouping variable based on document name\n        def get_group(filename):\n            if 'positive_test.txt' in filename:\n                return 'positive_group'\n            if 'negative_test.txt' in filename:\n                return 'negative_group'\n            return None\n            \n        data['group'] = data['document_name'].apply(get_group)\n        \n        # Filter out any documents that don't fit the grouping\n        grouped_data = data.dropna(subset=['group'])\n        \n        if grouped_data['group'].nunique() < 2:\n            return None # Not enough groups to compare\n\n        positive_docs = grouped_data[grouped_data['group'] == 'positive_group']\n        negative_docs = grouped_data[grouped_data['group'] == 'negative_group']\n\n        if len(positive_docs) < 2 or len(negative_docs) < 2:\n            return None # Not enough samples in one or both groups for a meaningful t-test\n\n        results = {}\n\n        # 1. Compare Positive Sentiment\n        pos_sent_pos_group = positive_docs['positive_sentiment_raw'].dropna()\n        pos_sent_neg_group = negative_docs['positive_sentiment_raw'].dropna()\n        \n        if len(pos_sent_pos_group) > 0 and len(pos_sent_neg_group) > 0:\n            ttest_pos = stats.ttest_ind(pos_sent_pos_group, pos_sent_neg_group, equal_var=False, nan_policy='omit')\n            results['positive_sentiment_comparison'] = {\n                'positive_group_mean': pos_sent_pos_group.mean(),\n                'positive_group_std': pos_sent_pos_group.std(),\n                'positive_group_n': len(pos_sent_pos_group),\n                'negative_group_mean': pos_sent_neg_group.mean(),\n                'negative_group_std': pos_sent_neg_group.std(),\n                'negative_group_n': len(pos_sent_neg_group),\n                't_statistic': ttest_pos.statistic,\n                'p_value': ttest_pos.pvalue\n            }\n\n        # 2. Compare Negative Sentiment\n        neg_sent_pos_group = positive_docs['negative_sentiment_raw'].dropna()\n        neg_sent_neg_group = negative_docs['negative_sentiment_raw'].dropna()\n\n        if len(neg_sent_pos_group) > 0 and len(neg_sent_neg_group) > 0:\n            ttest_neg = stats.ttest_ind(neg_sent_pos_group, neg_sent_neg_group, equal_var=False, nan_policy='omit')\n            results['negative_sentiment_comparison'] = {\n                'positive_group_mean': neg_sent_pos_group.mean(),\n                'positive_group_std': neg_sent_pos_group.std(),\n                'positive_group_n': len(neg_sent_pos_group),\n                'negative_group_mean': neg_sent_neg_group.mean(),\n                'negative_group_std': neg_sent_neg_group.std(),\n                'negative_group_n': len(neg_sent_neg_group),\n                't_statistic': ttest_neg.statistic,\n                'p_value': ttest_neg.pvalue\n            }\n            \n        return results if results else None\n\n    except Exception:\n        return None\n\ndef analyze_dimension_correlation(data, **kwargs):\n    \"\"\"\n    Calculates the correlation between positive and negative sentiment scores.\n\n    This function provides insight into the relationship between the two primary\n    dimensions of the framework. For this binary sentiment framework, a strong\n    negative correlation is expected (i.e., as positive sentiment increases,\n    negative sentiment decreases). This analysis helps validate the conceptual\n    opposition of the two dimensions.\n\n    Methodology:\n    - Uses the Pearson correlation coefficient to measure the linear relationship\n      between 'positive_sentiment_raw' and 'negative_sentiment_raw'.\n    - The Pearson r value ranges from -1 (perfect negative correlation) to +1\n      (perfect positive correlation), with 0 indicating no linear correlation.\n    - A p-value is also returned to assess the statistical significance of the\n      correlation.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the Pearson correlation coefficient and\n              the p-value, or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    from scipy import stats\n\n    try:\n        required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        clean_data = data[required_cols].dropna()\n\n        if len(clean_data) < 3:\n            # Correlation is not meaningful with fewer than 3 data points\n            return None\n\n        correlation, p_value = stats.pearsonr(clean_data['positive_sentiment_raw'], clean_data['negative_sentiment_raw'])\n\n        if pd.isna(correlation):\n            return None\n\n        return {\n            'pearson_correlation_coefficient': correlation,\n            'p_value': p_value,\n            'sample_size': len(clean_data)\n        }\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}