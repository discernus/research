{'generation_metadata': {'status': 'success', 'functions_generated': 3, 'output_file': 'automatedstatisticalanalysisagent_functions.py', 'module_size': 13922, 'function_code_content': '"""\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: nano_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-08-30T02:25:01.186158+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n"""\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings(\'ignore\', category=RuntimeWarning)\n\n\ndef descriptive_statistics(data, **kwargs):\n    """\n    Calculates descriptive statistics for sentiment scores, grouped by document type.\n\n    This function addresses the research question about pipeline functionality by summarizing\n    the dimensional scores. It creates a \'document_type\' group based on filenames\n    (e.g., \'positive_test.txt\' -> \'positive\', \'negative_test.txt\' -> \'negative\')\n    and calculates the count, mean, standard deviation, min, and max for each\n    sentiment dimension within each group. This provides a foundational view of the\n    data distribution and central tendency.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             like \'document_name\', \'positive_sentiment_raw\', etc.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each sentiment\n              dimension, grouped by document type. Returns None if required\n              columns are missing or data is insufficient.\n    """\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = [\n            \'document_name\', \'positive_sentiment_raw\', \'negative_sentiment_raw\',\n            \'positive_sentiment_salience\', \'negative_sentiment_salience\',\n            \'positive_sentiment_confidence\', \'negative_sentiment_confidence\'\n        ]\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        if data.empty:\n            return None\n\n        # Create grouping variable based on document name (thin architecture)\n        def get_doc_type(name):\n            if \'positive\' in name:\n                return \'positive\'\n            if \'negative\' in name:\n                return \'negative\'\n            return \'unknown\'\n        \n        data[\'document_type\'] = data[\'document_name\'].apply(get_doc_type)\n\n        if \'unknown\' in data[\'document_type\'].unique():\n            # Handle cases where grouping is not possible\n            pass\n\n        score_cols = [col for col in required_cols if col != \'document_name\']\n        \n        # Use .agg() for specific statistics to avoid scientific notation from .describe()\n        stats = data.groupby(\'document_type\')[score_cols].agg([\'count\', \'mean\', \'std\', \'min\', \'max\'])\n        \n        # Convert the multi-index dataframe to a nested dictionary for JSON compatibility\n        results = stats.to_dict(orient=\'index\')\n        \n        # Reformat the nested dictionary for cleaner output\n        output = {}\n        for group, group_stats in results.items():\n            output[group] = {}\n            for stat_col, stat_values in group_stats.items():\n                # stat_col is a tuple like (\'positive_sentiment_raw\', \'mean\')\n                col_name, stat_name = stat_col\n                if col_name not in output[group]:\n                    output[group][col_name] = {}\n                # Round to 4 decimal places for readability\n                output[group][col_name][stat_name] = round(stat_values, 4) if pd.notna(stat_values) else None\n\n        return output if output else None\n\n    except Exception:\n        return None\n\ndef compare_sentiment_groups(data, **kwargs):\n    """\n    Compares sentiment scores between positive and negative document groups.\n\n    This function directly addresses the "clear distinction" expected outcome by\n    comparing the means of sentiment scores between document groups. It uses an\n    independent samples t-test to determine if the differences in mean scores\n    (e.g., positive_sentiment_raw for the \'positive\' group vs. the \'negative\' group)\n    are statistically significant. The function handles small sample sizes by\n    reporting means even if a t-test is not viable.\n\n    Methodology:\n    1.  Groups documents into \'positive\' and \'negative\' based on filenames.\n    2.  For each dimension (\'positive_sentiment_raw\', \'negative_sentiment_raw\'),\n        it performs an independent t-test between the two groups.\n    3.  Reports the mean scores for each group, the t-statistic, and the p-value.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary with comparison results, including means and t-test\n              statistics for each dimension. Returns None on error or if\n              insufficient data for comparison.\n    """\n    import pandas as pd\n    import numpy as np\n    from scipy import stats\n\n    try:\n        required_cols = [\'document_name\', \'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        if data.empty:\n            return None\n\n        # Create grouping variable based on document name\n        def get_doc_type(name):\n            if \'positive\' in name:\n                return \'positive\'\n            if \'negative\' in name:\n                return \'negative\'\n            return \'unknown\'\n        \n        data[\'document_type\'] = data[\'document_name\'].apply(get_doc_type)\n\n        positive_group = data[data[\'document_type\'] == \'positive\']\n        negative_group = data[data[\'document_type\'] == \'negative\']\n\n        if positive_group.empty or negative_group.empty:\n            return {"error": "Insufficient data: one or both document groups are empty."}\n\n        results = {}\n        dimensions_to_test = [\'positive_sentiment_raw\', \'negative_sentiment_raw\']\n\n        for dim in dimensions_to_test:\n            pos_scores = positive_group[dim].dropna()\n            neg_scores = negative_group[dim].dropna()\n\n            # Check for sufficient data for a t-test (at least 2 samples per group)\n            can_run_ttest = len(pos_scores) > 1 and len(neg_scores) > 1\n\n            test_result = {\n                \'positive_group_mean\': pos_scores.mean(),\n                \'negative_group_mean\': neg_scores.mean(),\n                \'t_statistic\': None,\n                \'p_value\': None,\n                \'notes\': None\n            }\n\n            if can_run_ttest:\n                # equal_var=False for Welch\'s t-test, which doesn\'t assume equal variance\n                t_stat, p_val = stats.ttest_ind(pos_scores, neg_scores, equal_var=False, nan_policy=\'omit\')\n                test_result[\'t_statistic\'] = t_stat\n                test_result[\'p_value\'] = p_val\n            else:\n                test_result[\'notes\'] = "T-test not performed due to insufficient sample size in one or both groups (n < 2)."\n            \n            results[dim] = test_result\n\n        return results if results else None\n\n    except Exception:\n        return None\n\ndef correlation_analysis(data, **kwargs):\n    """\n    Calculates the correlation between positive and negative sentiment scores.\n\n    This function provides insight into the relationship between the two primary\n    dimensions of the framework. A strong negative correlation is expected,\n    indicating that as positive sentiment increases, negative sentiment decreases.\n    This helps validate the conceptual independence and opposition of the\n    framework\'s dimensions.\n\n    Methodology:\n    - Uses the Pearson correlation coefficient to measure the linear relationship\n      between \'positive_sentiment_raw\' and \'negative_sentiment_raw\'.\n    - Reports the correlation coefficient (r) and the p-value to assess the\n      statistical significance of the correlation.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing the Pearson correlation coefficient and\n              the p-value. Returns None if data is insufficient (fewer than 2 rows)\n              or if required columns are missing.\n    """\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import pearsonr\n\n    try:\n        required_cols = [\'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        # Drop rows with missing values in the columns of interest\n        clean_data = data[required_cols].dropna()\n\n        if len(clean_data) < 2:\n            return {"error": "Insufficient data for correlation analysis (requires at least 2 data points)."}\n\n        # Calculate Pearson correlation\n        correlation, p_value = pearsonr(clean_data[\'positive_sentiment_raw\'], clean_data[\'negative_sentiment_raw\'])\n\n        results = {\n            \'pearson_correlation_coefficient\': correlation,\n            \'p_value\': p_value,\n            \'sample_size\': len(clean_data)\n        }\n\n        return results\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    """\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    """\n    results = {\n        \'analysis_metadata\': {\n            \'timestamp\': pd.Timestamp.now().isoformat(),\n            \'sample_size\': len(data),\n            \'alpha_level\': alpha,\n            \'variables_analyzed\': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith((\'calculate_\', \'perform_\', \'test_\')) and \n            name != \'run_complete_statistical_analysis\'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if \'alpha\' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {\'error\': f\'Analysis failed: {str(e)}\'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    """\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    """\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    """\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    """\n    report_lines = []\n    report_lines.append("STATISTICAL ANALYSIS SUMMARY REPORT")\n    report_lines.append("=" * 50)\n    \n    metadata = analysis_results.get(\'analysis_metadata\', {})\n    report_lines.append(f"Analysis Timestamp: {metadata.get(\'timestamp\', \'Unknown\')}")\n    report_lines.append(f"Sample Size: {metadata.get(\'sample_size\', \'Unknown\')}")\n    report_lines.append(f"Alpha Level: {metadata.get(\'alpha_level\', \'Unknown\')}")\n    report_lines.append(f"Variables: {len(metadata.get(\'variables_analyzed\', []))}")\n    report_lines.append("")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != \'analysis_metadata\' and isinstance(result, dict):\n            if \'error\' not in result:\n                report_lines.append(f"{analysis_name.replace(\'_\', \' \').title()}:")\n                \n                # Extract key statistics based on analysis type\n                if \'p_value\' in result:\n                    p_val = result[\'p_value\']\n                    significance = "significant" if p_val < metadata.get(\'alpha_level\', 0.05) else "not significant"\n                    report_lines.append(f"  - p-value: {p_val:.4f} ({significance})")\n                \n                if \'effect_size\' in result:\n                    report_lines.append(f"  - Effect size: {result[\'effect_size\']:.4f}")\n                \n                if \'correlation_matrix\' in result:\n                    report_lines.append(f"  - Correlation matrix generated with {len(result[\'correlation_matrix\'])} variables")\n                \n                if \'cronbach_alpha\' in result:\n                    alpha_val = result[\'cronbach_alpha\']\n                    reliability = "excellent" if alpha_val > 0.9 else "good" if alpha_val > 0.8 else "acceptable" if alpha_val > 0.7 else "questionable"\n                    report_lines.append(f"  - Cronbach\'s Î±: {alpha_val:.3f} ({reliability})")\n                \n                report_lines.append("")\n            else:\n                report_lines.append(f"{analysis_name}: ERROR - {result[\'error\']}")\n                report_lines.append("")\n    \n    return "\\n".join(report_lines)\n', 'cached_with_code': True}, 'statistical_data': {'compare_sentiment_groups': {'positive_sentiment_raw': {'positive_group_mean': 1.0, 'negative_group_mean': 0.0, 't_statistic': None, 'p_value': None, 'notes': 'T-test not performed due to insufficient sample size in one or both groups (n < 2).'}, 'negative_sentiment_raw': {'positive_group_mean': 0.0, 'negative_group_mean': 1.0, 't_statistic': None, 'p_value': None, 'notes': 'T-test not performed due to insufficient sample size in one or both groups (n < 2).'}}, 'correlation_analysis': {'pearson_correlation_coefficient': -1.0, 'p_value': 1.0, 'sample_size': 2}, 'descriptive_statistics': {'negative': {'positive_sentiment_raw': {'count': 1, 'mean': 0.0, 'std': None, 'min': 0.0, 'max': 0.0}, 'negative_sentiment_raw': {'count': 1, 'mean': 1.0, 'std': None, 'min': 1.0, 'max': 1.0}, 'positive_sentiment_salience': {'count': 1, 'mean': 0.0, 'std': None, 'min': 0.0, 'max': 0.0}, 'negative_sentiment_salience': {'count': 1, 'mean': 1.0, 'std': None, 'min': 1.0, 'max': 1.0}, 'positive_sentiment_confidence': {'count': 1, 'mean': 1.0, 'std': None, 'min': 1.0, 'max': 1.0}, 'negative_sentiment_confidence': {'count': 1, 'mean': 1.0, 'std': None, 'min': 1.0, 'max': 1.0}}, 'positive': {'positive_sentiment_raw': {'count': 1, 'mean': 1.0, 'std': None, 'min': 1.0, 'max': 1.0}, 'negative_sentiment_raw': {'count': 1, 'mean': 0.0, 'std': None, 'min': 0.0, 'max': 0.0}, 'positive_sentiment_salience': {'count': 1, 'mean': 1.0, 'std': None, 'min': 1.0, 'max': 1.0}, 'negative_sentiment_salience': {'count': 1, 'mean': 0.0, 'std': None, 'min': 0.0, 'max': 0.0}, 'positive_sentiment_confidence': {'count': 1, 'mean': 1.0, 'std': None, 'min': 1.0, 'max': 1.0}, 'negative_sentiment_confidence': {'count': 1, 'mean': 1.0, 'std': None, 'min': 1.0, 'max': 1.0}}}, 'generate_statistical_summary_report': 'STATISTICAL ANALYSIS SUMMARY REPORT\n==================================================\nAnalysis Timestamp: Unknown\nSample Size: Unknown\nAlpha Level: Unknown\nVariables: 0\n', 'perform_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-08-29T22:27:16.532606', 'sample_size': 2, 'alpha_level': 0.05, 'variables_analyzed': ['positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence', 'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence']}}, 'run_complete_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-08-29T22:27:16.535192', 'sample_size': 2, 'alpha_level': 0.05, 'variables_analyzed': ['positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence', 'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence']}}}, 'status': 'success_with_data', 'validation_passed': True}