{
  "batch_id": "stats_20250916T201123Z",
  "statistical_analysis": {
    "batch_id": "stats_20250916T201123Z",
    "step": "statistical_execution",
    "model_used": "vertex_ai/gemini-2.5-pro",
    "statistical_functions_and_results": "```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nfrom typing import Dict, Any, Optional, List\\nimport json\\nimport re\\n\\ndef _prepare_data(data: List[Dict[str, Any]]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Parses raw analysis artifacts into a clean pandas DataFrame.\\n\\n    This function handles the specific, sometimes inconsistent, JSON structures found\\n    in the score extraction artifacts for this experiment. It maps analysis IDs to\\n    document filenames based on reasonable assumptions for a small test suite.\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A pandas DataFrame with scores and metadata, or None if parsing fails.\\n    \\\"\\\"\\\"\\n    score_artifacts = [a for a in data if a.get('step') == 'score_extraction']\\n    if not score_artifacts:\\n        return None\\n\\n    # HACK: For this specific experiment, analysis IDs must be mapped to documents.\\n    # Based on the corpus manifest, we can infer the mapping.\\n    # analysis_1d91dd78 -> positive_test.txt\\n    # analysis_12ed6a5a -> negative_test.txt\\n    analysis_id_to_doc = {\\n        'analysis_1d91dd78': 'positive_test.txt',\\n        'analysis_12ed6a5a': 'negative_test.txt'\\n    }\\n\\n    records = []\\n    for artifact in score_artifacts:\\n        analysis_id = artifact.get('analysis_id')\\n        json_string = artifact.get('scores_extraction', '{}')\\n        \\n        # Clean the string: remove markdown backticks and language identifiers\\n        cleaned_string = re.sub(r'```json\\\\n|```', '', json_string).strip()\\n\\n        try:\\n            scores_data = json.loads(cleaned_string)\\n        except json.JSONDecodeError:\\n            continue # Skip malformed JSON\\n\\n        # The JSON structure varies across artifacts. We need to handle both cases.\\n        # Case 1: Keys are dimensions (e.g., \\\"positive_sentiment\\\")\\n        if 'positive_sentiment' in scores_data and 'raw_score' in scores_data['positive_sentiment']:\\n            doc_name = analysis_id_to_doc.get(analysis_id)\\n            if doc_name:\\n                for dim, values in scores_data.items():\\n                    if isinstance(values, dict):\\n                        record = {\\n                            'document_name': doc_name,\\n                            'dimension': dim,\\n                            'raw_score': values.get('raw_score'),\\n                            'salience': values.get('salience'),\\n                            'confidence': values.get('confidence')\\n                        }\\n                        records.append(record)\\n        # Case 2: Top-level key is the document name\\n        else:\\n            for doc_name, dimensions in scores_data.items():\\n                if isinstance(dimensions, dict):\\n                    for dim, values in dimensions.items():\\n                        if isinstance(values, dict):\\n                            record = {\\n                                'document_name': doc_name,\\n                                'dimension': dim,\\n                                'raw_score': values.get('raw_score'),\\n                                'salience': values.get('salience'),\\n                                'confidence': values.get('confidence')\\n                            }\\n                            records.append(record)\\n\\n    if not records:\\n        return None\\n\\n    df = pd.DataFrame.from_records(records)\\n    df.dropna(subset=['raw_score'], inplace=True)\\n\\n    # Create grouping variable based on corpus manifest metadata\\n    group_mapping = {\\n        'positive_test.txt': 'positive',\\n        'negative_test.txt': 'negative'\\n    }\\n    df['document_group'] = df['document_name'].map(group_mapping)\\n\\n    return df\\n\\ndef calculate_descriptive_statistics(data: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates and returns descriptive statistics for each dimension, grouped by document sentiment.\\n\\n    This is the primary analysis for this exploratory (Tier 3) study. It directly addresses\\n    the research questions by comparing the scores of the positive and negative test documents.\\n\\n    Methodology:\\n    - Data is parsed and organized into a DataFrame.\\n    - Scores are grouped by the document's designated sentiment ('positive' or 'negative') and the analysis dimension.\\n    - Descriptive statistics (mean, std, min, max, count) are calculated for each group.\\n    - With N=1 per group, mean equals the raw score and standard deviation is NaN (calculated as 0 for clarity).\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A dictionary containing the descriptive statistics, or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    df = _prepare_data(data)\\n    if df is None or df.empty:\\n        return None\\n\\n    try:\\n        # Group by the metadata field and dimension\\n        grouped = df.groupby(['document_group', 'dimension'])\\n        desc_stats = grouped['raw_score'].agg(['mean', 'std', 'min', 'max', 'count']).reset_index()\\n        \\n        # Pandas calculates std as NaN for n=1, replace with 0 for cleaner output\\n        desc_stats['std'] = desc_stats['std'].fillna(0)\\n        \\n        # Rename for clarity\\n        desc_stats.rename(columns={'document_group': 'group'}, inplace=True)\\n        \\n        return desc_stats.to_dict('records')\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef perform_correlation_analysis(data: List[Dict[str, Any]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Placeholder for correlation analysis.\\n\\n    Methodology:\\n    - Skipped. Correlation analysis requires a minimum of N<15 for exploratory (Tier 3) analysis, \\n      but N=2 is insufficient to produce any meaningful correlation matrix.\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A dictionary indicating that the analysis was skipped due to insufficient sample size.\\n    \\\"\\\"\\\"\\n    return {\\n        \\\"status\\\": \\\"Skipped\\\",\\n        \\\"reason\\\": \\\"Correlation analysis is not meaningful for N=2. Requires a larger sample size for even exploratory insights.\\\"\\n    }\\n\\n\\ndef perform_group_comparison(data: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs a simple comparison of means between the two document groups.\\n\\n    Methodology:\\n    - This is an exploratory (Tier 3) analysis.\\n    - Inferential tests like t-tests are not appropriate for N=1 per group.\\n    - Instead, this function calculates the difference in mean scores between the 'positive' and 'negative' groups for each dimension.\\n    - This directly addresses the research question about whether there is a 'clear distinction' between the documents.\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A dictionary containing the mean difference for each dimension, or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    df = _prepare_data(data)\\n    if df is None or df.empty or len(df['document_group'].unique()) < 2:\\n        return None\\n\\n    try:\\n        results = {}\\n        for dimension in df['dimension'].unique():\\n            dim_df = df[df['dimension'] == dimension]\\n            \\n            pos_group = dim_df[dim_df['document_group'] == 'positive']\\n            neg_group = dim_df[dim_df['document_group'] == 'negative']\\n\\n            if pos_group.empty or neg_group.empty:\\n                continue\\n\\n            pos_mean = pos_group['raw_score'].mean()\\n            neg_mean = neg_group['raw_score'].mean()\\n            mean_difference = pos_mean - neg_mean\\n            \\n            results[dimension] = {\\n                'positive_group_mean': pos_mean,\\n                'negative_group_mean': neg_mean,\\n                'mean_difference': mean_difference\\n            }\\n        return results if results else None\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef calculate_reliability_analysis(data: List[Dict[str, Any]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Placeholder for reliability analysis.\\n\\n    Methodology:\\n    - Skipped. Reliability analysis (e.g., Cronbach's alpha) measures the internal consistency of a scale \\n      composed of multiple items. This experiment uses single-item dimensions, making this analysis inapplicable.\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A dictionary indicating that the analysis is not applicable.\\n    \\\"\\\"\\\"\\n    return {\\n        \\\"status\\\": \\\"Skipped\\\",\\n        \\\"reason\\\": \\\"Reliability analysis like Cronbach's alpha is not applicable as dimensions are not scales with multiple items.\\\"\\n    }\\n\\ndef perform_statistical_analysis(data: List[Dict[str, Any]], **kwargs) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function that executes all statistical analyses for the experiment.\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n        **kwargs: Additional parameters (not used in this implementation).\\n\\n    Returns:\\n        A dictionary containing the results of all executed analyses.\\n    \\\"\\\"\\\"\\n    results = {}\\n    results['descriptive_statistics'] = calculate_descriptive_statistics(data)\\n    results['correlation_analysis'] = perform_correlation_analysis(data)\\n    # Renamed anova_analysis to a more appropriate group_comparison for this N\\n    results['group_comparison'] = perform_group_comparison(data)\\n    results['reliability_analysis'] = calculate_reliability_analysis(data)\\n    results['additional_analyses'] = {}\\n\\n    return results\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": [\n      {\n        \"group\": \"negative\",\n        \"dimension\": \"negative_sentiment\",\n        \"mean\": 1.0,\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"count\": 1\n      },\n      {\n        \"group\": \"negative\",\n        \"dimension\": \"positive_sentiment\",\n        \"mean\": 0.0,\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"count\": 1\n      },\n      {\n        \"group\": \"positive\",\n        \"dimension\": \"negative_sentiment\",\n        \"mean\": 0.0,\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"count\": 1\n      },\n      {\n        \"group\": \"positive\",\n        \"dimension\": \"positive_sentiment\",\n        \"mean\": 0.9,\n        \"std\": 0.0,\n        \"min\": 0.9,\n        \"max\": 0.9,\n        \"count\": 1\n      }\n    ],\n    \"correlation_analysis\": {\n      \"status\": \"Skipped\",\n      \"reason\": \"Correlation analysis is not meaningful for N=2. Requires a larger sample size for even exploratory insights.\"\n    },\n    \"group_comparison\": {\n      \"positive_sentiment\": {\n        \"positive_group_mean\": 0.9,\n        \"negative_group_mean\": 0.0,\n        \"mean_difference\": 0.9\n      },\n      \"negative_sentiment\": {\n        \"positive_group_mean\": 0.0,\n        \"negative_group_mean\": 1.0,\n        \"mean_difference\": -1.0\n      }\n    },\n    \"reliability_analysis\": {\n      \"status\": \"Skipped\",\n      \"reason\": \"Reliability analysis like Cronbach's alpha is not applicable as dimensions are not scales with multiple items.\"\n    },\n    \"additional_analyses\": {}\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 2,\n    \"tier_classification\": \"TIER 3\",\n    \"power_notes\": \"The analysis is exploratory with N=2 (1 document per group). This sample size is insufficient for any inferential statistical tests (e.g., t-tests, ANOVA). The analysis is limited to descriptive statistics and direct comparison of the two data points. Conclusions cannot be generalized.\"\n  },\n  \"methodology_summary\": \"The statistical analysis was conducted at Tier 3 (Exploratory) due to the extremely small sample size of N=2. The primary method involved calculating descriptive statistics (mean, std) for the 'positive_sentiment' and 'negative_sentiment' scores, grouped by the document's designated sentiment ('positive' vs. 'negative'). A direct comparison of mean scores between the two groups was performed to quantify the difference. Inferential statistics, correlations, and reliability analyses were determined to be inappropriate and were skipped. The focus is on pattern validation rather than statistical inference.\"\n}\n```",
    "analysis_artifacts_processed": 4,
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-pro",
      "execution_time_seconds": 46.728558,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0,
      "prompt_length": 16125,
      "response_length": 12242
    },
    "timestamp": "2025-09-16T20:12:09.976941+00:00",
    "artifact_hash": "3b8dc34e7d1a1926e1faafd343f6f0fdb9b79ad1ca598657b137599b1207100e"
  },
  "verification": {
    "batch_id": "stats_20250916T201123Z",
    "step": "verification",
    "model_used": "vertex_ai/gemini-2.5-flash-lite",
    "verification_status": "verified",
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-flash-lite",
      "execution_time_seconds": 0.65399,
      "prompt_length": 12740,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0
    },
    "timestamp": "2025-09-16T20:12:10.636276+00:00",
    "artifact_hash": "f9aa46d937d446b8ec5057c9029739dcee66ee39912eb65094b71b76d2b2ca04"
  },
  "csv_generation": {
    "batch_id": "stats_20250916T201123Z",
    "step": "csv_generation",
    "model_used": "vertex_ai/gemini-2.5-flash",
    "csv_files": [
      {
        "filename": "scores.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250916T201123Z/data/scores.csv",
        "size": 149
      },
      {
        "filename": "evidence.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250916T201123Z/data/evidence.csv",
        "size": 27
      },
      {
        "filename": "metadata.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250916T201123Z/data/metadata.csv",
        "size": 227
      }
    ],
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-flash",
      "execution_time_seconds": 11.519141,
      "prompt_length": 3822,
      "artifacts_processed": 2,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0
    },
    "timestamp": "2025-09-16T20:12:22.273689+00:00",
    "artifact_hash": "03c4360f56a3665d4a81a0be07ad40e661a7be068dc01b8c3aab9e4c55b2e2db"
  },
  "total_cost_info": {
    "total_cost_usd": 0.0,
    "total_execution_time_seconds": 58.901689,
    "total_tokens": 0,
    "cost_breakdown": {
      "statistical_execution": 0.0,
      "verification": 0.0,
      "csv_generation": 0.0
    },
    "performance_breakdown": {
      "statistical_execution_time": 46.728558,
      "verification_time": 0.65399,
      "csv_generation_time": 11.519141
    },
    "models_used": [
      "vertex_ai/gemini-2.5-pro",
      "vertex_ai/gemini-2.5-flash-lite",
      "vertex_ai/gemini-2.5-flash"
    ]
  },
  "timestamp": "2025-09-16T20:12:22.274863+00:00",
  "agent_name": "StatisticalAgent"
}