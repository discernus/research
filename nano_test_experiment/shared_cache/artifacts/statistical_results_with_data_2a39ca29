{'generation_metadata': {'status': 'success', 'functions_generated': 4, 'output_file': 'automatedstatisticalanalysisagent_functions.py', 'module_size': 15084, 'function_code_content': '"""\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: nano_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-08-30T21:51:45.375829+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n"""\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings(\'ignore\', category=RuntimeWarning)\n\n\ndef calculate_overall_sentiment_descriptives(data, **kwargs):\n    """\n    Calculates overall descriptive statistics for all sentiment-related dimensions\n    across the entire dataset.\n\n    Statistical Methodology:\n    This function computes basic descriptive statistics (count, mean, standard deviation,\n    minimum, 25th percentile, 50th percentile (median), 75th percentile, and maximum)\n    for the raw sentiment scores, salience, and confidence metrics. These statistics\n    provide a summary of the central tendency, dispersion, and range of the sentiment\n    measurements across all documents.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the analysis data with\n                             columns like \'positive_sentiment_raw\',\n                             \'negative_sentiment_raw\', etc.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A dictionary where keys are the sentiment-related column names and\n              values are dictionaries of their descriptive statistics, or None\n              if the input data is empty or required columns are missing.\n    """\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if not isinstance(data, pd.DataFrame) or data.empty:\n            return None\n\n        sentiment_cols = [\n            \'positive_sentiment_raw\', \'positive_sentiment_salience\', \'positive_sentiment_confidence\',\n            \'negative_sentiment_raw\', \'negative_sentiment_salience\', \'negative_sentiment_confidence\'\n        ]\n\n        # Filter for columns that actually exist in the DataFrame\n        existing_cols = [col for col in sentiment_cols if col in data.columns]\n\n        if not existing_cols:\n            return None\n\n        descriptives = data[existing_cols].describe().to_dict()\n\n        # Convert numpy types to native Python types for JSON serialization\n        for col, stats in descriptives.items():\n            for stat_name, value in stats.items():\n                if isinstance(value, (np.float32, np.float64)):\n                    stats[stat_name] = float(value)\n                elif isinstance(value, (np.int32, np.int64)):\n                    stats[stat_name] = int(value)\n        return descriptives\n    except Exception as e:\n        # Log the exception for debugging purposes\n        # print(f"Error in calculate_overall_sentiment_descriptives: {e}")\n        return None\n\ndef calculate_document_sentiment_descriptives(data, **kwargs):\n    """\n    Calculates descriptive statistics for sentiment-related dimensions, grouped by\n    \'document_name\'.\n\n    Statistical Methodology:\n    This function groups the input DataFrame by the \'document_name\' column and then\n    computes descriptive statistics (mean, standard deviation, minimum, maximum,\n    and quartiles) for each sentiment-related metric within each document. This\n    allows for a direct comparison of sentiment scores across different documents,\n    addressing the research question about clear distinctions between test documents.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the analysis data with\n                             \'document_name\' and sentiment-related columns.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A nested dictionary where the first level keys are \'document_name\'s,\n              and the second level keys are sentiment-related column names, with\n              values being dictionaries of their descriptive statistics. Returns\n              None if input data is invalid or required columns are missing.\n    """\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if not isinstance(data, pd.DataFrame) or data.empty:\n            return None\n        if \'document_name\' not in data.columns:\n            return None\n\n        sentiment_cols = [\n            \'positive_sentiment_raw\', \'positive_sentiment_salience\', \'positive_sentiment_confidence\',\n            \'negative_sentiment_raw\', \'negative_sentiment_salience\', \'negative_sentiment_confidence\'\n        ]\n\n        existing_cols = [col for col in sentiment_cols if col in data.columns]\n        if not existing_cols:\n            return None\n\n        # Group by document_name and describe\n        grouped_descriptives = data.groupby(\'document_name\')[existing_cols].describe().to_dict()\n\n        # Reformat the dictionary for better readability and JSON serialization\n        results = {}\n        for (col, stat_name), value in grouped_descriptives.items():\n            for doc_name, val in value.items():\n                if doc_name not in results:\n                    results[doc_name] = {}\n                if col not in results[doc_name]:\n                    results[doc_name][col] = {}\n                if isinstance(val, (np.float32, np.float64)):\n                    results[doc_name][col][stat_name] = float(val)\n                elif isinstance(val, (np.int32, np.int64)):\n                    results[doc_name][col][stat_name] = int(val)\n                else:\n                    results[doc_name][col][stat_name] = val\n        return results\n    except Exception as e:\n        # print(f"Error in calculate_document_sentiment_descriptives: {e}")\n        return None\n\ndef compare_positive_negative_within_documents(data, **kwargs):\n    """\n    Compares the raw positive and negative sentiment scores within each document.\n\n    Statistical Methodology:\n    For each document, this function calculates the difference between its\n    \'positive_sentiment_raw\' and \'negative_sentiment_raw\' scores. This direct\n    comparison helps to assess the dominant sentiment within a given document\n    and validate if the pipeline correctly assigns higher scores to the\n    expected sentiment dimension for each test document (e.g., positive_test.txt\n    should have positive > negative). Given the nature of the experiment (N=1\n    observation per document for these scores), a simple difference is the most\n    appropriate metric rather than inferential tests.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing \'document_name\',\n                             \'positive_sentiment_raw\', and \'negative_sentiment_raw\' columns.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A dictionary where keys are \'document_name\'s and values are the\n              difference (positive_sentiment_raw - negative_sentiment_raw) for\n              that document. Returns None if input data is invalid or columns are missing.\n    """\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if not isinstance(data, pd.DataFrame) or data.empty:\n            return None\n        required_cols = [\'document_name\', \'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        results = {}\n        for doc_name, group in data.groupby(\'document_name\'):\n            if not group.empty:\n                pos_score = group[\'positive_sentiment_raw\'].iloc[0]\n                neg_score = group[\'negative_sentiment_raw\'].iloc[0]\n                difference = float(pos_score - neg_score)\n                results[doc_name] = {\n                    \'positive_raw\': float(pos_score),\n                    \'negative_raw\': float(neg_score),\n                    \'difference_pos_neg\': difference\n                }\n        return results\n    except Exception as e:\n        # print(f"Error in compare_positive_negative_within_documents: {e}")\n        return None\n\ndef compare_sentiment_across_documents(data, **kwargs):\n    """\n    Compares specific sentiment raw scores (positive and negative) across different documents.\n\n    Statistical Methodology:\n    This function extracts the raw sentiment scores for \'positive_sentiment_raw\'\n    and \'negative_sentiment_raw\' for each unique document. It then presents these\n    scores side-by-side, allowing for a direct comparison of how a specific sentiment\n    dimension (e.g., positive sentiment) varies between documents (e.g.,\n    \'positive_test.txt\' vs \'negative_test.txt\'). This directly addresses the\n    "clear distinction between positive and negative sentiment scores across the\n    two test documents" expected outcome. Given the small number of documents\n    (typically 2 for this framework), a direct comparison of values is sufficient.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing \'document_name\',\n                             \'positive_sentiment_raw\', and \'negative_sentiment_raw\' columns.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A dictionary containing the raw scores for positive and negative\n              sentiment for each document. Returns None if input data is invalid\n              or required columns are missing.\n    """\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if not isinstance(data, pd.DataFrame) or data.empty:\n            return None\n        required_cols = [\'document_name\', \'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        results = {\n            \'positive_sentiment_raw_by_document\': {},\n            \'negative_sentiment_raw_by_document\': {}\n        }\n\n        for doc_name, group in data.groupby(\'document_name\'):\n            if not group.empty:\n                pos_score = group[\'positive_sentiment_raw\'].iloc[0]\n                neg_score = group[\'negative_sentiment_raw\'].iloc[0]\n                results[\'positive_sentiment_raw_by_document\'][doc_name] = float(pos_score)\n                results[\'negative_sentiment_raw_by_document\'][doc_name] = float(neg_score)\n        return results\n    except Exception as e:\n        # print(f"Error in compare_sentiment_across_documents: {e}")\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    """\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    """\n    results = {\n        \'analysis_metadata\': {\n            \'timestamp\': pd.Timestamp.now().isoformat(),\n            \'sample_size\': len(data),\n            \'alpha_level\': alpha,\n            \'variables_analyzed\': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith((\'calculate_\', \'perform_\', \'test_\')) and \n            name != \'run_complete_statistical_analysis\'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if \'alpha\' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {\'error\': f\'Analysis failed: {str(e)}\'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    """\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    """\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    """\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    """\n    report_lines = []\n    report_lines.append("STATISTICAL ANALYSIS SUMMARY REPORT")\n    report_lines.append("=" * 50)\n    \n    metadata = analysis_results.get(\'analysis_metadata\', {})\n    report_lines.append(f"Analysis Timestamp: {metadata.get(\'timestamp\', \'Unknown\')}")\n    report_lines.append(f"Sample Size: {metadata.get(\'sample_size\', \'Unknown\')}")\n    report_lines.append(f"Alpha Level: {metadata.get(\'alpha_level\', \'Unknown\')}")\n    report_lines.append(f"Variables: {len(metadata.get(\'variables_analyzed\', []))}")\n    report_lines.append("")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != \'analysis_metadata\' and isinstance(result, dict):\n            if \'error\' not in result:\n                report_lines.append(f"{analysis_name.replace(\'_\', \' \').title()}:")\n                \n                # Extract key statistics based on analysis type\n                if \'p_value\' in result:\n                    p_val = result[\'p_value\']\n                    significance = "significant" if p_val < metadata.get(\'alpha_level\', 0.05) else "not significant"\n                    report_lines.append(f"  - p-value: {p_val:.4f} ({significance})")\n                \n                if \'effect_size\' in result:\n                    report_lines.append(f"  - Effect size: {result[\'effect_size\']:.4f}")\n                \n                if \'correlation_matrix\' in result:\n                    report_lines.append(f"  - Correlation matrix generated with {len(result[\'correlation_matrix\'])} variables")\n                \n                if \'cronbach_alpha\' in result:\n                    alpha_val = result[\'cronbach_alpha\']\n                    reliability = "excellent" if alpha_val > 0.9 else "good" if alpha_val > 0.8 else "acceptable" if alpha_val > 0.7 else "questionable"\n                    report_lines.append(f"  - Cronbach\'s Î±: {alpha_val:.3f} ({reliability})")\n                \n                report_lines.append("")\n            else:\n                report_lines.append(f"{analysis_name}: ERROR - {result[\'error\']}")\n                report_lines.append("")\n    \n    return "\\n".join(report_lines)\n', 'cached_with_code': True}, 'statistical_data': {'calculate_document_sentiment_descriptives': {'negative_test.txt': {'positive_sentiment_raw': {'count': 1.0, 'mean': 0.1, 'std': nan, 'min': 0.1, '25%': 0.1, '50%': 0.1, '75%': 0.1, 'max': 0.1}, 'positive_sentiment_salience': {'count': 1.0, 'mean': 0.1, 'std': nan, 'min': 0.1, '25%': 0.1, '50%': 0.1, '75%': 0.1, 'max': 0.1}, 'positive_sentiment_confidence': {'count': 1.0, 'mean': 0.8, 'std': nan, 'min': 0.8, '25%': 0.8, '50%': 0.8, '75%': 0.8, 'max': 0.8}, 'negative_sentiment_raw': {'count': 1.0, 'mean': 0.9, 'std': nan, 'min': 0.9, '25%': 0.9, '50%': 0.9, '75%': 0.9, 'max': 0.9}, 'negative_sentiment_salience': {'count': 1.0, 'mean': 0.9, 'std': nan, 'min': 0.9, '25%': 0.9, '50%': 0.9, '75%': 0.9, 'max': 0.9}, 'negative_sentiment_confidence': {'count': 1.0, 'mean': 0.95, 'std': nan, 'min': 0.95, '25%': 0.95, '50%': 0.95, '75%': 0.95, 'max': 0.95}}, 'positive_test.txt': {'positive_sentiment_raw': {'count': 1.0, 'mean': 0.95, 'std': nan, 'min': 0.95, '25%': 0.95, '50%': 0.95, '75%': 0.95, 'max': 0.95}, 'positive_sentiment_salience': {'count': 1.0, 'mean': 1.0, 'std': nan, 'min': 1.0, '25%': 1.0, '50%': 1.0, '75%': 1.0, 'max': 1.0}, 'positive_sentiment_confidence': {'count': 1.0, 'mean': 0.95, 'std': nan, 'min': 0.95, '25%': 0.95, '50%': 0.95, '75%': 0.95, 'max': 0.95}, 'negative_sentiment_raw': {'count': 1.0, 'mean': 0.0, 'std': nan, 'min': 0.0, '25%': 0.0, '50%': 0.0, '75%': 0.0, 'max': 0.0}, 'negative_sentiment_salience': {'count': 1.0, 'mean': 0.0, 'std': nan, 'min': 0.0, '25%': 0.0, '50%': 0.0, '75%': 0.0, 'max': 0.0}, 'negative_sentiment_confidence': {'count': 1.0, 'mean': 1.0, 'std': nan, 'min': 1.0, '25%': 1.0, '50%': 1.0, '75%': 1.0, 'max': 1.0}}}, 'calculate_overall_sentiment_descriptives': {'positive_sentiment_raw': {'count': 2.0, 'mean': 0.525, 'std': 0.6010407640085653, 'min': 0.1, '25%': 0.3125, '50%': 0.5249999999999999, '75%': 0.7374999999999999, 'max': 0.95}, 'positive_sentiment_salience': {'count': 2.0, 'mean': 0.55, 'std': 0.6363961030678927, 'min': 0.1, '25%': 0.325, '50%': 0.55, '75%': 0.775, 'max': 1.0}, 'positive_sentiment_confidence': {'count': 2.0, 'mean': 0.875, 'std': 0.10606601717798207, 'min': 0.8, '25%': 0.8375, '50%': 0.875, '75%': 0.9125, 'max': 0.95}, 'negative_sentiment_raw': {'count': 2.0, 'mean': 0.45, 'std': 0.6363961030678927, 'min': 0.0, '25%': 0.225, '50%': 0.45, '75%': 0.675, 'max': 0.9}, 'negative_sentiment_salience': {'count': 2.0, 'mean': 0.45, 'std': 0.6363961030678927, 'min': 0.0, '25%': 0.225, '50%': 0.45, '75%': 0.675, 'max': 0.9}, 'negative_sentiment_confidence': {'count': 2.0, 'mean': 0.975, 'std': 0.03535533905932741, 'min': 0.95, '25%': 0.9624999999999999, '50%': 0.975, '75%': 0.9875, 'max': 1.0}}, 'compare_positive_negative_within_documents': {'negative_test.txt': {'positive_raw': 0.1, 'negative_raw': 0.9, 'difference_pos_neg': -0.8}, 'positive_test.txt': {'positive_raw': 0.95, 'negative_raw': 0.0, 'difference_pos_neg': 0.95}}, 'compare_sentiment_across_documents': {'positive_sentiment_raw_by_document': {'negative_test.txt': 0.1, 'positive_test.txt': 0.95}, 'negative_sentiment_raw_by_document': {'negative_test.txt': 0.9, 'positive_test.txt': 0.0}}, 'generate_statistical_summary_report': 'STATISTICAL ANALYSIS SUMMARY REPORT\n==================================================\nAnalysis Timestamp: Unknown\nSample Size: Unknown\nAlpha Level: Unknown\nVariables: 0\n', 'perform_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-08-30T17:54:56.800065', 'sample_size': 2, 'alpha_level': 0.05, 'variables_analyzed': ['positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence', 'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence']}}, 'run_complete_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-08-30T17:54:56.806843', 'sample_size': 2, 'alpha_level': 0.05, 'variables_analyzed': ['positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence', 'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence']}}}, 'status': 'success_with_data', 'validation_passed': True}