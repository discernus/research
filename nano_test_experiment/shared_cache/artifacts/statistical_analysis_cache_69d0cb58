{
  "batch_id": "stats_20250916T111340Z",
  "statistical_analysis": {
    "batch_id": "stats_20250916T111340Z",
    "step": "statistical_execution",
    "model_used": "vertex_ai/gemini-2.5-pro",
    "statistical_functions_and_results": "```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nfrom typing import Dict, Any, Optional, List\\nimport json\\nimport re\\n\\ndef _clean_json_string(s: str) -> Optional[Dict]:\\n    \\\"\\\"\\\"\\n    Cleans a JSON string that might be wrapped in markdown code fences.\\n    \\\"\\\"\\\"\\n    if not isinstance(s, str):\\n        return None\\n    match = re.search(r'```json\\\\n(.*?)\\\\n```', s, re.DOTALL)\\n    if match:\\n        s = match.group(1)\\n    try:\\n        return json.loads(s)\\n    except (json.JSONDecodeError, TypeError):\\n        return None\\n\\ndef _create_dataframe_from_artifacts(data: List[Dict[str, Any]]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Parses the raw analysis artifacts into a clean pandas DataFrame.\\n\\n    Args:\\n        data: A list of analysis artifact dictionaries.\\n\\n    Returns:\\n        A pandas DataFrame containing the scores, or None if parsing fails.\\n    \\\"\\\"\\\"\\n    records = []\\n    # Infer document mapping as it's not explicitly provided\\n    # In a two-document test, one artifact names the doc, the other is inferred.\\n    doc_map = {}\\n    all_analysis_ids = {art['analysis_id'] for art in data}\\n    processed_ids = set()\\n\\n    for artifact in data:\\n        if artifact.get('step') != 'score_extraction':\\n            continue\\n\\n        scores_data = _clean_json_string(artifact.get('scores_extraction', ''))\\n        if not scores_data:\\n            continue\\n\\n        analysis_id = artifact['analysis_id']\\n        doc_filename = None\\n\\n        # Case 1: Filename is the top-level key\\n        if any(key.endswith('.txt') for key in scores_data.keys()):\\n            doc_filename = list(scores_data.keys())[0]\\n            scores_dict = scores_data[doc_filename]\\n            doc_map[analysis_id] = doc_filename\\n            processed_ids.add(analysis_id)\\n        # Case 2: Scores are at the top level (filename must be inferred)\\n        else:\\n            scores_dict = scores_data\\n\\n        if doc_filename:\\n            for dimension, values in scores_dict.items():\\n                if isinstance(values, dict):\\n                    record = {\\n                        'document_filename': doc_filename,\\n                        'analysis_id': analysis_id,\\n                        'dimension': dimension,\\n                        'raw_score': values.get('raw_score'),\\n                        'salience': values.get('salience'),\\n                        'confidence': values.get('confidence')\\n                    }\\n                    records.append(record)\\n\\n    # Second pass for inferred documents\\n    unprocessed_ids = all_analysis_ids - processed_ids\\n    if len(unprocessed_ids) == 1:\\n        inferred_analysis_id = list(unprocessed_ids)[0]\\n        # Based on corpus manifest\\n        all_docs = {'positive_test.txt', 'negative_test.txt'}\\n        processed_docs = set(doc_map.values())\\n        inferred_doc = list(all_docs - processed_docs)[0]\\n\\n        for artifact in data:\\n            if artifact['analysis_id'] == inferred_analysis_id and artifact.get('step') == 'score_extraction':\\n                scores_data = _clean_json_string(artifact.get('scores_extraction', ''))\\n                if scores_data and not any(key.endswith('.txt') for key in scores_data.keys()):\\n                     for dimension, values in scores_data.items():\\n                        if isinstance(values, dict):\\n                            record = {\\n                                'document_filename': inferred_doc,\\n                                'analysis_id': inferred_analysis_id,\\n                                'dimension': dimension,\\n                                'raw_score': values.get('raw_score'),\\n                                'salience': values.get('salience'),\\n                                'confidence': values.get('confidence')\\n                            }\\n                            records.append(record)\\n\\n    if not records:\\n        return None\\n\\n    return pd.DataFrame(records)\\n\\ndef _get_grouping_mapping() -> Dict[str, str]:\\n    \\\"\\\"\\\"\\n    Creates a mapping from document filename to sentiment group based on the corpus manifest.\\n    \\\"\\\"\\\"\\n    return {\\n        'positive_test.txt': 'positive',\\n        'negative_test.txt': 'negative'\\n    }\\n\\ndef calculate_descriptive_statistics(df: pd.DataFrame, group_mapping: Dict[str, str]) -> Optional[Dict]:\\n    \\\"\\\"\\\"\\n    Calculates and returns descriptive statistics for scores, grouped by sentiment and dimension.\\n\\n    Args:\\n        df: The input DataFrame with score data.\\n        group_mapping: A dictionary mapping filenames to groups.\\n\\n    Returns:\\n        A dictionary of descriptive statistics, or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty:\\n        return {'error': 'Input DataFrame is empty or None.'}\\n\\n    try:\\n        df['group'] = df['document_filename'].map(group_mapping)\\n        if df['group'].isnull().any():\\n            return {'error': 'Not all documents could be mapped to a group.'}\\n\\n        desc_stats = df.groupby(['group', 'dimension'])['raw_score'].describe()\\n        # Convert to a more JSON-friendly format\\n        results = {}\\n        for index, row in desc_stats.iterrows():\\n            group, dimension = index\\n            if group not in results:\\n                results[group] = {}\\n            results[group][dimension] = row.to_dict()\\n        return results\\n    except Exception as e:\\n        return {'error': f'An error occurred: {str(e)}'}\\n\\ndef perform_group_comparison(df: pd.DataFrame, group_mapping: Dict[str, str]) -> Optional[Dict]:\\n    \\\"\\\"\\\"\\n    Performs group comparisons on scores. For N < 8 per group (Tier 3), this function\\n    focuses on descriptive differences rather than formal t-tests.\\n\\n    Args:\\n        df: The input DataFrame with score data.\\n        group_mapping: A dictionary mapping filenames to groups.\\n\\n    Returns:\\n        A dictionary summarizing the comparison, or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty:\\n        return {'error': 'Input DataFrame is empty or None.'}\\n\\n    try:\\n        df['group'] = df['document_filename'].map(group_mapping)\\n        results = {}\\n        dimensions = df['dimension'].unique()\\n        groups = list(group_mapping.values())\\n\\n        if len(groups) != 2:\\n            return {'note': 'Group comparison requires exactly two groups.'}\\n\\n        for dim in dimensions:\\n            dim_df = df[df['dimension'] == dim]\\n            group1_scores = dim_df[dim_df['group'] == groups[0]]['raw_score'].dropna()\\n            group2_scores = dim_df[dim_df['group'] == groups[1]]['raw_score'].dropna()\\n\\n            # TIER 3 Check: N<8 per group\\n            if len(group1_scores) < 2 or len(group2_scores) < 2:\\n                mean1 = group1_scores.mean() if not group1_scores.empty else 0\\n                mean2 = group2_scores.mean() if not group2_scores.empty else 0\\n                results[dim] = {\\n                    'status': 'Exploratory comparison due to N < 2 per group.',\\n                    'groups': {\\n                        groups[0]: {'mean': mean1, 'n': len(group1_scores)},\\n                        groups[1]: {'mean': mean2, 'n': len(group2_scores)}\\n                    },\\n                    'mean_difference': mean1 - mean2\\n                }\\n            else:\\n                # Standard t-test for larger samples\\n                ttest_res = pg.ttest(group1_scores, group2_scores, correction=True)\\n                results[dim] = json.loads(ttest_res.to_json(orient='records'))[0]\\n\\n        return results\\n    except Exception as e:\\n        return {'error': f'An error occurred: {str(e)}'}\\n\\ndef perform_correlation_analysis(df: pd.DataFrame) -> Optional[Dict]:\\n    \\\"\\\"\\\"\\n    Calculates the Pearson correlation between the sentiment dimensions.\\n\\n    Args:\\n        df: The input DataFrame with score data.\\n\\n    Returns:\\n        A dictionary with the correlation matrix and a note on statistical power.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty:\\n        return {'error': 'Input DataFrame is empty or None.'}\\n\\n    try:\\n        pivot_df = df.pivot_table(index='document_filename', columns='dimension', values='raw_score')\\n\\n        note = 'Standard correlation analysis.'\\n        if len(pivot_df) < 15:\\n            note = 'TIER 3 (N < 15): Correlation is exploratory and may be unstable.'\\n\\n        if len(pivot_df) < 2:\\n            return {\\n                'note': 'Insufficient data for correlation (N < 2).',\\n                'correlation_matrix': None\\n            }\\n\\n        corr_matrix = pivot_df.corr(method='pearson')\\n        return {\\n            'note': note,\\n            'correlation_matrix': corr_matrix.to_dict()\\n        }\\n    except Exception as e:\\n        return {'error': f'An error occurred: {str(e)}'}\\n\\ndef calculate_reliability_analysis(df: pd.DataFrame) -> Dict:\\n    \\\"\\\"\\\"\\n    Assesses if reliability analysis (e.g., Cronbach's alpha) is applicable.\\n\\n    Args:\\n        df: The input DataFrame with score data.\\n\\n    Returns:\\n        A dictionary with a note on applicability.\\n    \\\"\\\"\\\"\\n    return {\\n        'status': 'Not Applicable',\\n        'reason': \\\"Cronbach's alpha requires multiple items measuring a single latent construct. The dimensions 'positive_sentiment' and 'negative_sentiment' are distinct constructs, not items of a single scale.\\\"\\n    }\\n\\ndef perform_statistical_analysis(artifacts: List[Dict[str, Any]]) -> Dict:\\n    \\\"\\\"\\\"\\n    Master function that prepares data and executes all statistical analyses.\\n\\n    Args:\\n        artifacts: A list of analysis artifact dictionaries.\\n\\n    Returns:\\n        A dictionary containing the results of all analyses.\\n    \\\"\\\"\\\"\\n    df = _create_dataframe_from_artifacts(artifacts)\\n    group_mapping = _get_grouping_mapping()\\n\\n    results = {}\\n    if df is not None:\\n        results['descriptive_statistics'] = calculate_descriptive_statistics(df, group_mapping)\\n        results['group_comparison'] = perform_group_comparison(df, group_mapping)\\n        results['correlation_analysis'] = perform_correlation_analysis(df)\\n    else:\\n        results['descriptive_statistics'] = {'error': 'Failed to create DataFrame from artifacts.'}\\n        results['group_comparison'] = {'error': 'DataFrame not available.'}\\n        results['correlation_analysis'] = {'error': 'DataFrame not available.'}\\n\\n    results['reliability_analysis'] = calculate_reliability_analysis(df) # Doesn't require df to run\\n    \\n    # Convert all results to be JSON serializable, replacing NaN with None\\n    def clean_json(d):\\n        if isinstance(d, dict):\\n            return {k: clean_json(v) for k, v in d.items()}\\n        elif isinstance(d, list):\\n            return [clean_json(i) for i in d]\\n        elif isinstance(d, float) and np.isnan(d):\\n            return None\\n        return d\\n\\n    return clean_json(results)\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": {\n      \"negative\": {\n        \"negative_sentiment\": {\n          \"count\": 1.0,\n          \"mean\": 1.0,\n          \"std\": null,\n          \"min\": 1.0,\n          \"25%\": 1.0,\n          \"50%\": 1.0,\n          \"75%\": 1.0,\n          \"max\": 1.0\n        },\n        \"positive_sentiment\": {\n          \"count\": 1.0,\n          \"mean\": 0.0,\n          \"std\": null,\n          \"min\": 0.0,\n          \"25%\": 0.0,\n          \"50%\": 0.0,\n          \"75%\": 0.0,\n          \"max\": 0.0\n        }\n      },\n      \"positive\": {\n        \"negative_sentiment\": {\n          \"count\": 1.0,\n          \"mean\": 0.0,\n          \"std\": null,\n          \"min\": 0.0,\n          \"25%\": 0.0,\n          \"50%\": 0.0,\n          \"75%\": 0.0,\n          \"max\": 0.0\n        },\n        \"positive_sentiment\": {\n          \"count\": 1.0,\n          \"mean\": 0.95,\n          \"std\": null,\n          \"min\": 0.95,\n          \"25%\": 0.95,\n          \"50%\": 0.95,\n          \"75%\": 0.95,\n          \"max\": 0.95\n        }\n      }\n    },\n    \"group_comparison\": {\n      \"positive_sentiment\": {\n        \"status\": \"Exploratory comparison due to N < 2 per group.\",\n        \"groups\": {\n          \"positive\": {\n            \"mean\": 0.95,\n            \"n\": 1\n          },\n          \"negative\": {\n            \"mean\": 0.0,\n            \"n\": 1\n          }\n        },\n        \"mean_difference\": 0.95\n      },\n      \"negative_sentiment\": {\n        \"status\": \"Exploratory comparison due to N < 2 per group.\",\n        \"groups\": {\n          \"positive\": {\n            \"mean\": 0.0,\n            \"n\": 1\n          },\n          \"negative\": {\n            \"mean\": 1.0,\n            \"n\": 1\n          }\n        },\n        \"mean_difference\": -1.0\n      }\n    },\n    \"correlation_analysis\": {\n      \"note\": \"TIER 3 (N < 15): Correlation is exploratory and may be unstable.\",\n      \"correlation_matrix\": {\n        \"negative_sentiment\": {\n          \"negative_sentiment\": 1.0,\n          \"positive_sentiment\": -1.0\n        },\n        \"positive_sentiment\": {\n          \"negative_sentiment\": -1.0,\n          \"positive_sentiment\": 1.0\n        }\n      }\n    },\n    \"reliability_analysis\": {\n      \"status\": \"Not Applicable\",\n      \"reason\": \"Cronbach's alpha requires multiple items measuring a single latent construct. The dimensions 'positive_sentiment' and 'negative_sentiment' are distinct constructs, not items of a single scale.\"\n    }\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 2,\n    \"tier_classification\": \"TIER 3\",\n    \"power_notes\": \"The sample size of N=2 (1 per group) is insufficient for any inferential statistical testing. The analysis is purely exploratory and descriptive, suitable for pipeline validation but not for generalizable research conclusions.\"\n  },\n  \"methodology_summary\": \"The statistical analysis was conducted under a TIER 3 (Exploratory) protocol due to the very small sample size (N=2). The primary methods included the calculation of descriptive statistics (mean, std dev) for each dimension, grouped by the document's intended sentiment ('positive' vs 'negative'). Group comparisons were performed by calculating the difference in means, as formal inferential tests like t-tests are not applicable. A Pearson correlation was calculated to explore the relationship between dimensions, with the caveat that results from N=2 are highly unstable. Reliability analysis was deemed not applicable. This approach directly addresses the research questions by descriptively validating the pipeline's ability to distinguish sentiment in the test documents.\"\n}\n```",
    "analysis_artifacts_processed": 4,
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-pro",
      "execution_time_seconds": 82.940839,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0,
      "prompt_length": 19731,
      "response_length": 14534
    },
    "timestamp": "2025-09-16T15:15:02.964204+00:00",
    "artifact_hash": "a81ab23f94a79b045c6066c62c06c9c81298f96be63b90e42cc7620cd2befae0"
  },
  "verification": {
    "batch_id": "stats_20250916T111340Z",
    "step": "verification",
    "model_used": "vertex_ai/gemini-2.5-flash-lite",
    "verification_status": "verified",
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-flash-lite",
      "execution_time_seconds": 34.589805,
      "prompt_length": 15032,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0
    },
    "timestamp": "2025-09-16T15:15:37.635384+00:00",
    "artifact_hash": "35ea6ffe23438c98107efc8e6a589c7f6d33aa1042028b761ffe1e6bc166261a"
  },
  "csv_generation": {
    "batch_id": "stats_20250916T111340Z",
    "step": "csv_generation",
    "model_used": "vertex_ai/gemini-2.5-flash",
    "csv_files": [
      {
        "filename": "scores.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250916T132939Z/data/scores.csv",
        "size": 360
      },
      {
        "filename": "metadata.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250916T132939Z/data/metadata.csv",
        "size": 457
      },
      {
        "filename": "evidence.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250916T132939Z/data/evidence.csv",
        "size": 74
      }
    ],
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-flash",
      "execution_time_seconds": 15.665283,
      "prompt_length": 6513,
      "artifacts_processed": 4,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0
    },
    "timestamp": "2025-09-16T15:15:53.338845+00:00",
    "artifact_hash": "e0f41f1e64dfaa2ea5a7d88e5216002aa818b1d192881e1c19a2a80f9c74a246"
  },
  "total_cost_info": {
    "total_cost_usd": 0.0,
    "total_execution_time_seconds": 133.19592699999998,
    "total_tokens": 0,
    "cost_breakdown": {
      "statistical_execution": 0.0,
      "verification": 0.0,
      "csv_generation": 0.0
    },
    "performance_breakdown": {
      "statistical_execution_time": 82.940839,
      "verification_time": 34.589805,
      "csv_generation_time": 15.665283
    },
    "models_used": [
      "vertex_ai/gemini-2.5-pro",
      "vertex_ai/gemini-2.5-flash-lite",
      "vertex_ai/gemini-2.5-flash"
    ]
  },
  "timestamp": "2025-09-16T15:15:53.343113+00:00",
  "agent_name": "StatisticalAgent"
}