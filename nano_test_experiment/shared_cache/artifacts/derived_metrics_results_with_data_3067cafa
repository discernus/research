{
  "generation_metadata": {
    "status": "success",
    "functions_generated": 6,
    "output_file": "automatedderivedmetricsagent_functions.py",
    "module_size": 11991,
    "function_code_content": "\"\"\"\nAutomated Derived Metrics Functions\n===================================\n\nGenerated by AutomatedDerivedMetricsAgent for experiment: Test Experiment\nDescription: Test experiment for derived metrics\nGenerated: 2025-08-30T02:24:26.663243+00:00\n\nThis module contains automatically generated calculation functions for derived metrics\nas specified in the framework's natural language descriptions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom typing import Optional, Dict, Any\n\n\ndef calculate_identity_tension(data, **kwargs):\n    \"\"\"\n    Calculate identity_tension: Conflict between tribal dominance and individual dignity dimensions.\n\n    This calculation is not possible with the provided 'Sentiment Binary Framework v1.0'\n    as it lacks the necessary dimensions ('tribal_dominance', 'individual_dignity').\n    The function will gracefully return None.\n\n    Hypothetical Formula: abs(tribal_dominance - individual_dignity)\n\n    Args:\n        data (pd.Series or pd.DataFrame): A single row of analysis data.\n        **kwargs: Additional parameters (unused).\n\n    Returns:\n        None: This calculation cannot be performed with the available data.\n    \"\"\"\n    # This function is designed to be a placeholder as the required dimensions\n    # 'tribal_dominance' and 'individual_dignity' are not present in the\n    # provided data structure. The framework context describes 'Positive Sentiment'\n    # and 'Negative Sentiment' dimensions, which are also not mapped to the\n    # actual data columns provided (e.g., 'analysis_result', 'raw_analysis_response').\n    # Therefore, the calculation is impossible, and we return None as per\n    # the requirement to handle missing data gracefully.\n    return None\n\ndef calculate_emotional_balance(data, **kwargs):\n    \"\"\"\n    Calculate emotional_balance: Difference between hope and fear scores\n\n    Formula: hope - fear\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        **kwargs: Additional parameters\n        \n    Returns:\n        float: Calculated result or None if insufficient data\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # The calculation is defined as the difference between 'hope' and 'fear' scores.\n        # This requires 'hope' and 'fear' to be columns in the input data.\n        hope_score = data['hope']\n        fear_score = data['fear']\n\n        # Ensure that the scores are not null (NaN, None, etc.) before proceeding.\n        if pd.isna(hope_score) or pd.isna(fear_score):\n            return None\n\n        # Perform the calculation and ensure the result is a standard float.\n        result = float(hope_score) - float(fear_score)\n        \n        return result\n        \n    except Exception:\n        # If the required 'hope' or 'fear' columns are missing (KeyError),\n        # or if the data within them is not numeric (TypeError), an exception\n        # will be caught. In this case, the calculation cannot be completed,\n        # so we return None to indicate missing or invalid data.\n        return None\n\ndef calculate_success_climate(data, **kwargs):\n    \"\"\"\n    Calculate success_climate: Difference between compersion and envy scores\n\n    Formula: compersion - envy\n    \n    Args:\n        data: pandas DataFrame or Series with dimension scores.\n        **kwargs: Additional parameters (unused).\n        \n    Returns:\n        float: Calculated result or None if insufficient data.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # The calculation requires 'compersion' and 'envy' scores.\n        # This will fail with a KeyError if columns are not present,\n        # which is handled by the except block.\n        compersion_score = data['compersion']\n        envy_score = data['envy']\n        \n        # Handle cases where columns exist but data is missing (NaN/None)\n        if pd.isna(compersion_score) or pd.isna(envy_score):\n            return None\n            \n        # Ensure values are numeric and calculate the result\n        result = float(compersion_score) - float(envy_score)\n        \n        # A final check for non-finite results like infinity\n        if not np.isfinite(result):\n            return None\n            \n        return result\n        \n    except Exception:\n        # Catches KeyError if columns are missing, TypeError if data is not\n        # subscriptable, ValueError if scores aren't numeric, or any other error.\n        return None\n\ndef calculate_relational_climate(data, **kwargs):\n    \"\"\"\n    Calculate relational_climate: Difference between amity and enmity scores\n\n    Formula: amity - enmity\n    \n    Args:\n        data: pandas DataFrame with dimension scores (single row treated as Series)\n        **kwargs: Additional parameters\n        \n    Returns:\n        float: Calculated result or None if insufficient data\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # The calculation is \"Difference between amity and enmity scores\".\n        # We expect 'amity' and 'enmity' to be columns in the input data.\n        amity_score = data['amity']\n        enmity_score = data['enmity']\n\n        # Handle cases where scores are missing (NaN, None, etc.)\n        if pd.isna(amity_score) or pd.isna(enmity_score):\n            return None\n\n        # Ensure scores are numeric and perform the calculation.\n        # The float conversion will raise an error for non-numeric strings,\n        # which is caught by the except block.\n        result = float(amity_score) - float(enmity_score)\n        \n        # Return the result, ensuring it's not NaN from an edge case calculation\n        return result if pd.notna(result) else None\n\n    except Exception:\n        # Catches KeyError if columns are missing, TypeError/ValueError if\n        # scores are not numeric, and any other unexpected errors.\n        return None\n\ndef calculate_goal_orientation(data, **kwargs):\n    \"\"\"\n    Calculate goal_orientation: Difference between cohesive goals and fragmentative goals\n\n    Formula: cohesive_goals - fragmentative_goals\n    \n    Args:\n        data: pandas DataFrame with dimension scores (passed as a Series for a single row)\n        **kwargs: Additional parameters\n        \n    Returns:\n        float: Calculated result or None if insufficient data\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # Retrieve the specific scores needed for the calculation from the data Series.\n        cohesive_goals = data['cohesive_goals']\n        fragmentative_goals = data['fragmentative_goals']\n        \n        # Gracefully handle missing data by checking for NaN or None values.\n        if pd.isna(cohesive_goals) or pd.isna(fragmentative_goals):\n            return None\n            \n        # Perform the calculation and ensure the result is a standard Python float.\n        result = float(cohesive_goals) - float(fragmentative_goals)\n        \n        return result\n        \n    except Exception:\n        # A broad exception handler is used as per the requirements.\n        # This will catch errors such as missing columns (KeyError) or\n        # non-numeric data that cannot be converted (TypeError, ValueError),\n        # ensuring the function returns None if the calculation cannot be completed.\n        return None\n\ndef calculate_overall_cohesion_index(data, **kwargs):\n    \"\"\"\n    Calculate overall_cohesion_index: Comprehensive measure combining all dimensions.\n\n    This index measures the balance between the positive and negative sentiment\n    dimensions. A score of 1.0 indicates perfect balance (e.g., positive and\n    negative sentiments are equal), while a score of 0.0 indicates that one\n    sentiment completely dominates the other.\n\n    Formula:\n    1 - |positive_sentiment - negative_sentiment|\n\n    Args:\n        data (pd.Series or pd.DataFrame): A single row of analysis data.\n        **kwargs: Additional parameters (unused).\n\n    Returns:\n        float: The calculated overall cohesion index (0.0 to 1.0), or None\n               if the necessary dimension scores are missing or invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    # The theoretical framework defines these dimensions.\n    # The function is written to operate on these dimensions.\n    # If the input data does not contain these columns, it will gracefully fail.\n    positive_col = 'positive_sentiment'\n    negative_col = 'negative_sentiment'\n\n    try:\n        # Ensure data is a pandas Series for consistent access\n        if isinstance(data, pd.DataFrame):\n            if len(data) != 1:\n                # This function is designed to operate on a single row.\n                return None\n            data = data.iloc[0]\n\n        # Check for required columns\n        if positive_col not in data.index or negative_col not in data.index:\n            return None\n\n        # Extract dimension scores\n        positive_score = data[positive_col]\n        negative_score = data[negative_col]\n\n        # Handle missing data for the specific scores\n        if pd.isna(positive_score) or pd.isna(negative_score):\n            return None\n\n        # Validate that scores are numeric floats or integers\n        if not all(isinstance(score, (int, float, np.number)) for score in [positive_score, negative_score]):\n            return None\n            \n        # Ensure scores are within the expected 0.0-1.0 range\n        if not (0.0 <= positive_score <= 1.0 and 0.0 <= negative_score <= 1.0):\n            # Out-of-range inputs are considered invalid for this calculation\n            return None\n\n        # Calculate the cohesion index\n        cohesion_index = 1.0 - abs(positive_score - negative_score)\n\n        return float(cohesion_index)\n\n    except (KeyError, TypeError, AttributeError, IndexError):\n        # KeyError: if columns are missing.\n        # TypeError: if data is not subscriptable or values are wrong type.\n        # AttributeError: if `data` is not a DataFrame/Series.\n        # IndexError: if DataFrame is empty.\n        return None\n    except Exception:\n        # Catch any other unexpected errors for production robustness\n        return None\n\ndef calculate_all_derived_metrics(data: pd.DataFrame) -> Dict[str, Optional[float]]:\n    \"\"\"\n    Calculate all derived metrics for the given dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        \n    Returns:\n        Dictionary mapping metric names to calculated values\n    \"\"\"\n    results = {}\n    \n    # Get all calculation functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith('calculate_') and \n            name not in ['calculate_all_derived_metrics', 'calculate_derived_metrics']):\n            try:\n                results[name.replace('calculate_', '')] = obj(data)\n            except Exception as e:\n                results[name.replace('calculate_', '')] = None\n                \n    return results\n\n\ndef calculate_derived_metrics(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Template-compatible wrapper function for derived metrics calculation.\n    \n    This function is called by the universal notebook template and returns\n    the original data with additional derived metric columns.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        \n    Returns:\n        DataFrame with original data plus derived metric columns\n    \"\"\"\n    # Calculate all derived metrics\n    derived_metrics = calculate_all_derived_metrics(data)\n    \n    # Create a copy of the original data\n    result = data.copy()\n    \n    # Add derived metrics as new columns\n    for metric_name, metric_value in derived_metrics.items():\n        if metric_value is not None:\n            # For scalar metrics, broadcast to all rows\n            result[metric_name] = metric_value\n        else:\n            # For failed calculations, use NaN\n            result[metric_name] = np.nan\n    \n    return result\n",
    "cached_with_code": true
  },
  "derived_metrics_data": {
    "status": "success",
    "original_count": 2,
    "derived_count": 2,
    "derived_metrics": [
      {
        "analysis_id": "analysis_2101dc6e8361",
        "result_hash": "2b8a7938c0a37f5f3aee48ccbc790b30fe366d40f7974b5a95012e2a144665ec",
        "result_content": {
          "analysis_id": "analysis_2101dc6e8361",
          "agent_name": "EnhancedAnalysisAgent",
          "agent_version": "enhanced_v2.1_raw_output",
          "experiment_name": "nano_test_experiment",
          "model_used": "vertex_ai/gemini-2.5-flash",
          "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 1.0,\n    \"analysis_notes\": \"Clear, designed test case for sentiment analysis, resulting in highly consistent scores across all three independent approaches.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"6950ccbc6b14...\",\n      \"document_name\": \"positive_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"This is a wonderful day! Everything is going perfectly. I feel great about the future. Success is everywhere. The team did an excellent job. We're achieving amazing results. Optimism fills the air. What a fantastic opportunity! I'm thrilled with the progress. Everything looks bright and promising.\",\n          \"confidence\": 1.0,\n          \"context_type\": \"explicit statement\"\n        },\n        {\n          \"dimension\": \"negative_sentiment\",\n          \"quote_text\": \"\",\n          \"confidence\": 1.0,\n          \"context_type\": \"absence of evidence\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
          "evidence_hash": "fa7467a32eb4b72b67b5442879b58f9caed8e698efa61d7604901fd17b4c5caf",
          "execution_metadata": {
            "start_time": "2025-08-30T02:20:41.697303+00:00",
            "end_time": "2025-08-30T02:20:54.472029+00:00",
            "duration_seconds": 12.774701
          },
          "input_artifacts": {
            "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
            "document_hashes": [
              "6950ccbc6b14eabf43508448cf888b2c590f44e549a32b8ae60e943710e74019"
            ],
            "num_documents": 1
          },
          "provenance": {
            "security_boundary": {
              "experiment_name": "nano_test_experiment",
              "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
              "boundary_type": "filesystem",
              "security_level": "experiment_scoped"
            },
            "audit_session_id": "20250830T022041Z_f3dc06d7"
          }
        },
        "cached": true
      },
      {
        "analysis_id": "analysis_4bc6d8eb51af",
        "result_hash": "0d1ba0ea1c1d7635082df4b21305a556fd5830852f6c90c2052bd267566441fa",
        "result_content": {
          "analysis_id": "analysis_4bc6d8eb51af",
          "agent_name": "EnhancedAnalysisAgent",
          "agent_version": "enhanced_v2.1_raw_output",
          "experiment_name": "nano_test_experiment",
          "model_used": "vertex_ai/gemini-2.5-flash",
          "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 1.0,\n    \"analysis_notes\": \"Aggregated from three independent analytical passes focusing on explicit evidence, rhetorical context, and linguistic patterns.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"1df25093a747...\",\n      \"document_name\": \"negative_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"\",\n          \"confidence\": 1.0,\n          \"context_type\": \"absence of evidence\"\n        },\n        {\n          \"dimension\": \"negative_sentiment\",\n          \"quote_text\": \"This is a terrible situation.\",\n          \"confidence\": 1.0,\n          \"context_type\": \"explicit statement\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
          "evidence_hash": "0fad04b8d9149f05001dd4794821c67f1efa2dc4d532a22ce1f19fdac21e27e5",
          "execution_metadata": {
            "start_time": "2025-08-30T02:20:54.475197+00:00",
            "end_time": "2025-08-30T02:21:04.553896+00:00",
            "duration_seconds": 10.078681
          },
          "input_artifacts": {
            "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
            "document_hashes": [
              "1df25093a747678c5dbe724ecfb963b88049f9a7a536a3f23f064fd592744f91"
            ],
            "num_documents": 1
          },
          "provenance": {
            "security_boundary": {
              "experiment_name": "nano_test_experiment",
              "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
              "boundary_type": "filesystem",
              "security_level": "experiment_scoped"
            },
            "audit_session_id": "20250830T022041Z_f3dc06d7"
          }
        },
        "cached": true
      }
    ],
    "columns_added": []
  },
  "status": "success_with_data",
  "validation_passed": true
}