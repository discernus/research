{
  "status": "success",
  "functions_generated": 3,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 15462,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: nano_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-11T23:20:58.508340+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef get_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates and returns descriptive statistics for all sentiment-related scores.\n\n    Methodology:\n    This function computes standard descriptive statistics (count, mean, standard deviation, min, quartiles, max)\n    for the primary dimensional scores and their associated salience and confidence values. As a seasoned\n    practitioner, I must emphasize that this is a foundational step in any analysis, providing a high-level\n    overview of the data distribution.\n\n    Sample Size and Power Assessment:\n    This is an exploratory analysis. The sample size (N) is determined by the number of documents in the input\n    DataFrame. For N < 15, these statistics are purely descriptive and should not be used for generalization.\n    The results are suggestive of patterns within this specific dataset only.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns for sentiment scores.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing a table of descriptive statistics, or None if the\n              required columns are not present or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = [\n            'positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence',\n            'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence'\n        ]\n\n        if not all(col in data.columns for col in required_cols):\n            # As a rigorous practitioner, I cannot proceed without the specified data columns.\n            return None\n\n        # Select only the columns of interest for this analysis\n        df_scores = data[required_cols]\n\n        # Ensure data is numeric, coercing errors to NaN\n        for col in df_scores.columns:\n            df_scores[col] = pd.to_numeric(df_scores[col], errors='coerce')\n\n        # Drop rows with any NaNs in the selected columns to ensure statistical validity\n        df_scores.dropna(inplace=True)\n        \n        n_samples = len(df_scores)\n        if n_samples < 1:\n            return {\"message\": \"No valid data available for descriptive statistics.\"}\n\n        # Calculate descriptive statistics\n        desc_stats = df_scores.describe().to_dict()\n\n        power_caveat = f\"Exploratory analysis - results are suggestive rather than conclusive (N={n_samples}).\"\n        if 15 <= n_samples < 30:\n            power_caveat = f\"Results should be interpreted with caution due to moderate sample size (N={n_samples}).\"\n        elif n_samples >= 30:\n            power_caveat = f\"Analysis is adequately powered (N={n_samples}).\"\n\n        return {\n            \"description\": \"Descriptive statistics for sentiment scores.\",\n            \"power_assessment\": power_caveat,\n            \"sample_size\": n_samples,\n            \"statistics\": desc_stats\n        }\n\n    except Exception as e:\n        # Proper error logging should occur in a production system.\n        # For this framework, returning None indicates failure.\n        print(f\"Error in get_descriptive_statistics: {e}\")\n        return None\n\ndef compare_sentiment_scores(data, **kwargs):\n    \"\"\"\n    Compares mean sentiment scores between document types (e.g., positive vs. negative).\n\n    Methodology:\n    This function addresses the core research question by directly comparing sentiment scores. It first\n    categorizes documents into groups based on their filenames (e.g., 'positive_test.txt' -> 'Positive',\n    'negative_test.txt' -> 'Negative'). It then calculates the mean 'positive_sentiment_raw' and\n    'negative_sentiment_raw' for each group. This is a direct test of the expected outcome: a \"clear\n    distinction between positive and negative sentiment scores\".\n\n    Given the extremely small sample size typical of this experiment (N=2), this is a Tier 3\n    exploratory analysis. No inferential tests (like t-tests) are appropriate or statistically valid.\n    The focus is on the magnitude of the difference in means as a descriptive measure.\n\n    Sample Size and Power Assessment:\n    Exploratory analysis - results are suggestive rather than conclusive. With N < 8 per group,\n    this analysis serves only to validate pipeline functionality by observing expected patterns in the\n    output scores. The findings are not generalizable.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing analysis data with 'document_name' and sentiment scores.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary with mean scores per group, or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        df = data[required_cols].copy()\n\n        # Methodologically sound data typing\n        df['positive_sentiment_raw'] = pd.to_numeric(df['positive_sentiment_raw'], errors='coerce')\n        df['negative_sentiment_raw'] = pd.to_numeric(df['negative_sentiment_raw'], errors='coerce')\n        df.dropna(subset=['positive_sentiment_raw', 'negative_sentiment_raw'], inplace=True)\n\n        if df.empty:\n            return {\"message\": \"No valid data for comparison.\"}\n\n        # Create grouping variable based on document name, a standard practice for test harnesses.\n        def assign_group(name):\n            if 'positive' in name:\n                return 'Positive Test Document'\n            elif 'negative' in name:\n                return 'Negative Test Document'\n            else:\n                return 'Uncategorized'\n        \n        df['group'] = df['document_name'].apply(assign_group)\n\n        if 'Positive Test Document' not in df['group'].values or 'Negative Test Document' not in df['group'].values:\n            return {\"message\": \"Data for both positive and negative test documents is required for comparison.\"}\n\n        # Group by the newly created category and calculate mean scores.\n        comparison_stats = df.groupby('group')[['positive_sentiment_raw', 'negative_sentiment_raw']].mean()\n        group_counts = df.groupby('group').size().to_dict()\n        \n        n_total = len(df)\n        \n        results = {\n            \"description\": \"Comparison of mean sentiment scores by document type.\",\n            \"power_assessment\": f\"Exploratory analysis - results are suggestive rather than conclusive (Total N={n_total}). This test validates expected score separation.\",\n            \"group_counts\": group_counts,\n            \"mean_scores_by_group\": comparison_stats.to_dict('index')\n        }\n        \n        return results\n\n    except Exception as e:\n        print(f\"Error in compare_sentiment_scores: {e}\")\n        return None\n\ndef analyze_sentiment_correlation(data, **kwargs):\n    \"\"\"\n    Calculates the correlation between positive and negative sentiment scores.\n\n    Methodology:\n    This function computes the Pearson correlation coefficient (r) between 'positive_sentiment_raw' and\n    'negative_sentiment_raw'. This helps to understand the relationship between the two dimensions: are they\n    independent, or does the presence of one preclude the other? A strong negative correlation is often\n    expected in simple sentiment frameworks.\n\n    Sample Size and Power Assessment:\n    This is a Tier 3 (Exploratory) analysis. Correlation analysis is highly sensitive to sample size.\n    - N < 15: Results are highly unstable and not statistically meaningful. They should be considered\n      purely descriptive of the current sample.\n    - With N=2, the correlation will mathematically be either +1, -1, or undefined. This result is an\n      artifact of the small sample and provides no real insight into the relationship between variables.\n      It is included here for pipeline validation purposes only.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary with the correlation matrix and interpretation caveats, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        df_scores = data[required_cols].copy()\n        \n        # Ensure data is numeric and handle missing values\n        for col in df_scores.columns:\n            df_scores[col] = pd.to_numeric(df_scores[col], errors='coerce')\n        df_scores.dropna(inplace=True)\n\n        n_samples = len(df_scores)\n\n        if n_samples < 3:\n            # With N<3, correlation is not a meaningful statistic.\n            # This check upholds my commitment to statistical rigor.\n            return {\n                \"description\": \"Correlation analysis between positive and negative sentiment.\",\n                \"power_assessment\": f\"Exploratory analysis - results are not statistically meaningful (N={n_samples}).\",\n                \"sample_size\": n_samples,\n                \"message\": \"Correlation requires at least 3 data points for a meaningful calculation. Analysis aborted.\",\n                \"correlation_matrix\": None\n            }\n\n        # Calculate the correlation matrix\n        correlation_matrix = df_scores.corr(method='pearson')\n\n        power_caveat = f\"Exploratory analysis - results are suggestive rather than conclusive (N={n_samples}).\"\n        if 15 <= n_samples < 30:\n            power_caveat = f\"Results should be interpreted with caution due to moderate sample size (N={n_samples}).\"\n        elif n_samples >= 30:\n            power_caveat = f\"Analysis is adequately powered (N={n_samples}).\"\n\n        return {\n            \"description\": \"Correlation analysis between positive and negative sentiment.\",\n            \"power_assessment\": power_caveat,\n            \"sample_size\": n_samples,\n            \"correlation_matrix\": correlation_matrix.to_dict()\n        }\n\n    except Exception as e:\n        print(f\"Error in analyze_sentiment_correlation: {e}\")\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}