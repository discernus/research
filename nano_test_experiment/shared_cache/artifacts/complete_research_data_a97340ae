{
  "raw_analysis_results": [
    {
      "analysis_id": "analysis_2101dc6e8361",
      "result_hash": "6b42184255fb2f4857389627f300e5167f51d9685f0a6820d44634cf80b92919",
      "result_content": {
        "analysis_id": "analysis_2101dc6e8361",
        "agent_name": "EnhancedAnalysisAgent",
        "agent_version": "enhanced_v2.1_raw_output",
        "experiment_name": "nano_test_experiment",
        "model_used": "vertex_ai/gemini-2.5-flash",
        "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.95,\n    \"analysis_notes\": \"Applied three independent analytical approaches with median aggregation. Document exhibits clear and strong positive sentiment with no discernible negative sentiment.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"6950ccbc6b14...\",\n      \"document_name\": \"positive_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 0.97\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"This is a wonderful day! Everything is going perfectly. I feel great about the future. Success is everywhere. The team did an excellent job. We're achieving amazing results. Optimism fills the air. What a fantastic opportunity! I'm thrilled with the progress. Everything looks bright and promising.\",\n          \"confidence\": 1.0,\n          \"context_type\": \"entire_document_theme\"\n        },\n        {\n          \"dimension\": \"negative_sentiment\",\n          \"quote_text\": \"\",\n          \"confidence\": 1.0,\n          \"context_type\": \"absence_of_evidence\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
        "evidence_hash": "4879cec64f5057bf7d11bb8430f17eef5bad20755f174dfe6addf98aaf88766c",
        "execution_metadata": {
          "start_time": "2025-09-09T17:42:45.358597+00:00",
          "end_time": "2025-09-09T17:42:59.035268+00:00",
          "duration_seconds": 13.676659
        },
        "input_artifacts": {
          "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
          "document_hashes": [
            "6950ccbc6b14eabf43508448cf888b2c590f44e549a32b8ae60e943710e74019"
          ],
          "num_documents": 1
        },
        "provenance": {
          "security_boundary": {
            "experiment_name": "nano_test_experiment",
            "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
            "boundary_type": "filesystem",
            "security_level": "experiment_scoped"
          },
          "audit_session_id": "20250909T174245Z_3c8bce26"
        }
      },
      "cached": true
    },
    {
      "analysis_id": "analysis_4bc6d8eb51af",
      "result_hash": "99e53f629ffbac36506af0c22421eed7551c582a11a871f9fffde2571e93fb4f",
      "result_content": {
        "analysis_id": "analysis_4bc6d8eb51af",
        "agent_name": "EnhancedAnalysisAgent",
        "agent_version": "enhanced_v2.1_raw_output",
        "experiment_name": "nano_test_experiment",
        "model_used": "vertex_ai/gemini-2.5-flash",
        "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.98,\n    \"analysis_notes\": \"Applied three independent analytical approaches with median aggregation to a document with clear sentiment.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"1df25093a747...\",\n      \"document_name\": \"negative_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"\",\n          \"confidence\": 0.0,\n          \"context_type\": \"absence_of_evidence\"\n        },\n        {\n          \"dimension\": \"negative_sentiment\",\n          \"quote_text\": \"This is a terrible situation. Everything is going wrong. I feel awful about the future. Failure surrounds us. The team did a horrible job. We're facing disaster. Pessimism fills the air. What a disastrous outcome! I'm devastated by the results. Everything looks dark and hopeless.\",\n          \"confidence\": 1.0,\n          \"context_type\": \"direct_statement\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
        "evidence_hash": "0f79a62068cbc77b623aa1afecddee6affa212eb2cc038f358b31aa6ba9d7eaa",
        "execution_metadata": {
          "start_time": "2025-09-09T17:42:59.036792+00:00",
          "end_time": "2025-09-09T17:43:17.464846+00:00",
          "duration_seconds": 18.428046
        },
        "input_artifacts": {
          "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
          "document_hashes": [
            "1df25093a747678c5dbe724ecfb963b88049f9a7a536a3f23f064fd592744f91"
          ],
          "num_documents": 1
        },
        "provenance": {
          "security_boundary": {
            "experiment_name": "nano_test_experiment",
            "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
            "boundary_type": "filesystem",
            "security_level": "experiment_scoped"
          },
          "audit_session_id": "20250909T174245Z_3c8bce26"
        }
      },
      "cached": true
    }
  ],
  "derived_metrics_results": {
    "status": "completed",
    "derived_metrics_hash": "479862731f8ccc10676b3639e624d7de8d1f59d05e923afd51345eab66f455f0",
    "functions_generated": 6,
    "derived_metrics_results": {
      "generation_metadata": {
        "status": "success",
        "functions_generated": 6,
        "output_file": "automatedderivedmetricsagent_functions.py",
        "module_size": 12332,
        "function_code_content": "\"\"\"\nAutomated Derived Metrics Functions\n===================================\n\nGenerated by AutomatedDerivedMetricsAgent for experiment: Test Experiment\nDescription: Test experiment for derived metrics\nGenerated: 2025-09-09T17:46:38.942547+00:00\n\nThis module contains automatically generated calculation functions for derived metrics\nas specified in the framework's natural language descriptions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom typing import Optional, Dict, Any\n\n\ndef calculate_identity_tension(data, **kwargs):\n    \"\"\"\n    Calculate identity_tension: Conflict between tribal dominance and individual dignity dimensions.\n\n    This calculation is requested by the research framework but the necessary dimensions\n    ('tribal_dominance', 'individual_dignity') are not present in the provided\n    data structure for the 'Sentiment Binary Framework v1.0'. This function is\n    designed to be forward-compatible and will return None until the required\n    data columns are available.\n\n    Formula: abs(tribal_dominance - individual_dignity)\n    \n    Args:\n        data (pd.Series): A single row of analysis data as a pandas Series.\n        **kwargs: Additional keyword arguments (unused).\n        \n    Returns:\n        float: The calculated identity tension score, or None if the necessary\n               columns ('tribal_dominance', 'individual_dignity') are missing\n               or contain non-numeric data.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # The prompt requires a calculation based on dimensions that are not present\n        # in the specified 'actual data structure'. This function attempts to\n        # access these theoretical columns and will gracefully fail, returning None,\n        # as per the requirements.\n        tribal_dominance = data['tribal_dominance']\n        individual_dignity = data['individual_dignity']\n\n        # Ensure both values are present and numeric before calculation\n        if pd.isna(tribal_dominance) or pd.isna(individual_dignity):\n            return None\n\n        # Calculate the absolute difference to represent tension\n        tension = np.abs(float(tribal_dominance) - float(individual_dignity))\n        \n        return float(tension)\n\n    except (KeyError, TypeError, ValueError):\n        # KeyError: Will trigger because the required columns are not in the data.\n        # TypeError/ValueError: Will trigger if columns exist but are not numeric.\n        # This ensures the function returns None as required by the prompt's constraints.\n        return None\n    except Exception:\n        # A final catch-all for any other unexpected errors.\n        return None\n\ndef calculate_emotional_balance(data, **kwargs):\n    \"\"\"\n    Calculate emotional_balance: Difference between hope and fear scores\n    Formula: hope - fear\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        **kwargs: Additional parameters\n        \n    Returns:\n        float: Calculated result or None if insufficient data\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # The calculation requires 'hope' and 'fear' scores.\n        # Per instructions, we must not assume column names exist if not specified\n        # in the data structure. We use .get() for safe access.\n        hope_score = data.get('hope')\n        fear_score = data.get('fear')\n\n        # The calculation cannot be performed if either score is missing (None)\n        # or is a non-numeric value like NaN.\n        if hope_score is None or fear_score is None or pd.isna(hope_score) or pd.isna(fear_score):\n            return None\n        \n        # Perform the calculation and ensure the result is a standard float.\n        return float(hope_score) - float(fear_score)\n\n    except (TypeError, ValueError):\n        # This handles cases where scores are present but not numeric (e.g., strings).\n        return None\n    except Exception:\n        # A general catch-all for any other unexpected errors, ensuring stability.\n        return None\n\ndef calculate_success_climate(data, **kwargs):\n    \"\"\"\n    Calculate success_climate: Difference between compersion and envy scores\n    \n    Formula: compersion - envy\n    \n    Args:\n        data: pandas DataFrame or Series with dimension scores.\n        **kwargs: Additional parameters (unused).\n        \n    Returns:\n        float: Calculated result or None if insufficient data.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # The calculation requires 'compersion' and 'envy' scores.\n        # .get() is used to safely retrieve values, returning None if the key is missing.\n        compersion_score = data.get('compersion')\n        envy_score = data.get('envy')\n        \n        # Check if either required score is missing (None or NaN).\n        # pd.isna() robustly handles both None and numpy.nan.\n        if pd.isna(compersion_score) or pd.isna(envy_score):\n            return None\n        \n        # Perform the calculation after ensuring data types are numeric.\n        result = float(compersion_score) - float(envy_score)\n        \n        return result\n        \n    except (TypeError, ValueError):\n        # Catches errors if scores are not convertible to float.\n        return None\n    except Exception:\n        # A general catch-all for any other unexpected errors.\n        return None\n\ndef calculate_relational_climate(data, **kwargs):\n    \"\"\"\n    Calculate relational_climate: Difference between amity and enmity scores.\n\n    Formula: 'Positive Sentiment' - 'Negative Sentiment'\n    \n    Args:\n        data (pd.Series): A single row of data with dimension scores.\n        **kwargs: Additional parameters (not used).\n        \n    Returns:\n        float: Calculated result or None if insufficient data.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # Per the 'Sentiment Binary Framework v1.0', amity is mapped to \n        # 'Positive Sentiment' and enmity to 'Negative Sentiment'.\n        amity_col = 'Positive Sentiment'\n        enmity_col = 'Negative Sentiment'\n\n        amity_score = data.get(amity_col)\n        enmity_score = data.get(enmity_col)\n\n        # pd.isna handles both None (from .get() on a missing column) and numpy.nan\n        if pd.isna(amity_score) or pd.isna(enmity_score):\n            return None\n        \n        # Calculate the difference, ensuring types are float for the operation\n        result = float(amity_score) - float(enmity_score)\n        \n        return result\n\n    except Exception:\n        # Catches any other error, such as non-numeric scores, and returns None.\n        return None\n\ndef calculate_goal_orientation(data, **kwargs):\n    \"\"\"\n    Calculate goal_orientation: Difference between cohesive goals and fragmentative goals\n\n    Formula: cohesive_goals - fragmentative_goals\n    \n    Args:\n        data: pandas DataFrame with dimension scores (expects a single row or Series)\n        **kwargs: Additional parameters\n        \n    Returns:\n        float: Calculated result or None if insufficient data\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # The calculation requires 'cohesive_goals' and 'fragmentative_goals' dimensions.\n        # These names are derived from the calculation's description.\n        cohesive_col = 'cohesive_goals'\n        fragmentative_col = 'fragmentative_goals'\n\n        # Accommodate both single-row DataFrame and Series input\n        if isinstance(data, pd.DataFrame):\n            if data.empty:\n                return None\n            s = data.iloc[0]\n        else:\n            s = data\n\n        # Use .get() to safely access values, returns None if key is missing\n        cohesive_score = s.get(cohesive_col)\n        fragmentative_score = s.get(fragmentative_col)\n\n        # Check for missing data (pd.isna handles None, np.nan, etc.)\n        if pd.isna(cohesive_score) or pd.isna(fragmentative_score):\n            return None\n\n        # Perform the calculation and ensure result is a standard float\n        result = float(cohesive_score) - float(fragmentative_score)\n        \n        return result\n\n    except (TypeError, ValueError):\n        # Catches errors if scores are not convertible to float\n        return None\n    except Exception:\n        # General catch-all for any other unexpected errors\n        return None\n\ndef calculate_overall_cohesion_index(data, **kwargs):\n    \"\"\"\n    Calculate overall_cohesion_index: Comprehensive measure combining all dimensions.\n\n    This index measures the degree to which the sentiment expressed is uniform rather than ambivalent.\n    A score of 1.0 indicates perfect cohesion (sentiment is purely positive or purely negative),\n    while a score of 0.0 indicates perfect ambivalence (equal presence of positive and negative sentiment).\n\n    Formula: 1 - abs(positive_sentiment - negative_sentiment)\n    \n    Args:\n        data (pd.Series or pd.DataFrame): A single row of data containing the dimension scores.\n                                           Must have 'positive_sentiment' and 'negative_sentiment' columns.\n        **kwargs: Additional parameters (not used in this calculation).\n        \n    Returns:\n        float: The calculated overall cohesion index, or None if required data is missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # The theoretical framework defines 'Positive Sentiment' and 'Negative Sentiment' as the dimensions.\n        # We map these to the expected column names 'positive_sentiment' and 'negative_sentiment'.\n        positive_score = data['positive_sentiment']\n        negative_score = data['negative_sentiment']\n\n        # Ensure both required scores are present and are numeric\n        if pd.isna(positive_score) or pd.isna(negative_score):\n            return None\n        \n        # The formula calculates cohesion. 1 means perfectly cohesive (one-sided sentiment).\n        # 0 means perfectly ambivalent (equal positive and negative sentiment).\n        cohesion_index = 1 - abs(float(positive_score) - float(negative_score))\n        \n        # Ensure the result is within the expected 0.0-1.0 range\n        return max(0.0, min(1.0, cohesion_index))\n\n    except (KeyError, TypeError, ValueError):\n        # KeyError if columns are missing.\n        # TypeError/ValueError if data is not numeric.\n        return None\n    except Exception:\n        # Catch any other unexpected errors\n        return None\n\ndef calculate_all_derived_metrics(data: pd.DataFrame) -> Dict[str, Optional[float]]:\n    \"\"\"\n    Calculate all derived metrics for the given dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        \n    Returns:\n        Dictionary mapping metric names to calculated values\n    \"\"\"\n    results = {}\n    \n    # Get all calculation functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith('calculate_') and \n            name not in ['calculate_all_derived_metrics', 'calculate_derived_metrics']):\n            try:\n                results[name.replace('calculate_', '')] = obj(data)\n            except Exception as e:\n                results[name.replace('calculate_', '')] = None\n                \n    return results\n\n\ndef calculate_derived_metrics(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Template-compatible wrapper function for derived metrics calculation.\n    \n    This function is called by the universal notebook template and returns\n    the original data with additional derived metric columns.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        \n    Returns:\n        DataFrame with original data plus derived metric columns\n    \"\"\"\n    # Calculate all derived metrics\n    derived_metrics = calculate_all_derived_metrics(data)\n    \n    # Create a copy of the original data\n    result = data.copy()\n    \n    # Add derived metrics as new columns\n    for metric_name, metric_value in derived_metrics.items():\n        if metric_value is not None:\n            # For scalar metrics, broadcast to all rows\n            result[metric_name] = metric_value\n        else:\n            # For failed calculations, use NaN\n            result[metric_name] = np.nan\n    \n    return result\n",
        "cached_with_code": true
      },
      "derived_metrics_data": {
        "status": "success",
        "original_count": 2,
        "derived_count": 2,
        "derived_metrics": [
          {
            "analysis_id": "analysis_2101dc6e8361",
            "result_hash": "6b42184255fb2f4857389627f300e5167f51d9685f0a6820d44634cf80b92919",
            "result_content": {
              "analysis_id": "analysis_2101dc6e8361",
              "agent_name": "EnhancedAnalysisAgent",
              "agent_version": "enhanced_v2.1_raw_output",
              "experiment_name": "nano_test_experiment",
              "model_used": "vertex_ai/gemini-2.5-flash",
              "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.95,\n    \"analysis_notes\": \"Applied three independent analytical approaches with median aggregation. Document exhibits clear and strong positive sentiment with no discernible negative sentiment.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"6950ccbc6b14...\",\n      \"document_name\": \"positive_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 0.97\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"This is a wonderful day! Everything is going perfectly. I feel great about the future. Success is everywhere. The team did an excellent job. We're achieving amazing results. Optimism fills the air. What a fantastic opportunity! I'm thrilled with the progress. Everything looks bright and promising.\",\n          \"confidence\": 1.0,\n          \"context_type\": \"entire_document_theme\"\n        },\n        {\n          \"dimension\": \"negative_sentiment\",\n          \"quote_text\": \"\",\n          \"confidence\": 1.0,\n          \"context_type\": \"absence_of_evidence\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
              "evidence_hash": "4879cec64f5057bf7d11bb8430f17eef5bad20755f174dfe6addf98aaf88766c",
              "execution_metadata": {
                "start_time": "2025-09-09T17:42:45.358597+00:00",
                "end_time": "2025-09-09T17:42:59.035268+00:00",
                "duration_seconds": 13.676659
              },
              "input_artifacts": {
                "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
                "document_hashes": [
                  "6950ccbc6b14eabf43508448cf888b2c590f44e549a32b8ae60e943710e74019"
                ],
                "num_documents": 1
              },
              "provenance": {
                "security_boundary": {
                  "experiment_name": "nano_test_experiment",
                  "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
                  "boundary_type": "filesystem",
                  "security_level": "experiment_scoped"
                },
                "audit_session_id": "20250909T174245Z_3c8bce26"
              }
            },
            "cached": true
          },
          {
            "analysis_id": "analysis_4bc6d8eb51af",
            "result_hash": "99e53f629ffbac36506af0c22421eed7551c582a11a871f9fffde2571e93fb4f",
            "result_content": {
              "analysis_id": "analysis_4bc6d8eb51af",
              "agent_name": "EnhancedAnalysisAgent",
              "agent_version": "enhanced_v2.1_raw_output",
              "experiment_name": "nano_test_experiment",
              "model_used": "vertex_ai/gemini-2.5-flash",
              "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.98,\n    \"analysis_notes\": \"Applied three independent analytical approaches with median aggregation to a document with clear sentiment.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"1df25093a747...\",\n      \"document_name\": \"negative_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"\",\n          \"confidence\": 0.0,\n          \"context_type\": \"absence_of_evidence\"\n        },\n        {\n          \"dimension\": \"negative_sentiment\",\n          \"quote_text\": \"This is a terrible situation. Everything is going wrong. I feel awful about the future. Failure surrounds us. The team did a horrible job. We're facing disaster. Pessimism fills the air. What a disastrous outcome! I'm devastated by the results. Everything looks dark and hopeless.\",\n          \"confidence\": 1.0,\n          \"context_type\": \"direct_statement\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
              "evidence_hash": "0f79a62068cbc77b623aa1afecddee6affa212eb2cc038f358b31aa6ba9d7eaa",
              "execution_metadata": {
                "start_time": "2025-09-09T17:42:59.036792+00:00",
                "end_time": "2025-09-09T17:43:17.464846+00:00",
                "duration_seconds": 18.428046
              },
              "input_artifacts": {
                "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
                "document_hashes": [
                  "1df25093a747678c5dbe724ecfb963b88049f9a7a536a3f23f064fd592744f91"
                ],
                "num_documents": 1
              },
              "provenance": {
                "security_boundary": {
                  "experiment_name": "nano_test_experiment",
                  "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
                  "boundary_type": "filesystem",
                  "security_level": "experiment_scoped"
                },
                "audit_session_id": "20250909T174245Z_3c8bce26"
              }
            },
            "cached": true
          }
        ],
        "columns_added": []
      },
      "status": "success_with_data",
      "validation_passed": true
    }
  },
  "statistical_results": {
    "generation_metadata": {
      "status": "success",
      "functions_generated": 3,
      "output_file": "automatedstatisticalanalysisagent_functions.py",
      "module_size": 12039,
      "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: nano_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-09T17:58:21.233449+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef summarize_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates and returns overall descriptive statistics for the sentiment dimensions.\n\n    Methodology:\n    This function provides a high-level summary of the distribution of scores for all\n    available numerical dimensions in the dataset. It computes the count, mean, standard\n    deviation, minimum, maximum, and quartile values for each relevant column.\n    Given the extremely small sample size of this experiment (N=2), these statistics\n    serve as a basic data validation check rather than a meaningful summary of\n    distribution. This is a Tier 3 (Exploratory) analysis.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             including 'positive_sentiment_raw' and 'negative_sentiment_raw'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the descriptive statistics for numerical columns,\n              or None if the data is insufficient or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n        \n        # Select only numeric columns for description\n        numeric_data = data.select_dtypes(include=np.number)\n        if numeric_data.empty:\n            return None\n\n        # As a conservative practitioner, I must note the sample size.\n        n_samples = len(data)\n        \n        stats = numeric_data.describe().to_dict()\n\n        return {\n            \"summary\": f\"Exploratory analysis - results are suggestive rather than conclusive (N={n_samples})\",\n            \"descriptive_statistics\": stats\n        }\n\n    except Exception as e:\n        # In a production environment, we would log the error `e`.\n        return None\n\ndef compare_sentiment_scores(data, **kwargs):\n    \"\"\"\n    Compares mean sentiment scores between document types.\n\n    Methodology:\n    This function directly addresses the research question about distinguishing between\n    positive and negative documents. It groups the data by 'document_name' and\n    calculates the mean score for 'positive_sentiment_raw' and 'negative_sentiment_raw'\n    for each document.\n\n    This is a Tier 3 (Exploratory) analysis. With a sample size of N=1 per group\n    (one positive, one negative document), inferential tests are not possible or\n    meaningful. The output is purely descriptive, intended to validate that the\n    positive test document has a high positive score and the negative test document\n    has a high negative score, as per the experiment's 'Expected Outcomes'.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             'document_name', 'positive_sentiment_raw', and\n                             'negative_sentiment_raw'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary with mean scores per document, or None if data is invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        required_cols = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        # There is no corpus manifest, so grouping is based on the document filename.\n        # This is the only available grouping variable.\n        if 'document_name' not in data.columns:\n            return None\n            \n        n_samples = len(data)\n        \n        # For N=1 per group, mean() simply returns the value, which is what is needed for this validation.\n        grouped_scores = data.groupby('document_name')[['positive_sentiment_raw', 'negative_sentiment_raw']].mean().to_dict('index')\n\n        if not grouped_scores:\n            return None\n\n        return {\n            \"summary\": f\"Exploratory analysis - results are suggestive rather than conclusive (N={n_samples})\",\n            \"comparison_by_document\": grouped_scores\n        }\n\n    except Exception as e:\n        # In a production environment, we would log the error `e`.\n        return None\n\ndef analyze_sentiment_correlation(data, **kwargs):\n    \"\"\"\n    Calculates the correlation between positive and negative sentiment scores.\n\n    Methodology:\n    This function computes the Pearson correlation coefficient (r) between the\n    'positive_sentiment_raw' and 'negative_sentiment_raw' dimensions. This is a\n    standard analysis to check if the two dimensions are independent or inversely\n    related.\n\n    This is a Tier 3 (Exploratory) analysis.\n    CRITICAL CAVEAT: The total sample size for this experiment is N=2. A correlation\n    coefficient calculated on two data points is not statistically meaningful or\n    interpretable. It will mathematically result in -1, 0, or +1. This function is\n    provided for framework completeness and to demonstrate functionality, but its\n    output for this specific experiment should NOT be used for any conclusion.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             'positive_sentiment_raw' and 'negative_sentiment_raw'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the correlation matrix, or None if data is\n              insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        # A correlation requires at least 2 data points.\n        if len(data) < 2:\n            return None\n            \n        n_samples = len(data)\n\n        correlation_matrix = data[required_cols].corr(method='pearson')\n        \n        # Extract the specific correlation value of interest\n        correlation_value = correlation_matrix.loc['positive_sentiment_raw', 'negative_sentiment_raw']\n\n        return {\n            \"summary\": f\"Exploratory analysis - results are suggestive rather than conclusive (N={n_samples})\",\n            \"caveat\": \"Correlation on N<15 is highly unstable. With N=2, the result is not statistically interpretable.\",\n            \"pearson_correlation_coefficient\": correlation_value,\n            \"correlation_matrix\": correlation_matrix.to_dict()\n        }\n\n    except Exception as e:\n        # In a production environment, we would log the error `e`.\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
      "cached_with_code": true
    },
    "statistical_data": {
      "analyze_sentiment_correlation": {
        "summary": "Exploratory analysis - results are suggestive rather than conclusive (N=2)",
        "caveat": "Correlation on N<15 is highly unstable. With N=2, the result is not statistically interpretable.",
        "pearson_correlation_coefficient": -1.0,
        "correlation_matrix": {
          "positive_sentiment_raw": {
            "positive_sentiment_raw": 1.0,
            "negative_sentiment_raw": -1.0
          },
          "negative_sentiment_raw": {
            "positive_sentiment_raw": -1.0,
            "negative_sentiment_raw": 1.0
          }
        }
      },
      "compare_sentiment_scores": {
        "summary": "Exploratory analysis - results are suggestive rather than conclusive (N=2)",
        "comparison_by_document": {
          "negative_test.txt": {
            "positive_sentiment_raw": 0.0,
            "negative_sentiment_raw": 1.0
          },
          "positive_test.txt": {
            "positive_sentiment_raw": 1.0,
            "negative_sentiment_raw": 0.0
          }
        }
      },
      "generate_statistical_summary_report": "STATISTICAL ANALYSIS SUMMARY REPORT\n==================================================\nAnalysis Timestamp: Unknown\nSample Size: Unknown\nAlpha Level: Unknown\nVariables: 0\n",
      "perform_statistical_analysis": {
        "analysis_metadata": {
          "timestamp": "2025-09-09T21:06:04.227018",
          "sample_size": 2,
          "alpha_level": 0.05,
          "variables_analyzed": [
            "positive_sentiment_raw",
            "positive_sentiment_salience",
            "positive_sentiment_confidence",
            "negative_sentiment_raw",
            "negative_sentiment_salience",
            "negative_sentiment_confidence"
          ]
        }
      },
      "run_complete_statistical_analysis": {
        "analysis_metadata": {
          "timestamp": "2025-09-09T21:06:04.230775",
          "sample_size": 2,
          "alpha_level": 0.05,
          "variables_analyzed": [
            "positive_sentiment_raw",
            "positive_sentiment_salience",
            "positive_sentiment_confidence",
            "negative_sentiment_raw",
            "negative_sentiment_salience",
            "negative_sentiment_confidence"
          ]
        }
      },
      "summarize_descriptive_statistics": {
        "summary": "Exploratory analysis - results are suggestive rather than conclusive (N=2)",
        "descriptive_statistics": {
          "positive_sentiment_raw": {
            "count": 2.0,
            "mean": 0.5,
            "std": 0.7071067811865476,
            "min": 0.0,
            "25%": 0.25,
            "50%": 0.5,
            "75%": 0.75,
            "max": 1.0
          },
          "positive_sentiment_salience": {
            "count": 2.0,
            "mean": 0.5,
            "std": 0.7071067811865476,
            "min": 0.0,
            "25%": 0.25,
            "50%": 0.5,
            "75%": 0.75,
            "max": 1.0
          },
          "positive_sentiment_confidence": {
            "count": 2.0,
            "mean": 0.985,
            "std": 0.021213203435596444,
            "min": 0.97,
            "25%": 0.9775,
            "50%": 0.985,
            "75%": 0.9924999999999999,
            "max": 1.0
          },
          "negative_sentiment_raw": {
            "count": 2.0,
            "mean": 0.5,
            "std": 0.7071067811865476,
            "min": 0.0,
            "25%": 0.25,
            "50%": 0.5,
            "75%": 0.75,
            "max": 1.0
          },
          "negative_sentiment_salience": {
            "count": 2.0,
            "mean": 0.5,
            "std": 0.7071067811865476,
            "min": 0.0,
            "25%": 0.25,
            "50%": 0.5,
            "75%": 0.75,
            "max": 1.0
          },
          "negative_sentiment_confidence": {
            "count": 2.0,
            "mean": 1.0,
            "std": 0.0,
            "min": 1.0,
            "25%": 1.0,
            "50%": 1.0,
            "75%": 1.0,
            "max": 1.0
          }
        }
      }
    },
    "status": "success_with_data",
    "validation_passed": true
  }
}