{
  "status": "success",
  "functions_generated": 4,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 10749,
  "function_code_content": "import pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Helper function to calculate Cohen's d for exploratory purposes\ndef _calculate_cohens_d(x, y, n1, n2):\n    \"\"\"Helper to calculate Cohen's d for two independent samples.\"\"\"\n    if n1 <= 1 or n2 <= 1:\n        return np.nan # Cannot compute pooled_std with n=1\n    s_pooled = np.sqrt(((n1 - 1) * np.var(x, ddof=1) + (n2 - 1) * np.var(y, ddof=1)) / (n1 + n2 - 2))\n    if s_pooled == 0:\n        return np.inf * np.sign(np.mean(x) - np.mean(y))\n    return (np.mean(x) - np.mean(y)) / s_pooled\n\ndef summarize_descriptive_statistics(data: pd.DataFrame, **kwargs: Any) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Calculates and returns overall descriptive statistics for all numeric score columns.\n\n    This function serves as a basic data validation step, confirming that dimensional\n    scores were generated and fall within the expected ranges. It computes count, mean,\n    standard deviation, min, and max for each relevant column.\n\n    Methodology:\n    This is a Tier 3 (Exploratory) analysis. The statistics are purely descriptive and\n    provide a high-level overview of the dataset's characteristics. No inferential\n    claims can be made from these results.\n\n    Args:\n        data (pd.DataFrame): A DataFrame containing the analysis data with columns\n                             for dimensional scores.\n        **kwargs: Additional keyword arguments (unused).\n\n    Returns:\n        Optional[Dict[str, Any]]: A dictionary where keys are score column names and\n                                  values are dictionaries of descriptive statistics.\n                                  Returns None if the input data is empty or invalid.\n    \"\"\"\n    try:\n        if data.empty:\n            warnings.warn(\"Input data is empty. Cannot generate descriptive statistics.\")\n            return None\n\n        score_columns = [\n            'positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence',\n            'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence'\n        ]\n\n        # Ensure columns exist, otherwise, this will raise a KeyError handled by the outer try-except\n        valid_columns = [col for col in score_columns if col in data.columns]\n        if not valid_columns:\n            warnings.warn(\"No valid score columns found in the data.\")\n            return None\n\n        df_numeric = data[valid_columns].apply(pd.to_numeric, errors='coerce')\n\n        # Tier 3 Power Assessment\n        n_total = len(df_numeric.dropna())\n        power_caveat = f\"Exploratory analysis - results are suggestive rather than conclusive (N={n_total}).\"\n\n        desc_stats = df_numeric.describe().to_dict()\n\n        results = {\n            \"metadata\": {\n                \"analysis_tier\": \"Tier 3 (Exploratory)\",\n                \"sample_size\": n_total,\n                \"notes\": power_caveat\n            },\n            \"descriptive_statistics\": desc_stats\n        }\n\n        return results\n\n    except (KeyError, ValueError, AttributeError) as e:\n        warnings.warn(f\"An error occurred during descriptive statistics calculation: {e}\")\n        return None\n\n\ndef compare_sentiment_groups(data: pd.DataFrame, **kwargs: Any) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Compares sentiment scores between the 'positive_test.txt' and 'negative_test.txt' documents.\n\n    This function directly addresses the research question about distinguishing between\n    positive and negative sentiment. It groups the data by 'document_name' and calculates\n    the mean scores for each sentiment dimension.\n\n    Methodology:\n    This is a Tier 3 (Exploratory) analysis due to the extremely small sample size (N=2).\n    The function calculates descriptive statistics (mean) for each group and the raw\n    difference between them. Inferential tests like t-tests are inappropriate and not\n    performed. The results are purely descriptive and serve to validate pipeline functionality.\n\n    Args:\n        data (pd.DataFrame): A DataFrame containing the analysis data, including\n                             'document_name', 'positive_sentiment_raw', and\n                             'negative_sentiment_raw' columns.\n        **kwargs: Additional keyword arguments (unused).\n\n    Returns:\n        Optional[Dict[str, Any]]: A dictionary containing the mean scores for each document\n                                  group and the difference between them. Returns None if\n                                  required columns or documents are missing.\n    \"\"\"\n    try:\n        required_columns = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_columns):\n            warnings.warn(f\"Data is missing one or more required columns: {required_columns}\")\n            return None\n\n        # Define the groups based on the known test document names\n        positive_doc_name = 'positive_test.txt'\n        negative_doc_name = 'negative_test.txt'\n\n        group_positive = data[data['document_name'] == positive_doc_name]\n        group_negative = data[data['document_name'] == negative_doc_name]\n\n        if group_positive.empty or group_negative.empty:\n            warnings.warn(\"One or both required test documents are not present in the data.\")\n            return None\n\n        n_positive = len(group_positive)\n        n_negative = len(group_negative)\n\n        # Tier 3 Power Assessment\n        power_caveat = (f\"Exploratory analysis - results are purely descriptive (N_positive={n_positive}, \"\n                        f\"N_negative={n_negative}). Inferential conclusions are not possible.\")\n\n        # Calculate mean scores for each group\n        mean_scores = {\n            positive_doc_name: {\n                'positive_sentiment_mean': group_positive['positive_sentiment_raw'].mean(),\n                'negative_sentiment_mean': group_positive['negative_sentiment_raw'].mean(),\n                'n': n_positive\n            },\n            negative_doc_name: {\n                'positive_sentiment_mean': group_negative['positive_sentiment_raw'].mean(),\n                'negative_sentiment_mean': group_negative['negative_sentiment_raw'].mean(),\n                'n': n_negative\n            }\n        }\n\n        # Calculate the difference in means to quantify the \"clear distinction\"\n        difference_in_means = {\n            'positive_sentiment_diff': (mean_scores[positive_doc_name]['positive_sentiment_mean'] -\n                                        mean_scores[negative_doc_name]['positive_sentiment_mean']),\n            'negative_sentiment_diff': (mean_scores[positive_doc_name]['negative_sentiment_mean'] -\n                                        mean_scores[negative_doc_name]['negative_sentiment_mean'])\n        }\n\n        results = {\n            \"metadata\": {\n                \"analysis_tier\": \"Tier 3 (Exploratory)\",\n                \"sample_sizes\": {'positive_group': n_positive, 'negative_group': n_negative},\n                \"notes\": power_caveat\n            },\n            \"group_means\": mean_scores,\n            \"difference_of_means\": difference_in_means\n        }\n\n        return results\n\n    except (KeyError, ValueError, AttributeError) as e:\n        warnings.warn(f\"An error occurred during group comparison: {e}\")\n        return None\n\n\ndef analyze_dimensional_relationship(data: pd.DataFrame, **kwargs: Any) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Analyzes the correlation between positive and negative sentiment scores.\n\n    This function explores the theoretical expectation that positive and negative\n    sentiment are inversely related. It calculates the Pearson correlation coefficient\n    between the two raw sentiment scores.\n\n    Methodology:\n    This is a Tier 3 (Exploratory) analysis. Given the extremely small sample size (N<15),\n    any correlation result is highly unstable and should not be used for inference.\n    With N=2, the correlation will deterministically be -1, 0, or +1. This function is\n    provided as a template for exploratory data analysis, but its output must be\n    interpreted with extreme caution.\n\n    Args:\n        data (pd.DataFrame): A DataFrame containing 'positive_sentiment_raw' and\n                             'negative_sentiment_raw' columns.\n        **kwargs: Additional keyword arguments (unused).\n\n    Returns:\n        Optional[Dict[str, Any]]: A dictionary with the correlation coefficient and\n                                  a p-value, along with strong caveats about\n                                  interpretation. Returns None if data is insufficient.\n    \"\"\"\n    try:\n        required_columns = ['positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_columns):\n            warnings.warn(f\"Data is missing one or more required columns: {required_columns}\")\n            return None\n\n        df_corr = data[required_columns].dropna()\n        n = len(df_corr)\n\n        if n < 2:\n            warnings.warn(f\"Insufficient data (N={n}) to calculate a correlation. At least 2 data points are required.\")\n            return None\n\n        # Tier 3 Power Assessment\n        power_caveat = (f\"Exploratory analysis - results are suggestive rather than conclusive (N={n}). \"\n                        \"Correlation with N<15 is highly unstable. With N=2, the result is deterministic \"\n                        \"and not statistically meaningful.\")\n\n        pos_scores = df_corr['positive_sentiment_raw']\n        neg_scores = df_corr['negative_sentiment_raw']\n\n        # Suppress ConstantInputWarning if one of the vectors is constant\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", stats.ConstantInputWarning)\n            correlation, p_value = stats.pearsonr(pos_scores, neg_scores)\n\n        # Handle case where correlation is undefined (e.g., constant input)\n        if np.isnan(correlation):\n            correlation = 0.0\n            p_value = 1.0\n            power_caveat += \" Correlation is undefined (likely due to constant scores) and has been set to 0.\"\n\n\n        results = {\n            \"metadata\": {\n                \"analysis_tier\": \"Tier 3 (Exploratory)\",\n                \"sample_size\": n,\n                \"notes\": power_caveat\n            },\n            \"pearson_correlation\": {\n                \"correlation_coefficient\": correlation,\n                \"p_value\": p_value,\n                \"interpretation_note\": \"The p-value is not meaningful with this sample size.\"\n            }\n        }\n\n        return results\n\n    except (KeyError, ValueError, AttributeError) as e:\n        warnings.warn(f\"An error occurred during correlation analysis: {e}\")\n        return None",
  "cached_with_code": true
}