{
  "generation_metadata": {
    "status": "success",
    "functions_generated": 6,
    "output_file": "automatedderivedmetricsagent_functions.py",
    "module_size": 17271,
    "function_code_content": "\"\"\"\nAutomated Derived Metrics Functions\n===================================\n\nGenerated by AutomatedDerivedMetricsAgent for experiment: Test Experiment\nDescription: Test experiment for derived metrics\nGenerated: 2025-08-30T21:51:27.314008+00:00\n\nThis module contains automatically generated calculation functions for derived metrics\nas specified in the framework's natural language descriptions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom typing import Optional, Dict, Any\n\n\ndef calculate_identity_tension(data, **kwargs):\n    \"\"\"\n    Calculate identity_tension: Conflict between tribal dominance and individual dignity dimensions\n    \n    Formula: identity_tension = Negative Sentiment - Positive Sentiment\n    This formula assumes that 'Negative Sentiment' represents aspects of tribal dominance\n    (e.g., group-think, conflict) and 'Positive Sentiment' represents individual dignity\n    (e.g., personal well-being, respect). Higher values indicate greater tension or conflict.\n    The result ranges from -1.0 to 1.0, given input sentiment scores are between 0.0 and 1.0.\n\n    Args:\n        data: pandas DataFrame or Series representing a single row of analysis data.\n              Expected to contain 'Positive Sentiment' and 'Negative Sentiment' columns.\n        **kwargs: Additional parameters (not used in this calculation but included for framework compatibility).\n        \n    Returns:\n        float: Calculated identity tension score, or None if required data is missing or invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # Ensure data is treated as a Series for consistent access, assuming single-row processing\n        if isinstance(data, pd.DataFrame):\n            if data.empty:\n                return None\n            row_data = data.iloc[0]\n        elif isinstance(data, pd.Series):\n            row_data = data\n        else:\n            # Handle unexpected data type\n            return None\n\n        # Retrieve sentiment scores using .get() for safe column access (returns None if column not found)\n        positive_sentiment = row_data.get('Positive Sentiment')\n        negative_sentiment = row_data.get('Negative Sentiment')\n\n        # Check for missing columns or NaN values\n        if pd.isna(positive_sentiment) or pd.isna(negative_sentiment):\n            return None\n        \n        # Ensure values are numeric and within expected range (0.0-1.0)\n        # Although calculation works with any numeric, validation helps with data integrity.\n        if not (isinstance(positive_sentiment, (int, float)) and isinstance(negative_sentiment, (int, float)) and\n                0.0 <= positive_sentiment <= 1.0 and 0.0 <= negative_sentiment <= 1.0):\n            return None\n\n        # Calculate identity tension\n        # Tension is defined as the difference between negative and positive sentiment.\n        # A higher negative sentiment relative to positive sentiment indicates more \"conflict\" or \"tension\".\n        tension = float(negative_sentiment - positive_sentiment)\n        \n        return tension\n\n    except Exception:\n        # Catch any unexpected errors during calculation and return None\n        return None\n\ndef calculate_emotional_balance(data, **kwargs):\n    \"\"\"\n    Calculate emotional_balance: Difference between hope and fear scores.\n    \n    Formula: emotional_balance = hope - fear\n    \n    Args:\n        data: pandas Series (representing a single row of data) containing score dimensions.\n              CRITICAL NOTE: While the 'ACTUAL DATA STRUCTURE' section did not explicitly\n              list 'hope' or 'fear' as column names, this function assumes the input\n              'data' Series will contain columns named 'hope' and 'fear' to perform\n              the requested calculation. If these columns are not present, or their\n              values are not numeric/missing, the function will return None.\n        **kwargs: Additional parameters (not used in this calculation).\n        \n    Returns:\n        float: The calculated emotional balance (hope - fear).\n        None: If 'hope' or 'fear' columns are missing in the input `data`,\n              or if their values are non-numeric or NaN.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # The 'data' parameter is expected to be a pandas Series (a single row).\n        # Check if 'hope' and 'fear' keys (column names) exist in the Series' index.\n        if 'hope' not in data.index or 'fear' not in data.index:\n            return None\n\n        hope_score = data['hope']\n        fear_score = data['fear']\n\n        # Attempt to convert scores to numeric values, coercing any errors to NaN.\n        hope_score_numeric = pd.to_numeric(hope_score, errors='coerce')\n        fear_score_numeric = pd.to_numeric(fear_score, errors='coerce')\n\n        # Check if either of the converted scores is NaN (missing or non-numeric).\n        if pd.isna(hope_score_numeric) or pd.isna(fear_score_numeric):\n            return None\n\n        # Perform the emotional balance calculation.\n        emotional_balance = hope_score_numeric - fear_score_numeric\n        \n        # Return the result as a standard Python float.\n        return float(emotional_balance)\n\n    except Exception:\n        # Catch any unexpected errors during the process and return None,\n        # ensuring graceful handling as per requirements.\n        return None\n\ndef calculate_success_climate(data, **kwargs):\n    \"\"\"\n    Calculate success_climate: Difference between compersion and envy scores.\n    Formula: Positive Sentiment - Negative Sentiment\n    \n    Args:\n        data: pandas DataFrame (or Series for a single row) containing 'Positive Sentiment'\n              and 'Negative Sentiment' scores.\n        **kwargs: Additional parameters (not used in this calculation).\n        \n    Returns:\n        float: Calculated success_climate or None if insufficient data.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # Based on the framework's \"Analytical Methodology\", 'Positive Sentiment'\n        # and 'Negative Sentiment' are defined as the core dimensions.\n        # It is assumed that 'compersion' conceptually maps to 'Positive Sentiment'\n        # and 'enfy' maps to 'Negative Sentiment' for this calculation.\n        positive_sentiment_col = 'Positive Sentiment'\n        negative_sentiment_col = 'Negative Sentiment'\n        \n        # Safely retrieve the scores from the 'data' object.\n        # The .get() method is used for robustness, returning a default (np.nan)\n        # if the column is not found or the value is missing/non-numeric.\n        # pd.to_numeric handles conversion to float and coerces errors to NaN.\n        positive_score = pd.to_numeric(data.get(positive_sentiment_col, np.nan), errors='coerce')\n        negative_score = pd.to_numeric(data.get(negative_sentiment_col, np.nan), errors='coerce')\n\n        # If 'data' was a single-row DataFrame, .get() might return a pandas Series\n        # containing one value. Extract the scalar value if that's the case.\n        if isinstance(positive_score, pd.Series) and not positive_score.empty:\n            positive_score = positive_score.iloc[0]\n        if isinstance(negative_score, pd.Series) and not negative_score.empty:\n            negative_score = negative_score.iloc[0]\n            \n        # Check if either of the scores is NaN after retrieval and conversion.\n        # This handles cases where columns are missing or contain non-numeric data.\n        if pd.isna(positive_score) or pd.isna(negative_score):\n            return None\n            \n        # Perform the calculation: Positive Sentiment minus Negative Sentiment.\n        result = float(positive_score - negative_score)\n        \n        return result\n        \n    except Exception:\n        # Catch any unexpected errors that might occur during processing,\n        # ensuring the function fails gracefully by returning None.\n        return None\n\ndef calculate_relational_climate(data, **kwargs):\n    \"\"\"\n    Calculate relational_climate: Difference between amity and enmity scores.\n    Formula: Positive Sentiment - Negative Sentiment\n\n    Args:\n        data: pandas Series (representing a single row of a DataFrame) with\n              'Positive Sentiment' and 'Negative Sentiment' scores.\n        **kwargs: Additional parameters (not used in this calculation but\n                  included for framework compatibility).\n\n    Returns:\n        float: The calculated relational climate score.\n        None: If required 'Positive Sentiment' or 'Negative Sentiment' data is\n              missing, non-numeric, or NaN.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # Ensure the required score dimensions exist and are not NaN\n        if 'Positive Sentiment' in data and \\\n           'Negative Sentiment' in data and \\\n           pd.notna(data['Positive Sentiment']) and \\\n           pd.notna(data['Negative Sentiment']):\n\n            amity_score = data['Positive Sentiment']\n            enmity_score = data['Negative Sentiment']\n\n            # Ensure scores are numeric before calculation\n            if pd.api.types.is_numeric_dtype(type(amity_score)) and \\\n               pd.api.types.is_numeric_dtype(type(enmity_score)):\n                \n                result = float(amity_score - enmity_score)\n                return result\n            else:\n                # One or both scores are not numeric\n                return None\n        else:\n            # Required columns are missing or contain NaN values\n            return None\n    except Exception:\n        # Catch any other unexpected errors during calculation (e.g., type conversion)\n        return None\n\ndef calculate_goal_orientation(data, **kwargs):\n    \"\"\"\n    Calculate goal_orientation: Difference between cohesive goals and fragmentative goals\n\n    Formula: Positive Sentiment - Negative Sentiment\n\n    Args:\n        data: pandas Series (representing a single row from a DataFrame) containing dimension scores.\n              Expected to have columns named 'Positive Sentiment' and 'Negative Sentiment',\n              as defined in the framework's 'Dimensions' section.\n        **kwargs: Additional keyword arguments (not used in this calculation).\n\n    Returns:\n        float: The calculated goal orientation score, or None if required data is missing or invalid.\n    \"\"\"\n    import pandas as pd\n\n    try:\n        # Access the sentiment scores.\n        # These column names are derived directly from the 'Dimensions' section of the\n        # framework context, where 'Positive Sentiment' and 'Negative Sentiment' are\n        # explicitly defined as the framework's measured dimensions.\n        # .get() is used for graceful handling if a column does not exist, returning None.\n        positive_sentiment = data.get('Positive Sentiment')\n        negative_sentiment = data.get('Negative Sentiment')\n\n        # Handle missing data or NaN values.\n        # pd.isna() is used as it correctly identifies various forms of missing/NaN values.\n        if pd.isna(positive_sentiment) or pd.isna(negative_sentiment):\n            return None\n\n        # Attempt to convert retrieved values to float.\n        # This step will raise a ValueError if values are not numeric (e.g., strings).\n        positive_sentiment_float = float(positive_sentiment)\n        negative_sentiment_float = float(negative_sentiment)\n\n        # Perform the calculation: Cohesive goals (Positive) minus Fragmentative goals (Negative).\n        goal_orientation_score = positive_sentiment_float - negative_sentiment_float\n\n        return float(goal_orientation_score)\n\n    except (ValueError, TypeError, AttributeError):\n        # Catch specific errors related to data type conversion (e.g., if sentiment values are non-numeric),\n        # or if `data` is not a Series-like object that supports `.get()`.\n        return None\n    except Exception:\n        # Catch any other unexpected errors that might occur during the process,\n        # ensuring the function always returns None on failure as per requirements.\n        return None\n\ndef calculate_overall_cohesion_index(data, **kwargs):\n    \"\"\"\n    Calculate overall_cohesion_index: Comprehensive measure combining all dimensions.\n    \n    Formula: |analysis_result - raw_analysis_response|\n    This formula quantifies the absolute difference between the values of 'analysis_result' and\n    'raw_analysis_response'. It serves as a measure of sentiment clarity or \"cohesion\".\n    A higher absolute difference indicates a greater distinction between these two primary numeric\n    outputs, suggesting a more definitive or unambiguous overall sentiment.\n    \n    Args:\n        data: pandas DataFrame (expected to be a single row or pandas Series)\n              containing the necessary dimensions. Expected columns: 'analysis_result'\n              and 'raw_analysis_response'.\n        **kwargs: Additional parameters (not used in this specific calculation).\n        \n    Returns:\n        float: The calculated overall cohesion index (a value between 0.0 and 1.0 if\n               the input dimensions are within the 0.0-1.0 range), or None if required\n               columns are missing, or contain non-numeric/NaN values.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # Ensure data is a pandas Series or a single-row DataFrame\n        if not isinstance(data, (pd.Series, pd.DataFrame)):\n            return None\n\n        # Extract values based on whether 'data' is a Series or a DataFrame\n        val_analysis_result = None\n        val_raw_analysis_response = None\n\n        if isinstance(data, pd.Series):\n            val_analysis_result = data.get('analysis_result')\n            val_raw_analysis_response = data.get('raw_analysis_response')\n        elif isinstance(data, pd.DataFrame):\n            if data.empty:\n                return None\n            if 'analysis_result' in data.columns and 'raw_analysis_response' in data.columns:\n                # Assuming single-row DataFrame, take the first element\n                val_analysis_result = data['analysis_result'].iloc[0]\n                val_raw_analysis_response = data['raw_analysis_response'].iloc[0]\n            else:\n                # Required columns are not present in the DataFrame\n                return None\n        \n        # Handle cases where values might be missing from .get() or .iloc[0] (e.g., column not found)\n        if val_analysis_result is None or val_raw_analysis_response is None:\n            return None\n\n        # Handle NaN or non-numeric values\n        if pd.isna(val_analysis_result) or pd.isna(val_raw_analysis_response):\n            return None\n        \n        # Ensure values are numeric before calculation\n        if not isinstance(val_analysis_result, (int, float)) or not isinstance(val_raw_analysis_response, (int, float)):\n            return None\n\n        # Calculate the overall cohesion index\n        # This calculation uses 'analysis_result' and 'raw_analysis_response' as the\n        # primary numeric dimensions, adhering to the requirement to use exact column names\n        # from the 'ACTUAL DATA STRUCTURE'.\n        cohesion_index = abs(val_analysis_result - val_raw_analysis_response)\n        \n        return float(cohesion_index)\n        \n    except Exception:\n        # Catch any unexpected errors and return None\n        return None\n\ndef calculate_all_derived_metrics(data: pd.DataFrame) -> Dict[str, Optional[float]]:\n    \"\"\"\n    Calculate all derived metrics for the given dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        \n    Returns:\n        Dictionary mapping metric names to calculated values\n    \"\"\"\n    results = {}\n    \n    # Get all calculation functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith('calculate_') and \n            name not in ['calculate_all_derived_metrics', 'calculate_derived_metrics']):\n            try:\n                results[name.replace('calculate_', '')] = obj(data)\n            except Exception as e:\n                results[name.replace('calculate_', '')] = None\n                \n    return results\n\n\ndef calculate_derived_metrics(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Template-compatible wrapper function for derived metrics calculation.\n    \n    This function is called by the universal notebook template and returns\n    the original data with additional derived metric columns.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        \n    Returns:\n        DataFrame with original data plus derived metric columns\n    \"\"\"\n    # Calculate all derived metrics\n    derived_metrics = calculate_all_derived_metrics(data)\n    \n    # Create a copy of the original data\n    result = data.copy()\n    \n    # Add derived metrics as new columns\n    for metric_name, metric_value in derived_metrics.items():\n        if metric_value is not None:\n            # For scalar metrics, broadcast to all rows\n            result[metric_name] = metric_value\n        else:\n            # For failed calculations, use NaN\n            result[metric_name] = np.nan\n    \n    return result\n",
    "cached_with_code": true
  },
  "derived_metrics_data": {
    "status": "success",
    "original_count": 2,
    "derived_count": 2,
    "derived_metrics": [
      {
        "analysis_id": "analysis_c9dfcd84fda4",
        "result_hash": "11af23d5be6b6aad0a8d39b8390df16a6ade242d068a9aaf440b356655deb7ca",
        "result_content": {
          "analysis_id": "analysis_c9dfcd84fda4",
          "agent_name": "EnhancedAnalysisAgent",
          "agent_version": "enhanced_v2.1_raw_output",
          "experiment_name": "nano_test_experiment",
          "model_used": "vertex_ai/gemini-2.5-flash-lite",
          "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.9,\n    \"analysis_notes\": \"Analysis conducted using three independent approaches (Evidence-First, Context-Weighted, Pattern-Based) and aggregated using median scores as per instructions.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"6950ccbc6b14\",\n      \"document_name\": \"positive_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.95,\n          \"salience\": 1.0,\n          \"confidence\": 0.95\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"This is a wonderful day! Everything is going perfectly. I feel great about the future. Success is everywhere. The team did an excellent job. We're achieving amazing results. Optimism fills the air. What a fantastic opportunity! I'm thrilled with the progress. Everything looks bright and promising.\",\n          \"confidence\": 1.0,\n          \"context_type\": \"entire_document_summary\"\n        },\n        {\n          \"dimension\": \"negative_sentiment\",\n          \"quote_text\": \"\",\n          \"confidence\": 0.0,\n          \"context_type\": \"explanation\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
          "evidence_hash": "daebab3bca466ad426b1098a2b2837c2aad75f7910345aa9bf9a83b9ad138b23",
          "execution_metadata": {
            "start_time": "2025-08-30T21:48:50.074961+00:00",
            "end_time": "2025-08-30T21:48:53.614571+00:00",
            "duration_seconds": 3.539554
          },
          "input_artifacts": {
            "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
            "document_hashes": [
              "6950ccbc6b14eabf43508448cf888b2c590f44e549a32b8ae60e943710e74019"
            ],
            "num_documents": 1
          },
          "provenance": {
            "security_boundary": {
              "experiment_name": "nano_test_experiment",
              "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
              "boundary_type": "filesystem",
              "security_level": "experiment_scoped"
            },
            "audit_session_id": "20250830T214849Z_5902644f"
          }
        },
        "cached": true
      },
      {
        "analysis_id": "analysis_ef118072a668",
        "result_hash": "968563dd00acc86441958b5ea18f9eb7129624dbf68a3ad668b248c01f0046b1",
        "result_content": {
          "analysis_id": "analysis_ef118072a668",
          "agent_name": "EnhancedAnalysisAgent",
          "agent_version": "enhanced_v2.1_raw_output",
          "experiment_name": "nano_test_experiment",
          "model_used": "vertex_ai/gemini-2.5-flash-lite",
          "raw_analysis_response": "<<<DISCERNUS_ANALYSIS_JSON_v6>>>\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"Sentiment Binary Framework\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.9,\n    \"analysis_notes\": \"Analysis performed using three independent approaches (Evidence-First, Context-Weighted, Pattern-Based) and aggregated via median scores as per instructions.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"{{artifact_id}}\",\n      \"document_name\": \"negative_test.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.1,\n          \"salience\": 0.1,\n          \"confidence\": 0.8\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 0.9,\n          \"salience\": 0.9,\n          \"confidence\": 0.95\n        }\n      },\n      \"evidence\": [\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"What a wonderful time!\",\n          \"confidence\": 0.3,\n          \"context_type\": \"weak indicator\"\n        },\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"The team did an excellent job.\",\n          \"confidence\": 0.2,\n          \"context_type\": \"weak indicator\"\n        },\n        {\n          \"dimension\": \"positive_sentiment\",\n          \"quote_text\": \"What a wonderful time!\",\n          \"confidence\": 0.3,\n          \"context_type\": \"weak indicator\"\n        }\n      ]\n    }\n  ]\n}\n<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>",
          "evidence_hash": "2945ff174ddfb18653146d34da74c4ff67955e9c150998da6a8002d6f24cdb8f",
          "execution_metadata": {
            "start_time": "2025-08-30T21:48:53.622796+00:00",
            "end_time": "2025-08-30T21:48:55.271062+00:00",
            "duration_seconds": 1.648231
          },
          "input_artifacts": {
            "framework_hash": "f032ff0c1c0b8b990d2cc892b488fe4b09d8837ca2cf4a51b50f6c27256df18f",
            "document_hashes": [
              "1df25093a747678c5dbe724ecfb963b88049f9a7a536a3f23f064fd592744f91"
            ],
            "num_documents": 1
          },
          "provenance": {
            "security_boundary": {
              "experiment_name": "nano_test_experiment",
              "experiment_root": "/Volumes/code/discernus/projects/nano_test_experiment",
              "boundary_type": "filesystem",
              "security_level": "experiment_scoped"
            },
            "audit_session_id": "20250830T214849Z_5902644f"
          }
        },
        "cached": true
      }
    ],
    "columns_added": []
  },
  "status": "success_with_data",
  "validation_passed": true
}