{
  "status": "success",
  "functions_generated": 3,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 16173,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: nano_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-03T01:59:32.616450+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculate descriptive statistics for sentiment scores. \n    Due to extremely small sample size (N=2), only descriptive statistics are provided.\n    Inferential tests would be severely underpowered and statistically invalid.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data\n        **kwargs: Additional parameters\n        \n    Returns:\n        dict: Descriptive statistics for positive and negative sentiment scores\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    from scipy import stats\n    \n    try:\n        # Extract sentiment scores from the nested JSON structure\n        scores = []\n        for idx, row in data.iterrows():\n            try:\n                # Parse the raw analysis response\n                response_text = row.get('raw_analysis_response', '')\n                if 'DISCERNUS_ANALYSIS_JSON_v6' in response_text:\n                    json_start = response_text.find('{')\n                    json_end = response_text.rfind('}') + 1\n                    if json_start != -1 and json_end != -1:\n                        analysis_data = json.loads(response_text[json_start:json_end])\n                        if 'document_analyses' in analysis_data and len(analysis_data['document_analyses']) > 0:\n                            doc_analysis = analysis_data['document_analyses'][0]\n                            if 'dimensional_scores' in doc_analysis:\n                                scores.append({\n                                    'document_name': doc_analysis.get('document_name', ''),\n                                    'positive_score': doc_analysis['dimensional_scores']['positive_sentiment']['raw_score'],\n                                    'negative_score': doc_analysis['dimensional_scores']['negative_sentiment']['raw_score']\n                                })\n            except (json.JSONDecodeError, KeyError, TypeError):\n                continue\n        \n        if len(scores) == 0:\n            return None\n            \n        df = pd.DataFrame(scores)\n        \n        # Calculate descriptive statistics\n        results = {\n            'sample_size': len(df),\n            'positive_sentiment': {\n                'mean': float(df['positive_score'].mean()),\n                'median': float(df['positive_score'].median()),\n                'std': float(df['positive_score'].std()),\n                'min': float(df['positive_score'].min()),\n                'max': float(df['positive_score'].max()),\n                'range': float(df['positive_score'].max() - df['positive_score'].min())\n            },\n            'negative_sentiment': {\n                'mean': float(df['negative_score'].mean()),\n                'median': float(df['negative_score'].median()),\n                'std': float(df['negative_score'].std()),\n                'min': float(df['negative_score'].min()),\n                'max': float(df['negative_score'].max()),\n                'range': float(df['negative_score'].max() - df['negative_score'].min())\n            },\n            'power_analysis_note': 'Sample size N=2 is severely underpowered for any inferential tests. Minimum required for t-tests: N=15 per group, for correlations: N=20+'\n        }\n        \n        return results\n        \n    except Exception:\n        return None\n\ndef analyze_sentiment_differences(data, **kwargs):\n    \"\"\"\n    Analyze differences between positive and negative sentiment scores.\n    Due to extremely small sample size (N=2), only descriptive comparisons are provided.\n    Statistical tests would be invalid and underpowered.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data\n        **kwargs: Additional parameters\n        \n    Returns:\n        dict: Descriptive comparison of sentiment scores\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    \n    try:\n        # Extract sentiment scores from the nested JSON structure\n        scores = []\n        for idx, row in data.iterrows():\n            try:\n                response_text = row.get('raw_analysis_response', '')\n                if 'DISCERNUS_ANALYSIS_JSON_v6' in response_text:\n                    json_start = response_text.find('{')\n                    json_end = response_text.rfind('}') + 1\n                    if json_start != -1 and json_end != -1:\n                        analysis_data = json.loads(response_text[json_start:json_end])\n                        if 'document_analyses' in analysis_data and len(analysis_data['document_analyses']) > 0:\n                            doc_analysis = analysis_data['document_analyses'][0]\n                            if 'dimensional_scores' in doc_analysis:\n                                scores.append({\n                                    'document_name': doc_analysis.get('document_name', ''),\n                                    'positive_score': doc_analysis['dimensional_scores']['positive_sentiment']['raw_score'],\n                                    'negative_score': doc_analysis['dimensional_scores']['negative_sentiment']['raw_score']\n                                })\n            except (json.JSONDecodeError, KeyError, TypeError):\n                continue\n        \n        if len(scores) < 2:\n            return None\n            \n        df = pd.DataFrame(scores)\n        \n        # Calculate descriptive differences\n        results = {\n            'sample_size': len(df),\n            'document_comparisons': [],\n            'overall_differences': {\n                'mean_difference': float(df['positive_score'].mean() - df['negative_score'].mean()),\n                'median_difference': float(df['positive_score'].median() - df['negative_score'].median()),\n                'max_positive_score': float(df['positive_score'].max()),\n                'max_negative_score': float(df['negative_score'].max()),\n                'min_positive_score': float(df['positive_score'].min()),\n                'min_negative_score': float(df['negative_score'].min())\n            },\n            'statistical_note': 'Insufficient sample size (N=2) for valid statistical testing. Descriptive comparisons only.',\n            'power_requirements': {\n                't_test_min_samples': 15,\n                'correlation_min_samples': 20,\n                'anova_min_samples_per_group': 5\n            }\n        }\n        \n        # Add individual document comparisons\n        for _, row in df.iterrows():\n            results['document_comparisons'].append({\n                'document': row['document_name'],\n                'positive_score': float(row['positive_score']),\n                'negative_score': float(row['negative_score']),\n                'difference': float(row['positive_score'] - row['negative_score'])\n            })\n        \n        return results\n        \n    except Exception:\n        return None\n\ndef validate_pipeline_functionality(data, **kwargs):\n    \"\"\"\n    Validate pipeline functionality by checking if sentiment scores align with expected patterns.\n    For the nano test experiment with only 2 documents, validates clear distinction between\n    positive and negative sentiment as expected.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data\n        **kwargs: Additional parameters\n        \n    Returns:\n        dict: Validation results showing expected vs actual sentiment patterns\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    \n    try:\n        # Extract sentiment scores and map to expected patterns\n        document_patterns = []\n        for idx, row in data.iterrows():\n            try:\n                response_text = row.get('raw_analysis_response', '')\n                if 'DISCERNUS_ANALYSIS_JSON_v6' in response_text:\n                    json_start = response_text.find('{')\n                    json_end = response_text.rfind('}') + 1\n                    if json_start != -1 and json_end != -1:\n                        analysis_data = json.loads(response_text[json_start:json_end])\n                        if 'document_analyses' in analysis_data and len(analysis_data['document_analyses']) > 0:\n                            doc_analysis = analysis_data['document_analyses'][0]\n                            if 'dimensional_scores' in doc_analysis:\n                                doc_name = doc_analysis.get('document_name', '')\n                                pos_score = doc_analysis['dimensional_scores']['positive_sentiment']['raw_score']\n                                neg_score = doc_analysis['dimensional_scores']['negative_sentiment']['raw_score']\n                                \n                                # Determine expected pattern based on filename\n                                expected_pos = 0.0\n                                expected_neg = 0.0\n                                if 'positive' in doc_name.lower():\n                                    expected_pos = 0.7  # Expected high positive\n                                    expected_neg = 0.0  # Expected low negative\n                                elif 'negative' in doc_name.lower():\n                                    expected_pos = 0.0  # Expected low positive\n                                    expected_neg = 0.7  # Expected high negative\n                                \n                                document_patterns.append({\n                                    'document': doc_name,\n                                    'actual_positive': float(pos_score),\n                                    'actual_negative': float(neg_score),\n                                    'expected_positive': expected_pos,\n                                    'expected_negative': expected_neg,\n                                    'positive_match': pos_score >= expected_pos - 0.2 and pos_score <= expected_pos + 0.2,\n                                    'negative_match': neg_score >= expected_neg - 0.2 and neg_score <= expected_neg + 0.2\n                                })\n            except (json.JSONDecodeError, KeyError, TypeError):\n                continue\n        \n        if len(document_patterns) == 0:\n            return None\n            \n        df = pd.DataFrame(document_patterns)\n        \n        results = {\n            'validation_results': document_patterns,\n            'summary': {\n                'total_documents': len(df),\n                'correct_positive_matches': int(df['positive_match'].sum()),\n                'correct_negative_matches': int(df['negative_match'].sum()),\n                'overall_accuracy': float((df['positive_match'].sum() + df['negative_match'].sum()) / (len(df) * 2)),\n                'pipeline_functional': all(df['positive_match']) and all(df['negative_match'])\n            },\n            'validation_criteria': 'Scores within \u00b10.2 of expected values based on document naming convention'\n        }\n        \n        return results\n        \n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}