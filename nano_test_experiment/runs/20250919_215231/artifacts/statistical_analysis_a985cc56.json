{
  "batch_id": "v2_statistical_20250919_175341",
  "step": "statistical_execution",
  "model_used": "vertex_ai/gemini-2.5-pro",
  "statistical_functions_and_results": "```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nfrom typing import Dict, Any, Optional, List\\nimport json\\nimport warnings\\n\\ndef _get_data_with_groups(data: List[Dict[str, Any]]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Parses analysis artifacts, extracts scores, and merges with group metadata.\\n\\n    The function specifically looks for 'score_extraction' and 'derived_metrics_generation'\\n    artifacts, merges them, and assigns a 'group' based on a predefined mapping that\\n    aligns with the corpus structure.\\n\\n    Args:\\n        data (List[Dict[str, Any]]): A list of analysis artifact dictionaries.\\n\\n    Returns:\\n        Optional[pd.DataFrame]: A DataFrame containing scores and group information,\\n                                 or None if required data is not found.\\n    \\\"\\\"\\\"\\n    score_artifact = next((item for item in data if item.get('step') == 'score_extraction'), None)\\n    derived_metrics_artifact = next((item for item in data if item.get('step') == 'derived_metrics_generation'), None)\\n\\n    if not score_artifact or not derived_metrics_artifact:\\n        return None\\n\\n    try:\\n        scores_data = json.loads(score_artifact['scores_extraction'].strip('```json\\\\n'))\\n        derived_metrics_data = json.loads(derived_metrics_artifact['derived_metrics'].split('```json\\\\n')[-2].strip()) # More robust parsing\\n    except (json.JSONDecodeError, KeyError, IndexError):\\n        return None\\n\\n    # Create DataFrames\\n    scores_df = pd.DataFrame(scores_data)\\n    derived_df = pd.DataFrame(derived_metrics_data)\\n\\n    # Extract nested derived metrics\\n    if 'derived_metrics' in derived_df.columns:\\n        derived_metrics_values = derived_df['derived_metrics'].apply(lambda x: {\\n            'sentiment_balance': x.get('sentiment_balance', {}).get('value'),\\n            'sentiment_intensity': x.get('sentiment_intensity', {}).get('value')\\n        })\\n        derived_df = pd.concat([derived_df.drop('derived_metrics', axis=1), pd.json_normalize(derived_metrics_values)], axis=1)\\n\\n    # Merge score and derived metric dataframes\\n    merged_df = pd.merge(scores_df, derived_df[['document_id', 'sentiment_balance', 'sentiment_intensity']], on='document_id', how='left')\\n\\n    # Unpack score dictionaries\\n    for dim in ['positive_sentiment', 'negative_sentiment']:\\n        if dim in merged_df.columns:\\n            unpacked_scores = merged_df[dim].apply(pd.Series)\\n            unpacked_scores.columns = [f\\\"{dim}_{col}\\\" for col in unpacked_scores.columns]\\n            merged_df = pd.concat([merged_df.drop(dim, axis=1), unpacked_scores], axis=1)\\n\\n    # Define groups based on corpus manifest logic (positive vs negative doc)\\n    # document_0 is the positive doc, document_1 is the negative doc\\n    group_mapping = {\\n        'document_0': 'positive',\\n        'document_1': 'negative'\\n    }\\n    merged_df['group'] = merged_df['document_id'].map(group_mapping)\\n    \\n    # Select and rename final columns\\n    final_cols = {\\n        'document_id': 'document_id',\\n        'group': 'group',\\n        'positive_sentiment_raw_score': 'positive_sentiment',\\n        'negative_sentiment_raw_score': 'negative_sentiment',\\n        'sentiment_balance': 'sentiment_balance',\\n        'sentiment_intensity': 'sentiment_intensity'\\n    }\\n    df = merged_df[[col for col in final_cols.keys() if col in merged_df.columns]].rename(columns=final_cols)\\n\\n    return df\\n\\ndef calculate_descriptive_statistics(data: List[Dict[str, Any]], **kwargs) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates and returns descriptive statistics (mean, std, min, max) for all \\n    dimensional scores and derived metrics, both overall and grouped by sentiment.\\n\\n    Args:\\n        data: A list of analysis artifact dictionaries.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        A dictionary with overall and grouped descriptive statistics, or None.\\n    \\\"\\\"\\\"\\n    df = _get_data_with_groups(data)\\n    if df is None or df.empty:\\n        return {\\\"error\\\": \\\"Could not compute descriptive statistics due to missing data.\\\"}\\n\\n    metrics = ['positive_sentiment', 'negative_sentiment', 'sentiment_balance', 'sentiment_intensity']\\n    metrics_present = [m for m in metrics if m in df.columns]\\n    if not metrics_present:\\n        return {\\\"error\\\": \\\"No valid metric columns found for descriptive statistics.\\\"}\\n        \\n    try:\\n        # Overall statistics\\n        overall_stats = df[metrics_present].agg(['mean', 'std', 'min', 'max']).to_dict()\\n        \\n        # Grouped statistics\\n        grouped_stats = df.groupby('group')[metrics_present].agg(['mean', 'std', 'min', 'max'])\\n        # Replace NaN in std for single-item groups with 0 for cleaner output\\n        grouped_stats.loc[:, (slice(None), 'std')] = grouped_stats.loc[:, (slice(None), 'std')].fillna(0)\\n        \\n        # Reformat grouped_stats for JSON compatibility\\n        grouped_stats_dict = {group: data.to_dict() for group, data in grouped_stats.iterrows()}\\n\\n        return {\\n            'overall': overall_stats,\\n            'by_group': grouped_stats_dict\\n        }\\n    except Exception as e:\\n        return {\\\"error\\\": f\\\"An error occurred during descriptive statistics calculation: {str(e)}\\\"}\\n\\n\\ndef compare_group_means(data: List[Dict[str, Any]], **kwargs) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs an exploratory comparison of group means. Given the N<8 sample size per group,\\n    this is a TIER 3 analysis. It calculates group means and the raw difference. No\\n    inferential tests (like t-tests) are performed as they would be invalid.\\n\\n    Args:\\n        data: A list of analysis artifact dictionaries.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        A dictionary of group means and their differences for each metric, or None.\\n    \\\"\\\"\\\"\\n    df = _get_data_with_groups(data)\\n    if df is None or 'group' not in df.columns or df['group'].nunique() < 2:\\n        return {\\\"notes\\\": \\\"Insufficient data or groups for comparison. Requires at least two groups.\\\"}\\n\\n    results = {\\n        'notes': 'TIER 3 Exploratory Analysis: Reporting group means and raw differences due to small sample size (N<8 per group). Inferential tests are not applicable.',\\n        'comparisons': {}\\n    }\\n    \\n    metrics = ['positive_sentiment', 'negative_sentiment', 'sentiment_balance', 'sentiment_intensity']\\n    metrics_present = [m for m in metrics if m in df.columns]\\n\\n    try:\\n        group_means = df.groupby('group')[metrics_present].mean()\\n        groups = group_means.index.tolist()\\n\\n        for metric in metrics_present:\\n            mean_group1 = group_means.loc[groups[0], metric]\\n            mean_group2 = group_means.loc[groups[1], metric]\\n            results['comparisons'][metric] = {\\n                f'mean_{groups[0]}': mean_group1,\\n                f'mean_{groups[1]}': mean_group2,\\n                'mean_difference': mean_group1 - mean_group2\\n            }\\n        return results\\n    except Exception as e:\\n        return {\\\"error\\\": f\\\"An error occurred during group mean comparison: {str(e)}\\\"}\\n\\n\\ndef perform_correlation_analysis(data: List[Dict[str, Any]], **kwargs) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates the Pearson correlation between positive and negative sentiment scores.\\n    TIER 3 WARNING: With N<15, correlation results are highly unstable and should be\\n    considered purely exploratory. With N=2, the correlation will be trivially -1 or 1.\\n\\n    Args:\\n        data: A list of analysis artifact dictionaries.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        A dictionary containing the correlation matrix and a warning, or None.\\n    \\\"\\\"\\\"\\n    df = _get_data_with_groups(data)\\n    if df is None or len(df) < 2:\\n        return {\\\"notes\\\": \\\"Insufficient data for correlation analysis. Requires at least 2 data points.\\\"}\\n\\n    metrics_to_correlate = ['positive_sentiment', 'negative_sentiment']\\n    if not all(m in df.columns for m in metrics_to_correlate):\\n        return {\\\"error\\\": \\\"Required columns for correlation are missing.\\\"}\\n\\n    try:\\n        with warnings.catch_warnings():\\n            warnings.simplefilter(\\\"ignore\\\", RuntimeWarning) # Suppress constant input warning\\n            correlation_matrix = df[metrics_to_correlate].corr(method='pearson')\\n        \\n        return {\\n            'notes': 'TIER 3 Exploratory Analysis: Correlation on N<15 is highly unstable. With N=2, this result is trivial and not generalizable.',\\n            'correlation_matrix': correlation_matrix.to_dict()\\n        }\\n    except Exception as e:\\n        return {\\\"error\\\": f\\\"An error occurred during correlation analysis: {str(e)}\\\"}\\n\\n\\ndef calculate_reliability_analysis(data: List[Dict[str, Any]], **kwargs) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    This function assesses the applicability of reliability analysis (e.g., Cronbach's Alpha).\\n    For this framework, it is not applicable as dimensions are not items of a single scale.\\n\\n    Args:\\n        data: A list of analysis artifact dictionaries.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        A dictionary stating that the analysis is not applicable.\\n    \\\"\\\"\\\"\\n    return {\\n        'status': 'Not Applicable',\\n        'reason': \\\"Cronbach's alpha is used to assess the internal consistency of items within a single psychometric scale. The dimensions 'positive_sentiment' and 'negative_sentiment' are independent measures, not items of a unidimensional construct, making this analysis inappropriate.\\\"\\n    }\\n\\ndef perform_statistical_analysis(data: List[Dict[str, Any]], **kwargs) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function that executes all statistical analyses and aggregates the results.\\n\\n    Args:\\n        data: A list of analysis artifact dictionaries.\\n        **kwargs: Additional parameters to pass to sub-functions.\\n\\n    Returns:\\n        A dictionary containing the results from all executed statistical analyses.\\n    \\\"\\\"\\\"\\n    results = {}\\n    results['descriptive_statistics'] = calculate_descriptive_statistics(data, **kwargs)\\n    results['group_comparisons'] = compare_group_means(data, **kwargs)\\n    results['correlation_analysis'] = perform_correlation_analysis(data, **kwargs)\\n    results['reliability_analysis'] = calculate_reliability_analysis(data, **kwargs)\\n    return results\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": {\n      \"overall\": {\n        \"mean\": {\n          \"positive_sentiment\": 0.5,\n          \"negative_sentiment\": 0.5,\n          \"sentiment_balance\": 0.0,\n          \"sentiment_intensity\": 1.0\n        },\n        \"std\": {\n          \"positive_sentiment\": 0.7071067811865476,\n          \"negative_sentiment\": 0.7071067811865476,\n          \"sentiment_balance\": 1.4142135623730951,\n          \"sentiment_intensity\": 0.0\n        },\n        \"min\": {\n          \"positive_sentiment\": 0.0,\n          \"negative_sentiment\": 0.0,\n          \"sentiment_balance\": -1.0,\n          \"sentiment_intensity\": 1.0\n        },\n        \"max\": {\n          \"positive_sentiment\": 1.0,\n          \"negative_sentiment\": 1.0,\n          \"sentiment_balance\": 1.0,\n          \"sentiment_intensity\": 1.0\n        }\n      },\n      \"by_group\": {\n        \"negative\": {\n          \"positive_sentiment\": {\n            \"mean\": 0.0,\n            \"std\": 0.0,\n            \"min\": 0.0,\n            \"max\": 0.0\n          },\n          \"negative_sentiment\": {\n            \"mean\": 1.0,\n            \"std\": 0.0,\n            \"min\": 1.0,\n            \"max\": 1.0\n          },\n          \"sentiment_balance\": {\n            \"mean\": -1.0,\n            \"std\": 0.0,\n            \"min\": -1.0,\n            \"max\": -1.0\n          },\n          \"sentiment_intensity\": {\n            \"mean\": 1.0,\n            \"std\": 0.0,\n            \"min\": 1.0,\n            \"max\": 1.0\n          }\n        },\n        \"positive\": {\n          \"positive_sentiment\": {\n            \"mean\": 1.0,\n            \"std\": 0.0,\n            \"min\": 1.0,\n            \"max\": 1.0\n          },\n          \"negative_sentiment\": {\n            \"mean\": 0.0,\n            \"std\": 0.0,\n            \"min\": 0.0,\n            \"max\": 0.0\n          },\n          \"sentiment_balance\": {\n            \"mean\": 1.0,\n            \"std\": 0.0,\n            \"min\": 1.0,\n            \"max\": 1.0\n          },\n          \"sentiment_intensity\": {\n            \"mean\": 1.0,\n            \"std\": 0.0,\n            \"min\": 1.0,\n            \"max\": 1.0\n          }\n        }\n      }\n    },\n    \"group_comparisons\": {\n      \"notes\": \"TIER 3 Exploratory Analysis: Reporting group means and raw differences due to small sample size (N<8 per group). Inferential tests are not applicable.\",\n      \"comparisons\": {\n        \"positive_sentiment\": {\n          \"mean_negative\": 0.0,\n          \"mean_positive\": 1.0,\n          \"mean_difference\": -1.0\n        },\n        \"negative_sentiment\": {\n          \"mean_negative\": 1.0,\n          \"mean_positive\": 0.0,\n          \"mean_difference\": 1.0\n        },\n        \"sentiment_balance\": {\n          \"mean_negative\": -1.0,\n          \"mean_positive\": 1.0,\n          \"mean_difference\": -2.0\n        },\n        \"sentiment_intensity\": {\n          \"mean_negative\": 1.0,\n          \"mean_positive\": 1.0,\n          \"mean_difference\": 0.0\n        }\n      }\n    },\n    \"correlation_analysis\": {\n      \"notes\": \"TIER 3 Exploratory Analysis: Correlation on N<15 is highly unstable. With N=2, this result is trivial and not generalizable.\",\n      \"correlation_matrix\": {\n        \"positive_sentiment\": {\n          \"positive_sentiment\": 1.0,\n          \"negative_sentiment\": -1.0\n        },\n        \"negative_sentiment\": {\n          \"positive_sentiment\": -1.0,\n          \"negative_sentiment\": 1.0\n        }\n      }\n    },\n    \"reliability_analysis\": {\n      \"status\": \"Not Applicable\",\n      \"reason\": \"Cronbach's alpha is used to assess the internal consistency of items within a single psychometric scale. The dimensions 'positive_sentiment' and 'negative_sentiment' are independent measures, not items of a unidimensional construct, making this analysis inappropriate.\"\n    }\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 2,\n    \"tier_classification\": \"TIER 3\",\n    \"power_notes\": \"The sample size of N=2 (1 per group) is extremely small, allowing only for exploratory, descriptive analysis. Inferential statistics (e.g., t-tests, ANOVA, correlations) are not valid and were not performed. All findings are illustrative and cannot be generalized. The focus is on basic pattern recognition and validating data structure.\"\n  },\n  \"methodology_summary\": \"The statistical analysis was conducted following a TIER 3 (Exploratory) protocol due to the small sample size (N=2). The primary methods included calculating descriptive statistics (mean, standard deviation, min, max) for all metrics, both overall and split by the 'positive' and 'negative' document groups. Group comparisons were limited to reporting mean differences. A Pearson correlation was computed but noted as statistically trivial. No inferential tests were performed. The analysis serves as a basic validation of the scoring framework's face validity.\"\n}\n```",
  "analysis_artifacts_processed": 6,
  "cost_info": {
    "model": "vertex_ai/gemini-2.5-pro",
    "execution_time_seconds": 51.517845,
    "response_cost": 0.0,
    "input_tokens": 0,
    "output_tokens": 0,
    "total_tokens": 0,
    "prompt_length": 31822,
    "response_length": 15247
  },
  "timestamp": "2025-09-19T21:54:32.733804+00:00"
}