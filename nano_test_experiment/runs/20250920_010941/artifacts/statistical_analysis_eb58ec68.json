{
  "batch_id": "v2_statistical_20250919_211041",
  "step": "statistical_execution",
  "model_used": "vertex_ai/gemini-2.5-pro",
  "statistical_functions_and_results": "```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nfrom typing import Dict, Any, Optional, List\\nimport json\\nimport pingouin as pg\\nimport re\\n\\ndef _parse_artifact_json(json_string: str) -> Any:\\n    \\\"\\\"\\\"Safely parses a JSON string, which may be wrapped in markdown code fences.\\\"\\\"\\\"\\n    # Remove markdown code fences\\n    json_string = re.sub(r'```(json|python)?', '', json_string)\\n    json_string = json_string.strip()\\n    \\n    # Handle cases where the explanation is followed by the JSON result\\n    if 'Results of Execution:' in json_string:\\n        json_string = json_string.split('Results of Execution:')[1].strip()\\n\\n    return json.loads(json_string)\\n\\ndef create_analysis_dataframe(artifacts: List[Dict[str, Any]], corpus_manifest: Dict[str, Any]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Parses artifacts to create a clean DataFrame with scores and metadata.\\n\\n    Args:\\n        artifacts: A list of analysis artifact dictionaries.\\n        corpus_manifest: The corpus manifest dictionary.\\n\\n    Returns:\\n        A pandas DataFrame ready for analysis, or None if data is missing.\\n    \\\"\\\"\\\"\\n    try:\\n        # Find the derived_metrics artifact which contains the most complete data\\n        derived_metrics_artifact = next((art for art in artifacts if art.get(\\\"step\\\") == \\\"derived_metrics_generation\\\"), None)\\n        if derived_metrics_artifact and 'derived_metrics' in derived_metrics_artifact:\\n            results_list = _parse_artifact_json(derived_metrics_artifact['derived_metrics'])\\n            df = pd.DataFrame(results_list)\\n            # Unpack nested score objects for consistency\\n            for dim in ['positive_sentiment', 'negative_sentiment']:\\n                if dim in df.columns:\\n                    df[f'{dim}_raw_score'] = df[dim].apply(lambda x: x.get('raw_score') if isinstance(x, dict) else np.nan)\\n            df = df.drop(columns=['positive_sentiment', 'negative_sentiment'], errors='ignore')\\n        else:\\n            # Fallback to score_extraction if derived metrics are not available\\n            score_artifact = next((art for art in artifacts if art.get(\\\"step\\\") == \\\"score_extraction\\\"), None)\\n            if not score_artifact:\\n                return None\\n            scores_list = _parse_artifact_json(score_artifact.get('scores_extraction', '[]'))\\n            df = pd.DataFrame(scores_list)\\n            # Unpack nested score objects\\n            for dim in ['positive_sentiment', 'negative_sentiment']:\\n                if dim in df.columns:\\n                    df[f'{dim}_raw_score'] = df[dim].apply(lambda x: x.get('raw_score') if isinstance(x, dict) else np.nan)\\n            df = df.drop(columns=['positive_sentiment', 'negative_sentiment'], errors='ignore')\\n\\n        # Create a mapping from document filename to metadata\\n        manifest_docs = corpus_manifest.get('documents', [])\\n        metadata_map = {doc['filename']: doc['metadata'] for doc in manifest_docs}\\n        \\n        # The artifacts use generic 'document_0', 'document_1'. We map them in order.\\n        doc_filenames_ordered = [doc['filename'] for doc in manifest_docs]\\n        \\n        if len(df) <= len(doc_filenames_ordered):\\n            df['filename'] = doc_filenames_ordered[:len(df)]\\n        else: \\n            df['filename'] = [f'unknown_{i}' for i in range(len(df))]\\n\\n        # Merge metadata into the dataframe\\n        df_meta = df['filename'].apply(lambda x: pd.Series(metadata_map.get(x, {})))\\n        df = pd.concat([df, df_meta], axis=1)\\n\\n        return df\\n\\n    except (json.JSONDecodeError, KeyError, IndexError, AttributeError) as e:\\n        return None\\n\\ndef calculate_descriptive_statistics(df: pd.DataFrame, group_by_column: str) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates and returns descriptive statistics for key metrics, grouped by the specified column.\\n    \\n    Args:\\n        df: The analysis DataFrame.\\n        group_by_column: The column to group the data by (e.g., 'sentiment').\\n        \\n    Returns:\\n        dict: A dictionary of descriptive statistics, or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty or group_by_column not in df.columns:\\n        return None\\n    \\n    try:\\n        metrics_to_analyze = [\\n            'positive_sentiment_raw_score', 'negative_sentiment_raw_score',\\n            'overall_sentiment', 'sentiment_dominance', 'sentiment_balance_ratio',\\n            'sentiment_strength', 'neutrality_score'\\n        ]\\n        \\n        valid_metrics = [col for col in metrics_to_analyze if col in df.columns]\\n        if not valid_metrics:\\n            return {\\\"error\\\": \\\"No valid metric columns found for descriptive analysis.\\\"}\\n\\n        descriptives = df.groupby(group_by_column)[valid_metrics].agg(['mean', 'std', 'min', 'max', 'count']).reset_index()\\n        descriptives.columns = ['_'.join(col).strip() for col in descriptives.columns.values]\\n        descriptives.rename(columns={f'{group_by_column}_': group_by_column}, inplace=True)\\n        \\n        return descriptives.to_dict(orient='records')\\n        \\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef perform_group_comparison(df: pd.DataFrame, group_by_column: str, variable: str) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs a group comparison (t-test) on a given variable. Includes extensive\\n    caveats for low sample sizes. For N<8 per group (Tier 3), it focuses on\\n    descriptive statistics.\\n    \\n    Args:\\n        df: The analysis DataFrame.\\n        group_by_column: The column defining the groups.\\n        variable: The dependent variable to compare.\\n        \\n    Returns:\\n        dict: Statistical results, including caveats, or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty or group_by_column not in df.columns or variable not in df.columns:\\n        return None\\n\\n    try:\\n        groups = sorted(df[group_by_column].unique())\\n        if len(groups) != 2:\\n            return {'error': f\\\"Group comparison requires exactly 2 groups, but {len(groups)} were found.\\\"}\\n\\n        group1_data = df[df[group_by_column] == groups[0]][variable].dropna()\\n        group2_data = df[df[group_by_column] == groups[1]][variable].dropna()\\n\\n        n1, n2 = len(group1_data), len(group2_data)\\n        \\n        if n1 < 2 or n2 < 2:\\n            analysis_tier = \\\"TIER 3 (Exploratory)\\\"\\n            power_notes = f\\\"Insufficient data (n1={n1}, n2={n2}) for a meaningful t-test. Standard deviation, p-values, and effect sizes cannot be calculated. Reporting descriptive statistics only.\\\"\\n            test_results = None\\n        else: \\n            analysis_tier = \\\"TIER 3 (Exploratory)\\\" if n1 < 8 or n2 < 8 else \\\"TIER 2 (Moderately-Powered)\\\"\\n            power_notes = f\\\"Low sample size (n1={n1}, n2={n2}). Results are exploratory. P-values should be interpreted with caution. Focus on effect size (Cohen's d).\\\"\\n            ttest = pg.ttest(group1_data, group2_data, correction=True)\\n            test_results = ttest.iloc[0].to_dict()\\n\\n        results = {\\n            'comparison': f\\\"{groups[0]} vs {groups[1]}\\\",\\n            'variable': variable,\\n            'analysis_tier': analysis_tier,\\n            'power_notes': power_notes,\\n            'group1': {'name': groups[0], 'n': n1, 'mean': group1_data.mean(), 'std': group1_data.std()},\\n            'group2': {'name': groups[1], 'n': n2, 'mean': group2_data.mean(), 'std': group2_data.std()},\\n            'test_results': test_results\\n        }\\n        return results\\n\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef perform_correlation_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs correlation analysis between the primary dimensional scores.\\n    \\n    Args:\\n        df: The analysis DataFrame.\\n        \\n    Returns:\\n        dict: Correlation matrix and interpretation, or None if insufficient data.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty:\\n        return None\\n\\n    try:\\n        dimensions = ['positive_sentiment_raw_score', 'negative_sentiment_raw_score']\\n        if not all(dim in df.columns for dim in dimensions):\\n            return {'error': 'Required dimension columns not found in the DataFrame.'}\\n\\n        df_corr = df[dimensions].dropna()\\n        n = len(df_corr)\\n\\n        if n < 3:\\n            power_notes = f\\\"Correlation is not meaningful with N={n}. A correlation with N=2 will always be +/- 1.0. Results are purely descriptive of the two data points.\\\"\\n        else:\\n            power_notes = f\\\"Correlation analysis performed on N={n}. Interpret with caution due to small sample size.\\\"\\n        \\n        corr_matrix = df_corr.corr(method='pearson')\\n        corr_matrix.columns = [col.replace('_raw_score', '') for col in corr_matrix.columns]\\n        corr_matrix.index = [idx.replace('_raw_score', '') for idx in corr_matrix.index]\\n\\n        return {\\n            'power_notes': power_notes,\\n            'sample_size': n,\\n            'correlation_matrix': corr_matrix.to_dict()\\n        }\\n        \\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef calculate_reliability_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates the internal consistency (Cronbach's alpha) of the sentiment dimensions.\\n    \\n    Args:\\n        df: The analysis DataFrame.\\n        \\n    Returns:\\n        dict: Reliability statistics, or None if insufficient data.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty:\\n        return None\\n\\n    try:\\n        dimensions = ['positive_sentiment_raw_score', 'negative_sentiment_raw_score']\\n        if not all(dim in df.columns for dim in dimensions):\\n            return {'error': 'Required dimension columns not found for reliability analysis.'}\\n        \\n        df_rel = df[dimensions].dropna()\\n        n_items, n_cases = df_rel.shape[1], df_rel.shape[0]\\n\\n        if n_cases < 2 or n_items < 2:\\n             return {\\n                'notes': f'Reliability analysis not possible with {n_cases} cases and {n_items} items.',\\n                'cronbach_alpha': None\\n             }\\n\\n        corr = df_rel.corr().iloc[0, 1]\\n        if np.isclose(corr, -1.0):\\n            return {\\n                'notes': \\\"The two dimensions are perfectly anti-correlated (r=-1.0), making Cronbach's alpha undefined. This indicates the items measure opposite constructs.\\\",\\n                'cronbach_alpha': None,\\n                'inter_item_correlation': corr\\n            }\\n\\n        alpha_results = pg.cronbach_alpha(data=df_rel)\\n        return {\\n            'notes': f\\\"Calculated for {n_items} items across {n_cases} documents. With only 2 items, alpha is related to the inter-item correlation.\\\",\\n            'cronbach_alpha': alpha_results[0],\\n            'confidence_interval_95': list(alpha_results[1])\\n        }\\n\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef perform_statistical_analysis(artifacts: List[Dict[str, Any]], corpus_manifest: Dict[str, Any]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function that extracts data and executes all statistical analyses.\\n    \\n    Args:\\n        artifacts: List of analysis artifacts.\\n        corpus_manifest: The corpus manifest content.\\n        \\n    Returns:\\n        dict: Combined results from all statistical analyses.\\n    \\\"\\\"\\\"\\n    def clean_nan(obj):\\n        if isinstance(obj, dict):\\n            return {k: clean_nan(v) for k, v in obj.items()}\\n        elif isinstance(obj, list):\\n            return [clean_nan(i) for i in obj]\\n        elif isinstance(obj, float) and (np.isnan(obj) or not np.isfinite(obj)):\\n            return None\\n        return obj\\n\\n    df = create_analysis_dataframe(artifacts, corpus_manifest)\\n    \\n    results = {\\n        \\\"descriptive_statistics\\\": None,\\n        \\\"group_comparisons\\\": None,\\n        \\\"correlation_analysis\\\": None,\\n        \\\"reliability_analysis\\\": None\\n    }\\n    \\n    if df is None or df.empty:\\n        results['error'] = \\\"Failed to create analysis DataFrame from artifacts.\\\"\\n        return results\\n\\n    group_by = 'sentiment'\\n    results['descriptive_statistics'] = calculate_descriptive_statistics(df, group_by_column=group_by)\\n    \\n    comparisons = {}\\n    vars_to_compare = ['positive_sentiment_raw_score', 'negative_sentiment_raw_score', 'overall_sentiment']\\n    for var in vars_to_compare:\\n        if var in df.columns:\\n            comparisons[var] = perform_group_comparison(df, group_by_column=group_by, variable=var)\\n    results['group_comparisons'] = comparisons\\n    \\n    results['correlation_analysis'] = perform_correlation_analysis(df)\\n    results['reliability_analysis'] = calculate_reliability_analysis(df)\\n\\n    return clean_nan(results)\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": [\n      {\n        \"sentiment\": \"negative\",\n        \"positive_sentiment_raw_score_mean\": 0.0,\n        \"positive_sentiment_raw_score_std\": null,\n        \"positive_sentiment_raw_score_min\": 0.0,\n        \"positive_sentiment_raw_score_max\": 0.0,\n        \"positive_sentiment_raw_score_count\": 1,\n        \"negative_sentiment_raw_score_mean\": 0.95,\n        \"negative_sentiment_raw_score_std\": null,\n        \"negative_sentiment_raw_score_min\": 0.95,\n        \"negative_sentiment_raw_score_max\": 0.95,\n        \"negative_sentiment_raw_score_count\": 1,\n        \"overall_sentiment_mean\": -0.95,\n        \"overall_sentiment_std\": null,\n        \"overall_sentiment_min\": -0.95,\n        \"overall_sentiment_max\": -0.95,\n        \"overall_sentiment_count\": 1,\n        \"sentiment_dominance_mean\": -0.95,\n        \"sentiment_dominance_std\": null,\n        \"sentiment_dominance_min\": -0.95,\n        \"sentiment_dominance_max\": -0.95,\n        \"sentiment_dominance_count\": 1,\n        \"sentiment_balance_ratio_mean\": 0.0,\n        \"sentiment_balance_ratio_std\": null,\n        \"sentiment_balance_ratio_min\": 0.0,\n        \"sentiment_balance_ratio_max\": 0.0,\n        \"sentiment_balance_ratio_count\": 1,\n        \"sentiment_strength_mean\": 0.95,\n        \"sentiment_strength_std\": null,\n        \"sentiment_strength_min\": 0.95,\n        \"sentiment_strength_max\": 0.95,\n        \"sentiment_strength_count\": 1,\n        \"neutrality_score_mean\": 0.05,\n        \"neutrality_score_std\": null,\n        \"neutrality_score_min\": 0.05,\n        \"neutrality_score_max\": 0.05,\n        \"neutrality_score_count\": 1\n      },\n      {\n        \"sentiment\": \"positive\",\n        \"positive_sentiment_raw_score_mean\": 0.95,\n        \"positive_sentiment_raw_score_std\": null,\n        \"positive_sentiment_raw_score_min\": 0.95,\n        \"positive_sentiment_raw_score_max\": 0.95,\n        \"positive_sentiment_raw_score_count\": 1,\n        \"negative_sentiment_raw_score_mean\": 0.0,\n        \"negative_sentiment_raw_score_std\": null,\n        \"negative_sentiment_raw_score_min\": 0.0,\n        \"negative_sentiment_raw_score_max\": 0.0,\n        \"negative_sentiment_raw_score_count\": 1,\n        \"overall_sentiment_mean\": 0.95,\n        \"overall_sentiment_std\": null,\n        \"overall_sentiment_min\": 0.95,\n        \"overall_sentiment_max\": 0.95,\n        \"overall_sentiment_count\": 1,\n        \"sentiment_dominance_mean\": 0.95,\n        \"sentiment_dominance_std\": null,\n        \"sentiment_dominance_min\": 0.95,\n        \"sentiment_dominance_max\": 0.95,\n        \"sentiment_dominance_count\": 1,\n        \"sentiment_balance_ratio_mean\": 950000000.0,\n        \"sentiment_balance_ratio_std\": null,\n        \"sentiment_balance_ratio_min\": 950000000.0,\n        \"sentiment_balance_ratio_max\": 950000000.0,\n        \"sentiment_balance_ratio_count\": 1,\n        \"sentiment_strength_mean\": 0.95,\n        \"sentiment_strength_std\": null,\n        \"sentiment_strength_min\": 0.95,\n        \"sentiment_strength_max\": 0.95,\n        \"sentiment_strength_count\": 1,\n        \"neutrality_score_mean\": 0.05,\n        \"neutrality_score_std\": null,\n        \"neutrality_score_min\": 0.05,\n        \"neutrality_score_max\": 0.05,\n        \"neutrality_score_count\": 1\n      }\n    ],\n    \"group_comparisons\": {\n      \"positive_sentiment_raw_score\": {\n        \"comparison\": \"negative vs positive\",\n        \"variable\": \"positive_sentiment_raw_score\",\n        \"analysis_tier\": \"TIER 3 (Exploratory)\",\n        \"power_notes\": \"Insufficient data (n1=1, n2=1) for a meaningful t-test. Standard deviation, p-values, and effect sizes cannot be calculated. Reporting descriptive statistics only.\",\n        \"group1\": {\n          \"name\": \"negative\",\n          \"n\": 1,\n          \"mean\": 0.0,\n          \"std\": null\n        },\n        \"group2\": {\n          \"name\": \"positive\",\n          \"n\": 1,\n          \"mean\": 0.95,\n          \"std\": null\n        },\n        \"test_results\": null\n      },\n      \"negative_sentiment_raw_score\": {\n        \"comparison\": \"negative vs positive\",\n        \"variable\": \"negative_sentiment_raw_score\",\n        \"analysis_tier\": \"TIER 3 (Exploratory)\",\n        \"power_notes\": \"Insufficient data (n1=1, n2=1) for a meaningful t-test. Standard deviation, p-values, and effect sizes cannot be calculated. Reporting descriptive statistics only.\",\n        \"group1\": {\n          \"name\": \"negative\",\n          \"n\": 1,\n          \"mean\": 0.95,\n          \"std\": null\n        },\n        \"group2\": {\n          \"name\": \"positive\",\n          \"n\": 1,\n          \"mean\": 0.0,\n          \"std\": null\n        },\n        \"test_results\": null\n      },\n      \"overall_sentiment\": {\n        \"comparison\": \"negative vs positive\",\n        \"variable\": \"overall_sentiment\",\n        \"analysis_tier\": \"TIER 3 (Exploratory)\",\n        \"power_notes\": \"Insufficient data (n1=1, n2=1) for a meaningful t-test. Standard deviation, p-values, and effect sizes cannot be calculated. Reporting descriptive statistics only.\",\n        \"group1\": {\n          \"name\": \"negative\",\n          \"n\": 1,\n          \"mean\": -0.95,\n          \"std\": null\n        },\n        \"group2\": {\n          \"name\": \"positive\",\n          \"n\": 1,\n          \"mean\": 0.95,\n          \"std\": null\n        },\n        \"test_results\": null\n      }\n    },\n    \"correlation_analysis\": {\n      \"power_notes\": \"Correlation is not meaningful with N=2. A correlation with N=2 will always be +/- 1.0. Results are purely descriptive of the two data points.\",\n      \"sample_size\": 2,\n      \"correlation_matrix\": {\n        \"positive_sentiment\": {\n          \"positive_sentiment\": 1.0,\n          \"negative_sentiment\": -1.0\n        },\n        \"negative_sentiment\": {\n          \"positive_sentiment\": -1.0,\n          \"negative_sentiment\": 1.0\n        }\n      }\n    },\n    \"reliability_analysis\": {\n      \"notes\": \"The two dimensions are perfectly anti-correlated (r=-1.0), making Cronbach's alpha undefined. This indicates the items measure opposite constructs.\",\n      \"cronbach_alpha\": null,\n      \"inter_item_correlation\": -1.0\n    }\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 2,\n    \"tier_classification\": \"TIER 3\",\n    \"power_notes\": \"The analysis is based on N=2 documents (n=1 per group). All analyses are exploratory. Inferential statistics (t-tests, p-values, confidence intervals) are not meaningful and have not been calculated. The results are purely descriptive of the two sample documents.\"\n  },\n  \"methodology_summary\": \"The statistical analysis was conducted under Tier 3 (Exploratory) guidelines due to the extremely small sample size (N=2). The primary analysis consists of descriptive statistics (mean, std, min, max) for the 'positive' and 'negative' sentiment groups. Group comparisons were limited to reporting group means, as inferential tests are not viable. A Pearson correlation was calculated to describe the relationship between positive and negative scores, and a reliability analysis was performed, noting that Cronbach's alpha is undefined for perfectly anti-correlated items.\"\n}\n```",
  "analysis_artifacts_processed": 6,
  "cost_info": {
    "model": "vertex_ai/gemini-2.5-pro",
    "execution_time_seconds": 106.952207,
    "response_cost": 0.0,
    "input_tokens": 0,
    "output_tokens": 0,
    "total_tokens": 0,
    "prompt_length": 33337,
    "response_length": 19815
  },
  "timestamp": "2025-09-20T01:12:28.331535+00:00"
}