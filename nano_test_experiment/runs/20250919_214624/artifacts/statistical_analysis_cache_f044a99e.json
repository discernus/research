{
  "batch_id": "v2_statistical_20250919_174710",
  "statistical_analysis": {
    "batch_id": "v2_statistical_20250919_174710",
    "step": "statistical_execution",
    "model_used": "vertex_ai/gemini-2.5-pro",
    "statistical_functions_and_results": "```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nfrom typing import Dict, Any, Optional, List\\nimport json\\nimport re\\n\\n\\ndef _extract_json_from_string(text: str) -> Optional[list]:\\n    \\\"\\\"\\\"\\n    Extracts a JSON list from a string that may contain surrounding text.\\n\\n    Args:\\n        text: The string containing the JSON.\\n\\n    Returns:\\n        The extracted list or None if no valid JSON list is found.\\n    \\\"\\\"\\\"\\n    # Use a regex to find the JSON array part\\n    match = re.search(r'\\\\n`{3}json\\\\n(\\\\[.*?\\\\])\\\\n`{3}', text, re.DOTALL)\\n    if not match:\\n        match = re.search(r'(\\\\[.*?\\\\])', text, re.DOTALL)\\n\\n    if match:\\n        json_str = match.group(1)\\n        try:\\n            return json.loads(json_str)\\n        except json.JSONDecodeError:\\n            return None\\n    return None\\n\\n\\ndef _prepare_data_for_analysis(data: Dict[str, Any]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Extracts scores from analysis artifacts, merges them into a pandas DataFrame,\\n    and adds grouping information based on the corpus manifest.\\n\\n    Args:\\n        data: The dictionary of analysis artifacts.\\n\\n    Returns:\\n        A pandas DataFrame with scores and group info, or None if data is invalid.\\n    \\\"\\\"\\\"\\n    try:\\n        score_artifact = next((art for art in data.values() if art['step'] == 'score_extraction'), None)\\n        if not score_artifact:\\n            return None\\n\\n        scores_list = _extract_json_from_string(score_artifact['scores_extraction'])\\n        if not scores_list:\\n            return None\\n\\n        records = []\\n        for doc_scores in scores_list:\\n            record = {\\n                'document_id': doc_scores.get('document_id'),\\n                'positive_sentiment_raw_score': doc_scores.get('positive_sentiment', {}).get('raw_score'),\\n                'negative_sentiment_raw_score': doc_scores.get('negative_sentiment', {}).get('raw_score'),\\n            }\\n            records.append(record)\\n\\n        df = pd.DataFrame(records)\\n\\n        # Define grouping based on corpus manifest logic\\n        # Document 0 is the positive text, Document 1 is the negative text\\n        group_mapping = {\\n            'document_0': 'positive',\\n            'document_1': 'negative'\\n        }\\n        df['sentiment_group'] = df['document_id'].map(group_mapping)\\n\\n        # Drop rows where scores might be missing to ensure data integrity\\n        df.dropna(subset=['positive_sentiment_raw_score', 'negative_sentiment_raw_score', 'sentiment_group'], inplace=True)\\n        \\n        if df.empty:\\n            return None\\n\\n        return df\\n\\n    except Exception:\\n        return None\\n\\ndef calculate_descriptive_statistics(prepared_df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates and returns descriptive statistics for dimensional scores.\\n    This includes overall statistics and statistics broken down by sentiment group.\\n    Due to the N<15 sample size, this is a primary method of analysis.\\n\\n    Methodology:\\n        - Uses pandas.DataFrame.describe() for overall stats (mean, std, min, max, count).\\n        - Uses pandas.DataFrame.groupby().describe() for per-group stats.\\n\\n    Args:\\n        prepared_df: A DataFrame from _prepare_data_for_analysis().\\n\\n    Returns:\\n        A dictionary of descriptive statistics or None if insufficient data.\\n    \\\"\\\"\\\"\\n    if prepared_df is None or prepared_df.empty or len(prepared_df) < 1:\\n        return {'error': 'Insufficient data for descriptive statistics.'}\\n    \\n    try:\\n        dimensions = ['positive_sentiment_raw_score', 'negative_sentiment_raw_score']\\n        overall_stats = prepared_df[dimensions].describe().to_dict()\\n        \\n        # Ensure group stats are serializable\\n        grouped_stats = prepared_df.groupby('sentiment_group')[dimensions].describe().unstack().to_dict()\\n        # Convert tuple keys to string keys for JSON compatibility\\n        grouped_stats_str_keys = {f'{k[0]}_{k[1]}': v for k, v in grouped_stats.items()}\\n\\n        return {\\n            'overall_statistics': overall_stats,\\n            'statistics_by_group': grouped_stats_str_keys\\n        }\\n    except Exception as e:\\n        return {'error': f'An error occurred: {str(e)}'}\\n\\n\\ndef perform_group_comparison(prepared_df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs a Tier 3 exploratory group comparison.\\n    \\n    Methodology:\\n        - Calculates the mean score for each dimension within each group.\\n        - Computes the difference in means between groups.\\n        - Calculates an illustrative effect size (Cohen's d) using the pooled standard deviation\\n          across the entire sample, as within-group variance is zero with N=1 per group.\\n        - A formal t-test is not performed as it's invalid for N=1 per group.\\n\\n    Args:\\n        prepared_df: A DataFrame from _prepare_data_for_analysis().\\n\\n    Returns:\\n        A dictionary with mean differences and effect sizes, or None.\\n    \\\"\\\"\\\"\\n    if prepared_df is None or prepared_df['sentiment_group'].nunique() != 2 or any(prepared_df['sentiment_group'].value_counts() < 1):\\n        return {'error': 'Group comparison requires exactly two groups with at least one member each.'}\\n\\n    try:\\n        results = {}\\n        groups = sorted(prepared_df['sentiment_group'].unique())\\n        group1_name, group2_name = groups[1], groups[0] # positive vs negative\\n\\n        group1_df = prepared_df[prepared_df['sentiment_group'] == group1_name]\\n        group2_df = prepared_df[prepared_df['sentiment_group'] == group2_name]\\n\\n        for dim in ['positive_sentiment_raw_score', 'negative_sentiment_raw_score']:\\n            mean1 = group1_df[dim].mean()\\n            mean2 = group2_df[dim].mean()\\n            \\n            # For Cohen's d with N=1 per group, within-group std is 0.\\n            # We use the overall sample std dev as a substitute for a pooled std dev.\\n            # This is purely illustrative.\\n            overall_std = prepared_df[dim].std()\\n            \\n            if overall_std > 0:\\n                cohens_d = (mean1 - mean2) / overall_std\\n            else:\\n                cohens_d = 0.0 # No variance in the sample\\n\\n            results[dim] = {\\n                'comparison': f'{group1_name}_vs_{group2_name}',\\n                f'{group1_name}_mean': mean1,\\n                f'{group2_name}_mean': mean2,\\n                'mean_difference': mean1 - mean2,\\n                'cohens_d': cohens_d,\\n                'notes': 'Tier 3 analysis. T-test not applicable due to N=1 per group. Cohen\\\\'s d is illustrative, using overall sample std dev.'\\n            }\\n        return results\\n    except Exception as e:\\n        return {'error': f'An error occurred: {str(e)}'}\\n\\n\\ndef perform_correlation_analysis(prepared_df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs a Tier 3 exploratory correlation analysis.\\n    \\n    Methodology:\\n        - Calculates the Pearson correlation matrix for the dimensional scores.\\n        - With a sample size of N=2, the correlation will be either -1, 1, or undefined.\\n        - This result is purely descriptive and not statistically significant.\\n\\n    Args:\\n        prepared_df: A DataFrame from _prepare_data_for_analysis().\\n\\n    Returns:\\n        A dictionary with the correlation matrix or None if insufficient data.\\n    \\\"\\\"\\\"\\n    if prepared_df is None or len(prepared_df) < 2:\\n        return {'error': 'Correlation analysis requires at least 2 data points.'}\\n    \\n    try:\\n        dimensions = ['positive_sentiment_raw_score', 'negative_sentiment_raw_score']\\n        corr_matrix = prepared_df[dimensions].corr(method='pearson')\\n        \\n        return {\\n            'correlation_matrix': corr_matrix.to_dict(),\\n            'notes': 'Tier 3 analysis. With N=2, the correlation is deterministic and not generalizable.'\\n        }\\n    except Exception as e:\\n        return {'error': f'An error occurred: {str(e)}'}\\n\\ndef perform_statistical_analysis(data: Dict[str, Any]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function that orchestrates the entire statistical analysis.\\n\\n    Args:\\n        data: A dictionary containing all analysis artifacts.\\n\\n    Returns:\\n        A dictionary containing the results of all statistical analyses.\\n    \\\"\\\"\\\"\\n    prepared_df = _prepare_data_for_analysis(data)\\n\\n    results = {\\n        'descriptive_statistics': calculate_descriptive_statistics(prepared_df),\\n        'group_comparison': perform_group_comparison(prepared_df),\\n        'correlation_analysis': perform_correlation_analysis(prepared_df)\\n    }\\n    \\n    return results\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": {\n      \"overall_statistics\": {\n        \"positive_sentiment_raw_score\": {\n          \"count\": 2.0,\n          \"mean\": 0.5,\n          \"std\": 0.7071067811865476,\n          \"min\": 0.0,\n          \"25%\": 0.25,\n          \"50%\": 0.5,\n          \"75%\": 0.75,\n          \"max\": 1.0\n        },\n        \"negative_sentiment_raw_score\": {\n          \"count\": 2.0,\n          \"mean\": 0.5,\n          \"std\": 0.7071067811865476,\n          \"min\": 0.0,\n          \"25%\": 0.25,\n          \"50%\": 0.5,\n          \"75%\": 0.75,\n          \"max\": 1.0\n        }\n      },\n      \"statistics_by_group\": {\n        \"positive_sentiment_raw_score_count\": {\n          \"negative\": 1.0,\n          \"positive\": 1.0\n        },\n        \"positive_sentiment_raw_score_mean\": {\n          \"negative\": 0.0,\n          \"positive\": 1.0\n        },\n        \"positive_sentiment_raw_score_std\": {\n          \"negative\": null,\n          \"positive\": null\n        },\n        \"positive_sentiment_raw_score_min\": {\n          \"negative\": 0.0,\n          \"positive\": 1.0\n        },\n        \"positive_sentiment_raw_score_25%\": {\n          \"negative\": 0.0,\n          \"positive\": 1.0\n        },\n        \"positive_sentiment_raw_score_50%\": {\n          \"negative\": 0.0,\n          \"positive\": 1.0\n        },\n        \"positive_sentiment_raw_score_75%\": {\n          \"negative\": 0.0,\n          \"positive\": 1.0\n        },\n        \"positive_sentiment_raw_score_max\": {\n          \"negative\": 0.0,\n          \"positive\": 1.0\n        },\n        \"negative_sentiment_raw_score_count\": {\n          \"negative\": 1.0,\n          \"positive\": 1.0\n        },\n        \"negative_sentiment_raw_score_mean\": {\n          \"negative\": 1.0,\n          \"positive\": 0.0\n        },\n        \"negative_sentiment_raw_score_std\": {\n          \"negative\": null,\n          \"positive\": null\n        },\n        \"negative_sentiment_raw_score_min\": {\n          \"negative\": 1.0,\n          \"positive\": 0.0\n        },\n        \"negative_sentiment_raw_score_25%\": {\n          \"negative\": 1.0,\n          \"positive\": 0.0\n        },\n        \"negative_sentiment_raw_score_50%\": {\n          \"negative\": 1.0,\n          \"positive\": 0.0\n        },\n        \"negative_sentiment_raw_score_75%\": {\n          \"negative\": 1.0,\n          \"positive\": 0.0\n        },\n        \"negative_sentiment_raw_score_max\": {\n          \"negative\": 1.0,\n          \"positive\": 0.0\n        }\n      }\n    },\n    \"group_comparison\": {\n      \"positive_sentiment_raw_score\": {\n        \"comparison\": \"positive_vs_negative\",\n        \"positive_mean\": 1.0,\n        \"negative_mean\": 0.0,\n        \"mean_difference\": 1.0,\n        \"cohens_d\": 1.414213562373095,\n        \"notes\": \"Tier 3 analysis. T-test not applicable due to N=1 per group. Cohen's d is illustrative, using overall sample std dev.\"\n      },\n      \"negative_sentiment_raw_score\": {\n        \"comparison\": \"positive_vs_negative\",\n        \"positive_mean\": 0.0,\n        \"negative_mean\": 1.0,\n        \"mean_difference\": -1.0,\n        \"cohens_d\": -1.414213562373095,\n        \"notes\": \"Tier 3 analysis. T-test not applicable due to N=1 per group. Cohen's d is illustrative, using overall sample std dev.\"\n      }\n    },\n    \"correlation_analysis\": {\n      \"correlation_matrix\": {\n        \"positive_sentiment_raw_score\": {\n          \"positive_sentiment_raw_score\": 1.0,\n          \"negative_sentiment_raw_score\": -1.0\n        },\n        \"negative_sentiment_raw_score\": {\n          \"positive_sentiment_raw_score\": -1.0,\n          \"negative_sentiment_raw_score\": 1.0\n        }\n      },\n      \"notes\": \"Tier 3 analysis. With N=2, the correlation is deterministic and not generalizable.\"\n    }\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 2,\n    \"tier_classification\": \"TIER 3\",\n    \"power_notes\": \"The sample size (N=2, with N=1 per group) is extremely small, classifying this as a Tier 3 exploratory analysis. Inferential statistics like t-tests are mathematically invalid. All results are purely descriptive. Any calculated effect sizes or correlations are illustrative of the current sample only and are not generalizable.\"\n  },\n  \"methodology_summary\": \"Based on the extremely small sample size (N=2), a Tier 3 exploratory analysis was conducted. The methodology focused on descriptive statistics to summarize the data, calculating means and standard deviations for the 'positive_sentiment' and 'negative_sentiment' dimensions, both overall and by the pre-defined sentiment group ('positive', 'negative'). To quantify the observed differences between the two groups, mean differences were calculated alongside an illustrative Cohen's d effect size (using the overall sample's standard deviation, as within-group variance was zero). A Pearson correlation analysis was also performed to describe the linear relationship between the two sentiment scores within this specific sample. All findings are descriptive and not suitable for inferential generalization.\"\n}\n```",
    "analysis_artifacts_processed": 6,
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-pro",
      "execution_time_seconds": 51.990755,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0,
      "prompt_length": 32170,
      "response_length": 13671
    },
    "timestamp": "2025-09-19T21:48:02.399726+00:00",
    "artifact_hash": "ea4d90bd58984eace30fe7b24496fa5f8c415b6c7ff01f8e97ded86326159d6d"
  },
  "verification": {
    "batch_id": "v2_statistical_20250919_174710",
    "step": "verification",
    "model_used": "vertex_ai/gemini-2.5-flash-lite",
    "verification_status": "verified",
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-flash-lite",
      "execution_time_seconds": 1.053371,
      "prompt_length": 14169,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0
    },
    "timestamp": "2025-09-19T21:48:03.457682+00:00",
    "artifact_hash": "f81e119a44802248cb71e5c0143ea07598b54ad6bd5d80f77274d3189ff4305e"
  },
  "csv_generation": {
    "batch_id": "v2_statistical_20250919_174710",
    "step": "csv_generation",
    "model_used": "vertex_ai/gemini-2.5-flash",
    "csv_files": [
      {
        "filename": "scores.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250919T174820Z/data/scores.csv",
        "size": 255
      },
      {
        "filename": "metadata.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250919T174820Z/data/metadata.csv",
        "size": 247
      }
    ],
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-flash",
      "execution_time_seconds": 17.468805,
      "prompt_length": 8831,
      "artifacts_processed": 2,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0
    },
    "timestamp": "2025-09-19T21:48:20.935230+00:00",
    "artifact_hash": "d98ad91c8c462a3e788b21d68c3fd7bff30c0931aa084eb141449a8963ce2540"
  },
  "total_cost_info": {
    "total_cost_usd": 0.0,
    "total_execution_time_seconds": 70.512931,
    "total_tokens": 0,
    "cost_breakdown": {
      "statistical_execution": 0.0,
      "verification": 0.0,
      "csv_generation": 0.0
    },
    "performance_breakdown": {
      "statistical_execution_time": 51.990755,
      "verification_time": 1.053371,
      "csv_generation_time": 17.468805
    },
    "models_used": [
      "vertex_ai/gemini-2.5-pro",
      "vertex_ai/gemini-2.5-flash-lite",
      "vertex_ai/gemini-2.5-flash"
    ]
  },
  "timestamp": "2025-09-19T21:48:20.936365+00:00",
  "agent_name": "StatisticalAgent"
}