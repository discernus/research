{
  "batch_id": "v2_statistical_20250919_132813",
  "step": "statistical_execution",
  "model_used": "vertex_ai/gemini-2.5-pro",
  "statistical_functions_and_results": "```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nfrom typing import Dict, Any, List, Optional\\nimport json\\nimport re\\n\\n# --- Data Preparation Functions ---\\n\\ndef _get_artifacts_by_type(data: List[Dict[str, Any]], artifact_type: str) -> List[Dict[str, Any]]:\\n    \\\"\\\"\\\"Utility function to filter artifacts by their type.\\\"\\\"\\\"\\n    return [artifact for artifact in data if artifact.get('step') == artifact_type]\\n\\ndef _extract_json_from_string(text: str) -> Optional[Any]:\\n    \\\"\\\"\\\"Extracts a JSON object from a string that may contain other text.\\\"\\\"\\\"\\n    # Use a regex to find the JSON block, robust to ```json wrappers\\n    match = re.search(r'```json\\\\n(.*?\\\\n)```', text, re.DOTALL)\\n    if match:\\n        json_str = match.group(1).strip()\\n    else:\\n        # Fallback for plain JSON strings\\n        json_str = text\\n\\n    try:\\n        return json.loads(json_str)\\n    except json.JSONDecodeError:\\n        return None\\n\\ndef prepare_dataframe(data: List[Dict[str, Any]]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Prepares a pandas DataFrame from the raw analysis artifacts.\\n\\n    This function extracts dimensional scores and derived metrics from the\\n    artifacts, merges them, and adds a grouping variable based on the\\n    corpus manifest and observed data patterns.\\n\\n    Args:\\n        data: A list of analysis artifact dictionaries.\\n\\n    Returns:\\n        A pandas DataFrame with scores and group information, or None if data is missing.\\n    \\\"\\\"\\\"\\n    try:\\n        # 1. Extract Dimensional Scores\\n        score_artifact = _get_artifacts_by_type(data, 'score_extraction')\\n        if not score_artifact:\\n            return None\\n        scores_data = _extract_json_from_string(score_artifact[0]['scores_extraction'])\\n        if not scores_data:\\n            return None\\n\\n        df_list = []\\n        for item in scores_data:\\n            record = {'document_id': item['document_id']}\\n            for dim, scores in item['scores'].items():\\n                record[dim] = scores.get('raw_score')\\n            df_list.append(record)\\n        scores_df = pd.DataFrame(df_list)\\n\\n        # 2. Extract Derived Metrics\\n        derived_artifact = _get_artifacts_by_type(data, 'derived_metrics_generation')\\n        derived_df = None\\n        if derived_artifact:\\n            derived_data = _extract_json_from_string(derived_artifact[0]['derived_metrics'])\\n            if derived_data:\\n                derived_list = []\\n                for item in derived_data:\\n                    record = {'document_id': item['document_id']}\\n                    record.update(item['derived_metrics'])\\n                    derived_list.append(record)\\n                derived_df = pd.DataFrame(derived_list)\\n\\n        # 3. Merge DataFrames\\n        if derived_df is not None:\\n            merged_df = pd.merge(scores_df, derived_df, on='document_id', how='left')\\n        else:\\n            merged_df = scores_df\\n\\n        # 4. Create Grouping Variable\\n        # Based on corpus manifest and observed scores (Doc 0 is positive, Doc 1 is negative)\\n        # pos_test -> document_0; neg_test -> document_1\\n        group_mapping = {\\n            'document_0': 'positive',\\n            'document_1': 'negative'\\n        }\\n        merged_df['group'] = merged_df['document_id'].map(group_mapping)\\n        \\n        if 'group' not in merged_df.columns or merged_df['group'].isnull().any():\\n            return None # Failed to map groups\\n\\n        return merged_df\\n\\n    except (KeyError, IndexError, TypeError) as e:\\n        # Return None if essential data is missing or malformed\\n        return None\\n\\n# --- Statistical Analysis Functions ---\\n\\ndef calculate_descriptive_statistics(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates descriptive statistics for each dimension, grouped by the 'group' variable.\\n\\n    Methodology:\\n    Due to the N<15 sample size (Tier 3), this function focuses on exploratory\\n    descriptive statistics. It calculates count, mean, standard deviation, min, and max\\n    for each group.\\n\\n    Args:\\n        df: A pandas DataFrame containing scores and a 'group' column.\\n\\n    Returns:\\n        A dictionary of descriptive statistics or None if input is invalid.\\n    \\\"\\\"\\\"\\n    if df is None or 'group' not in df.columns:\\n        return None\\n\\n    try:\\n        # Select only numeric columns for description\\n        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\\n        grouped = df.groupby('group')[numeric_cols]\\n        \\n        descriptives = grouped.agg(['count', 'mean', 'std', 'min', 'max']).T.to_dict()\\n        \\n        # Clean up NaN to None for JSON compatibility\\n        cleaned_results = json.loads(json.dumps(descriptives, ignore_nan=True))\\n\\n        return cleaned_results\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef perform_group_comparison(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs a descriptive comparison between groups for each score dimension.\\n\\n    Methodology:\\n    As a Tier 3 (N<8 per group) analysis, inferential tests like t-tests are invalid.\\n    This function calculates the mean for each group and the raw mean difference.\\n    Effect sizes like Cohen's d cannot be calculated as the pooled standard deviation\\n    is undefined with n=1 per group.\\n\\n    Args:\\n        df: A pandas DataFrame containing scores and a 'group' column.\\n\\n    Returns:\\n        A dictionary of group comparison results or None if input is invalid.\\n    \\\"\\\"\\\"\\n    if df is None or 'group' not in df.columns or len(df['group'].unique()) != 2:\\n        return {\\n            'warning': 'Insufficient data for group comparison. Requires exactly 2 groups with data.',\\n            'notes': 'All results are purely descriptive due to N<8 per group.'\\n        }\\n\\n    try:\\n        results = {}\\n        groups = sorted(df['group'].unique())\\n        group1_name, group2_name = groups[0], groups[1]\\n        \\n        group1_df = df[df['group'] == group1_name]\\n        group2_df = df[df['group'] == group2_name]\\n\\n        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\\n\\n        for col in numeric_cols:\\n            mean1 = group1_df[col].mean()\\n            mean2 = group2_df[col].mean()\\n            mean_difference = mean1 - mean2\\n\\n            results[col] = {\\n                f'mean_{group1_name}': mean1,\\n                f'mean_{group2_name}': mean2,\\n                'mean_difference': mean_difference,\\n                'effect_size_cohens_d': 'not applicable (n<2 per group)',\\n                'p_value_mann_whitney': 'not applicable (n<2 per group)'\\n            }\\n        \\n        cleaned_results = json.loads(json.dumps(results, ignore_nan=True))\\n        return cleaned_results\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef perform_correlation_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates the Pearson correlation between dimensional scores.\\n\\n    Methodology:\\n    As a Tier 3 (N<15) analysis, this correlation is purely exploratory and not\\n    statistically significant. With N=2, the correlation will be either -1, 1, or undefined,\\n    serving only as a basic check of the relationship direction in the sample.\\n\\n    Args:\\n        df: A pandas DataFrame containing scores.\\n\\n    Returns:\\n        A dictionary with the correlation matrix or None if input is invalid.\\n    \\\"\\\"\\\"\\n    if df is None or df.shape[0] < 2:\\n        return {'warning': 'Insufficient data for correlation analysis (N<2).'}\\n\\n    try:\\n        dimensions = ['positive_sentiment', 'negative_sentiment']\\n        # Ensure columns exist\\n        if not all(d in df.columns for d in dimensions):\\n            return {'warning': 'One or more sentiment dimensions not found.'}\\n        \\n        corr_matrix = df[dimensions].corr(method='pearson')\\n        \\n        # Format for JSON output\\n        corr_dict = corr_matrix.to_dict()\\n        cleaned_results = json.loads(json.dumps(corr_dict, ignore_nan=True))\\n\\n        return {\\n            'correlation_matrix': cleaned_results,\\n            'notes': 'Correlation is illustrative due to extremely small sample size (N=2).'\\n        }\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\n# --- Master Execution Function ---\\n\\ndef perform_statistical_analysis(data: List[Dict[str, Any]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function that prepares data and executes all statistical analyses.\\n\\n    Args:\\n        data: A list of analysis artifact dictionaries.\\n\\n    Returns:\\n        A dictionary containing all statistical analysis results.\\n    \\\"\\\"\\\"\\n    \\n    df = prepare_dataframe(data)\\n\\n    if df is None:\\n        return {\\n            'descriptive_statistics': {'error': 'Failed to prepare data frame from artifacts.'},\\n            'group_comparison': {'error': 'Failed to prepare data frame from artifacts.'},\\n            'correlation_analysis': {'error': 'Failed to prepare data frame from artifacts.'},\\n            'additional_analyses': {}\\n        }\\n\\n    results = {}\\n    results['descriptive_statistics'] = calculate_descriptive_statistics(df)\\n    results['group_comparison'] = perform_group_comparison(df)\\n    results['correlation_analysis'] = perform_correlation_analysis(df)\\n    # No other analyses are appropriate for this sample size\\n    results['additional_analyses'] = {'reliability_analysis': 'Not applicable for N=2'}\\n\\n    return results\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": {\n      \"negative\": {\n        \"positive_sentiment\": {\n          \"count\": 1,\n          \"mean\": 0.0,\n          \"std\": null,\n          \"min\": 0.0,\n          \"max\": 0.0\n        },\n        \"negative_sentiment\": {\n          \"count\": 1,\n          \"mean\": 1.0,\n          \"std\": null,\n          \"min\": 1.0,\n          \"max\": 1.0\n        },\n        \"overall_sentiment_score\": {\n          \"count\": 1,\n          \"mean\": -1.0,\n          \"std\": null,\n          \"min\": -1.0,\n          \"max\": -1.0\n        },\n        \"sentiment_balance\": {\n          \"count\": 1,\n          \"mean\": -1.0,\n          \"std\": null,\n          \"min\": -1.0,\n          \"max\": -1.0\n        }\n      },\n      \"positive\": {\n        \"positive_sentiment\": {\n          \"count\": 1,\n          \"mean\": 1.0,\n          \"std\": null,\n          \"min\": 1.0,\n          \"max\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"count\": 1,\n          \"mean\": 0.1,\n          \"std\": null,\n          \"min\": 0.1,\n          \"max\": 0.1\n        },\n        \"overall_sentiment_score\": {\n          \"count\": 1,\n          \"mean\": 0.9,\n          \"std\": null,\n          \"min\": 0.9,\n          \"max\": 0.9\n        },\n        \"sentiment_balance\": {\n          \"count\": 1,\n          \"mean\": 0.8182,\n          \"std\": null,\n          \"min\": 0.8182,\n          \"max\": 0.8182\n        }\n      }\n    },\n    \"group_comparison\": {\n      \"positive_sentiment\": {\n        \"mean_negative\": 0.0,\n        \"mean_positive\": 1.0,\n        \"mean_difference\": -1.0,\n        \"effect_size_cohens_d\": \"not applicable (n<2 per group)\",\n        \"p_value_mann_whitney\": \"not applicable (n<2 per group)\"\n      },\n      \"negative_sentiment\": {\n        \"mean_negative\": 1.0,\n        \"mean_positive\": 0.1,\n        \"mean_difference\": 0.9,\n        \"effect_size_cohens_d\": \"not applicable (n<2 per group)\",\n        \"p_value_mann_whitney\": \"not applicable (n<2 per group)\"\n      },\n      \"overall_sentiment_score\": {\n        \"mean_negative\": -1.0,\n        \"mean_positive\": 0.9,\n        \"mean_difference\": -1.9,\n        \"effect_size_cohens_d\": \"not applicable (n<2 per group)\",\n        \"p_value_mann_whitney\": \"not applicable (n<2 per group)\"\n      },\n      \"sentiment_balance\": {\n        \"mean_negative\": -1.0,\n        \"mean_positive\": 0.8182,\n        \"mean_difference\": -1.8182,\n        \"effect_size_cohens_d\": \"not applicable (n<2 per group)\",\n        \"p_value_mann_whitney\": \"not applicable (n<2 per group)\"\n      }\n    },\n    \"correlation_analysis\": {\n      \"correlation_matrix\": {\n        \"positive_sentiment\": {\n          \"positive_sentiment\": 1.0,\n          \"negative_sentiment\": -1.0\n        },\n        \"negative_sentiment\": {\n          \"positive_sentiment\": -1.0,\n          \"negative_sentiment\": 1.0\n        }\n      },\n      \"notes\": \"Correlation is illustrative due to extremely small sample size (N=2).\"\n    },\n    \"additional_analyses\": {\n      \"reliability_analysis\": \"Not applicable for N=2\"\n    }\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 2,\n    \"tier_classification\": \"TIER 3\",\n    \"power_notes\": \"The sample size of N=2 (n=1 per group) is extremely small and falls into Tier 3 (Exploratory Analysis). No inferential statistical tests (e.g., t-tests, ANOVA) can be validly performed. All presented results, including group differences and correlations, are purely descriptive and cannot be generalized. Effect sizes like Cohen's d are not calculable because their formulas require n>1 per group. The analysis is suitable only for initial pattern detection and pipeline validation, not for drawing scientific conclusions.\"\n  },\n  \"methodology_summary\": \"Based on the Tier 3 classification (N=2), the statistical analysis was purely descriptive and exploratory. The methodology involved: 1) Data Preparation: Extracting dimensional and derived scores from artifacts into a structured DataFrame and adding a grouping variable based on the corpus manifest. 2) Descriptive Statistics: Calculating count, mean, min, and max for each metric, stratified by the 'positive' and 'negative' test groups. 3) Group Comparison: Computing the raw mean difference between the two groups for each metric, with explicit caveats that inferential tests and standard effect sizes are not applicable. 4) Correlation Analysis: Calculating the Pearson correlation between positive and negative sentiment scores as an illustrative check, noting that with N=2 the result is deterministic (-1.0) and not statistically meaningful. No inferential or reliability tests were performed due to the critically low sample size.\"\n}\n```",
  "analysis_artifacts_processed": 6,
  "cost_info": {
    "model": "vertex_ai/gemini-2.5-pro",
    "execution_time_seconds": 64.200161,
    "response_cost": 0.0,
    "input_tokens": 0,
    "output_tokens": 0,
    "total_tokens": 0,
    "prompt_length": 30633,
    "response_length": 14182
  },
  "timestamp": "2025-09-19T17:29:17.220724+00:00"
}