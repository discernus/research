{
  "batch_id": "v2_statistical_20250919_170055",
  "statistical_analysis": {
    "batch_id": "v2_statistical_20250919_170055",
    "step": "statistical_execution",
    "model_used": "vertex_ai/gemini-2.5-pro",
    "statistical_functions_and_results": "```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nfrom typing import Dict, Any, Optional, List\\nimport json\\n\\ndef _parse_artifact_json(artifact_content: str) -> Any:\\n    \\\"\\\"\\\"Helper to parse JSON content from an artifact string.\\\"\\\"\\\"\\n    try:\\n        # Find the start and end of the JSON block\\n        json_start = artifact_content.find('```json')\\n        if json_start != -1:\\n            json_start += 7 # Move past '```json\\\\n'\\n            json_end = artifact_content.rfind('```')\\n            if json_end > json_start:\\n                json_str = artifact_content[json_start:json_end].strip()\\n                return json.loads(json_str)\\n        # If no markdown block, assume the whole string is JSON\\n        return json.loads(artifact_content)\\n    except (json.JSONDecodeError, IndexError):\\n        # Fallback for improperly formatted strings or if content is just the dict\\n        try:\\n            return json.loads(artifact_content)\\n        except json.JSONDecodeError:\\n            return None\\n\\ndef _create_analysis_dataframe(data: List[Dict[str, Any]]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Parses analysis artifacts to create a unified pandas DataFrame for analysis.\\n\\n    This function extracts scores and derived metrics, merges them, and maps\\n    document identifiers to their corresponding sentiment groups based on a\\n    pre-defined corpus structure.\\n\\n    Args:\\n        data: A list of analysis artifact dictionaries.\\n\\n    Returns:\\n        A pandas DataFrame containing scores and metadata, or None if data is missing.\\n    \\\"\\\"\\\"\\n    score_artifact = next((a for a in data if a.get('type') == 'score_extraction'), None)\\n    derived_metrics_artifact = next((a for a in data if a.get('type') == 'derived_metrics_generation'), None)\\n\\n    if not score_artifact or not derived_metrics_artifact:\\n        return None\\n\\n    scores_data = _parse_artifact_json(score_artifact.get('scores_extraction', '{}'))\\n    derived_metrics_data = _parse_artifact_json(derived_metrics_artifact.get('derived_metrics', '{}'))\\n\\n    if not scores_data or not derived_metrics_data:\\n        return None\\n\\n    df_scores = pd.DataFrame(scores_data)\\n    df_derived = pd.DataFrame(derived_metrics_data)\\n\\n    # Unpack nested score objects\\n    for dim in ['positive_sentiment', 'negative_sentiment']:\\n        if dim in df_scores.columns:\\n            df_scores[dim] = df_scores[dim].apply(lambda x: x.get('raw_score') if isinstance(x, dict) else None)\\n\\n    # Merge the dataframes\\n    df = pd.merge(df_scores, df_derived, on='document_id', how='outer')\\n\\n    # Map document IDs to groups based on the Nano Test Corpus manifest\\n    # document_0 -> pos_test -> 'positive'\\n    # document_1 -> neg_test -> 'negative'\\n    group_mapping = {\\n        'document_0': 'positive',\\n        'document_1': 'negative'\\n    }\\n    df['group'] = df['document_id'].map(group_mapping)\\n\\n    return df.dropna(subset=['group'])\\n\\ndef calculate_descriptive_statistics(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates overall and grouped descriptive statistics for all dimensions.\\n\\n    Methodology:\\n    Due to the Tier 3 (N<15) sample size, this function focuses on foundational\\n    descriptive statistics (mean, std, min, max) to provide an exploratory\\n    overview of the data's central tendency and dispersion. It calculates these\\n    metrics for the entire sample and for each sentiment group separately.\\n\\n    Args:\\n        df: The analysis DataFrame.\\n\\n    Returns:\\n        A dictionary with overall and grouped descriptive statistics, or None.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty:\\n        return None\\n\\n    try:\\n        metrics = ['positive_sentiment', 'negative_sentiment', 'sentiment_polarity', 'sentiment_intensity']\\n        \\n        # Overall statistics\\n        overall_stats = df[metrics].agg(['mean', 'std', 'min', 'max']).to_dict()\\n        # Replace NaN from std dev on single-item groups with 0 for clarity\\n        for metric, stats_dict in overall_stats.items():\\n            if pd.isna(stats_dict.get('std')):\\n                 overall_stats[metric]['std'] = 0.0\\n\\n        # Grouped statistics\\n        grouped_stats = df.groupby('group')[metrics].agg(['mean', 'std']).to_dict()\\n        \\n        # Clean up grouped stats keys and handle NaNs\\n        cleaned_grouped_stats = {}\\n        for metric_tuple, values in grouped_stats.items():\\n            metric, stat = metric_tuple\\n            if metric not in cleaned_grouped_stats:\\n                cleaned_grouped_stats[metric] = {}\\n            for group, value in values.items():\\n                if group not in cleaned_grouped_stats[metric]:\\n                    cleaned_grouped_stats[metric][group] = {}\\n                cleaned_grouped_stats[metric][group][stat] = 0.0 if pd.isna(value) else value\\n\\n        return {\\n            \\\"overall_statistics\\\": overall_stats,\\n            \\\"grouped_statistics\\\": cleaned_grouped_stats\\n        }\\n    except Exception:\\n        return None\\n\\ndef perform_correlation_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs a correlation analysis between the primary sentiment dimensions.\\n\\n    Methodology:\\n    Calculates the Pearson correlation coefficient (r) between 'positive_sentiment'\\n    and 'negative_sentiment'.\\n    \\n    Tier 3 Caveat (N=2): The result is purely illustrative of the relationship within this\\n    tiny sample. With only two data points, the correlation will necessarily be -1 or 1,\\n    and it has no statistical power or generalizability.\\n\\n    Args:\\n        df: The analysis DataFrame.\\n\\n    Returns:\\n        A dictionary containing the correlation matrix, or None.\\n    \\\"\\\"\\\"\\n    if df is None or len(df) < 2:\\n        return {\\n            \\\"notes\\\": \\\"Correlation requires at least 2 data points.\\\",\\n            \\\"correlation_matrix\\\": None\\n        }\\n\\n    try:\\n        dims = ['positive_sentiment', 'negative_sentiment']\\n        corr_matrix = df[dims].corr(method='pearson')\\n        return {\\n            \\\"notes\\\": \\\"Tier 3 (N=2) result is illustrative, not generalizable.\\\",\\n            \\\"correlation_matrix\\\": corr_matrix.to_dict()\\n        }\\n    except Exception:\\n        return None\\n\\ndef compare_group_means(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Provides an exploratory comparison of mean scores between sentiment groups.\\n\\n    Methodology:\\n    For each scoring dimension, this function calculates the mean score for the 'positive'\\n    and 'negative' groups and reports the absolute difference. \\n\\n    Tier 3 Caveat (N=2, n=1 per group): This is not an inferential test (e.g., t-test).\\n    The differences are simple observations. Effect size metrics like Cohen's d are not\\n    meaningful as within-group variance is zero.\\n\\n    Args:\\n        df: The analysis DataFrame.\\n\\n    Returns:\\n        A dictionary of mean differences per dimension, or None.\\n    \\\"\\\"\\\"\\n    if df is None or 'group' not in df.columns or df['group'].nunique() < 2:\\n        return None\\n        \\n    try:\\n        results = {}\\n        metrics = ['positive_sentiment', 'negative_sentiment', 'sentiment_polarity', 'sentiment_intensity']\\n        \\n        means = df.groupby('group')[metrics].mean()\\n        if 'positive' not in means.index or 'negative' not in means.index:\\n            return {\\\"notes\\\": \\\"Both 'positive' and 'negative' groups must be present.\\\"}\\n\\n        for metric in metrics:\\n            mean_pos = means.loc['positive', metric]\\n            mean_neg = means.loc['negative', metric]\\n            difference = mean_pos - mean_neg\\n            results[metric] = {\\n                'positive_group_mean': mean_pos,\\n                'negative_group_mean': mean_neg,\\n                'mean_difference': difference\\n            }\\n        \\n        return results\\n    except Exception:\\n        return None\\n\\ndef perform_statistical_analysis(data: List[Dict[str, Any]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function that prepares data and executes all statistical analyses.\\n    \\n    Args:\\n        data: A list of analysis artifact dictionaries.\\n        \\n    Returns:\\n        A dictionary containing the results of all analyses.\\n    \\\"\\\"\\\"\\n    results = {\\n        'descriptive_statistics': None,\\n        'correlation_analysis': None,\\n        'group_mean_comparison': None\\n    }\\n    \\n    df = _create_analysis_dataframe(data)\\n    \\n    if df is None:\\n        return results # Return empty results if data prep fails\\n\\n    results['descriptive_statistics'] = calculate_descriptive_statistics(df)\\n    results['correlation_analysis'] = perform_correlation_analysis(df)\\n    results['group_mean_comparison'] = compare_group_means(df)\\n    \\n    return results\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": {\n      \"overall_statistics\": {\n        \"positive_sentiment\": {\n          \"mean\": 0.5,\n          \"std\": 0.7071067811865476,\n          \"min\": 0.0,\n          \"max\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"mean\": 0.5,\n          \"std\": 0.7071067811865476,\n          \"min\": 0.0,\n          \"max\": 1.0\n        },\n        \"sentiment_polarity\": {\n          \"mean\": 0.0,\n          \"std\": 1.4142135623730951,\n          \"min\": -1.0,\n          \"max\": 1.0\n        },\n        \"sentiment_intensity\": {\n          \"mean\": 1.0,\n          \"std\": 0.0,\n          \"min\": 1.0,\n          \"max\": 1.0\n        }\n      },\n      \"grouped_statistics\": {\n        \"positive_sentiment\": {\n          \"negative\": {\n            \"mean\": 0.0,\n            \"std\": 0.0\n          },\n          \"positive\": {\n            \"mean\": 1.0,\n            \"std\": 0.0\n          }\n        },\n        \"negative_sentiment\": {\n          \"negative\": {\n            \"mean\": 1.0,\n            \"std\": 0.0\n          },\n          \"positive\": {\n            \"mean\": 0.0,\n            \"std\": 0.0\n          }\n        },\n        \"sentiment_polarity\": {\n          \"negative\": {\n            \"mean\": -1.0,\n            \"std\": 0.0\n          },\n          \"positive\": {\n            \"mean\": 1.0,\n            \"std\": 0.0\n          }\n        },\n        \"sentiment_intensity\": {\n          \"negative\": {\n            \"mean\": 1.0,\n            \"std\": 0.0\n          },\n          \"positive\": {\n            \"mean\": 1.0,\n            \"std\": 0.0\n          }\n        }\n      }\n    },\n    \"correlation_analysis\": {\n      \"notes\": \"Tier 3 (N=2) result is illustrative, not generalizable.\",\n      \"correlation_matrix\": {\n        \"positive_sentiment\": {\n          \"positive_sentiment\": 1.0,\n          \"negative_sentiment\": -1.0\n        },\n        \"negative_sentiment\": {\n          \"positive_sentiment\": -1.0,\n          \"negative_sentiment\": 1.0\n        }\n      }\n    },\n    \"group_mean_comparison\": {\n      \"positive_sentiment\": {\n        \"positive_group_mean\": 1.0,\n        \"negative_group_mean\": 0.0,\n        \"mean_difference\": 1.0\n      },\n      \"negative_sentiment\": {\n        \"positive_group_mean\": 0.0,\n        \"negative_group_mean\": 1.0,\n        \"mean_difference\": -1.0\n      },\n      \"sentiment_polarity\": {\n        \"positive_group_mean\": 1.0,\n        \"negative_group_mean\": -1.0,\n        \"mean_difference\": 2.0\n      },\n      \"sentiment_intensity\": {\n        \"positive_group_mean\": 1.0,\n        \"negative_group_mean\": 1.0,\n        \"mean_difference\": 0.0\n      }\n    },\n    \"anova_analysis\": null,\n    \"reliability_analysis\": null,\n    \"additional_analyses\": null\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 2,\n    \"tier_classification\": \"TIER 3\",\n    \"power_notes\": \"With a total sample size of N=2 (n=1 per group), the analysis is classified as Tier 3 (Exploratory). All statistical findings are purely descriptive observations of this specific dataset. Inferential statistics (e.g., p-values, confidence intervals) are not applicable or meaningful, and results cannot be generalized. The purpose of this analysis is to demonstrate methodological execution rather than to draw statistically significant conclusions.\"\n  },\n  \"methodology_summary\": \"The statistical analysis was conducted following a Tier 3 exploratory protocol due to the small sample size (N=2). The methodology involved three main steps. First, a data preparation function parsed and merged the score and derived metric artifacts into a single analysis DataFrame, mapping documents to their 'positive' or 'negative' groups based on the corpus manifest. Second, descriptive statistics (mean, standard deviation, min, max) were calculated for the entire sample and for each group to summarize the data's characteristics. Finally, an illustrative Pearson correlation was computed between positive and negative sentiment scores, and a direct comparison of group means was performed to observe the magnitude of differences between the two sentiment groups. No inferential tests were conducted due to the lack of statistical power.\"\n}\n```",
    "analysis_artifacts_processed": 6,
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-pro",
      "execution_time_seconds": 49.287208,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0,
      "prompt_length": 28687,
      "response_length": 12967
    },
    "timestamp": "2025-09-19T21:01:44.545945+00:00",
    "artifact_hash": "e8e334c33c79569480668cbf8471e322a2902604259b11851f7369a565991e18"
  },
  "verification": {
    "batch_id": "v2_statistical_20250919_170055",
    "step": "verification",
    "model_used": "vertex_ai/gemini-2.5-flash-lite",
    "verification_status": "unknown",
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-flash-lite",
      "execution_time_seconds": 81.053627,
      "prompt_length": 13465,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0
    },
    "timestamp": "2025-09-19T21:03:05.604548+00:00",
    "artifact_hash": "7f0858cc0fc03a1688cc89792d3aebb8e2e5b9a86cf516d41cd4b6d9ffbd255b"
  },
  "csv_generation": {
    "batch_id": "v2_statistical_20250919_170055",
    "step": "csv_generation",
    "model_used": "vertex_ai/gemini-2.5-flash",
    "csv_files": [
      {
        "filename": "scores.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250919T170318Z/data/scores.csv",
        "size": 224
      },
      {
        "filename": "derived_metrics.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250919T170318Z/data/derived_metrics.csv",
        "size": 89
      },
      {
        "filename": "metadata.csv",
        "path": "/Volumes/code/discernus/projects/nano_test_experiment/runs/20250919T170318Z/data/metadata.csv",
        "size": 389
      }
    ],
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-flash",
      "execution_time_seconds": 12.615607,
      "prompt_length": 4586,
      "artifacts_processed": 2,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0
    },
    "timestamp": "2025-09-19T21:03:18.230416+00:00",
    "artifact_hash": "fe6c745a1540b967c199d163138c98f77a9666454038d2d5a3b80ceda9dfc522"
  },
  "total_cost_info": {
    "total_cost_usd": 0.0,
    "total_execution_time_seconds": 142.956442,
    "total_tokens": 0,
    "cost_breakdown": {
      "statistical_execution": 0.0,
      "verification": 0.0,
      "csv_generation": 0.0
    },
    "performance_breakdown": {
      "statistical_execution_time": 49.287208,
      "verification_time": 81.053627,
      "csv_generation_time": 12.615607
    },
    "models_used": [
      "vertex_ai/gemini-2.5-pro",
      "vertex_ai/gemini-2.5-flash-lite",
      "vertex_ai/gemini-2.5-flash"
    ]
  },
  "timestamp": "2025-09-19T21:03:18.232151+00:00",
  "agent_name": "StatisticalAgent"
}