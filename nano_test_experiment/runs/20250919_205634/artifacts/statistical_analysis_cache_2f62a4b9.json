{
  "batch_id": "v2_statistical_20250919_165731",
  "statistical_analysis": {
    "batch_id": "v2_statistical_20250919_165731",
    "step": "statistical_execution",
    "model_used": "vertex_ai/gemini-2.5-pro",
    "statistical_functions_and_results": "```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nfrom typing import Dict, Any, Optional, List, Union\\nimport json\\n\\n# --- Data Preparation --- #\\n\\ndef create_analysis_dataframe(data: Dict[str, Any]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Parses analysis artifacts to create a clean DataFrame with scores and metadata.\\n\\n    This function extracts dimensional scores from the 'enhanced_composite_analysis_generation'\\n    artifact and merges it with metadata from the corpus manifest.\\n\\n    It makes a reasoned assumption for mapping document indices from the artifact\\n    (e.g., 'Document 0', 'Document 1') to the ordered list of documents in the manifest.\\n\\n    Args:\\n        data (Dict[str, Any]): A dictionary containing all analysis artifacts.\\n\\n    Returns:\\n        Optional[pd.DataFrame]: A DataFrame ready for statistical analysis, or None if key data is missing.\\n    \\\"\\\"\\\"\\n    # Find the main analysis artifact\\n    analysis_artifact = None\\n    for artifact in data.values():\\n        if artifact.get('step') == 'enhanced_composite_analysis_generation':\\n            # The actual data is often a JSON string within the artifact\\n            if isinstance(artifact.get('raw_analysis_response'), str):\\n                try:\\n                    analysis_artifact = json.loads(artifact['raw_analysis_response'])\\n                except json.JSONDecodeError:\\n                    continue\\n            else:\\n                analysis_artifact = artifact\\n            break\\n\\n    if not analysis_artifact or 'document_analyses' not in analysis_artifact:\\n        return None\\n\\n    # Hardcoded corpus manifest as per the problem description\\n    corpus_manifest = {\\n        \\\"documents\\\": [\\n            {\\n                \\\"filename\\\": \\\"corpus/positive_test.txt\\\",\\n                \\\"document_id\\\": \\\"pos_test\\\",\\n                \\\"metadata\\\": {\\\"type\\\": \\\"test\\\", \\\"sentiment\\\": \\\"positive\\\"}\\n            },\\n            {\\n                \\\"filename\\\": \\\"corpus/negative_test.txt\\\",\\n                \\\"document_id\\\": \\\"neg_test\\\",\\n                \\\"metadata\\\": {\\\"type\\\": \\\"test\\\", \\\"sentiment\\\": \\\"negative\\\"}\\n            }\\n        ]\\n    }\\n\\n    records = []\\n    doc_analyses = analysis_artifact.get('document_analyses', [])\\n    manifest_docs = corpus_manifest.get('documents', [])\\n\\n    if len(doc_analyses) != len(manifest_docs):\\n        # Fallback if lengths don't match, though they should for this problem\\n        return None\\n\\n    for i, doc_analysis in enumerate(doc_analyses):\\n        dimensional_scores = doc_analysis.get('dimensional_scores', {})\\n        record = {\\n            'document_name': manifest_docs[i]['filename'],\\n            'document_id': manifest_docs[i]['document_id']\\n        }\\n        # Add metadata\\n        record.update(manifest_docs[i].get('metadata', {}))\\n        \\n        # Add scores\\n        for dim, scores in dimensional_scores.items():\\n            record[f\\\"{dim}_raw_score\\\"] = scores.get('raw_score')\\n            record[f\\\"{dim}_salience\\\"] = scores.get('salience')\\n            record[f\\\"{dim}_confidence\\\"] = scores.get('confidence')\\n            \\n        records.append(record)\\n\\n    return pd.DataFrame(records)\\n\\n\\n# --- Statistical Analyses --- #\\n\\ndef calculate_descriptive_statistics(df: pd.DataFrame, group_by: Optional[str] = 'sentiment') -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates and returns descriptive statistics for numerical columns.\\n\\n    Args:\\n        df (pd.DataFrame): The input DataFrame containing analysis scores.\\n        group_by (Optional[str]): The column to group the statistics by. If None, computes overall stats.\\n\\n    Returns:\\n        Optional[Dict[str, Any]]: A dictionary of descriptive statistics, or None if input is invalid.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty:\\n        return {\\\"error\\\": \\\"Input DataFrame is empty or None.\\\"}\\n\\n    score_cols = [col for col in df.columns if 'raw_score' in col]\\n    if not score_cols:\\n        return {\\\"error\\\": \\\"No score columns found in the DataFrame.\\\"}\\n\\n    try:\\n        if group_by and group_by in df.columns:\\n            # Use .agg() for specific statistics\\n            desc_stats = df.groupby(group_by)[score_cols].agg(['mean', 'std', 'min', 'max', 'count'])\\n            return json.loads(desc_stats.to_json(orient='index'))\\n        else:\\n            desc_stats = df[score_cols].agg(['mean', 'std', 'min', 'max', 'count'])\\n            return json.loads(desc_stats.to_json(orient='index'))\\n    except Exception as e:\\n        return {\\\"error\\\": f\\\"An error occurred during descriptive statistics calculation: {str(e)}\\\"}\\n\\n\\ndef perform_group_comparison(df: pd.DataFrame, dv: str, iv: str) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs a group comparison. Due to N<8 per group (Tier 3), this function focuses\\n    on descriptive statistics and effect size rather than inferential tests like t-tests,\\n    which are not valid for this sample size.\\n\\n    Args:\\n        df (pd.DataFrame): The input DataFrame.\\n        dv (str): The dependent variable (a score column).\\n        iv (str): The independent variable (the grouping column, e.g., 'sentiment').\\n\\n    Returns:\\n        Optional[Dict[str, Any]]: A dictionary with group descriptives and a note on statistical power.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty or iv not in df.columns or dv not in df.columns:\\n        return {\\\"error\\\": \\\"Invalid input DataFrame, dependent variable, or independent variable.\\\"}\\n    \\n    groups = df[iv].unique()\\n    if len(groups) < 2:\\n        return {\\\"error\\\": \\\"Group comparison requires at least two groups.\\\"}\\n\\n    # Tier 3: N<8 per group. Focus on descriptive statistics.\\n    descriptives = df.groupby(iv)[dv].agg(['mean', 'std', 'count']).reset_index()\\n\\n    group1_name = groups[0]\\n    group2_name = groups[1]\\n    \\n    group1_stats = descriptives[descriptives[iv] == group1_name]\\n    group2_stats = descriptives[descriptives[iv] == group2_name]\\n    \\n    if group1_stats.empty or group2_stats.empty or group1_stats['count'].iloc[0] < 1 or group2_stats['count'].iloc[0] < 1:\\n        return {\\\"error\\\": \\\"One or both groups have no data.\\\"}\\n\\n    mean_diff = float(group1_stats['mean'].iloc[0] - group2_stats['mean'].iloc[0])\\n\\n    results = {\\n        'dependent_variable': dv,\\n        'independent_variable': iv,\\n        'analysis_type': 'Exploratory Descriptive Comparison (Tier 3)',\\n        'notes': 'Inferential tests (e.g., t-test) were not performed due to extremely small sample size (N<8 per group), which violates assumptions and provides no statistical power. Analysis is limited to comparing descriptive statistics.',\\n        'groups': json.loads(descriptives.to_json(orient='records')),\\n        'mean_difference': {\\n            'comparison': f\\\"mean({group1_name}) - mean({group2_name})\\\",\\n            'value': mean_diff\\n        }\\n    }\\n    return results\\n\\n\\ndef perform_correlation_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs correlation analysis on score columns. Returns a message if data is insufficient.\\n\\n    Args:\\n        df (pd.DataFrame): The input DataFrame.\\n\\n    Returns:\\n        Optional[Dict[str, Any]]: A dictionary with the correlation matrix or an insufficiency note.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty:\\n        return {\\\"error\\\": \\\"Input DataFrame is empty or None.\\\"}\\n\\n    score_cols = [col for col in df.columns if 'raw_score' in col]\\n    if len(df) < 3:\\n        return {\\n            'status': 'skipped',\\n            'reason': f'Correlation analysis requires at least 3 data points. Provided: {len(df)}.'\\n        }\\n    \\n    try:\\n        corr_matrix = df[score_cols].corr()\\n        return json.loads(corr_matrix.to_json(orient='index'))\\n    except Exception as e:\\n        return {\\\"error\\\": f\\\"An error occurred during correlation analysis: {str(e)}\\\"}\\n\\n\\ndef perform_all_analyses(data: Dict[str, Any]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function to orchestrate and execute all statistical analyses.\\n\\n    Args:\\n        data (Dict[str, Any]): A dictionary containing all analysis artifacts.\\n\\n    Returns:\\n        Dict[str, Any]: A nested dictionary containing the results of all statistical analyses.\\n    \\\"\\\"\\\"\\n    df = create_analysis_dataframe(data)\\n    \\n    if df is None:\\n        return {\\\"error\\\": \\\"Failed to create analysis DataFrame from artifacts.\\\"}\\n\\n    results = {\\n        'descriptive_statistics': calculate_descriptive_statistics(df, group_by='sentiment'),\\n        'group_comparisons': {\\n             'positive_sentiment_by_sentiment': perform_group_comparison(df, dv='positive_sentiment_raw_score', iv='sentiment'),\\n             'negative_sentiment_by_sentiment': perform_group_comparison(df, dv='negative_sentiment_raw_score', iv='sentiment')\\n        },\\n        'correlation_analysis': perform_correlation_analysis(df)\\n    }\\n\\n    return results\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": {\n      \"negative\": {\n        \"positive_sentiment_raw_score\": {\n          \"mean\": 0.0,\n          \"std\": null,\n          \"min\": 0.0,\n          \"max\": 0.0,\n          \"count\": 1\n        },\n        \"negative_sentiment_raw_score\": {\n          \"mean\": 0.98,\n          \"std\": null,\n          \"min\": 0.98,\n          \"max\": 0.98,\n          \"count\": 1\n        }\n      },\n      \"positive\": {\n        \"positive_sentiment_raw_score\": {\n          \"mean\": 0.98,\n          \"std\": null,\n          \"min\": 0.98,\n          \"max\": 0.98,\n          \"count\": 1\n        },\n        \"negative_sentiment_raw_score\": {\n          \"mean\": 0.0,\n          \"std\": null,\n          \"min\": 0.0,\n          \"max\": 0.0,\n          \"count\": 1\n        }\n      }\n    },\n    \"group_comparisons\": {\n      \"positive_sentiment_by_sentiment\": {\n        \"dependent_variable\": \"positive_sentiment_raw_score\",\n        \"independent_variable\": \"sentiment\",\n        \"analysis_type\": \"Exploratory Descriptive Comparison (Tier 3)\",\n        \"notes\": \"Inferential tests (e.g., t-test) were not performed due to extremely small sample size (N<8 per group), which violates assumptions and provides no statistical power. Analysis is limited to comparing descriptive statistics.\",\n        \"groups\": [\n          {\n            \"sentiment\": \"negative\",\n            \"mean\": 0.0,\n            \"std\": null,\n            \"count\": 1\n          },\n          {\n            \"sentiment\": \"positive\",\n            \"mean\": 0.98,\n            \"std\": null,\n            \"count\": 1\n          }\n        ],\n        \"mean_difference\": {\n          \"comparison\": \"mean(negative) - mean(positive)\",\n          \"value\": -0.98\n        }\n      },\n      \"negative_sentiment_by_sentiment\": {\n        \"dependent_variable\": \"negative_sentiment_raw_score\",\n        \"independent_variable\": \"sentiment\",\n        \"analysis_type\": \"Exploratory Descriptive Comparison (Tier 3)\",\n        \"notes\": \"Inferential tests (e.g., t-test) were not performed due to extremely small sample size (N<8 per group), which violates assumptions and provides no statistical power. Analysis is limited to comparing descriptive statistics.\",\n        \"groups\": [\n          {\n            \"sentiment\": \"negative\",\n            \"mean\": 0.98,\n            \"std\": null,\n            \"count\": 1\n          },\n          {\n            \"sentiment\": \"positive\",\n            \"mean\": 0.0,\n            \"std\": null,\n            \"count\": 1\n          }\n        ],\n        \"mean_difference\": {\n          \"comparison\": \"mean(negative) - mean(positive)\",\n          \"value\": 0.98\n        }\n      }\n    },\n    \"correlation_analysis\": {\n      \"status\": \"skipped\",\n      \"reason\": \"Correlation analysis requires at least 3 data points. Provided: 2.\"\n    },\n    \"additional_analyses\": {}\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 2,\n    \"tier_classification\": \"TIER 3\",\n    \"power_notes\": \"The sample size is N=2 (n=1 per group). This is the absolute minimum for a two-group comparison and falls squarely in Tier 3 (Exploratory Analysis). Statistical power is zero, and inferential tests (like t-tests, ANOVA) are mathematically impossible and conceptually invalid. All conclusions must be strictly limited to describing the patterns observed in this specific sample without any generalization.\"\n  },\n  \"methodology_summary\": \"Due to the extremely small sample size (N=2), the analysis was restricted to a Tier 3 exploratory approach. A data preparation function was created to parse the analysis artifacts and merge them with the corpus manifest metadata, creating two groups based on the 'sentiment' field ('positive', 'negative'). The primary analysis consisted of calculating descriptive statistics (mean, std, min, max) for the 'positive_sentiment' and 'negative_sentiment' scores, both overall and for each group. Group comparisons were limited to presenting these descriptive statistics side-by-side to highlight differences. Inferential tests like t-tests and correlation analyses were explicitly skipped due to insufficient data, and appropriate notifications were generated. The methodology prioritizes statistical validity by avoiding tests that would be inappropriate for the given data.\"\n}\n```",
    "analysis_artifacts_processed": 6,
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-pro",
      "execution_time_seconds": 50.317745,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0,
      "prompt_length": 28280,
      "response_length": 13246
    },
    "timestamp": "2025-09-19T20:58:22.204931+00:00",
    "artifact_hash": "e66d54a3b7d9155115e8c3e2a9eb99d3a92ba16337f27a0f949ec0b99de89a99"
  },
  "verification": {
    "batch_id": "v2_statistical_20250919_165731",
    "step": "verification",
    "model_used": "vertex_ai/gemini-2.5-flash-lite",
    "verification_status": "unknown",
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-flash-lite",
      "execution_time_seconds": 38.003129,
      "prompt_length": 13744,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0
    },
    "timestamp": "2025-09-19T20:59:00.211626+00:00",
    "artifact_hash": "e0fcf15adb3f33c191a15bc83dea0c2f9fe2bae588bfd4d76bb7b00d1422e661"
  },
  "csv_generation": {
    "batch_id": "v2_statistical_20250919_165731",
    "step": "csv_generation",
    "model_used": "vertex_ai/gemini-2.5-flash",
    "csv_files": [],
    "cost_info": {
      "model": "vertex_ai/gemini-2.5-flash",
      "execution_time_seconds": 30.836172,
      "prompt_length": 4022,
      "artifacts_processed": 2,
      "response_cost": 0.0,
      "input_tokens": 0,
      "output_tokens": 0,
      "total_tokens": 0
    },
    "timestamp": "2025-09-19T20:59:31.052838+00:00",
    "artifact_hash": "2ec31dcf67eacf1475da4d4e2eaa891c01ebfe3f9d3c7ec4599127fad50520c5"
  },
  "total_cost_info": {
    "total_cost_usd": 0.0,
    "total_execution_time_seconds": 119.15704600000001,
    "total_tokens": 0,
    "cost_breakdown": {
      "statistical_execution": 0.0,
      "verification": 0.0,
      "csv_generation": 0.0
    },
    "performance_breakdown": {
      "statistical_execution_time": 50.317745,
      "verification_time": 38.003129,
      "csv_generation_time": 30.836172
    },
    "models_used": [
      "vertex_ai/gemini-2.5-pro",
      "vertex_ai/gemini-2.5-flash-lite",
      "vertex_ai/gemini-2.5-flash"
    ]
  },
  "timestamp": "2025-09-19T20:59:31.055004+00:00",
  "agent_name": "StatisticalAgent"
}