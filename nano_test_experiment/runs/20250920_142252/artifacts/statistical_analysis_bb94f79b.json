{
  "batch_id": "v2_statistical_20250920_102411",
  "step": "statistical_execution",
  "model_used": "vertex_ai/gemini-2.5-pro",
  "statistical_functions_and_results": "```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nfrom typing import Dict, Any, Optional, List, Union\\nimport json\\n\\n# --- Data Preparation ---\\n\\ndef _get_corpus_manifest_mapping() -> Dict[int, Dict[str, str]]:\\n    \\\"\\\"\\\"\\n    Parses the provided corpus manifest to create a document index to metadata mapping.\\n\\n    Returns:\\n        Dict[int, Dict[str, str]]: A dictionary mapping document index to its metadata.\\n    \\\"\\\"\\\"\\n    manifest = {\\n        'name': 'Nano Test Corpus',\\n        'version': '1.0',\\n        'spec_version': '8.0',\\n        'total_documents': 2,\\n        'date_range': '2024',\\n        'documents': [\\n            {\\n                'filename': 'corpus/positive_test.txt',\\n                'document_id': 'pos_test',\\n                'metadata': {\\n                    'type': 'test',\\n                    'sentiment': 'positive'\\n                }\\n            },\\n            {\\n                'filename': 'corpus/negative_test.txt',\\n                'document_id': 'neg_test',\\n                'metadata': {\\n                    'type': 'test',\\n                    'sentiment': 'negative'\\n                }\\n            }\\n        ]\\n    }\\n    mapping = {i: doc for i, doc in enumerate(manifest['documents'])}\\n    return mapping\\n\\ndef _prepare_data(data: List[Dict[str, Any]]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Extracts scores and metadata from analysis artifacts into a structured pandas DataFrame.\\n    This function handles the extraction of data from various artifact types and merges them\\n    into a single, analyzable DataFrame, linking documents to their corpus metadata via\\n    the 'document_index'.\\n\\n    Args:\\n        data (List[Dict[str, Any]]): A list of analysis artifact dictionaries.\\n\\n    Returns:\\n        Optional[pd.DataFrame]: A DataFrame ready for analysis, or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    scores_data = {}\\n    derived_metrics_data = {}\\n    doc_index_map = {}\\n\\n    for artifact in data:\\n        try:\\n            if artifact.get('step') == 'score_extraction' and 'scores_extraction' in artifact:\\n                scores_content = json.loads(artifact['scores_extraction'].strip('```json\\\\n').strip('\\\\n```'))\\n                scores_data.update(scores_content)\\n\\n            elif artifact.get('step') == 'derived_metrics_generation' and 'raw_metrics_response' in artifact:\\n                # Extract the final JSON block from the response\\n                json_str = artifact['raw_metrics_response'].split('```json')[-1].strip('\\\\n```\\\\n')\\n                derived_content = json.loads(json_str)\\n                derived_metrics_data.update(derived_content)\\n            \\n            if 'document_name' in artifact and 'document_index' in artifact:\\n                doc_index_map[artifact['document_name']] = artifact['document_index']\\n\\n        except (json.JSONDecodeError, KeyError, IndexError):\\n            continue\\n\\n    if not scores_data:\\n        return None\\n\\n    manifest_mapping = _get_corpus_manifest_mapping()\\n    records = []\\n\\n    for doc_name, scores in scores_data.items():\\n        doc_index = doc_index_map.get(doc_name)\\n        if doc_index is None:\\n            continue\\n        \\n        manifest_info = manifest_mapping.get(doc_index, {})\\n        metadata = manifest_info.get('metadata', {})\\n        \\n        record = {\\n            'document_id': manifest_info.get('document_id', doc_name),\\n            'group': metadata.get('sentiment', 'unknown'),\\n            'positive_sentiment': scores.get('positive_sentiment', {}).get('raw_score'),\\n            'negative_sentiment': scores.get('negative_sentiment', {}).get('raw_score'),\\n        }\\n\\n        if doc_name in derived_metrics_data:\\n            record.update(derived_metrics_data[doc_name])\\n        \\n        records.append(record)\\n\\n    if not records:\\n        return None\\n\\n    df = pd.DataFrame(records)\\n    return df\\n\\n# --- Statistical Analysis Functions ---\\n\\ndef calculate_descriptive_statistics(df: pd.DataFrame, dimensions: List[str]) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates and returns descriptive statistics (mean, std, count, etc.) for specified dimensions,\\n    grouped by the 'group' column.\\n\\n    Methodology:\\n    - Tier 3 Analysis (N<15): Focuses on descriptive statistics as the primary output due to low power.\\n    - Groups data by the 'group' column.\\n    - Calculates mean, standard deviation, min, max, and count for each dimension.\\n    - Handles cases with single-member groups where standard deviation is NaN.\\n\\n    Args:\\n        df (pd.DataFrame): The input data.\\n        dimensions (List[str]): A list of column names for which to calculate statistics.\\n\\n    Returns:\\n        Optional[Dict[str, Any]]: A nested dictionary of descriptive statistics, or None.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty or 'group' not in df.columns:\\n        return {\\\"status\\\": \\\"ERROR\\\", \\\"message\\\": \\\"Input DataFrame is invalid or missing 'group' column.\\\"}\\n    \\n    try:\\n        grouped = df.groupby('group')\\n        results = {}\\n        \\n        for group_name, group_df in grouped:\\n            group_stats = {}\\n            for dim in dimensions:\\n                if dim in group_df.columns:\\n                    stats_series = group_df[dim].agg(['mean', 'std', 'min', 'max', 'count']).to_dict()\\n                    # Replace NaN std with 0 for single-element groups for cleaner output\\n                    if pd.isna(stats_series['std']):\\n                        stats_series['std'] = 0.0\\n                    group_stats[dim] = stats_series\\n            results[group_name] = group_stats\\n        \\n        return {\\\"status\\\": \\\"OK\\\", \\\"statistics\\\": results}\\n    except Exception as e:\\n        return {\\\"status\\\": \\\"ERROR\\\", \\\"message\\\": f\\\"An error occurred: {str(e)}\\\"}\\n\\ndef perform_group_comparison(df: pd.DataFrame, dv: str, iv: str) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Performs a group comparison (e.g., t-test) if data is sufficient. Given the sample size,\\n    this is a Tier 3 exploratory analysis and will likely be skipped.\\n\\n    Methodology:\\n    - Checks for minimum sample size requirements for a meaningful comparison.\\n    - With N < 2 groups or any group having N < 2, the test is skipped.\\n    - This function serves as a placeholder for a well-powered analysis and demonstrates\\n      the principle of tiered analysis.\\n\\n    Args:\\n        df (pd.DataFrame): The input data.\\n        dv (str): The dependent variable column name.\\n        iv (str): The independent variable (grouping) column name.\\n\\n    Returns:\\n        Dict[str, Any]: A dictionary containing the analysis results or a skip message.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty or iv not in df.columns or dv not in df.columns:\\n        return {\\\"status\\\": \\\"SKIPPED\\\", \\\"message\\\": \\\"Input DataFrame is invalid or missing required columns.\\\"}\\n\\n    groups = df[iv].unique()\\n    if len(groups) < 2:\\n        return {\\n            \\\"status\\\": \\\"SKIPPED\\\",\\n            \\\"analysis_type\\\": \\\"T-test / ANOVA\\\",\\n            \\\"message\\\": f\\\"Insufficient number of groups for comparison. Found {len(groups)}, require at least 2.\\\"\\n        }\\n\\n    group_sizes = df[iv].value_counts()\\n    if any(group_sizes < 2):\\n        return {\\n            \\\"status\\\": \\\"SKIPPED\\\",\\n            \\\"analysis_type\\\": \\\"T-test / ANOVA\\\",\\n            \\\"message\\\": f\\\"One or more groups have fewer than 2 data points, making variance calculation impossible. Group sizes: {group_sizes.to_dict()}\\\"\\n        }\\n    \\n    # This part would execute if data were sufficient\\n    # For this specific dataset, this code will not be reached.\\n    try:\\n        if len(groups) == 2:\\n            group1 = df[df[iv] == groups[0]][dv]\\n            group2 = df[df[iv] == groups[1]][dv]\\n            ttest_result = pg.ttest(group1, group2, correction=True) # Welch's t-test\\n            return {\\\"status\\\": \\\"OK\\\", \\\"analysis_type\\\": \\\"T-test\\\", \\\"results\\\": json.loads(ttest_result.to_json(orient='records'))[0]}\\n        else: # More than 2 groups\\n            aov_result = pg.anova(data=df, dv=dv, between=iv)\\n            return {\\\"status\\\": \\\"OK\\\", \\\"analysis_type\\\": \\\"ANOVA\\\", \\\"results\\\": json.loads(aov_result.to_json(orient='records'))[0]}\\n    except Exception as e:\\n        return {\\\"status\\\": \\\"ERROR\\\", \\\"message\\\": f\\\"An error occurred during group comparison: {str(e)}\\\"}\\n\\ndef perform_correlation_analysis(df: pd.DataFrame, dimensions: List[str]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Performs a correlation analysis between specified dimensions.\\n\\n    Methodology:\\n    - Tier 3 Analysis (N<15): Acknowledges that correlations with very small N are unstable.\\n    - Checks if N >= 3, as correlation is undefined or trivial for N < 3.\\n    - If sufficient data exists, calculates Pearson correlation coefficient, p-value, and sample size.\\n\\n    Args:\\n        df (pd.DataFrame): The input data.\\n        dimensions (List[str]): A list of 2 or more column names for correlation.\\n\\n    Returns:\\n        Dict[str, Any]: A dictionary with correlation results or a skip message.\\n    \\\"\\\"\\\"\\n    if df is None or df.shape[0] < 3:\\n        return {\\n            \\\"status\\\": \\\"SKIPPED\\\",\\n            \\\"message\\\": f\\\"Insufficient data for meaningful correlation analysis. Found {df.shape[0] if df is not None else 0} data points, require at least 3.\\\"\\n        }\\n    \\n    try:\\n        corr_matrix = df[dimensions].corr(method='pearson')\\n        # Convert to a more JSON-friendly format of pairwise results\\n        corr_matrix = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\\n        corr_pairs = corr_matrix.stack().reset_index()\\n        corr_pairs.columns = ['var1', 'var2', 'correlation']\\n        results = corr_pairs.to_dict(orient='records')\\n        return {\\\"status\\\": \\\"OK\\\", \\\"correlation_type\\\": \\\"Pearson\\\", \\\"results\\\": results, \\\"n\\\": df.shape[0]}\\n    except Exception as e:\\n        return {\\\"status\\\": \\\"ERROR\\\", \\\"message\\\": f\\\"An error occurred during correlation analysis: {str(e)}\\\"}\\n\\n\\ndef calculate_reliability_analysis(df: pd.DataFrame, dimensions: List[str]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Assesses the applicability of reliability analysis (e.g., Cronbach's alpha).\\n\\n    Methodology:\\n    - Cronbach's alpha measures the internal consistency of a scale composed of multiple items\\n      intended to measure the same underlying construct.\\n    - The dimensions 'positive_sentiment' and 'negative_sentiment' are designed to be distinct,\\n      not items of a single scale.\\n    - Therefore, this analysis is not applicable to this framework.\\n\\n    Args:\\n        df (pd.DataFrame): The input data.\\n        dimensions (List[str]): The dimensions to consider.\\n\\n    Returns:\\n        Dict[str, Any]: A dictionary explaining why the analysis is not applicable.\\n    \\\"\\\"\\\"\\n    return {\\n        \\\"status\\\": \\\"NOT_APPLICABLE\\\",\\n        \\\"message\\\": \\\"Cronbach's alpha is used to assess the internal consistency of items within a single scale. The dimensions 'positive_sentiment' and 'negative_sentiment' are distinct constructs, not items of a unidimensional scale. Therefore, reliability analysis is not appropriate.\\\"\\n    }\\n\\n# --- Master Execution Function ---\\n\\ndef perform_statistical_analysis(data: List[Dict[str, Any]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function that prepares data and executes all statistical analyses.\\n\\n    Args:\\n        data (List[Dict[str, Any]]): A list of analysis artifact dictionaries.\\n\\n    Returns:\\n        Dict[str, Any]: A dictionary containing the results of all executed analyses.\\n    \\\"\\\"\\\"\\n    df = _prepare_data(data)\\n    dimensions_to_analyze = ['positive_sentiment', 'negative_sentiment']\\n    \\n    results = {\\n        'descriptive_statistics': calculate_descriptive_statistics(df, dimensions_to_analyze),\\n        'anova_analysis': perform_group_comparison(df, dv='positive_sentiment', iv='group'), # Example DV\\n        'correlation_analysis': perform_correlation_analysis(df, dimensions=dimensions_to_analyze),\\n        'reliability_analysis': calculate_reliability_analysis(df, dimensions=dimensions_to_analyze)\\n    }\\n    \\n    return results\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": {\n      \"status\": \"OK\",\n      \"statistics\": {\n        \"negative\": {\n          \"positive_sentiment\": {\n            \"mean\": 0.0,\n            \"std\": 0.0,\n            \"min\": 0.0,\n            \"max\": 0.0,\n            \"count\": 1\n          },\n          \"negative_sentiment\": {\n            \"mean\": 1.0,\n            \"std\": 0.0,\n            \"min\": 1.0,\n            \"max\": 1.0,\n            \"count\": 1\n          }\n        }\n      }\n    },\n    \"correlation_analysis\": {\n      \"status\": \"SKIPPED\",\n      \"message\": \"Insufficient data for meaningful correlation analysis. Found 1 data points, require at least 3.\"\n    },\n    \"anova_analysis\": {\n      \"status\": \"SKIPPED\",\n      \"analysis_type\": \"T-test / ANOVA\",\n      \"message\": \"Insufficient number of groups for comparison. Found 1, require at least 2.\"\n    },\n    \"reliability_analysis\": {\n      \"status\": \"NOT_APPLICABLE\",\n      \"message\": \"Cronbach's alpha is used to assess the internal consistency of items within a single scale. The dimensions 'positive_sentiment' and 'negative_sentiment' are distinct constructs, not items of a unidimensional scale. Therefore, reliability analysis is not appropriate.\"\n    },\n    \"additional_analyses\": null\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 1,\n    \"tier_classification\": \"TIER 3: Exploratory Analysis\",\n    \"power_notes\": \"The analysis was conducted on a single data point (N=1). This sample size is far below the threshold for any form of inferential statistics (e.g., t-tests, ANOVA, correlations). All statistical tests requiring variance or multiple groups/data points have been skipped. The results are purely descriptive of this single document and cannot be generalized. This falls into the lowest end of Tier 3, representing a single-case observation rather than a sample-based analysis.\"\n  },\n  \"methodology_summary\": \"The statistical analysis was severely constrained by a sample size of N=1. Consequently, the methodology was restricted to a Tier 3 exploratory approach, focusing exclusively on descriptive statistics for the single available data point. Functions for group comparison (ANOVA/t-test) and correlation were included to demonstrate a complete analytical pipeline, but they were automatically skipped due to insufficient data. Reliability analysis (Cronbach's alpha) was deemed not applicable for the framework's distinct dimensions. The analysis successfully extracted the scores for the single document and reported its descriptive characteristics.\"\n}\n```",
  "analysis_artifacts_processed": 6,
  "cost_info": {
    "model": "vertex_ai/gemini-2.5-pro",
    "execution_time_seconds": 73.616442,
    "response_cost": 0.0,
    "input_tokens": 0,
    "output_tokens": 0,
    "total_tokens": 0,
    "prompt_length": 24958,
    "response_length": 14827
  },
  "timestamp": "2025-09-20T14:25:24.834882+00:00"
}