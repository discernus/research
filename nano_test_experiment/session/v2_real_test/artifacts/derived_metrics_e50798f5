{
  "analysis_id": "analysis_d6b4b48d",
  "step": "derived_metrics_generation",
  "model_used": "vertex_ai/gemini-2.5-flash-lite",
  "derived_metrics": "```python\nimport pandas as pd\n\n# Input scores\nscores_data = {\n  \"document_0\": {\n    \"positive_sentiment\": {\n      \"raw_score\": 0.0,\n      \"salience\": 0.0,\n      \"confidence\": 0.95\n    },\n    \"negative_sentiment\": {\n      \"raw_score\": 0.0,\n      \"salience\": 0.0,\n      \"confidence\": 0.95\n    }\n  }\n}\n\n# Convert scores to DataFrame for easier manipulation\ndf = pd.DataFrame.from_dict(scores_data, orient='index')\n\n# Flatten the nested structure for easier access to scores\ndf = df.apply(pd.Series.explode).apply(pd.Series.explode)\n\n# Convert relevant columns to numeric, coercing errors to NaN\nfor col in ['raw_score', 'salience', 'confidence']:\n    if col in df.columns:\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n\n# --- Derived Metrics Calculation ---\n\n# Calculate Sentiment Intensity (Average of positive and negative raw scores)\n# This metric aims to capture the overall emotional intensity, regardless of valence.\nif 'positive_sentiment' in df.columns and 'negative_sentiment' in df.columns:\n    df['sentiment_intensity'] = (df['positive_sentiment'].apply(lambda x: x.get('raw_score', 0) if isinstance(x, dict) else 0) +\n                                 df['negative_sentiment'].apply(lambda x: x.get('raw_score', 0) if isinstance(x, dict) else 0)) / 2\nelse:\n    df['sentiment_intensity'] = 0.0\n\n# Calculate Valence (Difference between positive and negative raw scores)\n# This metric indicates the dominant sentiment direction.\nif 'positive_sentiment' in df.columns and 'negative_sentiment' in df.columns:\n    df['valence'] = (df['positive_sentiment'].apply(lambda x: x.get('raw_score', 0) if isinstance(x, dict) else 0) -\n                     df['negative_sentiment'].apply(lambda x: x.get('raw_score', 0) if isinstance(x, dict) else 0))\nelse:\n    df['valence'] = 0.0\n\n# Calculate Overall Confidence (Average confidence of all dimensions)\n# This metric reflects the system's overall certainty in its sentiment assessment.\nconfidence_scores = []\nif 'positive_sentiment' in df.columns and isinstance(df.loc[df.index[0], 'positive_sentiment'], dict):\n    confidence_scores.append(df.loc[df.index[0], 'positive_sentiment'].get('confidence', 0))\nif 'negative_sentiment' in df.columns and isinstance(df.loc[df.index[0], 'negative_sentiment'], dict):\n    confidence_scores.append(df.loc[df.index[0], 'negative_sentiment'].get('confidence', 0))\n\nif confidence_scores:\n    df['overall_confidence'] = sum(confidence_scores) / len(confidence_scores)\nelse:\n    df['overall_confidence'] = 0.0\n\n\n# Prepare the output in the specified JSON format\nderived_metrics_results = {}\nfor index, row in df.iterrows():\n    derived_metrics_results[index] = {\n        \"sentiment_intensity\": round(row.get('sentiment_intensity', 0.0), 4),\n        \"valence\": round(row.get('valence', 0.0), 4),\n        \"overall_confidence\": round(row.get('overall_confidence', 0.0), 4)\n    }\n\nprint(derived_metrics_results)\n```\n\n**Results:**\n\n```json\n{\n  \"document_0\": {\n    \"sentiment_intensity\": 0.0,\n    \"valence\": 0.0,\n    \"overall_confidence\": 0.95\n  }\n}\n```",
  "timestamp": "2025-09-19T15:57:55.163424+00:00"
}