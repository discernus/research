{
  "status": "success",
  "functions_generated": 2,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 13301,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: nano_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-09T18:28:18.538481+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef get_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates and returns descriptive statistics for the numeric columns in the dataset.\n\n    This function serves as a foundational step in any analysis, providing a summary of the\n    distribution of each measured variable. It computes the count, mean, standard deviation,\n    minimum, 25th percentile, median (50th), 75th percentile, and maximum for all\n    numeric columns.\n\n    Methodology:\n    This is a purely descriptive analysis. Given the potential for small sample sizes in\n    initial pipeline tests, no inferential claims are made. The statistics provide a\n    quantitative overview of the analysis results.\n\n    Sample Size and Power Assessment:\n    This function is suitable for any sample size (N\u22651) as it is descriptive.\n    For N < 15, these results are considered exploratory.\n    For N = 15-29, these results can supplement cautious inferential analysis.\n    For N \u2265 30, these results form the basis of well-powered inferential analysis.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the analysis data. It must\n                             include the numeric columns for sentiment scores.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        dict: A dictionary containing the descriptive statistics, keyed by column name.\n              Returns None if the input data is invalid, empty, or lacks numeric columns.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    if data is None or data.empty:\n        return None\n\n    try:\n        # Select only numeric columns for descriptive statistics\n        numeric_cols = data.select_dtypes(include=np.number).columns.tolist()\n        if not numeric_cols:\n            return {\"message\": \"No numeric columns found for descriptive statistics.\"}\n\n        # Calculate descriptive statistics\n        desc_stats = data[numeric_cols].describe().to_dict()\n\n        # Format the output for better readability\n        results = {}\n        for col, stats in desc_stats.items():\n            results[col] = {\n                'count': int(stats.get('count', 0)),\n                'mean': stats.get('mean'),\n                'std_dev': stats.get('std'),\n                'min': stats.get('min'),\n                '25th_percentile': stats.get('25%'),\n                'median': stats.get('50%'),\n                '75th_percentile': stats.get('75%'),\n                'max': stats.get('max')\n            }\n        return results\n\n    except Exception as e:\n        # In a production environment, one might log the error `e`\n        return None\n\ndef summarize_sentiment_by_group(data, **kwargs):\n    \"\"\"\n    Compares sentiment scores between positive and negative test documents.\n\n    Methodology:\n    This function addresses the research question: \"Does the pipeline correctly identify\n    positive vs negative sentiment?\". It operates by first categorizing documents into\n    'Positive' and 'Negative' groups based on their filenames ('positive_test.txt',\n    'negative_test.txt'). It then calculates descriptive statistics (mean, std. dev., count)\n    for 'positive_sentiment_raw' and 'negative_sentiment_raw' for each group.\n\n    This is a TIER 3 (Exploratory) analysis. Given the extremely small sample size (N=2, n=1\n    per group), inferential statistics like t-tests are not applicable or valid. The\n    standard error of the mean cannot be computed for a group of size 1. Therefore, the\n    analysis is strictly descriptive. The \"mean\" for each group will simply be the score\n    of the single document in that group. The primary output is the clear difference in\n    scores between the two documents, which serves to validate the basic functionality\n    of the sentiment analysis pipeline.\n\n    Sample Size and Power Assessment:\n    Exploratory analysis - results are suggestive rather than conclusive (N=2).\n    The purpose of this test is not to draw generalizable conclusions but to verify\n    that the analysis pipeline produces directionally correct results for clear-cut cases.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the analysis data. It must\n                             include 'document_name', 'positive_sentiment_raw', and\n                             'negative_sentiment_raw' columns.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each sentiment dimension,\n              broken down by the document group ('Positive' vs. 'Negative'). Returns None\n              if data is insufficient or required columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    if data is None or data.shape[0] < 2:\n        return {\"error\": \"Insufficient data for comparison. At least two documents are required.\"}\n\n    required_cols = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n    if not all(col in data.columns for col in required_cols):\n        return {\"error\": f\"Dataframe must contain the following columns: {required_cols}\"}\n\n    try:\n        df = data.copy()\n\n        # Create grouping variable based on document name\n        # This mapping is based on the experiment design which uses specific filenames for testing.\n        def assign_group(doc_name):\n            if 'positive_test.txt' in doc_name:\n                return 'Positive'\n            elif 'negative_test.txt' in doc_name:\n                return 'Negative'\n            else:\n                return 'Uncategorized'\n\n        df['sentiment_group'] = df['document_name'].apply(assign_group)\n\n        # Filter out any documents that don't match the expected pattern\n        analysis_df = df[df['sentiment_group'].isin(['Positive', 'Negative'])]\n        if analysis_df.shape[0] < 2 or len(analysis_df['sentiment_group'].unique()) < 2:\n             return {\"error\": \"Could not find both 'Positive' and 'Negative' test documents for comparison.\"}\n\n        # Group by the new variable and calculate descriptive statistics\n        # With n=1, std() will be 0 or NaN. We will report it as 0 for clarity.\n        grouped_stats = analysis_df.groupby('sentiment_group')[['positive_sentiment_raw', 'negative_sentiment_raw']].agg(['mean', 'std', 'count']).fillna(0)\n\n        # Format the output for clear interpretation\n        results = {\n            \"analysis_tier\": \"Tier 3 (Exploratory)\",\n            \"sample_size\": int(analysis_df.shape[0]),\n            \"caveat\": \"Results are purely descriptive due to extremely small sample size (n=1 per group). Inferential tests are not applicable.\",\n            \"group_summary\": {}\n        }\n\n        for group in ['Positive', 'Negative']:\n            if group in grouped_stats.index:\n                results[\"group_summary\"][group] = {\n                    \"count\": int(grouped_stats.loc[group, ('positive_sentiment_raw', 'count')]),\n                    \"positive_sentiment_mean\": grouped_stats.loc[group, ('positive_sentiment_raw', 'mean')],\n                    \"negative_sentiment_mean\": grouped_stats.loc[group, ('negative_sentiment_raw', 'mean')]\n                }\n\n        # Calculate the difference in means to directly address the \"clear distinction\" expectation\n        if 'Positive' in results[\"group_summary\"] and 'Negative' in results[\"group_summary\"]:\n            pos_group_means = results[\"group_summary\"]['Positive']\n            neg_group_means = results[\"group_summary\"]['Negative']\n            results[\"mean_differences\"] = {\n                \"positive_sentiment_diff (Pos - Neg)\": pos_group_means['positive_sentiment_mean'] - neg_group_means['positive_sentiment_mean'],\n                \"negative_sentiment_diff (Neg - Pos)\": neg_group_means['negative_sentiment_mean'] - pos_group_means['negative_sentiment_mean']\n            }\n\n        return results\n\n    except Exception as e:\n        # In a production environment, one might log the error `e`\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}