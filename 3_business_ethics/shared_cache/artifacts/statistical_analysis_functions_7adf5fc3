{
  "status": "success",
  "functions_generated": 5,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 25719,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: business_ethics_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-04T12:17:12.458707+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates and returns descriptive statistics for all numerical columns.\n\n    This function provides a foundational overview of the dataset, including measures of\n    central tendency (mean), dispersion (std), and range (min, max) for each\n    dimensional score, salience score, and derived metric. This is a critical\n    first step in any rigorous statistical analysis.\n\n    Methodology:\n    - Utilizes pandas' .describe() method for efficient computation.\n    - Filters for numeric columns to avoid errors with non-numeric data.\n    - Transposes the output for better readability.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the analysis data with columns\n                             matching the Business Ethics Framework v10.0 specification.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        dict: A dictionary of descriptive statistics for each numeric column.\n              Returns None if the input data is empty or not a DataFrame.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if not isinstance(data, pd.DataFrame) or data.empty:\n        return None\n\n    try:\n        # Select only numeric columns for description\n        numeric_cols = data.select_dtypes(include=np.number).columns.tolist()\n        if not numeric_cols:\n            return {\"error\": \"No numeric columns found in the data.\"}\n\n        descriptives = data[numeric_cols].describe()\n\n        # Format for JSON-friendly output\n        results = descriptives.to_dict()\n        \n        # Add sample size\n        results['sample_size'] = int(data.shape[0])\n\n        return results\n    except Exception as e:\n        # Log the exception in a real application\n        # For this context, we return a dictionary with the error\n        return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n\ndef analyze_domain_coherence(data, **kwargs):\n    \"\"\"\n    Tests H1 (Ethical Domain Coherence) by analyzing correlations within ethical domains.\n\n    This function assesses the internal coherence of the framework's domains by\n    calculating Pearson correlation matrices for the dimensions within each of the\n    three major domains: Stakeholder Relations, Operational Integrity, and Strategic Vision.\n    Positive correlations among conceptually related dimensions (e.g., customer_service\n    and employee_development) would support the hypothesis of ethical domain coherence.\n\n    Methodology:\n    - The analysis is tiered based on sample size (N) as per the protocol.\n    - Tier 1 (N>=30): Full inferential analysis with p-values.\n    - Tier 2 (N=15-29): Inferential analysis with a cautionary note about statistical power.\n    - Tier 3 (N<15): Exploratory analysis focusing on the magnitude of correlations.\n      Spearman's rank correlation is used as a non-parametric alternative due to the\n      small sample size's sensitivity to outliers and non-normality.\n    - The function calculates separate correlation matrices for each domain.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame with raw scores.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing correlation matrices and statistical details for\n              each domain, or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import pearsonr, spearmanr\n\n    if not isinstance(data, pd.DataFrame) or data.empty:\n        return None\n\n    n = len(data)\n    results = {'sample_size': n, 'analysis_tier': '', 'notes': ''}\n\n    # Tiered Power Analysis\n    if n >= 30:\n        tier = 'Tier 1: Well-Powered'\n        note = 'Standard significance testing with confidence.'\n    elif 15 <= n < 30:\n        tier = 'Tier 2: Moderately-Powered'\n        note = 'Results should be interpreted with caution due to moderate sample size.'\n    else:\n        tier = 'Tier 3: Exploratory'\n        note = 'Exploratory analysis - results are suggestive rather than conclusive. Using Spearman correlation.'\n\n    results['analysis_tier'] = tier\n    results['notes'] = note\n\n    domains = {\n        \"stakeholder_relations\": [\n            \"customer_service_raw\", \"employee_development_raw\",\n            \"customer_exploitation_raw\", \"employee_exploitation_raw\"\n        ],\n        \"operational_integrity\": [\n            \"accountability_raw\", \"financial_responsibility_raw\",\n            \"opacity_raw\", \"financial_manipulation_raw\"\n        ],\n        \"strategic_vision\": [\n            \"sustainable_purpose_raw\", \"short_term_extraction_raw\"\n        ]\n    }\n\n    domain_results = {}\n\n    try:\n        for domain_name, cols in domains.items():\n            if not all(c in data.columns for c in cols):\n                domain_results[domain_name] = {\"error\": f\"Missing one or more required columns for domain: {domain_name}\"}\n                continue\n            \n            df_domain = data[cols].dropna()\n            \n            if len(df_domain) < 2:\n                domain_results[domain_name] = {\"error\": \"Not enough data to compute correlations.\"}\n                continue\n\n            # Use Spearman for Tier 3, Pearson for Tiers 1 & 2\n            if tier == 'Tier 3: Exploratory':\n                corr_matrix = df_domain.corr(method='spearman')\n                p_values = df_domain.corr(method=lambda x, y: spearmanr(x, y)[1])\n            else:\n                corr_matrix = df_domain.corr(method='pearson')\n                p_values = df_domain.corr(method=lambda x, y: pearsonr(x, y)[0 if np.isnan(pearsonr(x, y)[1]) else 1])\n\n\n            domain_results[domain_name] = {\n                'correlation_matrix': corr_matrix.to_dict(),\n                'p_values': p_values.to_dict()\n            }\n        \n        results['domain_correlations'] = domain_results\n        return results\n\n    except Exception as e:\n        return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n\ndef calculate_internal_consistency(data, **kwargs):\n    \"\"\"\n    Calculates Cronbach's alpha to measure internal consistency of ethical sub-scales.\n\n    This function further tests H1 (Ethical Domain Coherence) by assessing the reliability\n    or internal consistency of multi-item sub-scales within the framework. It calculates\n    Cronbach's alpha for the positive and negative dimensions within the Stakeholder\n    Relations and Operational Integrity domains. A higher alpha value (typically > 0.7)\n    suggests that the items are reliably measuring the same underlying latent construct.\n\n    Methodology:\n    - The analysis is tiered based on sample size (N).\n    - Tier 1 (N>=30): Alpha is considered relatively stable.\n    - Tier 2 (N=15-29): Alpha is reported with a caution about potential instability.\n    - Tier 3 (N<15): Alpha is reported but noted as highly unstable and purely exploratory.\n    - Uses the `pingouin` library for robust calculation of Cronbach's alpha.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame with raw scores.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing Cronbach's alpha values for each sub-scale,\n              or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import pingouin as pg\n\n    if not isinstance(data, pd.DataFrame) or data.empty:\n        return None\n\n    n = len(data)\n    results = {'sample_size': n, 'analysis_tier': '', 'notes': ''}\n\n    # Tiered Power Analysis\n    if n >= 30:\n        tier = 'Tier 1: Well-Powered'\n        note = 'Cronbach\\'s alpha is considered relatively stable.'\n    elif 15 <= n < 30:\n        tier = 'Tier 2: Moderately-Powered'\n        note = 'Results should be interpreted with caution; alpha estimates may be unstable.'\n    else:\n        tier = 'Tier 3: Exploratory'\n        note = 'Exploratory analysis - alpha estimates are highly unstable and should be considered suggestive.'\n\n    results['analysis_tier'] = tier\n    results['notes'] = note\n\n    subscales = {\n        \"stakeholder_positive_orientation\": [\"customer_service_raw\", \"employee_development_raw\"],\n        \"stakeholder_negative_orientation\": [\"customer_exploitation_raw\", \"employee_exploitation_raw\"],\n        \"operational_positive_orientation\": [\"accountability_raw\", \"financial_responsibility_raw\"],\n        \"operational_negative_orientation\": [\"opacity_raw\", \"financial_manipulation_raw\"]\n    }\n\n    alpha_results = {}\n\n    try:\n        for scale_name, cols in subscales.items():\n            if not all(c in data.columns for c in cols):\n                alpha_results[scale_name] = {\"error\": f\"Missing one or more required columns for scale: {scale_name}\"}\n                continue\n\n            df_scale = data[cols].dropna()\n            \n            if len(df_scale) < 2:\n                alpha_results[scale_name] = {\"error\": \"Not enough data to compute alpha.\"}\n                continue\n            \n            # Check for zero variance in columns, which makes alpha undefined\n            if (df_scale.var() == 0).any():\n                 alpha_results[scale_name] = {\n                    'cronbach_alpha': None,\n                    'confidence_interval_95': [None, None],\n                    'note': 'Cannot compute alpha, one or more items have zero variance.'\n                }\n                 continue\n\n            alpha_val = pg.cronbach_alpha(data=df_scale)\n            alpha_results[scale_name] = {\n                'cronbach_alpha': alpha_val[0],\n                'confidence_interval_95': list(alpha_val[1])\n            }\n        \n        results['internal_consistency'] = alpha_results\n        return results\n\n    except Exception as e:\n        return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n\ndef analyze_salience_intensity_alignment(data, **kwargs):\n    \"\"\"\n    Tests H3 (Salience-Intensity Alignment) by correlating raw scores with salience scores.\n\n    This function investigates the relationship between the intensity of an ethical\n    dimension (raw_score) and its strategic emphasis (salience). A positive correlation\n    would support the hypothesis that corporations strategically emphasize the ethical\n    dimensions they reason about most intensely.\n\n    Methodology:\n    - The analysis is tiered based on sample size (N).\n    - Tier 1 (N>=30): Pearson correlation with p-values.\n    - Tier 2 (N=15-29): Pearson correlation with a cautionary note about power.\n    - Tier 3 (N<15): Spearman's rank correlation, reported descriptively.\n    - Calculates the correlation for each of the 10 core dimensions separately.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame with raw and salience scores.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary of correlation coefficients and p-values for each dimension,\n              or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import pearsonr, spearmanr\n\n    if not isinstance(data, pd.DataFrame) or data.empty:\n        return None\n\n    n = len(data)\n    results = {'sample_size': n, 'analysis_tier': '', 'notes': ''}\n\n    # Tiered Power Analysis\n    if n >= 30:\n        tier = 'Tier 1: Well-Powered'\n        note = 'Standard significance testing with confidence.'\n    elif 15 <= n < 30:\n        tier = 'Tier 2: Moderately-Powered'\n        note = 'Results should be interpreted with caution due to moderate sample size.'\n    else:\n        tier = 'Tier 3: Exploratory'\n        note = 'Exploratory analysis - results are suggestive. Using Spearman correlation.'\n\n    results['analysis_tier'] = tier\n    results['notes'] = note\n\n    dimensions = [\n        \"customer_service\", \"customer_exploitation\", \"employee_development\",\n        \"employee_exploitation\", \"accountability\", \"opacity\",\n        \"financial_responsibility\", \"financial_manipulation\",\n        \"sustainable_purpose\", \"short_term_extraction\"\n    ]\n\n    alignment_results = {}\n\n    try:\n        for dim in dimensions:\n            raw_col = f\"{dim}_raw\"\n            salience_col = f\"{dim}_salience\"\n\n            if raw_col not in data.columns or salience_col not in data.columns:\n                alignment_results[dim] = {\"error\": \"Missing raw or salience column.\"}\n                continue\n\n            df_dim = data[[raw_col, salience_col]].dropna()\n\n            if len(df_dim) < 2 or df_dim[raw_col].nunique() < 2 or df_dim[salience_col].nunique() < 2:\n                alignment_results[dim] = {\"error\": \"Not enough data or variance to compute correlation.\"}\n                continue\n\n            if tier == 'Tier 3: Exploratory':\n                corr, p_val = spearmanr(df_dim[raw_col], df_dim[salience_col])\n            else:\n                corr, p_val = pearsonr(df_dim[raw_col], df_dim[salience_col])\n\n            alignment_results[dim] = {\n                'correlation': corr,\n                'p_value': p_val\n            }\n        \n        results['salience_intensity_correlations'] = alignment_results\n        return results\n\n    except Exception as e:\n        return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n\ndef compare_document_types(data, **kwargs):\n    \"\"\"\n    Tests H2 (Strategic Ethical Positioning) by comparing scores across document types.\n\n    This function operationalizes H2 by creating a 'document_type' grouping variable\n    based on document filenames and then comparing the means of all dimensional scores\n    and derived metrics across these groups. It uses ANOVA (for >2 groups) or\n    independent t-tests (for 2 groups) to test for significant differences.\n\n    Methodology:\n    - A helper function maps document names (e.g., 'csr_report.txt') to categories\n      ('CSR Report', 'Shareholder Report', 'Crisis Comms', etc.).\n    - The analysis is tiered based on the smallest group size.\n    - Tier 1 (ANOVA N>=10/group, t-test N>=15/group): Full inferential tests,\n      including Levene's test for homogeneity of variances and Tukey's HSD post-hoc\n      for ANOVA.\n    - Tier 2 (ANOVA N=5-9/group, t-test N=8-14/group): Inferential tests with power\n      caveats. Effect sizes (eta-squared, Cohen's d) are emphasized.\n    - Tier 3 (ANOVA N<5/group, t-test N<8/group): Exploratory analysis. Non-parametric\n      alternatives (Kruskal-Wallis, Mann-Whitney U) are used, and results are\n      presented as suggestive patterns.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame with scores and a 'document_name' column.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary of statistical test results for each dependent variable,\n              or None if data is insufficient for grouping.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import f_oneway, ttest_ind, levene, kruskal, mannwhitneyu\n    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n    \n    def _get_document_type(filename):\n        \"\"\"Maps filename to a document type category.\"\"\"\n        if not isinstance(filename, str):\n            return 'Other'\n        fn_lower = filename.lower()\n        if 'csr' in fn_lower or 'social_responsibility' in fn_lower:\n            return 'CSR Report'\n        if 'shareholder' in fn_lower or 'primacy' in fn_lower:\n            return 'Shareholder Report'\n        if 'crisis' in fn_lower or 'apology' in fn_lower:\n            return 'Crisis Comms'\n        if 'annual_report' in fn_lower:\n            return 'Annual Report'\n        if 'product_launch' in fn_lower:\n            return 'Product Launch'\n        return 'Other'\n\n    if not isinstance(data, pd.DataFrame) or data.empty or 'document_name' not in data.columns:\n        return None\n\n    try:\n        df = data.copy()\n        df['document_type'] = df['document_name'].apply(_get_document_type)\n        \n        group_counts = df['document_type'].value_counts()\n        if len(group_counts) < 2:\n            return {\"error\": \"Cannot perform comparison, only one document type found.\", \"group_counts\": group_counts.to_dict()}\n\n        min_group_size = group_counts.min()\n        num_groups = len(group_counts)\n        \n        # Tiered Power Analysis\n        if num_groups > 2: # ANOVA path\n            test_type = 'ANOVA'\n            if min_group_size >= 10:\n                tier = 'Tier 1: Well-Powered'\n                note = 'ANOVA with Tukey HSD post-hoc tests.'\n            elif 5 <= min_group_size < 10:\n                tier = 'Tier 2: Moderately-Powered'\n                note = 'ANOVA results should be interpreted with caution. Focus on effect sizes.'\n            else:\n                tier = 'Tier 3: Exploratory'\n                note = 'Using non-parametric Kruskal-Wallis test. Results are suggestive.'\n        else: # t-test path\n            test_type = 'T-test'\n            if min_group_size >= 15:\n                tier = 'Tier 1: Well-Powered'\n                note = 'Independent samples t-test.'\n            elif 8 <= min_group_size < 15:\n                tier = 'Tier 2: Moderately-Powered'\n                note = 'T-test results should be interpreted with caution. Focus on effect sizes.'\n            else:\n                tier = 'Tier 3: Exploratory'\n                note = 'Using non-parametric Mann-Whitney U test. Results are suggestive.'\n\n        results = {\n            'sample_size': len(df),\n            'group_counts': group_counts.to_dict(),\n            'analysis_tier': tier,\n            'test_type': test_type,\n            'notes': note,\n            'comparisons': {}\n        }\n\n        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n        \n        for col in numeric_cols:\n            groups = [df[col][df['document_type'] == g].dropna() for g in group_counts.index]\n            \n            # Skip if any group is empty after dropping NaNs\n            if any(g.empty for g in groups):\n                results['comparisons'][col] = {'error': 'One or more groups have no data for this variable.'}\n                continue\n\n            if tier == 'Tier 3: Exploratory':\n                if test_type == 'ANOVA':\n                    stat, p_val = kruskal(*groups)\n                    results['comparisons'][col] = {'test': 'Kruskal-Wallis', 'statistic': stat, 'p_value': p_val}\n                else:\n                    stat, p_val = mannwhitneyu(groups[0], groups[1], alternative='two-sided')\n                    results['comparisons'][col] = {'test': 'Mann-Whitney U', 'statistic': stat, 'p_value': p_val}\n            else: # Tiers 1 & 2\n                # Levene's test for homogeneity of variances\n                levene_stat, levene_p = levene(*groups)\n                equal_var = levene_p > 0.05\n\n                if test_type == 'ANOVA':\n                    f_stat, p_val = f_oneway(*groups)\n                    # Eta-squared effect size\n                    ss_between = sum(len(g) * (g.mean() - df[col].mean())**2 for g in groups)\n                    ss_total = sum((x - df[col].mean())**2 for x in df[col].dropna())\n                    eta_squared = ss_between / ss_total if ss_total > 0 else 0\n                    \n                    test_results = {\n                        'test': 'One-way ANOVA', 'f_statistic': f_stat, 'p_value': p_val,\n                        'eta_squared': eta_squared, 'levene_test_p_value': levene_p\n                    }\n                    if tier == 'Tier 1: Well-Powered' and p_val < 0.05:\n                        tukey_res = pairwise_tukeyhsd(endog=df[col].dropna(), groups=df['document_type'].dropna(), alpha=0.05)\n                        test_results['tukey_hsd'] = str(tukey_res)\n                    results['comparisons'][col] = test_results\n                else: # t-test\n                    t_stat, p_val = ttest_ind(groups[0], groups[1], equal_var=equal_var, nan_policy='omit')\n                    # Cohen's d effect size\n                    n1, n2 = len(groups[0]), len(groups[1])\n                    s1, s2 = groups[0].std(), groups[1].std()\n                    pooled_std = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2)) if (n1 + n2 - 2) > 0 else 0\n                    cohen_d = (groups[0].mean() - groups[1].mean()) / pooled_std if pooled_std > 0 else 0\n\n                    results['comparisons'][col] = {\n                        'test': 'Independent T-test', 't_statistic': t_stat, 'p_value': p_val,\n                        'cohen_d': cohen_d, 'equal_variances_assumed': equal_var\n                    }\n        return results\n\n    except Exception as e:\n        return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}