{
  "status": "success",
  "functions_generated": 9,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 25124,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: emotional_climate_factorial_analysis\nDescription: Statistical analysis experiment\nGenerated: 2025-08-29T03:10:32.095785+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef _create_metadata_mapping(data):\n    \"\"\"\n    Internal helper function to add 'ideology' and 'era' columns to the DataFrame.\n\n    This function maps document names to their corresponding ideology and political era\n    based on the 2x3 factorial design specified in the experiment. It standardizes\n    document names by removing file extensions before mapping.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with a 'document_name' column.\n\n    Returns:\n        pd.DataFrame: The DataFrame with added 'ideology' and 'era' columns.\n                      Returns the original DataFrame if 'document_name' is missing.\n    \"\"\"\n    import pandas as pd\n    from pathlib import Path\n\n    if 'document_name' not in data.columns:\n        return data\n\n    # Create a standardized key from the document name\n    data['doc_key'] = data['document_name'].apply(lambda x: Path(x).stem)\n\n    ideology_map = {\n        # Progressive\n        \"john_lewis_march_on_washington\": \"Progressive\",\n        \"cory_booker_2018_first_step_act\": \"Progressive\",\n        \"bernie_sanders_2025_oligarchy\": \"Progressive\",\n        \"aoc_2025_oligarchy\": \"Progressive\",\n        # Conservative\n        \"john_mccain_2008_concession\": \"Conservative\",\n        \"mitt_romney_2020_impeachment\": \"Conservative\",\n        \"jd_vance_2022_natcon_conference\": \"Conservative\",\n        \"steve_king_2017_house_floor\": \"Conservative\",\n    }\n\n    era_map = {\n        # Civil Rights Era\n        \"john_lewis_march_on_washington\": \"Civil Rights Era\",\n        # Institutional Era\n        \"john_mccain_2008_concession\": \"Institutional Era\",\n        \"mitt_romney_2020_impeachment\": \"Institutional Era\",\n        \"cory_booker_2018_first_step_act\": \"Institutional Era\",\n        # Populist Era\n        \"bernie_sanders_2025_oligarchy\": \"Populist Era\",\n        \"aoc_2025_oligarchy\": \"Populist Era\",\n        \"jd_vance_2022_natcon_conference\": \"Populist Era\",\n        \"steve_king_2017_house_floor\": \"Populist Era\",\n    }\n\n    data['ideology'] = data['doc_key'].map(ideology_map)\n    data['era'] = data['doc_key'].map(era_map)\n    data = data.drop(columns=['doc_key'])\n\n    return data\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates derived emotional climate metrics based on the ECF v10.0 specification.\n\n    This function computes axis-level balances and summary indices using the formulas\n    provided in the framework. It handles the discrepancy between the 'compersion'\n    dimension in the framework specification and the 'compassion' dimension in the\n    actual data by substituting 'compassion' scores where 'compersion' is required.\n    A small epsilon (0.001) is added to salience totals to prevent division by zero.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing raw and salience scores for each\n                             of the six emotional dimensions.\n        **kwargs: Not used.\n\n    Returns:\n        pd.DataFrame: The input DataFrame with added columns for each derived metric.\n                      Returns None if required columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        df = data.copy()\n        \n        # NOTE: The framework specifies 'compersion', but the data provides 'compassion'.\n        # We will proceed by substituting 'compassion' for 'compersion' in all formulas.\n        required_cols = [\n            'fear_raw', 'fear_salience', 'hope_raw', 'hope_salience',\n            'enmity_raw', 'enmity_salience', 'amity_raw', 'amity_salience',\n            'envy_raw', 'envy_salience', 'compassion_raw', 'compassion_salience'\n        ]\n        if not all(col in df.columns for col in required_cols):\n            # Missing one or more required columns for calculation\n            return None\n\n        # Intermediate salience totals (add epsilon to avoid division by zero)\n        epsilon = 0.001\n        df['threat_opportunity_salience_total'] = df['fear_salience'] + df['hope_salience'] + epsilon\n        df['social_relations_salience_total'] = df['enmity_salience'] + df['amity_salience'] + epsilon\n        df['resource_attitudes_salience_total'] = df['envy_salience'] + df['compassion_salience'] + epsilon\n        df['total_emotional_salience'] = (df['threat_opportunity_salience_total'] +\n                                          df['social_relations_salience_total'] +\n                                          df['resource_attitudes_salience_total'])\n\n        # Axis-level climate indices\n        df['threat_opportunity_balance'] = ((df['hope_raw'] * df['hope_salience']) - (df['fear_raw'] * df['fear_salience'])) / df['threat_opportunity_salience_total']\n        df['social_relations_balance'] = ((df['amity_raw'] * df['amity_salience']) - (df['enmity_raw'] * df['enmity_salience'])) / df['social_relations_salience_total']\n        df['resource_attitudes_balance'] = ((df['compassion_raw'] * df['compassion_salience']) - (df['envy_raw'] * df['envy_salience'])) / df['resource_attitudes_salience_total']\n\n        # Summary metrics\n        numerator = ((df['threat_opportunity_balance'] * df['threat_opportunity_salience_total']) +\n                     (df['social_relations_balance'] * df['social_relations_salience_total']) +\n                     (df['resource_attitudes_balance'] * df['resource_attitudes_salience_total']))\n        df['emotional_climate_index'] = numerator / df['total_emotional_salience']\n\n        df['climate_intensity'] = (df['fear_raw'] + df['hope_raw'] + df['enmity_raw'] + df['amity_raw'] + df['envy_raw'] + df['compassion_raw']) / 6\n        df['positive_emotional_index'] = (df['hope_raw'] + df['amity_raw'] + df['compassion_raw']) / 3\n        df['negative_emotional_index'] = (df['fear_raw'] + df['enmity_raw'] + df['envy_raw']) / 3\n\n        return df\n\n    except (KeyError, TypeError, Exception):\n        return None\n\ndef run_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for key emotional climate metrics.\n\n    This function first calculates derived metrics and adds metadata groupings.\n    It then computes descriptive statistics (mean, std, count) for all raw scores\n    and primary derived indices, grouped by ideology, era, and their interaction.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing DataFrames of descriptive statistics\n              for each grouping, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        df = _create_metadata_mapping(data.copy())\n        df = calculate_derived_metrics(df)\n\n        if df is None or 'ideology' not in df.columns or 'era' not in df.columns:\n            return None\n\n        metrics_to_describe = [\n            'fear_raw', 'hope_raw', 'enmity_raw', 'amity_raw', 'envy_raw', 'compassion_raw',\n            'threat_opportunity_balance', 'social_relations_balance', 'resource_attitudes_balance',\n            'emotional_climate_index', 'climate_intensity', 'positive_emotional_index', 'negative_emotional_index'\n        ]\n        \n        # Ensure all metrics exist in the dataframe\n        metrics_to_describe = [m for m in metrics_to_describe if m in df.columns]\n        if not metrics_to_describe:\n            return None\n\n        results = {\n            'by_ideology': df.groupby('ideology')[metrics_to_describe].agg(['mean', 'std', 'count']).to_dict(),\n            'by_era': df.groupby('era')[metrics_to_describe].agg(['mean', 'std', 'count']).to_dict(),\n            'by_ideology_era': df.groupby(['ideology', 'era'])[metrics_to_describe].agg(['mean', 'std', 'count']).to_dict()\n        }\n        return results\n\n    except Exception:\n        return None\n\ndef run_two_way_anova(data, **kwargs):\n    \"\"\"\n    Performs a two-way ANOVA on emotional climate metrics.\n\n    This function tests for main effects of 'ideology' and 'era', as well as their\n    interaction effect, on specified dependent variables. It uses the statsmodels\n    library to perform the analysis. The function first prepares the data by\n    calculating derived metrics and adding metadata.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs:\n            dependent_variable (str): The name of the column to use as the\n                                      dependent variable for the ANOVA.\n                                      Defaults to 'emotional_climate_index'.\n\n    Returns:\n        dict: A dictionary where keys are dependent variables and values are\n              the ANOVA table results as a dictionary. Returns None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import statsmodels.api as sm\n    from statsmodels.formula.api import ols\n\n    try:\n        dependent_variable = kwargs.get('dependent_variable', 'emotional_climate_index')\n\n        df = _create_metadata_mapping(data.copy())\n        df = calculate_derived_metrics(df)\n\n        if df is None or dependent_variable not in df.columns or 'ideology' not in df.columns or 'era' not in df.columns:\n            return None\n        \n        # Drop rows with missing values in the relevant columns\n        df_clean = df.dropna(subset=[dependent_variable, 'ideology', 'era'])\n        if len(df_clean) < 3: # Not enough data for ANOVA\n            return None\n\n        # Check for sufficient data in each group\n        group_counts = df_clean.groupby(['ideology', 'era']).size()\n        if (group_counts < 2).any():\n            # ANOVA requires at least 2 observations per group for variance calculation\n            return {'error': 'Insufficient data; at least one group has fewer than 2 observations.'}\n\n        # Formula for the two-way ANOVA\n        formula = f'`{dependent_variable}` ~ C(ideology) * C(era)'\n        model = ols(formula, data=df_clean).fit()\n        anova_table = sm.stats.anova_lm(model, typ=2)\n\n        results = {\n            'dependent_variable': dependent_variable,\n            'anova_table': anova_table.to_dict()\n        }\n        return results\n\n    except Exception:\n        return None\n\ndef run_correlation_analysis(data, **kwargs):\n    \"\"\"\n    Calculates the Pearson correlation matrix for emotional climate dimensions.\n\n    This function analyzes the relationships between the six core emotional dimensions\n    (raw scores) and the primary derived indices. It returns a correlation matrix\n    which helps answer research questions about cross-axis relationships.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing the correlation matrix, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        df = calculate_derived_metrics(data.copy())\n        if df is None:\n            return None\n\n        correlation_vars = [\n            'fear_raw', 'hope_raw', 'enmity_raw', 'amity_raw', 'envy_raw', 'compassion_raw',\n            'emotional_climate_index', 'climate_intensity',\n            'positive_emotional_index', 'negative_emotional_index'\n        ]\n        \n        # Ensure all variables exist in the dataframe\n        correlation_vars = [v for v in correlation_vars if v in df.columns]\n        if len(correlation_vars) < 2:\n            return None\n\n        correlation_matrix = df[correlation_vars].corr(method='pearson')\n\n        # Replace NaN with None for JSON compatibility\n        correlation_matrix = correlation_matrix.where(pd.notnull(correlation_matrix), None)\n\n        return {'correlation_matrix': correlation_matrix.to_dict()}\n\n    except Exception:\n        return None\n\ndef run_climate_polarization_analysis(data, **kwargs):\n    \"\"\"\n    Analyzes emotional climate polarization between ideologies.\n\n    This function performs an independent samples t-test to compare the means of a\n    specified emotional metric between 'Progressive' and 'Conservative' ideologies.\n    This helps quantify the emotional distance between the two groups.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs:\n            metric (str): The metric to test for polarization.\n                          Defaults to 'emotional_climate_index'.\n\n    Returns:\n        dict: A dictionary with the t-statistic and p-value, or None on error.\n    \"\"\"\n    import pandas as pd\n    from scipy.stats import ttest_ind\n\n    try:\n        metric = kwargs.get('metric', 'emotional_climate_index')\n\n        df = _create_metadata_mapping(data.copy())\n        df = calculate_derived_metrics(df)\n\n        if df is None or metric not in df.columns or 'ideology' not in df.columns:\n            return None\n\n        df_clean = df.dropna(subset=[metric, 'ideology'])\n        \n        group1 = df_clean[df_clean['ideology'] == 'Progressive'][metric]\n        group2 = df_clean[df_clean['ideology'] == 'Conservative'][metric]\n\n        if len(group1) < 2 or len(group2) < 2:\n            return {'error': 'Insufficient data for one or both ideology groups.'}\n\n        # Perform t-test\n        t_stat, p_value = ttest_ind(group1, group2, equal_var=False) # Welch's t-test\n\n        results = {\n            'metric_tested': metric,\n            't_statistic': t_stat,\n            'p_value': p_value,\n            'group1_mean': group1.mean(),\n            'group2_mean': group2.mean(),\n            'group1_name': 'Progressive',\n            'group2_name': 'Conservative'\n        }\n        return results\n\n    except Exception:\n        return None\n\ndef run_emotional_climate_clustering(data, **kwargs):\n    \"\"\"\n    Performs K-Means clustering to identify emotional climate profiles.\n\n    This function groups documents into a specified number of clusters based on their\n    raw emotional scores. It helps identify common emotional signatures in the corpus.\n    The results include the cluster assignment for each document and the centroid\n    coordinates for each cluster.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs:\n            n_clusters (int): The number of clusters to form. Defaults to 3.\n\n    Returns:\n        dict: A dictionary containing cluster assignments and centroids, or None on error.\n    \"\"\"\n    import pandas as pd\n    from sklearn.cluster import KMeans\n    from sklearn.preprocessing import StandardScaler\n\n    try:\n        n_clusters = kwargs.get('n_clusters', 3)\n\n        df = data.copy()\n        \n        # Note: Using 'compassion_raw' as per data structure\n        cluster_features = ['fear_raw', 'hope_raw', 'enmity_raw', 'amity_raw', 'envy_raw', 'compassion_raw']\n        \n        if not all(col in df.columns for col in cluster_features):\n            return None\n\n        df_clean = df.dropna(subset=cluster_features)\n        if len(df_clean) < n_clusters:\n            return {'error': 'Not enough data points to form the requested number of clusters.'}\n\n        X = df_clean[cluster_features]\n        \n        # Scale data for better clustering performance\n        scaler = StandardScaler()\n        X_scaled = scaler.fit_transform(X)\n\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n        kmeans.fit(X_scaled)\n\n        # Add cluster labels back to the dataframe\n        df_clean['cluster'] = kmeans.labels_\n\n        # Get centroids and un-scale them back to the original data space\n        centroids_scaled = kmeans.cluster_centers_\n        centroids = scaler.inverse_transform(centroids_scaled)\n        \n        centroid_df = pd.DataFrame(centroids, columns=cluster_features)\n\n        results = {\n            'cluster_assignments': df_clean[['document_name', 'cluster']].to_dict('records'),\n            'cluster_centroids': centroid_df.to_dict('index')\n        }\n        return results\n\n    except Exception:\n        return None\n\ndef run_multi_evaluation_reliability(data, **kwargs):\n    \"\"\"\n    Calculates multi-evaluation reliability using Intraclass Correlation (ICC).\n\n    This function assesses the consistency of scores for documents that have been\n    evaluated multiple times. It requires a DataFrame where each row is a single\n    evaluation, identified by 'document_name'.\n\n    Methodology:\n    - Uses the pingouin library to calculate ICC.\n    - ICC(2,1) is used: a two-way random effects model, assessing consistency of single raters.\n    - The function calculates ICC for each of the six raw emotional scores.\n\n    Args:\n        data (pd.DataFrame): DataFrame with multiple rows per 'document_name',\n                             representing different evaluations.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary of ICC results for each emotional dimension, or None on error.\n    \"\"\"\n    import pandas as pd\n    import pingouin as pg\n\n    try:\n        df = data.copy()\n        \n        required_cols = ['document_name', 'fear_raw', 'hope_raw', 'enmity_raw', 'amity_raw', 'envy_raw', 'compassion_raw']\n        if not all(col in df.columns for col in required_cols):\n            return None\n\n        # The data needs a 'rater' or evaluation ID. If not present, we create one.\n        if 'rater_id' not in df.columns:\n            df['rater_id'] = df.groupby('document_name').cumcount() + 1\n\n        # Check if there are multiple ratings per document\n        if df.groupby('document_name').size().max() < 2:\n            return {'error': 'Insufficient data: No document has more than one evaluation.'}\n\n        icc_results = {}\n        dimensions = ['fear_raw', 'hope_raw', 'enmity_raw', 'amity_raw', 'envy_raw', 'compassion_raw']\n\n        for dim in dimensions:\n            # Pivot the data into the required format for pingouin: targets x raters\n            icc_df = df.pivot(index='document_name', columns='rater_id', values=dim).dropna()\n            \n            if icc_df.shape[0] < 2 or icc_df.shape[1] < 2:\n                icc_results[dim] = {'error': f'Insufficient data for {dim} to calculate ICC.'}\n                continue\n\n            icc = pg.intraclass_corr(data=icc_df, targets='document_name', raters='rater_id', ratings=dim)\n            icc_val = icc.set_index('Type').loc['ICC2']['ICC']\n            icc_results[dim] = {'ICC(2,1)': icc_val}\n\n        return icc_results\n\n    except Exception:\n        return None\n\ndef run_emotional_axis_trajectory(data, **kwargs):\n    \"\"\"\n    Analyzes the trajectory of emotional axes across different eras.\n\n    This function calculates the mean scores for the three primary emotional balance\n    axes (Threat-Opportunity, Social Relations, Resource Attitudes) for each\n    political era defined in the experiment. This provides insight into how the\n    emotional climate has evolved over time.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing the mean axis balances for each era,\n              or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n\n    try:\n        df = _create_metadata_mapping(data.copy())\n        df = calculate_derived_metrics(df)\n\n        if df is None or 'era' not in df.columns:\n            return None\n        \n        # Define the order of eras for chronological trajectory\n        era_order = ['Civil Rights Era', 'Institutional Era', 'Populist Era']\n        df['era'] = pd.Categorical(df['era'], categories=era_order, ordered=True)\n        \n        df_clean = df.dropna(subset=['era'])\n\n        axis_balances = [\n            'threat_opportunity_balance',\n            'social_relations_balance',\n            'resource_attitudes_balance'\n        ]\n        \n        if not all(col in df.columns for col in axis_balances):\n            return None\n\n        trajectory_data = df_clean.groupby('era')[axis_balances].mean()\n        \n        # Sort by the categorical order\n        trajectory_data = trajectory_data.sort_index()\n\n        return {'axis_trajectory_by_era': trajectory_data.to_dict('index')}\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}