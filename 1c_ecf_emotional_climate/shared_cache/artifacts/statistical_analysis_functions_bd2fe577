{
  "status": "success",
  "functions_generated": 7,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 23996,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: emotional_climate_factorial_analysis\nDescription: Statistical analysis experiment\nGenerated: 2025-08-27T17:43:54.516779+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef _load_and_merge_metadata(data, **kwargs):\n    \"\"\"\n    Internal helper to load and merge corpus metadata.\n\n    This function attempts to find and load a 'corpus_manifest.json' file from the\n    current directory. It assumes the manifest is a dictionary where keys are\n    filenames and values are dictionaries of metadata. It merges this metadata\n    into the provided DataFrame based on the 'document_name' column.\n\n    This function is designed to fail gracefully if the manifest is not found or\n    if merging fails, returning the original DataFrame. This handles the case\n    where no corpus manifest is available, as specified in the project brief.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with a 'document_name' column.\n        **kwargs: Not used directly, but present for signature consistency.\n\n    Returns:\n        pd.DataFrame: The DataFrame, merged with metadata if successful, otherwise\n                      the original DataFrame.\n    \"\"\"\n    import pandas as pd\n    import json\n    from pathlib import Path\n\n    manifest_path = Path('corpus_manifest.json')\n    if not manifest_path.exists():\n        # Per instructions, handle missing manifest gracefully.\n        return data\n\n    try:\n        with open(manifest_path, 'r') as f:\n            manifest = json.load(f)\n\n        # Assuming manifest is a dict like: {\"filename.txt\": {\"speaker\": \"X\", \"party\": \"Y\"}}\n        # Or a list of dicts: [{\"document_name\": \"filename.txt\", \"speaker\": \"X\"}]\n        if isinstance(manifest, dict):\n            meta_df = pd.DataFrame.from_dict(manifest, orient='index')\n            meta_df.index.name = 'document_name'\n            meta_df.reset_index(inplace=True)\n        elif isinstance(manifest, list):\n            meta_df = pd.DataFrame(manifest)\n        else:\n            # Unsupported manifest format\n            return data\n\n        # Ensure document_name in meta_df matches the format in the main data\n        # The sample data shows filenames like 'mitt_romney_2020_impeachment.txt'\n        if 'document_name' not in meta_df.columns:\n            return data\n\n        # Merge metadata into the main dataframe\n        # Use a left merge to keep all original data rows\n        merged_data = pd.merge(data, meta_df, on='document_name', how='left')\n        return merged_data\n\n    except (json.JSONDecodeError, KeyError, FileNotFoundError):\n        # Gracefully return original data if manifest is malformed or missing\n        return data\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived metrics as specified in the ECF v10.0 framework.\n\n    This function takes a DataFrame containing the raw and salience scores for the six\n    primary emotional dimensions and computes the eleven derived metrics. These include\n    intermediate salience totals, axis-level balance scores, and final summary indices\n    like the Emotional Climate Index and Climate Intensity. The formulas are implemented\n    exactly as described in the framework's 'derived_metrics' section. A small epsilon\n    (0.001) is added to denominators to prevent division by zero, mirroring the spec.\n\n    Args:\n        data (pd.DataFrame): A DataFrame containing the analysis data. Must include\n                             columns for '{dim}_raw' and '{dim}_salience' for each of\n                             the six dimensions (fear, hope, enmity, amity, envy, compersion).\n        **kwargs: Not used.\n\n    Returns:\n        pd.DataFrame: The original DataFrame with added columns for each derived metric,\n                      or None if essential input columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        df = data.copy()\n        required_cols = [\n            'fear_raw', 'fear_salience', 'hope_raw', 'hope_salience',\n            'enmity_raw', 'enmity_salience', 'amity_raw', 'amity_salience',\n            'envy_raw', 'envy_salience', 'compersion_raw', 'compersion_salience'\n        ]\n        if not all(col in df.columns for col in required_cols):\n            # Missing one or more required columns for calculation\n            return None\n\n        # Intermediate calculations for salience weighting (with epsilon for stability)\n        epsilon = 0.001\n        df['threat_opportunity_salience_total'] = df['fear_salience'] + df['hope_salience'] + epsilon\n        df['social_relations_salience_total'] = df['enmity_salience'] + df['amity_salience'] + epsilon\n        df['resource_attitudes_salience_total'] = df['envy_salience'] + df['compersion_salience'] + epsilon\n        df['total_emotional_salience'] = (df['threat_opportunity_salience_total'] +\n                                          df['social_relations_salience_total'] +\n                                          df['resource_attitudes_salience_total'])\n\n        # Axis-level climate indices\n        df['threat_opportunity_balance'] = ((df['hope_raw'] * df['hope_salience']) - (df['fear_raw'] * df['fear_salience'])) / df['threat_opportunity_salience_total']\n        df['social_relations_balance'] = ((df['amity_raw'] * df['amity_salience']) - (df['enmity_raw'] * df['enmity_salience'])) / df['social_relations_salience_total']\n        df['resource_attitudes_balance'] = ((df['compersion_raw'] * df['compersion_salience']) - (df['envy_raw'] * df['envy_salience'])) / df['resource_attitudes_salience_total']\n\n        # Summary metrics\n        numerator_eci = ((df['threat_opportunity_balance'] * df['threat_opportunity_salience_total']) +\n                         (df['social_relations_balance'] * df['social_relations_salience_total']) +\n                         (df['resource_attitudes_balance'] * df['resource_attitudes_salience_total']))\n        df['emotional_climate_index'] = numerator_eci / df['total_emotional_salience']\n\n        df['climate_intensity'] = (df['fear_raw'] + df['hope_raw'] + df['enmity_raw'] + df['amity_raw'] + df['envy_raw'] + df['compersion_raw']) / 6\n        df['positive_emotional_index'] = (df['hope_raw'] + df['amity_raw'] + df['compersion_raw']) / 3\n        df['negative_emotional_index'] = (df['fear_raw'] + df['enmity_raw'] + df['envy_raw']) / 3\n\n        return df\n\n    except Exception:\n        return None\n\ndef get_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Computes descriptive statistics for all raw scores, salience, and derived metrics.\n\n    This function first calculates the derived metrics using the framework's formulas.\n    It then generates summary statistics (mean, std, min, max, quartiles) for all 18\n    primary dimension scores (6 raw, 6 salience, 6 confidence) and the 11 derived metrics.\n    This provides a comprehensive overview of the dataset's emotional climate characteristics.\n\n    Args:\n        data (pd.DataFrame): A DataFrame containing the analysis data with columns for\n                             raw scores, salience, and confidence for each dimension.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing the descriptive statistics table (as a JSON string),\n              or None if the input data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n\n    try:\n        # First, ensure derived metrics are calculated and present\n        data_with_metrics = calculate_derived_metrics(data)\n        if data_with_metrics is None:\n            return None\n\n        # Select all relevant columns for description\n        all_cols = [\n            'fear_raw', 'fear_salience', 'fear_confidence',\n            'hope_raw', 'hope_salience', 'hope_confidence',\n            'enmity_raw', 'enmity_salience', 'enmity_confidence',\n            'amity_raw', 'amity_salience', 'amity_confidence',\n            'envy_raw', 'envy_salience', 'envy_confidence',\n            'compersion_raw', 'compersion_salience', 'compersion_confidence',\n            'threat_opportunity_balance', 'social_relations_balance',\n            'resource_attitudes_balance', 'emotional_climate_index',\n            'climate_intensity', 'positive_emotional_index', 'negative_emotional_index'\n        ]\n        \n        # Filter to only include columns that exist in the dataframe\n        existing_cols = [col for col in all_cols if col in data_with_metrics.columns]\n        \n        if not existing_cols:\n            return None\n\n        stats = data_with_metrics[existing_cols].describe().round(4)\n        \n        # Convert to a JSON-serializable format\n        return json.loads(stats.to_json(orient='index'))\n\n    except Exception:\n        return None\n\ndef analyze_dimension_correlations(data, **kwargs):\n    \"\"\"\n    Computes the Pearson correlation matrix for raw scores and salience scores.\n\n    This analysis reveals the relationships between the emotional dimensions. For example,\n    it can show if 'fear' and 'enmity' are commonly used together, or if 'hope' and 'amity'\n    are positively correlated. The correlation is calculated separately for the raw intensity\n    scores and the salience (rhetorical prominence) scores to provide a nuanced view of\n    both emotional content and strategic emphasis patterns.\n\n    Args:\n        data (pd.DataFrame): A DataFrame containing the analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing two keys, 'raw_score_correlations' and\n              'salience_correlations', each holding a correlation matrix as a\n              JSON-serializable dictionary. Returns None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n\n    try:\n        raw_cols = [col for col in data.columns if col.endswith('_raw')]\n        salience_cols = [col for col in data.columns if col.endswith('_salience')]\n\n        if len(raw_cols) < 2 or len(salience_cols) < 2:\n            return None\n\n        # Calculate correlation for raw scores\n        raw_corr = data[raw_cols].corr(method='pearson').round(4)\n        raw_corr_dict = json.loads(raw_corr.to_json(orient='index'))\n\n        # Calculate correlation for salience scores\n        salience_corr = data[salience_cols].corr(method='pearson').round(4)\n        salience_corr_dict = json.loads(salience_corr.to_json(orient='index'))\n\n        return {\n            'raw_score_correlations': raw_corr_dict,\n            'salience_correlations': salience_corr_dict\n        }\n\n    except Exception:\n        return None\n\ndef perform_two_way_anova(data, **kwargs):\n    \"\"\"\n    Performs a two-way ANOVA on a specified dependent variable.\n\n    This function addresses the research question about interaction effects between\n    ideology and political era. It requires 'ideology' and 'era' to be specified\n    in the kwargs and present as columns in the data. It first attempts to load\n    metadata from a corpus manifest. The analysis tests the main effects of each\n    factor and their interaction effect on a dependent variable (e.g.,\n    'emotional_climate_index').\n\n    Methodology: Uses Ordinary Least Squares (OLS) from the statsmodels library\n    to fit the model `dependent_var ~ C(factor1) * C(factor2)`, where C() treats\n    the factors as categorical. The results include F-statistics and p-values for\n    each factor and the interaction term.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame.\n        **kwargs:\n            dependent_var (str): The column name of the dependent variable for the analysis\n                                 (e.g., 'emotional_climate_index').\n            factor1 (str): The column name of the first independent variable (e.g., 'ideology').\n            factor2 (str): The column name of the second independent variable (e.g., 'era').\n\n    Returns:\n        dict: A dictionary containing the ANOVA table results, or None if requirements are not met.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import statsmodels.api as sm\n    from statsmodels.formula.api import ols\n    import json\n\n    try:\n        dependent_var = kwargs.get('dependent_var')\n        factor1 = kwargs.get('factor1')\n        factor2 = kwargs.get('factor2')\n\n        if not all([dependent_var, factor1, factor2]):\n            return {\"error\": \"dependent_var, factor1, and factor2 must be provided in kwargs.\"}\n\n        # Attempt to load metadata and calculate derived metrics\n        df = _load_and_merge_metadata(data)\n        df = calculate_derived_metrics(df)\n\n        required_cols = [dependent_var, factor1, factor2]\n        if not all(col in df.columns for col in required_cols):\n            return {\"error\": f\"One or more required columns missing from data: {required_cols}\"}\n\n        # Drop rows with missing values in the relevant columns\n        df_clean = df.dropna(subset=required_cols)\n\n        # Check for sufficient data and variance\n        if len(df_clean) < 10 or df_clean[factor1].nunique() < 2 or df_clean[factor2].nunique() < 2:\n            return {\"error\": \"Insufficient data or variance in factors for ANOVA.\"}\n\n        # Construct the model formula\n        model_formula = f\"`{dependent_var}` ~ C(`{factor1}`) * C(`{factor2}`)\"\n        \n        # Fit the model\n        model = ols(model_formula, data=df_clean).fit()\n        \n        # Get the ANOVA table\n        anova_table = sm.stats.anova_lm(model, typ=2)\n        \n        # Format for JSON output\n        anova_table.index.name = 'source'\n        anova_table.reset_index(inplace=True)\n        anova_table.rename(columns={'PR(>F)': 'p_value'}, inplace=True)\n        \n        return json.loads(anova_table.to_json(orient='records'))\n\n    except Exception:\n        return None\n\ndef identify_emotional_climate_clusters(data, **kwargs):\n    \"\"\"\n    Identifies emotional climate clusters using K-Means clustering.\n\n    This function groups documents into a specified number of clusters based on their\n    emotional profiles. This helps identify recurring \"emotional signatures\" in the\n    corpus (e.g., a 'high-fear, high-enmity' signature vs. a 'high-hope, high-amity'\n    signature). The clustering is performed on the six raw emotional scores.\n\n    Methodology: Uses the K-Means algorithm from scikit-learn. The function returns\n    the coordinates of the cluster centers (representing the average emotional profile\n    for each cluster) and adds a 'cluster_label' column to the input data.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame.\n        **kwargs:\n            n_clusters (int): The number of clusters to create. Defaults to 3.\n\n    Returns:\n        dict: A dictionary with 'cluster_centers' and 'data_with_clusters' (as a\n              JSON string), or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.cluster import KMeans\n    import json\n\n    try:\n        n_clusters = kwargs.get('n_clusters', 3)\n        if not isinstance(n_clusters, int) or n_clusters < 2:\n            return {\"error\": \"n_clusters must be an integer greater than 1.\"}\n\n        df = data.copy()\n        feature_cols = [\n            'fear_raw', 'hope_raw', 'enmity_raw',\n            'amity_raw', 'envy_raw', 'compersion_raw'\n        ]\n\n        if not all(col in df.columns for col in feature_cols):\n            return {\"error\": f\"Missing one or more feature columns: {feature_cols}\"}\n        \n        df_clean = df.dropna(subset=feature_cols)\n        if len(df_clean) < n_clusters:\n            return {\"error\": \"Not enough data points to form the requested number of clusters.\"}\n\n        X = df_clean[feature_cols].values\n        \n        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n        cluster_labels = kmeans.fit_predict(X)\n        \n        # Add cluster labels back to the cleaned dataframe\n        df_clean['cluster_label'] = cluster_labels\n        \n        # Create a dataframe for cluster centers\n        centers_df = pd.DataFrame(kmeans.cluster_centers_, columns=feature_cols).round(4)\n        centers_df['cluster_label'] = centers_df.index\n        \n        results = {\n            'cluster_centers': json.loads(centers_df.to_json(orient='records')),\n            'data_with_clusters': json.loads(df_clean[['document_name', 'cluster_label']].to_json(orient='records'))\n        }\n        \n        return results\n\n    except Exception:\n        return None\n\ndef analyze_climate_polarization(data, **kwargs):\n    \"\"\"\n    Analyzes emotional climate polarization across a specified grouping variable.\n\n    This function measures the spread (polarization) of emotional climates within\n    different groups (e.g., ideologies, eras). It calculates the standard deviation\n    of the three axis-balance scores and the main Emotional Climate Index for each\n    group defined by the 'grouping_var'. A higher standard deviation indicates greater\n    divergence or polarization of emotional expression within that group.\n\n    Methodology: The function groups the data by the specified factor, then calculates\n    the standard deviation for key derived metrics. It requires metadata to be present\n    in the DataFrame.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame.\n        **kwargs:\n            grouping_var (str): The column name to group by for polarization analysis\n                                (e.g., 'ideology' or 'era').\n\n    Returns:\n        dict: A dictionary where keys are group names and values are dictionaries of\n              polarization scores (standard deviations). Returns None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n\n    try:\n        grouping_var = kwargs.get('grouping_var')\n        if not grouping_var:\n            return {\"error\": \"A 'grouping_var' must be provided in kwargs.\"}\n\n        # Attempt to load metadata and calculate derived metrics\n        df = _load_and_merge_metadata(data)\n        df = calculate_derived_metrics(df)\n\n        metrics_to_analyze = [\n            'threat_opportunity_balance', 'social_relations_balance',\n            'resource_attitudes_balance', 'emotional_climate_index'\n        ]\n        \n        required_cols = metrics_to_analyze + [grouping_var]\n        if not all(col in df.columns for col in required_cols):\n            return {\"error\": f\"One or more required columns missing from data: {required_cols}\"}\n\n        df_clean = df.dropna(subset=required_cols)\n        if df_clean.empty:\n            return {\"error\": \"No valid data remains after cleaning.\"}\n\n        # Group by the specified variable and calculate standard deviation for each metric\n        polarization_scores = df_clean.groupby(grouping_var)[metrics_to_analyze].std().round(4)\n        \n        if polarization_scores.empty:\n            return {\"error\": \"Could not compute polarization scores. Check variance in groups.\"}\n            \n        polarization_scores.index.name = grouping_var\n        polarization_scores.reset_index(inplace=True)\n\n        return json.loads(polarization_scores.to_json(orient='records'))\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}