{
  "generation_metadata": {
    "status": "success",
    "functions_generated": 5,
    "output_file": "automatedstatisticalanalysisagent_functions.py",
    "module_size": 20829,
    "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: emotional_climate_factorial_analysis\nDescription: Statistical analysis experiment\nGenerated: 2025-08-27T16:23:26.076666+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates derived emotional climate metrics based on the ECF v10.0 spec.\n\n    This function takes a DataFrame with raw and salience scores and adds columns\n    for the derived metrics defined in the framework. It adapts the 'Resource\n    Attitudes' axis to use 'compassion' as found in the actual data structure,\n    instead of 'compersion' from the spec. A small epsilon (0.001) is added to\n    salience totals to prevent division by zero, as specified.\n\n    Methodology:\n    - Axis Balances: Calculated as the salience-weighted difference between the\n      positive and negative dimensions on an axis, normalized by the total\n      salience of that axis. Ranges from -1.0 to 1.0.\n    - Emotional Climate Index: The primary summary metric, calculated as the\n      weighted average of the three axis balances, where each balance is weighted\n      by the total salience of its axis.\n    - Climate Intensity: The unweighted average of all six raw emotional scores.\n    - Positive/Negative Indices: The unweighted average of the three positive\n      (hope, amity, compassion) or three negative (fear, enmity, envy) raw scores.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data with columns\n                                 for raw scores and salience (e.g., 'fear_raw',\n                                 'fear_salience').\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pandas.DataFrame: The input DataFrame with added columns for derived metrics,\n                          or None if required columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        df = data.copy()\n        \n        # Define required columns based on the provided data structure\n        required_cols = [\n            'fear_raw', 'fear_salience', 'hope_raw', 'hope_salience',\n            'enmity_raw', 'enmity_salience', 'amity_raw', 'amity_salience',\n            'envy_raw', 'envy_salience', 'compassion_raw', 'compassion_salience'\n        ]\n        if not all(col in df.columns for col in required_cols):\n            # Missing one or more required columns\n            return None\n\n        # Intermediate salience calculations with epsilon for stability\n        epsilon = 0.001\n        df['threat_opportunity_salience_total'] = df['fear_salience'] + df['hope_salience'] + epsilon\n        df['social_relations_salience_total'] = df['enmity_salience'] + df['amity_salience'] + epsilon\n        # NOTE: Using 'compassion' as per actual data structure, not 'compersion' from spec\n        df['resource_attitudes_salience_total'] = df['envy_salience'] + df['compassion_salience'] + epsilon\n        df['total_emotional_salience'] = (df['threat_opportunity_salience_total'] +\n                                          df['social_relations_salience_total'] +\n                                          df['resource_attitudes_salience_total'])\n\n        # Axis-level climate indices\n        df['threat_opportunity_balance'] = ((df['hope_raw'] * df['hope_salience']) - (df['fear_raw'] * df['fear_salience'])) / df['threat_opportunity_salience_total']\n        df['social_relations_balance'] = ((df['amity_raw'] * df['amity_salience']) - (df['enmity_raw'] * df['enmity_salience'])) / df['social_relations_salience_total']\n        # NOTE: Using 'compassion' as per actual data structure\n        df['resource_attitudes_balance'] = ((df['compassion_raw'] * df['compassion_salience']) - (df['envy_raw'] * df['envy_salience'])) / df['resource_attitudes_salience_total']\n\n        # Summary metrics\n        numerator = ((df['threat_opportunity_balance'] * df['threat_opportunity_salience_total']) +\n                     (df['social_relations_balance'] * df['social_relations_salience_total']) +\n                     (df['resource_attitudes_balance'] * df['resource_attitudes_salience_total']))\n        df['emotional_climate_index'] = numerator / df['total_emotional_salience']\n\n        df['climate_intensity'] = (df['fear_raw'] + df['hope_raw'] + df['enmity_raw'] + df['amity_raw'] + df['envy_raw'] + df['compassion_raw']) / 6\n        \n        # NOTE: Using 'compassion' as per actual data structure\n        df['positive_emotional_index'] = (df['hope_raw'] + df['amity_raw'] + df['compassion_raw']) / 3\n        df['negative_emotional_index'] = (df['fear_raw'] + df['enmity_raw'] + df['envy_raw']) / 3\n\n        return df\n\n    except (KeyError, TypeError, Exception):\n        return None\n\ndef get_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Computes descriptive statistics for all emotional dimensions and derived metrics.\n\n    This function first calculates the derived metrics using the ECF v10.0 framework\n    and then computes summary statistics (mean, std, min, 25%, 50%, 75%, max) for\n    both the base dimension scores (raw and salience) and the calculated derived metrics.\n    This provides a foundational overview of the dataset's characteristics.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing the descriptive statistics as a JSON-serializable\n              string, or None if the analysis fails.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # First, calculate derived metrics to include them in the analysis\n        data_with_metrics = calculate_derived_metrics(data)\n        if data_with_metrics is None:\n            return None\n\n        # Select only numeric columns for description\n        numeric_cols = data_with_metrics.select_dtypes(include=np.number)\n        \n        if numeric_cols.empty:\n            return None\n\n        # Generate descriptive statistics\n        desc_stats = numeric_cols.describe()\n        \n        # Convert to a JSON-friendly format\n        return {'descriptive_statistics': desc_stats.to_dict()}\n\n    except Exception:\n        return None\n\ndef calculate_correlation_matrix(data, **kwargs):\n    \"\"\"\n    Calculates the Pearson correlation matrix for emotional dimensions and derived metrics.\n\n    This analysis helps answer research questions about the statistical relationships\n    between different emotional climate measures. The function first computes the\n    derived metrics, then calculates a Pearson correlation matrix for all numeric\n    scores. The result shows how different emotional dimensions and indices co-vary.\n\n    Methodology:\n    - Pearson correlation coefficient (r) is used to measure the linear relationship\n      between two variables. Values range from -1 (perfect negative correlation) to\n      +1 (perfect positive correlation), with 0 indicating no linear correlation.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing the correlation matrix as a JSON-serializable\n              object, or None if the analysis fails.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # First, calculate derived metrics to include them in the analysis\n        data_with_metrics = calculate_derived_metrics(data)\n        if data_with_metrics is None:\n            return None\n\n        # Select only numeric columns for correlation\n        numeric_cols = data_with_metrics.select_dtypes(include=np.number)\n        \n        if numeric_cols.shape[1] < 2:\n            return None # Not enough columns to correlate\n\n        # Calculate the correlation matrix\n        correlation_matrix = numeric_cols.corr(method='pearson')\n        \n        # Replace NaN values that can occur if a column has zero variance\n        correlation_matrix.fillna(0, inplace=True)\n\n        return {'correlation_matrix': correlation_matrix.to_dict()}\n\n    except Exception:\n        return None\n\ndef analyze_climate_by_group(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for key emotional metrics, grouped by a metadata variable.\n\n    This function addresses research questions about how emotional climate patterns\n    differ across groups (e.g., by ideology or era). It attempts to load a corpus\n    manifest to merge metadata with the analysis data. If the manifest or the specified\n    grouping variable is not available, it returns None.\n\n    Methodology:\n    - The function groups the data by the provided 'grouping_variable'.\n    - For each group, it calculates the mean and standard deviation of key derived\n      metrics: 'emotional_climate_index', 'climate_intensity', 'positive_emotional_index',\n      and 'negative_emotional_index'.\n    - This allows for direct comparison of emotional signatures between different groups.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs:\n            grouping_variable (str): The name of the metadata column to group by (e.g., 'ideology', 'era').\n            manifest_path (str, optional): Path to the corpus manifest file. Defaults to 'corpus_manifest.json'.\n\n    Returns:\n        dict: A dictionary with the grouped analysis results, or None if the grouping\n              variable is not found or data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    from pathlib import Path\n\n    def _load_and_merge_metadata_local(df, m_path):\n        \"\"\"Internal helper to load and merge metadata gracefully.\"\"\"\n        manifest_path = Path(m_path)\n        if not manifest_path.is_file():\n            return df\n        try:\n            with open(manifest_path, 'r') as f:\n                manifest_data = json.load(f)\n            \n            if isinstance(manifest_data, dict):\n                manifest_data = [dict(item, document_name=key) for key, item in manifest_data.items()]\n            \n            if not isinstance(manifest_data, list): return df\n\n            meta_df = pd.DataFrame(manifest_data)\n            if 'document_name' not in meta_df.columns or 'document_name' not in df.columns:\n                return df\n            \n            return pd.merge(df, meta_df, on='document_name', how='left')\n        except Exception:\n            return df\n\n    try:\n        grouping_variable = kwargs.get('grouping_variable')\n        if not grouping_variable:\n            return None\n\n        # Attempt to load metadata\n        manifest_path = kwargs.get(\"manifest_path\", \"corpus_manifest.json\")\n        data_with_meta = _load_and_merge_metadata_local(data.copy(), manifest_path)\n\n        if grouping_variable not in data_with_meta.columns:\n            # Handle gracefully: grouping variable not found in data or manifest\n            return None\n\n        # Calculate derived metrics\n        df = calculate_derived_metrics(data_with_meta)\n        if df is None:\n            return None\n            \n        # Drop rows where the grouping variable is NaN\n        df.dropna(subset=[grouping_variable], inplace=True)\n        if df.empty:\n            return None\n\n        metrics_to_analyze = [\n            'emotional_climate_index', 'climate_intensity',\n            'positive_emotional_index', 'negative_emotional_index'\n        ]\n        \n        # Ensure metrics columns exist\n        if not all(col in df.columns for col in metrics_to_analyze):\n            return None\n\n        # Group and aggregate\n        grouped_analysis = df.groupby(grouping_variable)[metrics_to_analyze].agg(['mean', 'std', 'count'])\n        \n        # Format for JSON output\n        grouped_analysis.columns = ['_'.join(col).strip() for col in grouped_analysis.columns.values]\n        \n        return {'grouped_analysis': grouped_analysis.to_dict()}\n\n    except Exception:\n        return None\n\ndef perform_two_way_anova(data, **kwargs):\n    \"\"\"\n    Performs a two-way ANOVA to analyze the interaction of two factors on an emotional metric.\n\n    This function directly addresses research questions about the interaction effects\n    of variables like 'ideology' and 'era' on emotional climate. It requires a corpus\n    manifest to provide these categorical factors.\n\n    Methodology:\n    - A two-way Analysis of Variance (ANOVA) is used to determine how the mean of a\n      quantitative dependent variable changes according to the levels of two\n      categorical independent variables.\n    - It tests three null hypotheses:\n      1. The means of the dependent variable are equal for the first factor.\n      2. The means are equal for the second factor.\n      3. There is no interaction effect between the two factors.\n    - The function uses the `statsmodels` library to fit an Ordinary Least Squares (OLS)\n      model and generate the ANOVA table.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs:\n            dependent_variable (str): The metric to be analyzed (e.g., 'emotional_climate_index').\n            factor_one (str): The first categorical factor (e.g., 'ideology').\n            factor_two (str): The second categorical factor (e.g., 'era').\n            manifest_path (str, optional): Path to the corpus manifest file. Defaults to 'corpus_manifest.json'.\n\n    Returns:\n        dict: A dictionary containing the ANOVA results table, or None if factors are\n              missing or data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    from pathlib import Path\n    import statsmodels.api as sm\n    from statsmodels.formula.api import ols\n\n    def _load_and_merge_metadata_local(df, m_path):\n        \"\"\"Internal helper to load and merge metadata gracefully.\"\"\"\n        manifest_path = Path(m_path)\n        if not manifest_path.is_file(): return df\n        try:\n            with open(manifest_path, 'r') as f: manifest_data = json.load(f)\n            if isinstance(manifest_data, dict): manifest_data = [dict(item, document_name=key) for key, item in manifest_data.items()]\n            if not isinstance(manifest_data, list): return df\n            meta_df = pd.DataFrame(manifest_data)\n            if 'document_name' not in meta_df.columns or 'document_name' not in df.columns: return df\n            return pd.merge(df, meta_df, on='document_name', how='left')\n        except Exception: return df\n\n    try:\n        dependent_variable = kwargs.get('dependent_variable')\n        factor_one = kwargs.get('factor_one')\n        factor_two = kwargs.get('factor_two')\n\n        if not all([dependent_variable, factor_one, factor_two]):\n            return None\n\n        # Load metadata and calculate derived metrics\n        manifest_path = kwargs.get(\"manifest_path\", \"corpus_manifest.json\")\n        data_with_meta = _load_and_merge_metadata_local(data.copy(), manifest_path)\n        df = calculate_derived_metrics(data_with_meta)\n        if df is None: return None\n\n        required_cols = [dependent_variable, factor_one, factor_two]\n        if not all(col in df.columns for col in required_cols):\n            return None\n\n        # Prepare data for ANOVA\n        df_anova = df[required_cols].dropna()\n        \n        # Ensure there's enough data to perform ANOVA\n        if df_anova.shape[0] < 10 or df_anova[factor_one].nunique() < 2 or df_anova[factor_two].nunique() < 2:\n            return None\n\n        # Create the formula for the OLS model\n        formula = f'`{dependent_variable}` ~ C(`{factor_one}`) * C(`{factor_two}`)'\n        \n        # Fit the model and perform ANOVA\n        model = ols(formula, data=df_anova).fit()\n        anova_table = sm.stats.anova_lm(model, typ=2)\n        \n        return {'anova_results': anova_table.to_dict()}\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
    "cached_with_code": true
  },
  "statistical_data": {
    "analyze_climate_by_group": null,
    "calculate_correlation_matrix": null,
    "calculate_derived_metrics": null,
    "generate_statistical_summary_report": "STATISTICAL ANALYSIS SUMMARY REPORT\n==================================================\nAnalysis Timestamp: Unknown\nSample Size: Unknown\nAlpha Level: Unknown\nVariables: 0\n",
    "get_descriptive_statistics": null,
    "perform_statistical_analysis": {
      "analysis_metadata": {
        "timestamp": "2025-08-27T13:28:00.770910",
        "sample_size": 8,
        "alpha_level": 0.05,
        "variables_analyzed": [
          "fear_raw",
          "fear_salience",
          "fear_confidence",
          "hope_raw",
          "hope_salience",
          "hope_confidence",
          "enmity_raw",
          "enmity_salience",
          "enmity_confidence",
          "amity_raw",
          "amity_salience",
          "amity_confidence",
          "envy_raw",
          "envy_salience",
          "envy_confidence",
          "compersion_raw",
          "compersion_salience",
          "compersion_confidence"
        ]
      }
    },
    "perform_two_way_anova": null,
    "run_complete_statistical_analysis": {
      "analysis_metadata": {
        "timestamp": "2025-08-27T13:28:01.023645",
        "sample_size": 8,
        "alpha_level": 0.05,
        "variables_analyzed": [
          "fear_raw",
          "fear_salience",
          "fear_confidence",
          "hope_raw",
          "hope_salience",
          "hope_confidence",
          "enmity_raw",
          "enmity_salience",
          "enmity_confidence",
          "amity_raw",
          "amity_salience",
          "amity_confidence",
          "envy_raw",
          "envy_salience",
          "envy_confidence",
          "compersion_raw",
          "compersion_salience",
          "compersion_confidence"
        ]
      }
    }
  },
  "status": "success_with_data",
  "validation_passed": true
}