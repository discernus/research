"""
Automated Derived Metrics Functions
===================================

Generated by AutomatedDerivedMetricsAgent for experiment: simple_test
Description: No description
Generated: 2025-08-15T18:51:44.009005+00:00

This module contains automatically generated calculation functions for derived metrics
as specified in the framework's natural language descriptions.
"""

import pandas as pd
import numpy as np
from typing import Optional, Dict, Any


def calculate_identity_tension(data, **kwargs):
    """
    Calculate identity_tension: Conflict between tribal dominance and individual dignity dimensions.

    Formula: identity_tension = Tribal_Dominance * (1 - Individual_Dignity)

    This formula captures the degree of conflict by multiplying the level of 'Tribal Dominance'
    by the inverse of 'Individual Dignity'. High tension occurs when Tribal Dominance is high
    and Individual Dignity is low, ranging from 0.0 (no tension) to 1.0 (maximal tension).

    Args:
        data: pandas DataFrame with at least 'Tribal Dominance' and 'Individual Dignity' columns.
              Each dimension is expected to be scored 0.0-1.0.
        **kwargs: Additional parameters (not used in this calculation but included for framework compatibility).

    Returns:
        float: The mean calculated identity tension across the valid rows of the DataFrame,
               or None if insufficient data (e.g., required columns missing, or all relevant
               data points are missing/invalid).
    """
    import pandas as pd
    import numpy as np

    try:
        required_columns = ['Tribal Dominance', 'Individual Dignity']

        # Check if all required columns exist in the DataFrame
        if not all(col in data.columns for col in required_columns):
            return None # Insufficient data if columns are missing

        # Create a working copy of the relevant columns and convert to numeric, coercing errors
        df_calc = data[required_columns].apply(pd.to_numeric, errors='coerce')

        # Drop rows where any of the required columns have NaN values, as they cannot be used in the calculation
        df_calc.dropna(subset=required_columns, inplace=True)

        # If no valid data remains after dropping NaNs, return None
        if df_calc.empty:
            return None # Insufficient data after cleaning

        # Clip values to ensure they are within the expected 0.0-1.0 range,
        # providing robustness against potential out-of-range source data.
        df_calc['Tribal Dominance'] = np.clip(df_calc['Tribal Dominance'], 0.0, 1.0)
        df_calc['Individual Dignity'] = np.clip(df_calc['Individual Dignity'], 0.0, 1.0)

        # Apply the identity_tension formula
        df_calc['identity_tension'] = df_calc['Tribal Dominance'] * (1 - df_calc['Individual Dignity'])

        # Return the mean of the calculated identity tension scores.
        # This provides a single aggregated score for the DataFrame.
        return float(df_calc['identity_tension'].mean())

    except Exception:
        # Catch any unexpected errors during processing and return None as per requirement
        return None

def calculate_emotional_balance(data, **kwargs):
    """
    Calculate emotional_balance: Difference between hope and fear scores.

    Formula: emotional_balance = hope_score - fear_score

    Args:
        data (pd.DataFrame): A pandas DataFrame expected to contain 'hope_score' and 'fear_score' columns.
                             It is assumed these columns contain the scalar scores for the calculation.
                             If multiple rows exist, the first non-null, numeric value in each column is used.
        **kwargs: Additional parameters (not used in this calculation).

    Returns:
        float: The calculated emotional balance score, ranging from -1.0 to 1.0.
               Returns None if:
               - `data` is not a pandas DataFrame.
               - 'hope_score' or 'fear_score' columns are missing.
               - Either score column is empty or contains only null values after dropping NaNs.
               - The extracted scores are not numeric.
    """
    import pandas as pd
    import numpy as np # Included as per template, though not directly used for this specific calc

    try:
        # 1. Validate input type
        if not isinstance(data, pd.DataFrame):
            return None

        # 2. Check for required columns
        required_columns = ['hope_score', 'fear_score']
        if not all(col in data.columns for col in required_columns):
            return None

        # 3. Extract scores. Assume single scalar values are expected.
        #    We take the first non-null value from each column.
        hope_score_series = data['hope_score'].dropna()
        fear_score_series = data['fear_score'].dropna()

        # 4. Handle cases where columns exist but are empty or contain only nulls after dropna()
        if hope_score_series.empty or fear_score_series.empty:
            return None

        hope_val = hope_score_series.iloc[0]
        fear_val = fear_score_series.iloc[0]

        # 5. Ensure the extracted values are numeric
        #    Checks for standard Python numeric types (int, float) and NumPy numeric types.
        if not (isinstance(hope_val, (int, float, np.integer, np.floating)) and
                isinstance(fear_val, (int, float, np.integer, np.floating))):
            return None

        # 6. Perform the calculation. Ensure result is float.
        emotional_balance = float(hope_val) - float(fear_val)

        return emotional_balance

    except Exception:
        # Catch any other unexpected errors during processing (e.g., type conversion issues)
        return None

def calculate_success_climate(data, **kwargs):
    """
    Calculate success_climate: Difference between the mean compersion score and the mean envy score.

    Args:
        data (pandas.DataFrame): A DataFrame expected to contain 'compersion' and 'envy'
                                 columns, each with scores typically ranging from 0.0 to 1.0.
        **kwargs: Additional parameters (not used in this calculation).

    Returns:
        float: The calculated success_climate score (mean_compersion - mean_envy),
               or None if 'compersion' or 'envy' columns are missing,
               or if no valid numeric data is found in these columns.
    """
    import pandas as pd
    import numpy as np

    try:
        # Check if required columns exist in the DataFrame
        required_columns = ['compersion', 'envy']
        if not all(col in data.columns for col in required_columns):
            # Log a warning or error if this were a full system, but for now, return None
            return None

        # Convert columns to numeric, coercing errors to NaN
        compersion_scores = pd.to_numeric(data['compersion'], errors='coerce')
        envy_scores = pd.to_numeric(data['envy'], errors='coerce')

        # Calculate the mean of each score, dropping NaNs.
        # If all values in a column are NaN, the mean will be NaN.
        mean_compersion = compersion_scores.mean()
        mean_envy = envy_scores.mean()

        # If either mean is NaN (meaning no valid numeric data was found in that column),
        # or if the DataFrame was empty to begin with, return None.
        if pd.isna(mean_compersion) or pd.isna(mean_envy):
            return None

        # Calculate the success_climate score
        success_climate_score = mean_compersion - mean_envy

        return float(success_climate_score)

    except Exception:
        # Catch any unexpected errors during calculation and return None
        return None

def calculate_relational_climate(data, **kwargs):
    """
    Calculate relational_climate: Difference between amity and enmity scores.
    
    Formula: relational_climate = mean(amity_scores) - mean(enmity_scores)
    
    Args:
        data: pandas DataFrame expected to contain 'amity' and 'enmity' columns.
              These columns should contain numeric scores (e.g., within 0.0-1.0 range).
        **kwargs: Additional parameters (not used in this specific calculation, but
                  included for framework compatibility).
        
    Returns:
        float: The calculated relational climate score. The score represents the
               difference between the mean 'amity' and mean 'enmity' scores.
               Returns None if 'amity' or 'enmity' columns are missing, if there's
               insufficient valid numeric data in these columns to perform the
               calculation, or if an unexpected error occurs.
    """
    import pandas as pd
    import numpy as np
    
    try:
        if not isinstance(data, pd.DataFrame) or data.empty:
            return None

        required_columns = ['amity', 'enmity']
        if not all(col in data.columns for col in required_columns):
            # Return None if any required column is missing
            return None

        # Convert columns to numeric, coercing any non-numeric values to NaN
        amity_scores = pd.to_numeric(data['amity'], errors='coerce')
        enmity_scores = pd.to_numeric(data['enmity'], errors='coerce')

        # Drop NaN values to ensure only valid numeric scores are used for the mean
        amity_valid = amity_scores.dropna()
        enmity_valid = enmity_scores.dropna()

        # If, after dropping NaNs, either series is empty, there's no valid data
        if amity_valid.empty or enmity_valid.empty:
            return None

        # Calculate the mean of the valid scores for each dimension
        mean_amity = amity_valid.mean()
        mean_enmity = enmity_valid.mean()

        # Calculate the difference
        result = mean_amity - mean_enmity
        
        # Ensure the final result is a float and not NaN (e.g., if means somehow resulted in NaN)
        if np.isnan(result):
            return None
            
        return float(result)

    except Exception:
        # Catch any unexpected errors during processing and return None
        return None

def calculate_goal_orientation(data, **kwargs):
    """
    Calculate goal_orientation: Difference between cohesive goals and fragmentative goals.

    Formula: Goal Orientation = Mean(Individual Dignity) - Mean(Tribal Dominance)

    Cohesive goals are represented by the 'Individual Dignity' dimension score,
    reflecting universal human worth and inclusive recognition.
    Fragmentative goals are represented by the 'Tribal Dominance' dimension score,
    reflecting in-group supremacy and exclusionary identity patterns.

    Args:
        data: pandas DataFrame expected to contain 'Individual Dignity' and 'Tribal Dominance' columns.
              Each row represents an observation where these dimensions have been scored (0.0-1.0).
        **kwargs: Additional parameters (not used in this calculation).

    Returns:
        float: Calculated aggregated goal orientation score for the DataFrame,
               ranging from -1.0 (fully fragmentative) to 1.0 (fully cohesive).
               Returns None if the input data is not a DataFrame, if required columns are missing,
               or if no valid numeric data is available for calculation in the relevant columns.
    """
    import pandas as pd
    import numpy as np
    
    try:
        if not isinstance(data, pd.DataFrame):
            return None

        required_columns = ['Individual Dignity', 'Tribal Dominance']
        if not all(col in data.columns for col in required_columns):
            return None

        # Convert columns to numeric, coercing any non-numeric values to NaN
        individual_dignity_scores = pd.to_numeric(data['Individual Dignity'], errors='coerce')
        tribal_dominance_scores = pd.to_numeric(data['Tribal Dominance'], errors='coerce')

        # Check if, after coercion, there's any valid numeric data in the columns.
        # If a column becomes entirely NaN, it means no valid data was present.
        if individual_dignity_scores.isnull().all() or tribal_dominance_scores.isnull().all():
            return None

        # Calculate the mean for each relevant dimension, automatically skipping NaN values.
        mean_individual_dignity = individual_dignity_scores.mean()
        mean_tribal_dominance = tribal_dominance_scores.mean()

        # If either mean is NaN (e.g., if a column was entirely NaN but had entries),
        # it indicates insufficient data for a meaningful calculation.
        if pd.isna(mean_individual_dignity) or pd.isna(mean_tribal_dominance):
            return None

        # Calculate the goal orientation score
        goal_orientation_score = mean_individual_dignity - mean_tribal_dominance

        return float(goal_orientation_score)
    except Exception:
        # Catch any unexpected errors and return None as per requirement
        return None

def calculate_overall_cohesion_index(data, **kwargs):
    """
    Calculate overall_cohesion_index: Comprehensive measure combining all dimensions.

    Formula: Overall Cohesion Index = (Average(Individual_Dignity_Score) + (1 - Average(Tribal_Dominance_Score))) / 2

    This index quantifies overall social cohesion by balancing positive and negative contributing dimensions.
    Higher values indicate greater cohesion.

    Args:
        data (pd.DataFrame): pandas DataFrame containing the dimension scores.
                             Expected columns: 'individual_dignity', 'tribal_dominance'.
                             Scores in these columns are expected to be numeric (float) between 0.0 and 1.0.
                             Each row in the DataFrame represents an observation.
        **kwargs: Additional parameters (not used in this version but available for future extensions).

    Returns:
        float: The calculated overall cohesion index (a single scalar value between 0.0 and 1.0)
               or None if the input data is insufficient, invalid, or an error occurs during calculation.
    """
    import pandas as pd
    import numpy as np

    # Ensure the input is a pandas DataFrame
    if not isinstance(data, pd.DataFrame):
        return None

    # Define the required dimension columns
    # 'individual_dignity' contributes positively to cohesion
    # 'tribal_dominance' contributes negatively to cohesion (hence 1 - score)
    required_columns = ['individual_dignity', 'tribal_dominance']

    # Check if all required columns exist in the DataFrame
    if not all(col in data.columns for col in required_columns):
        return None

    try:
        # Convert required columns to numeric, coercing any non-numeric values to NaN.
        # This handles potential string or incorrect data types gracefully.
        individual_dignity_scores = pd.to_numeric(data['individual_dignity'], errors='coerce')
        tribal_dominance_scores = pd.to_numeric(data['tribal_dominance'], errors='coerce')

        # Calculate the mean for each score column, skipping NaN values.
        # This provides an average score for the entire dataset for each dimension.
        mean_individual_dignity = individual_dignity_scores.mean(skipna=True)
        mean_tribal_dominance = tribal_dominance_scores.mean(skipna=True)

        # If either of the critical dimension means is NaN (i.e., the column was entirely NaN
        # or empty after conversion), return None as there's insufficient data for calculation.
        if pd.isna(mean_individual_dignity) or pd.isna(mean_tribal_dominance):
            return None

        # Apply the overall cohesion index formula:
        # Individual Dignity directly adds to cohesion.
        # Tribal Dominance's impact is inverted: a high dominance score means low cohesion (1 - score).
        # The result is then averaged across the two contributing factors.
        overall_cohesion_index = (mean_individual_dignity + (1.0 - mean_tribal_dominance)) / 2.0

        # The calculated index should naturally fall between 0.0 and 1.0 if input scores
        # were within this range. No explicit clamping is typically needed.

        return float(overall_cohesion_index)

    except Exception:
        # Catch any unforeseen errors during the calculation process and return None.
        return None

def calculate_all_derived_metrics(data: pd.DataFrame) -> Dict[str, Optional[float]]:
    """
    Calculate all derived metrics for the given dataset.
    
    Args:
        data: pandas DataFrame with dimension scores
        
    Returns:
        Dictionary mapping metric names to calculated values
    """
    results = {}
    
    # Get all calculation functions from this module
    import inspect
    current_module = inspect.getmodule(inspect.currentframe())
    
    for name, obj in inspect.getmembers(current_module):
        if (inspect.isfunction(obj) and 
            name.startswith('calculate_') and 
            name != 'calculate_all_derived_metrics'):
            try:
                results[name.replace('calculate_', '')] = obj(data)
            except Exception as e:
                results[name.replace('calculate_', '')] = None
                
    return results
