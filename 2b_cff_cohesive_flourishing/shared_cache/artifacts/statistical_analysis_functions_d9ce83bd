{
  "status": "success",
  "functions_generated": 5,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 22179,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: cohesive_flourishing_factorial_analysis\nDescription: Statistical analysis experiment\nGenerated: 2025-08-28T02:36:33.899414+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived metrics as defined in the Cohesive Flourishing Framework v10.0 spec\n    and adds them as new columns to the DataFrame. This function is foundational for other analyses.\n\n    Methodology:\n    This function implements the formulas for Tension Indices, the Strategic Contradiction Index,\n    and the three Salience-Weighted Cohesion Indices. It uses the raw and salience scores for each of the\n    10 core dimensions. The calculations are performed row-wise for each document in the dataset.\n    - Tension Indices quantify the rhetorical contradiction between opposing dimensions.\n    - Strategic Contradiction Index is the average of all tension indices, measuring overall message incoherence.\n    - Cohesion Indices are salience-weighted scores that measure the overall leaning of the discourse\n      towards cohesive or fragmentative patterns, normalized to a -1.0 to +1.0 scale. An epsilon of 0.001\n      is added to denominators to prevent division by zero.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns for raw and salience scores\n                             for all 10 dimensions (e.g., 'tribal_dominance_raw', 'tribal_dominance_salience').\n        **kwargs: Not used.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with the original data plus added columns for each derived metric,\n                      or None if essential input columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        df = data.copy()\n\n        required_cols = [\n            'tribal_dominance_raw', 'tribal_dominance_salience', 'individual_dignity_raw', 'individual_dignity_salience',\n            'fear_raw', 'fear_salience', 'hope_raw', 'hope_salience', 'envy_raw', 'envy_salience',\n            'compersion_raw', 'compersion_salience', 'enmity_raw', 'enmity_salience', 'amity_raw', 'amity_salience',\n            'fragmentative_goals_raw', 'fragmentative_goals_salience', 'cohesive_goals_raw', 'cohesive_goals_salience'\n        ]\n        if not all(col in df.columns for col in required_cols):\n            # Missing one or more required columns for calculation\n            return None\n\n        # 1. Calculate Tension Indices\n        df['identity_tension'] = np.minimum(df['tribal_dominance_raw'], df['individual_dignity_raw']) * \\\n                                 abs(df['tribal_dominance_salience'] - df['individual_dignity_salience'])\n        df['emotional_tension'] = np.minimum(df['fear_raw'], df['hope_raw']) * \\\n                                  abs(df['fear_salience'] - df['hope_salience'])\n        df['success_tension'] = np.minimum(df['envy_raw'], df['compersion_raw']) * \\\n                                abs(df['envy_salience'] - df['compersion_salience'])\n        df['relational_tension'] = np.minimum(df['enmity_raw'], df['amity_raw']) * \\\n                                   abs(df['enmity_salience'] - df['amity_salience'])\n        df['goal_tension'] = np.minimum(df['fragmentative_goals_raw'], df['cohesive_goals_raw']) * \\\n                             abs(df['fragmentative_goals_salience'] - df['cohesive_goals_salience'])\n\n        # 2. Calculate Strategic Contradiction Index\n        tension_cols = ['identity_tension', 'emotional_tension', 'success_tension', 'relational_tension', 'goal_tension']\n        df['strategic_contradiction_index'] = df[tension_cols].mean(axis=1)\n\n        # 3. Calculate Intermediate Cohesion Components\n        df['identity_cohesion_component'] = (df['individual_dignity_raw'] * df['individual_dignity_salience']) - \\\n                                             (df['tribal_dominance_raw'] * df['tribal_dominance_salience'])\n        df['emotional_cohesion_component'] = (df['hope_raw'] * df['hope_salience']) - \\\n                                              (df['fear_raw'] * df['fear_salience'])\n        df['success_cohesion_component'] = (df['compersion_raw'] * df['compersion_salience']) - \\\n                                            (df['envy_raw'] * df['envy_salience'])\n        df['relational_cohesion_component'] = (df['amity_raw'] * df['amity_salience']) - \\\n                                               (df['enmity_raw'] * df['enmity_salience'])\n        df['goal_cohesion_component'] = (df['cohesive_goals_raw'] * df['cohesive_goals_salience']) - \\\n                                         (df['fragmentative_goals_raw'] * df['fragmentative_goals_salience'])\n\n        # 4. Calculate Salience Totals for Normalization\n        epsilon = 0.001\n        descriptive_salience_cols = ['hope_salience', 'fear_salience', 'compersion_salience', 'envy_salience', 'amity_salience', 'enmity_salience']\n        df['descriptive_salience_total'] = df[descriptive_salience_cols].sum(axis=1)\n\n        motivational_salience_cols = descriptive_salience_cols + ['cohesive_goals_salience', 'fragmentative_goals_salience']\n        df['motivational_salience_total'] = df[motivational_salience_cols].sum(axis=1)\n\n        full_salience_cols = motivational_salience_cols + ['individual_dignity_salience', 'tribal_dominance_salience']\n        df['full_salience_total'] = df[full_salience_cols].sum(axis=1)\n\n        # 5. Calculate Final Salience-Weighted Cohesion Indices\n        df['descriptive_cohesion_index'] = (df['emotional_cohesion_component'] + df['success_cohesion_component'] + df['relational_cohesion_component']) / \\\n                                           (df['descriptive_salience_total'] + epsilon)\n        df['motivational_cohesion_index'] = (df['emotional_cohesion_component'] + df['success_cohesion_component'] + df['relational_cohesion_component'] + df['goal_cohesion_component']) / \\\n                                            (df['motivational_salience_total'] + epsilon)\n        df['full_cohesion_index'] = (df['identity_cohesion_component'] + df['emotional_cohesion_component'] + df['success_cohesion_component'] + df['relational_cohesion_component'] + df['goal_cohesion_component']) / \\\n                                     (df['full_salience_total'] + epsilon)\n        \n        # Clean up intermediate columns\n        intermediate_cols = [\n            'identity_cohesion_component', 'emotional_cohesion_component', 'success_cohesion_component',\n            'relational_cohesion_component', 'goal_cohesion_component', 'descriptive_salience_total',\n            'motivational_salience_total', 'full_salience_total'\n        ]\n        df = df.drop(columns=intermediate_cols)\n\n        return df\n\n    except Exception:\n        return None\n\ndef get_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates and returns descriptive statistics for all core and derived CFF metrics.\n\n    Methodology:\n    This function first computes all derived metrics using the `calculate_derived_metrics` function.\n    It then uses the pandas `.describe()` method to calculate count, mean, standard deviation,\n    minimum, 25th percentile, median (50th), 75th percentile, and maximum for all numerical\n    columns. This provides a comprehensive overview of the dataset's characteristics and helps\n    address Research Question 1: \"What patterns exist in discourse framing dimensions...?\"\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing the descriptive statistics for each metric,\n              or None if calculations fail.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # First, calculate all derived metrics to include them in the statistics\n        metrics_df = calculate_derived_metrics(data)\n        if metrics_df is None:\n            return None\n\n        # Select only numeric columns for description\n        numeric_df = metrics_df.select_dtypes(include=np.number)\n        \n        # Calculate descriptive statistics\n        stats = numeric_df.describe()\n        \n        # Convert to a dictionary for a structured JSON output\n        return stats.to_dict()\n\n    except Exception:\n        return None\n\ndef analyze_dimension_correlations(data, **kwargs):\n    \"\"\"\n    Calculates the Pearson correlation matrix for all core and derived CFF metrics.\n\n    Methodology:\n    This function first computes all derived metrics. It then calculates a Pearson correlation\n    matrix for all raw scores, salience scores, and derived indices. The Pearson correlation\n    coefficient measures the linear relationship between two datasets. The output helps identify\n    which rhetorical dimensions tend to co-occur and how they relate to composite measures like\n    cohesion and tension. This directly addresses Research Question 4: \"What statistical\n    relationships exist between opposing dimensional axes and strategic tension measures?\"\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs:\n            method (str): The correlation method to use ('pearson', 'kendall', 'spearman').\n                          Defaults to 'pearson'.\n\n    Returns:\n        dict: A dictionary representing the correlation matrix, or None if calculations fail.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # First, calculate all derived metrics to include them in the correlation\n        metrics_df = calculate_derived_metrics(data)\n        if metrics_df is None:\n            return None\n\n        # Select only numeric columns for correlation\n        numeric_df = metrics_df.select_dtypes(include=np.number)\n        \n        # Get correlation method from kwargs, default to 'pearson'\n        method = kwargs.get('method', 'pearson')\n        \n        # Calculate the correlation matrix\n        corr_matrix = numeric_df.corr(method=method)\n        \n        # Replace NaN with None for JSON compatibility\n        corr_matrix = corr_matrix.where(pd.notnull(corr_matrix), None)\n        \n        return corr_matrix.to_dict('index')\n\n    except Exception:\n        return None\n\ndef analyze_strategic_tension(data, **kwargs):\n    \"\"\"\n    Analyzes the statistical relationships between opposing dimensional axes and their corresponding tension scores.\n\n    Methodology:\n    This function calculates Pearson correlations for each of the five CFF axes. For each axis (e.g., Emotional),\n    it correlates the raw scores of the two opposing dimensions (e.g., 'fear_raw', 'hope_raw') with each other\n    and with their corresponding tension index (e.g., 'emotional_tension'). This provides a focused view on\n    the dynamics of rhetorical contradiction, directly addressing Research Question 4. A negative correlation\n    between opposing dimensions is expected, while the correlation with the tension index reveals how the\n    interplay of the two dimensions contributes to strategic incoherence.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary where each key is an axis name (e.g., 'identity_axis') and the value is its\n              correlation matrix, or None if calculations fail.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        metrics_df = calculate_derived_metrics(data)\n        if metrics_df is None:\n            return None\n\n        axes = {\n            \"identity_axis\": ['tribal_dominance_raw', 'individual_dignity_raw', 'identity_tension'],\n            \"emotional_axis\": ['fear_raw', 'hope_raw', 'emotional_tension'],\n            \"success_axis\": ['envy_raw', 'compersion_raw', 'success_tension'],\n            \"relational_axis\": ['enmity_raw', 'amity_raw', 'relational_tension'],\n            \"goal_axis\": ['fragmentative_goals_raw', 'cohesive_goals_raw', 'goal_tension']\n        }\n\n        results = {}\n        for axis_name, cols in axes.items():\n            if not all(col in metrics_df.columns for col in cols):\n                results[axis_name] = \"Missing one or more required columns for analysis.\"\n                continue\n            \n            axis_df = metrics_df[cols]\n            corr_matrix = axis_df.corr(method='pearson')\n            corr_matrix = corr_matrix.where(pd.notnull(corr_matrix), None)\n            results[axis_name] = corr_matrix.to_dict('index')\n            \n        return results\n\n    except Exception:\n        return None\n\ndef analyze_group_comparison(data, **kwargs):\n    \"\"\"\n    Compares key CFF metrics across different groups based on corpus metadata.\n\n    Methodology:\n    This function attempts to load a corpus manifest (`corpus_manifest.json` or `.csv`) to enrich the data\n    with metadata such as speaker, year, or speech type. It does NOT parse filenames. If a manifest is found\n    and a valid `group_by_col` (e.g., 'speaker') is provided and exists in the data, the function groups the\n    data and calculates the mean and standard deviation for key derived metrics ('full_cohesion_index',\n    'strategic_contradiction_index') and the core raw scores. This addresses RQ2 (\"...relationships exist\n    between temporal factors...\") and RQ3 (\"How do framing patterns vary across different speech contexts...\").\n    If the manifest or the specified grouping column is not available, the function returns a message\n    indicating that the analysis cannot be performed, thereby handling missing metadata gracefully.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs:\n            group_by_col (str): The metadata column to group by (e.g., 'speaker', 'year', 'speech_type').\n                                Required for analysis.\n\n    Returns:\n        dict: A dictionary of aggregated statistics for each group, or a message indicating failure.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from pathlib import Path\n    import json\n\n    def _load_and_merge_manifest(df):\n        manifest_path_json = Path('corpus_manifest.json')\n        manifest_path_csv = Path('corpus_manifest.csv')\n        manifest_df = None\n        try:\n            if manifest_path_json.exists():\n                manifest_df = pd.read_json(manifest_path_json)\n            elif manifest_path_csv.exists():\n                manifest_df = pd.read_csv(manifest_path_csv)\n        except Exception:\n            return df\n\n        if manifest_df is not None:\n            if 'filename' in manifest_df.columns and 'document_name' not in manifest_df.columns:\n                manifest_df = manifest_df.rename(columns={'filename': 'document_name'})\n            if 'document_name' in manifest_df.columns and 'document_name' in df.columns:\n                cols_to_merge = [col for col in manifest_df.columns if col not in df.columns or col == 'document_name']\n                if len(cols_to_merge) > 1:\n                    return pd.merge(df, manifest_df[cols_to_merge], on='document_name', how='left')\n        return df\n\n    try:\n        group_by_col = kwargs.get('group_by_col')\n        if not group_by_col:\n            return {\"error\": \"The 'group_by_col' parameter is required for this analysis (e.g., 'speaker', 'year').\"}\n\n        # 1. Load metadata and calculate metrics\n        data_with_meta = _load_and_merge_manifest(data)\n        metrics_df = calculate_derived_metrics(data_with_meta)\n\n        if metrics_df is None:\n            return {\"error\": \"Failed to calculate derived metrics. Check input data.\"}\n\n        # 2. Check if grouping is possible\n        if group_by_col not in metrics_df.columns:\n            return {\n                \"error\": f\"Grouping column '{group_by_col}' not found. \"\n                         \"Ensure a corpus manifest is available and contains this column.\"\n            }\n        \n        if metrics_df[group_by_col].isnull().all():\n            return {\n                \"error\": f\"Grouping column '{group_by_col}' contains only null values. \"\n                         \"Analysis cannot be performed.\"\n            }\n\n        # 3. Define columns for aggregation and perform grouping\n        agg_cols = [\n            'full_cohesion_index', 'strategic_contradiction_index', 'tribal_dominance_raw',\n            'individual_dignity_raw', 'fear_raw', 'hope_raw', 'envy_raw', 'compersion_raw',\n            'enmity_raw', 'amity_raw', 'fragmentative_goals_raw', 'cohesive_goals_raw'\n        ]\n        \n        # Ensure all aggregation columns exist\n        agg_cols_exist = [col for col in agg_cols if col in metrics_df.columns]\n        if not agg_cols_exist:\n            return {\"error\": \"None of the specified aggregation columns exist in the data.\"}\n\n        grouped_stats = metrics_df.groupby(group_by_col)[agg_cols_exist].agg(['mean', 'std', 'count'])\n        \n        # Format for clean JSON output\n        grouped_stats.columns = ['_'.join(col).strip() for col in grouped_stats.columns.values]\n        grouped_stats = grouped_stats.where(pd.notnull(grouped_stats), None)\n        \n        return grouped_stats.to_dict('index')\n\n    except Exception as e:\n        return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}