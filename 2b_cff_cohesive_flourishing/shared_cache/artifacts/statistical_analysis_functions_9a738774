{
  "status": "success",
  "functions_generated": 6,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 26107,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: cohesive_flourishing_factorial_analysis\nDescription: Statistical analysis experiment\nGenerated: 2025-08-29T11:41:33.131167+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef _add_metadata_and_derived_metrics(data):\n    \"\"\"\n    Internal helper function to preprocess the data. It adds metadata columns\n    based on document names and calculates all derived metrics as specified\n    in the Cohesive Flourishing Framework v10.0.\n\n    This function is not intended to be called directly by the user.\n\n    Args:\n        data (pd.DataFrame): The raw analysis data.\n\n    Returns:\n        pd.DataFrame: The augmented DataFrame with metadata and derived metrics,\n                      or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import re\n\n    if data is None or data.empty:\n        return None\n\n    df = data.copy()\n\n    # --- Add Metadata Columns ---\n    # This mapping is based on the \"Corpus Design\" section of the experiment spec.\n    # Since no formal corpus manifest was provided, metadata is derived from filenames.\n    def get_year(name):\n        match = re.search(r'(\\d{4})', str(name))\n        return int(match.group(1)) if match else None\n\n    df['year'] = df['document_name'].apply(get_year)\n\n    time_period_map = {\n        2017: 'Early Presidential Period',\n        2018: 'Mid Presidential Period',\n        2020: 'Late Presidential Period',\n        2025: 'Recent Presidential Period'\n    }\n    df['time_period'] = df['year'].map(time_period_map)\n\n    def get_speech_type(name):\n        name_lower = str(name).lower()\n        if 'inaugural' in name_lower:\n            return 'Inaugural Address'\n        if 'sotu' in name_lower:\n            return 'State of the Union'\n        return 'Unknown'\n\n    df['speech_type'] = df['document_name'].apply(get_speech_type)\n\n    # --- Calculate Derived Metrics ---\n    # Rename columns for easier formula implementation\n    score_map = {\n        'tribal_dominance_raw': 'td_s', 'tribal_dominance_salience': 'td_sal',\n        'individual_dignity_raw': 'id_s', 'individual_dignity_salience': 'id_sal',\n        'fear_raw': 'fear_s', 'fear_salience': 'fear_sal',\n        'hope_raw': 'hope_s', 'hope_salience': 'hope_sal',\n        'envy_raw': 'envy_s', 'envy_salience': 'envy_sal',\n        'compersion_raw': 'comp_s', 'compersion_salience': 'comp_sal',\n        'enmity_raw': 'enmity_s', 'enmity_salience': 'enmity_sal',\n        'amity_raw': 'amity_s', 'amity_salience': 'amity_sal',\n        'fragmentative_goals_raw': 'fg_s', 'fragmentative_goals_salience': 'fg_sal',\n        'cohesive_goals_raw': 'cg_s', 'cohesive_goals_salience': 'cg_sal',\n    }\n    df.rename(columns=score_map, inplace=True)\n\n    # Tension Indices\n    df['identity_tension'] = np.minimum(df['td_s'], df['id_s']) * np.abs(df['td_sal'] - df['id_sal'])\n    df['emotional_tension'] = np.minimum(df['fear_s'], df['hope_s']) * np.abs(df['fear_sal'] - df['hope_sal'])\n    df['success_tension'] = np.minimum(df['envy_s'], df['comp_s']) * np.abs(df['envy_sal'] - df['comp_sal'])\n    df['relational_tension'] = np.minimum(df['enmity_s'], df['amity_s']) * np.abs(df['enmity_sal'] - df['amity_sal'])\n    df['goal_tension'] = np.minimum(df['fg_s'], df['cg_s']) * np.abs(df['fg_sal'] - df['cg_sal'])\n\n    # Strategic Contradiction Index\n    tension_cols = ['identity_tension', 'emotional_tension', 'success_tension', 'relational_tension', 'goal_tension']\n    df['strategic_contradiction_index'] = df[tension_cols].mean(axis=1)\n\n    # Cohesion Index Components\n    df['identity_cohesion_component'] = (df['id_s'] * df['id_sal'] - df['td_s'] * df['td_sal'])\n    df['emotional_cohesion_component'] = (df['hope_s'] * df['hope_sal'] - df['fear_s'] * df['fear_sal'])\n    df['success_cohesion_component'] = (df['comp_s'] * df['comp_sal'] - df['envy_s'] * df['envy_sal'])\n    df['relational_cohesion_component'] = (df['amity_s'] * df['amity_sal'] - df['enmity_s'] * df['enmity_sal'])\n    df['goal_cohesion_component'] = (df['cg_s'] * df['cg_sal'] - df['fg_s'] * df['fg_sal'])\n\n    # Salience Totals\n    epsilon = 0.001\n    df['descriptive_salience_total'] = (df['hope_sal'] + df['fear_sal'] + df['comp_sal'] + df['envy_sal'] + df['amity_sal'] + df['enmity_sal']) + epsilon\n    df['motivational_salience_total'] = (df['descriptive_salience_total'] - epsilon + df['cg_sal'] + df['fg_sal']) + epsilon\n    df['full_salience_total'] = (df['motivational_salience_total'] - epsilon + df['id_sal'] + df['td_sal']) + epsilon\n\n    # Final Cohesion Indices\n    df['descriptive_cohesion_index'] = (df['emotional_cohesion_component'] + df['success_cohesion_component'] + df['relational_cohesion_component']) / df['descriptive_salience_total']\n    df['motivational_cohesion_index'] = (df['emotional_cohesion_component'] + df['success_cohesion_component'] + df['relational_cohesion_component'] + df['goal_cohesion_component']) / df['motivational_salience_total']\n    df['full_cohesion_index'] = (df['identity_cohesion_component'] + df['emotional_cohesion_component'] + df['success_cohesion_component'] + df['relational_cohesion_component'] + df['goal_cohesion_component']) / df['full_salience_total']\n\n    # Revert column names for consistency\n    inv_score_map = {v: k for k, v in score_map.items()}\n    df.rename(columns=inv_score_map, inplace=True)\n\n    return df\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for all raw scores, salience scores,\n    and derived metrics from the Cohesive Flourishing Framework.\n\n    This function addresses RQ1: \"What patterns exist in discourse framing\n    dimensions across presidential speeches?\" by providing a quantitative\n    summary of all measured and calculated variables, grouped by document.\n\n    Methodology:\n    1. Augments the input data with metadata and derived metrics.\n    2. Groups the data by 'document_name'.\n    3. For each document, calculates the mean, standard deviation, min, and max\n       for each raw score, salience score, and derived metric across all\n       evaluations for that document.\n    \n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data with columns\n                             matching the framework specification.\n        **kwargs: Additional parameters (not used).\n        \n    Returns:\n        dict: A dictionary containing a pandas DataFrame with the descriptive\n              statistics. Returns None if data is insufficient or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty or 'document_name' not in data.columns:\n            return None\n\n        # Augment data with metadata and derived metrics\n        augmented_data = _add_metadata_and_derived_metrics(data)\n        if augmented_data is None:\n            return None\n\n        # Identify all numeric columns to be analyzed\n        numeric_cols = augmented_data.select_dtypes(include=np.number).columns.tolist()\n        # Exclude confidence scores and year from main aggregation\n        cols_to_exclude = [c for c in augmented_data.columns if 'confidence' in c] + ['year']\n        analysis_cols = [c for c in numeric_cols if c not in cols_to_exclude]\n\n        # Group by document and calculate descriptive statistics\n        descriptives = augmented_data.groupby('document_name')[analysis_cols].agg(['mean', 'std', 'min', 'max'])\n        \n        if descriptives.empty:\n            return None\n\n        return {\"descriptive_statistics\": descriptives.to_dict()}\n\n    except Exception as e:\n        # In a real environment, you might log the error e\n        return None\n\ndef analyze_temporal_patterns(data, **kwargs):\n    \"\"\"\n    Analyzes temporal patterns in framing dimensions and strategic measures\n    using one-way ANOVA.\n\n    This function addresses RQ2: \"What relationships exist between temporal\n    factors and strategic framing measures?\" by testing for significant\n    differences in key metrics across different time periods.\n\n    Methodology:\n    1. Augments data with metadata (including 'time_period') and derived metrics.\n    2. Averages the scores for each document across multiple evaluations to get a\n       single value per document.\n    3. For a predefined set of key metrics (raw scores and derived indices),\n       it performs a one-way ANOVA with 'time_period' as the independent variable.\n    4. Levene's test is performed to check for homogeneity of variances.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n        \n    Returns:\n        dict: A dictionary where keys are the tested metrics and values are\n              dictionaries containing the F-statistic, p-value, and Levene's\n              test results. Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n    from scipy.stats import f_oneway, levene\n\n    try:\n        if data is None or data.empty or 'document_name' not in data.columns:\n            return None\n\n        augmented_data = _add_metadata_and_derived_metrics(data)\n        if augmented_data is None or 'time_period' not in augmented_data.columns:\n            return None\n        \n        # Average scores across evaluations for each document\n        doc_avg_data = augmented_data.groupby('document_name').mean(numeric_only=True).reset_index()\n        \n        # Merge metadata back after averaging\n        metadata_cols = ['document_name', 'time_period', 'speech_type']\n        doc_avg_data = pd.merge(doc_avg_data, augmented_data[metadata_cols].drop_duplicates(), on='document_name')\n\n        if doc_avg_data['time_period'].nunique() < 2:\n            return None # Not enough groups to compare\n\n        # Define key metrics for temporal analysis\n        metrics_to_test = [\n            'tribal_dominance_raw', 'individual_dignity_raw', 'fear_raw', 'hope_raw',\n            'envy_raw', 'compersion_raw', 'enmity_raw', 'amity_raw',\n            'fragmentative_goals_raw', 'cohesive_goals_raw',\n            'strategic_contradiction_index', 'full_cohesion_index'\n        ]\n        \n        results = {}\n        \n        for metric in metrics_to_test:\n            if metric not in doc_avg_data.columns:\n                continue\n            \n            groups = [\n                group[metric].dropna() for name, group in doc_avg_data.groupby('time_period')\n            ]\n            \n            # Ensure there are at least two groups with data to compare\n            valid_groups = [g for g in groups if len(g) > 0]\n            if len(valid_groups) < 2:\n                continue\n\n            # Levene's test for homogeneity of variances\n            levene_stat, levene_p = levene(*valid_groups)\n            \n            # One-way ANOVA\n            f_stat, p_value = f_oneway(*valid_groups)\n            \n            results[metric] = {\n                'levene_test': {'statistic': levene_stat, 'p_value': levene_p},\n                'anova': {'f_statistic': f_stat, 'p_value': p_value}\n            }\n            \n        return results if results else None\n\n    except Exception as e:\n        return None\n\ndef analyze_speech_context_effects(data, **kwargs):\n    \"\"\"\n    Analyzes how framing patterns vary across speech contexts (Inaugural vs. SOTU)\n    using independent samples t-tests.\n\n    This function addresses RQ3: \"How do framing patterns vary across different\n    speech contexts and types?\" by comparing key metrics between the two speech types.\n\n    Methodology:\n    1. Augments data with metadata (including 'speech_type') and derived metrics.\n    2. Averages scores for each document across multiple evaluations.\n    3. For key metrics, it performs an independent samples t-test with 'speech_type'\n       as the grouping variable.\n    4. Levene's test is performed to check for homogeneity of variances, which\n       informs the 'equal_var' parameter of the t-test.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n        \n    Returns:\n        dict: A dictionary of t-test results for each tested metric, including\n              the t-statistic and p-value. Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n    from scipy.stats import ttest_ind, levene\n\n    try:\n        if data is None or data.empty or 'document_name' not in data.columns:\n            return None\n\n        augmented_data = _add_metadata_and_derived_metrics(data)\n        if augmented_data is None or 'speech_type' not in augmented_data.columns:\n            return None\n\n        doc_avg_data = augmented_data.groupby('document_name').mean(numeric_only=True).reset_index()\n        metadata_cols = ['document_name', 'speech_type']\n        doc_avg_data = pd.merge(doc_avg_data, augmented_data[metadata_cols].drop_duplicates(), on='document_name')\n\n        speech_types = doc_avg_data['speech_type'].unique()\n        if len(speech_types) != 2 or 'Inaugural Address' not in speech_types or 'State of the Union' not in speech_types:\n            return None # Requires exactly two specific groups to compare\n\n        group1 = doc_avg_data[doc_avg_data['speech_type'] == 'Inaugural Address']\n        group2 = doc_avg_data[doc_avg_data['speech_type'] == 'State of the Union']\n\n        metrics_to_test = [\n            'tribal_dominance_raw', 'individual_dignity_raw', 'fear_raw', 'hope_raw',\n            'envy_raw', 'compersion_raw', 'enmity_raw', 'amity_raw',\n            'fragmentative_goals_raw', 'cohesive_goals_raw',\n            'strategic_contradiction_index', 'full_cohesion_index'\n        ]\n        \n        results = {}\n        \n        for metric in metrics_to_test:\n            if metric not in doc_avg_data.columns:\n                continue\n            \n            g1_data = group1[metric].dropna()\n            g2_data = group2[metric].dropna()\n\n            if len(g1_data) < 2 or len(g2_data) < 2:\n                continue\n\n            # Levene's test to decide if variances are equal\n            levene_stat, levene_p = levene(g1_data, g2_data)\n            equal_variances = levene_p > 0.05\n\n            # Independent t-test\n            t_stat, p_value = ttest_ind(g1_data, g2_data, equal_var=equal_variances, nan_policy='omit')\n            \n            results[metric] = {\n                'levene_p_value': levene_p,\n                'equal_variances_assumed': equal_variances,\n                't_statistic': t_stat,\n                'p_value': p_value\n            }\n            \n        return results if results else None\n\n    except Exception as e:\n        return None\n\ndef analyze_strategic_tension_and_correlations(data, **kwargs):\n    \"\"\"\n    Analyzes relationships between opposing dimensional axes and strategic tension\n    measures using a Pearson correlation matrix.\n\n    This function addresses RQ4: \"What statistical relationships exist between\n    opposing dimensional axes and strategic tension measures?\"\n\n    Methodology:\n    1. Augments data with derived metrics (tension indices, SCI, cohesion indices).\n    2. Averages scores for each document across multiple evaluations.\n    3. Selects all raw scores and key derived metrics.\n    4. Computes a Pearson correlation matrix for these variables to reveal\n       inter-relationships, such as how raw scores relate to tension.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n        \n    Returns:\n        dict: A dictionary containing the correlation matrix. Returns None if\n              data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty or 'document_name' not in data.columns:\n            return None\n\n        augmented_data = _add_metadata_and_derived_metrics(data)\n        if augmented_data is None:\n            return None\n\n        doc_avg_data = augmented_data.groupby('document_name').mean(numeric_only=True).reset_index()\n\n        # Select columns for correlation analysis\n        correlation_cols = [\n            'tribal_dominance_raw', 'individual_dignity_raw', 'fear_raw', 'hope_raw',\n            'envy_raw', 'compersion_raw', 'enmity_raw', 'amity_raw',\n            'fragmentative_goals_raw', 'cohesive_goals_raw',\n            'identity_tension', 'emotional_tension', 'success_tension',\n            'relational_tension', 'goal_tension', 'strategic_contradiction_index',\n            'descriptive_cohesion_index', 'motivational_cohesion_index', 'full_cohesion_index'\n        ]\n        \n        # Ensure columns exist\n        valid_cols = [col for col in correlation_cols if col in doc_avg_data.columns]\n        if len(valid_cols) < 2:\n            return None\n\n        correlation_matrix = doc_avg_data[valid_cols].corr(method='pearson')\n        \n        if correlation_matrix.empty:\n            return None\n\n        # Convert to dict for JSON serialization, replacing NaNs\n        return {\"correlation_matrix\": correlation_matrix.replace({np.nan: None}).to_dict()}\n\n    except Exception as e:\n        return None\n\ndef calculate_multi_evaluation_reliability(data, **kwargs):\n    \"\"\"\n    Calculates the multi-evaluation reliability for each dimension and derived\n    metric using Cronbach's Alpha.\n\n    This function addresses the \"Multi-evaluation reliability\" and \"Reliability\n    analysis\" requirements of the experiment. It measures the consistency of\n    scores across the multiple evaluations performed for each document.\n\n    Methodology:\n    1. Augments data with derived metrics.\n    2. For each metric, it creates a pivot table with documents as rows and\n       evaluators (raters) as columns.\n    3. It calculates Cronbach's Alpha for each metric, which indicates the\n       internal consistency of the ratings. A higher alpha (typically > 0.7)\n       suggests good reliability.\n    4. Requires the `pingouin` library.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n        \n    Returns:\n        dict: A dictionary where keys are the metrics and values are their\n              Cronbach's Alpha scores. Returns None if data is insufficient\n              or the pingouin library is not installed.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n    \n    try:\n        import pingouin as pg\n    except ImportError:\n        # pingouin is required for this function.\n        # In a real environment, this might raise an error or log a warning.\n        return {\"error\": \"pingouin library not installed. Please install it via 'pip install pingouin'.\"}\n\n    try:\n        if data is None or data.empty or 'document_name' not in data.columns:\n            return None\n\n        augmented_data = _add_metadata_and_derived_metrics(data)\n        if augmented_data is None:\n            return None\n\n        # Add a 'rater' ID for each evaluation within a document group\n        augmented_data['rater'] = augmented_data.groupby('document_name').cumcount()\n\n        # Check if there are multiple raters\n        if augmented_data['rater'].max() < 1:\n            return {\"message\": \"Insufficient data for reliability analysis (only one rater found).\"}\n\n        metrics_to_test = [\n            'tribal_dominance_raw', 'individual_dignity_raw', 'fear_raw', 'hope_raw',\n            'envy_raw', 'compersion_raw', 'enmity_raw', 'amity_raw',\n            'fragmentative_goals_raw', 'cohesive_goals_raw',\n            'tribal_dominance_salience', 'individual_dignity_salience', 'fear_salience',\n            'hope_salience', 'envy_salience', 'compersion_salience', 'enmity_salience',\n            'amity_salience', 'fragmentative_goals_salience', 'cohesive_goals_salience',\n            'strategic_contradiction_index', 'full_cohesion_index'\n        ]\n        \n        reliability_scores = {}\n\n        for metric in metrics_to_test:\n            if metric not in augmented_data.columns:\n                continue\n            \n            # Create a wide-format DataFrame: documents x raters\n            pivot_df = augmented_data.pivot(index='document_name', columns='rater', values=metric)\n            \n            # Drop documents that weren't rated by all raters\n            pivot_df.dropna(inplace=True)\n            \n            if pivot_df.shape[0] < 2 or pivot_df.shape[1] < 2:\n                # Need at least 2 items and 2 raters\n                reliability_scores[metric] = None\n                continue\n            \n            # Calculate Cronbach's Alpha\n            alpha = pg.cronbach_alpha(data=pivot_df)\n            reliability_scores[metric] = alpha[0] # alpha[0] is the alpha value\n\n        return reliability_scores if reliability_scores else None\n\n    except Exception as e:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}