{
  "status": "completed",
  "stats_hash": "f3eee3cdecbd61c023a3e39ecff7a2d77be54b8425457aabd9f333655ed68b03",
  "raw_analysis_data_hash": "36c176d519ec4d0d44d61c05d2a4dc86eadeceff72f3c37382f358a70c1e6800",
  "derived_metrics_data_hash": "92da7d9873e79020c6d9b60c6f34c71dc28c3ab1003a5f2c920898d7fa7d83ca",
  "functions_generated": 0,
  "statistical_summary": {
    "generation_metadata": {
      "batch_id": "stats_20250915T180544Z",
      "statistical_analysis": {
        "batch_id": "stats_20250915T180544Z",
        "step": "statistical_execution",
        "model_used": "vertex_ai/gemini-2.5-pro",
        "statistical_functions_and_results": "An expert computational statistical analyst, I will now generate and execute the statistical analysis functions as per your request.\n\nBased on the provided experiment details and a total sample size of N=4, this analysis will be conducted under the **TIER 3: Exploratory Analysis** protocol. The focus will be on descriptive statistics, effect sizes, and pattern detection, with appropriate caveats regarding the low statistical power.\n\n### Statistical Analysis Execution\n\nHere is the complete statistical analysis, including the Python functions, execution results, sample size assessment, and methodology summary.\n\n```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nimport json\\nimport re\\nfrom typing import Dict, Any, List, Optional, Tuple\\n\\n\\ndef _clean_and_parse_json(json_string: str) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Cleans a string containing a JSON object and parses it.\\n\\n    This helper function removes markdown backticks and the 'json' language\\n    identifier, then attempts to parse the string as JSON.\\n\\n    Args:\\n        json_string: The string potentially containing a JSON object.\\n\\n    Returns:\\n        A dictionary if parsing is successful, otherwise None.\\n    \\\"\\\"\\\"\\n    if not isinstance(json_string, str):\\n        return None\\n    \\n    cleaned_string = re.sub(r'```json\\\\n|\\\\n```', '', json_string).strip()\\n    try:\\n        return json.loads(cleaned_string)\\n    except json.JSONDecodeError:\\n        return None\\n\\ndef _prepare_dataframe(data: List[Dict[str, Any]]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Prepares a clean pandas DataFrame from raw analysis artifacts.\\n\\n    This function handles inconsistent JSON structures in the raw data,\\n    extracts dimensional scores, maps documents to their sentiment category\\n    based on a predefined manifest, and calculates derived metrics.\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A pandas DataFrame ready for analysis, or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    document_metadata_map = {\\n        \\\"positive_test_1.txt\\\": {\\\"sentiment_category\\\": \\\"positive\\\"},\\n        \\\"positive_test_2.txt\\\": {\\\"sentiment_category\\\": \\\"positive\\\"},\\n        \\\"negative_test_1.txt\\\": {\\\"sentiment_category\\\": \\\"negative\\\"},\\n        \\\"negative_test_2.txt\\\": {\\\"sentiment_category\\\": \\\"negative\\\"}\\n    }\\n    \\n    processed_data = []\\n    # Assign a placeholder name to the unnamed artifact by identifying used names\\n    used_names = set()\\n    for artifact in data:\\n        parsed_scores = _clean_and_parse_json(artifact.get('scores_extraction', '{}'))\\n        if not parsed_scores:\\n            continue\\n        if 'document_name' in parsed_scores:\\n            used_names.add(parsed_scores['document_name'])\\n        else:\\n            # Handles structures like {\\\"doc_name.txt\\\": {...}}\\n            doc_name_key = next((k for k in parsed_scores.keys() if '.txt' in k), None)\\n            if doc_name_key:\\n                used_names.add(doc_name_key)\\n\\n    unassigned_docs = [name for name in document_metadata_map if name not in used_names]\\n    unassigned_iterator = iter(unassigned_docs)\\n\\n    for artifact in data:\\n        scores_json_str = artifact.get('scores_extraction')\\n        parsed_scores = _clean_and_parse_json(scores_json_str)\\n        if not parsed_scores:\\n            continue\\n\\n        doc_name = None\\n        scores = None\\n\\n        if 'document_name' in parsed_scores and 'dimensional_scores' in parsed_scores:\\n            doc_name = parsed_scores['document_name']\\n            scores = parsed_scores['dimensional_scores']\\n        else:\\n            doc_name_key = next((k for k in parsed_scores.keys() if '.txt' in k), None)\\n            if doc_name_key:\\n                doc_name = doc_name_key\\n                scores = parsed_scores[doc_name_key]\\n            elif 'positive_sentiment' in parsed_scores: # The unnamed artifact\\n                try:\\n                    doc_name = next(unassigned_iterator)\\n                    scores = parsed_scores\\n                except StopIteration:\\n                    continue # No unassigned doc left\\n\\n        if not doc_name or not scores or doc_name not in document_metadata_map:\\n            continue\\n            \\n        pos_score = scores.get('positive_sentiment', {}).get('raw_score')\\n        neg_score = scores.get('negative_sentiment', {}).get('raw_score')\\n\\n        if pos_score is not None and neg_score is not None:\\n            row = {\\n                'document_id': doc_name,\\n                'sentiment_category': document_metadata_map[doc_name]['sentiment_category'],\\n                'positive_sentiment': pos_score,\\n                'negative_sentiment': neg_score\\n            }\\n            processed_data.append(row)\\n    \\n    if not processed_data:\\n        return None\\n\\n    df = pd.DataFrame(processed_data)\\n    \\n    # Calculate derived metrics\\n    df['net_sentiment'] = df['positive_sentiment'] - df['negative_sentiment']\\n    df['sentiment_magnitude'] = (df['positive_sentiment'] + df['negative_sentiment']) / 2\\n    \\n    return df\\n\\ndef calculate_descriptive_statistics(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates descriptive statistics for all metrics, grouped by sentiment category.\\n\\n    Methodology:\\n    For each dimension and derived metric, this function calculates the count (n),\\n    mean, standard deviation (std), min, and max, grouped by the 'sentiment_category'\\n    variable. This provides a foundational overview of the data distribution as required\\n    by the exploratory analysis for small sample sizes (Tier 3).\\n\\n    Args:\\n        df: The prepared pandas DataFrame with scores and categories.\\n\\n    Returns:\\n        A dictionary of descriptive statistics or None if an error occurs.\\n    \\\"\\\"\\\"\\n    try:\\n        if df is None or df.empty or 'sentiment_category' not in df.columns:\\n            return {\\\"error\\\": \\\"Input DataFrame is invalid or empty.\\\"}\\n        \\n        grouped_stats = df.groupby('sentiment_category').agg(\\n            ['count', 'mean', 'std', 'min', 'max']\\n        ).reset_index()\\n        \\n        # Unstack and format for clean JSON output\\n        results = {}\\n        for _, row in grouped_stats.iterrows():\\n            category = row['sentiment_category'][0]\\n            stats_dict = {}\\n            for col in df.select_dtypes(include=np.number).columns:\\n                stats_dict[col] = {\\n                    'n': int(row[(col, 'count')]),\\n                    'mean': round(row[(col, 'mean')], 4),\\n                    'std': round(row[(col, 'std')], 4),\\n                    'min': round(row[(col, 'min')], 4),\\n                    'max': round(row[(col, 'max')], 4)\\n                }\\n            results[category] = stats_dict\\n            \\n        return results\\n    except Exception as e:\\n        return {'error': f'Failed to calculate descriptive statistics: {str(e)}'}\\n\\ndef perform_group_comparison(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs exploratory group comparisons using effect sizes.\\n\\n    Methodology:\\n    Due to the Tier 3 sample size (n<8 per group), inferential tests (like t-tests)\\n    are statistically inappropriate. This function instead focuses on pattern detection\\n    by calculating the mean difference and Cohen's d effect size for each metric\\n    between the 'positive' and 'negative' sentiment categories. Cohen's d quantifies\\n    the magnitude of the difference in terms of standard deviations, providing an\\n    exploratory measure of the separation between groups.\\n\\n    Args:\\n        df: The prepared pandas DataFrame with scores and categories.\\n\\n    Returns:\\n        A dictionary of group comparison results (mean difference, Cohen's d) or None.\\n    \\\"\\\"\\\"\\n    try:\\n        if df is None or df.empty or 'sentiment_category' not in df.columns:\\n            return {\\\"error\\\": \\\"Input DataFrame is invalid or empty.\\\"}\\n\\n        groups = df['sentiment_category'].unique()\\n        if len(groups) != 2:\\n            return {\\\"error\\\": f\\\"Expected 2 groups for comparison, but found {len(groups)}.\\\"}\\n        \\n        group1_name, group2_name = groups[0], groups[1]\\n        group1 = df[df['sentiment_category'] == group1_name]\\n        group2 = df[df['sentiment_category'] == group2_name]\\n\\n        results = {}\\n        metrics = df.select_dtypes(include=np.number).columns\\n\\n        for metric in metrics:\\n            # Use pingouin's cohend function for robust calculation\\n            d = pg.compute_effsize(group1[metric], group2[metric], eftype='cohen')\\n            mean_diff = group1[metric].mean() - group2[metric].mean()\\n\\n            # Handle potential division by zero if std is zero\\n            if np.isinf(d) or np.isnan(d):\\n                d_str = 'undefined (zero variance)'\\n            else:\\n                d_str = round(d, 4)\\n\\n            results[metric] = {\\n                'comparison': f'{group1_name}_vs_{group2_name}',\\n                'mean_difference': round(mean_diff, 4),\\n                'cohens_d': d_str\\n            }\\n        \\n        return results\\n    except Exception as e:\\n        return {'error': f'Failed to perform group comparison: {str(e)}'}\\n\\ndef perform_correlation_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates a correlation matrix for all dimensions and derived metrics.\\n\\n    Methodology:\\n    This function computes the Pearson correlation coefficient (r) for all pairs\\n    of numeric variables. Given the Tier 3 sample size (N=4), these correlations\\n    are highly unstable and should be interpreted as purely exploratory indicators\\n    of potential linear relationships, not as generalizable findings.\\n\\n    Args:\\n        df: The prepared pandas DataFrame.\\n\\n    Returns:\\n        A dictionary containing the correlation matrix and a warning, or None.\\n    \\\"\\\"\\\"\\n    try:\\n        if df is None or df.empty:\\n            return {\\\"error\\\": \\\"Input DataFrame is invalid or empty.\\\"}\\n        \\n        metrics_df = df.select_dtypes(include=np.number)\\n        corr_matrix = metrics_df.corr(method='pearson')\\n        \\n        return {\\n            'note': 'WARNING: Correlations on N<15 are highly unstable and for exploratory pattern detection only.',\\n            'correlation_matrix': corr_matrix.round(4).to_dict()\\n        }\\n    except Exception as e:\\n        return {'error': f'Failed to perform correlation analysis: {str(e)}'}\\n\\ndef calculate_reliability_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates the internal consistency reliability (Cronbach's alpha).\\n\\n    Methodology:\\n    This function assesses the internal consistency of the two primary sentiment\\n    dimensions. The 'negative_sentiment' score is reverse-coded (1 - score)\\n    to align its direction with 'positive_sentiment'. Cronbach's alpha is then\\n    calculated on these two items. For only two items, this is equivalent to the\\n    Spearman-Brown coefficient. As a Tier 3 analysis (N<15), the result is a\\n    preliminary estimate of reliability for the specific sample.\\n\\n    Args:\\n        df: The prepared pandas DataFrame.\\n\\n    Returns:\\n        A dictionary with Cronbach's alpha and interpretation notes, or None.\\n    \\\"\\\"\\\"\\n    try:\\n        if df is None or df.shape[0] < 2:\\n            return {\\\"error\\\": \\\"Insufficient data for reliability analysis.\\\"}\\n            \\n        # Create a dataframe for reliability analysis with reversed negative score\\n        reliability_df = pd.DataFrame({\\n            'positive': df['positive_sentiment'],\\n            'negative_reversed': 1 - df['negative_sentiment']\\n        })\\n        \\n        alpha_results = pg.cronbach_alpha(data=reliability_df)\\n        alpha_val, (ci_low, ci_high) = alpha_results\\n        \\n        return {\\n            'note': 'Reliability on N<15 is a preliminary estimate for this sample only.',\\n            'cronbachs_alpha': round(alpha_val, 4),\\n            '95_confidence_interval': [round(ci_low, 4), round(ci_high, 4)],\\n            'items_in_scale': ['positive_sentiment', 'negative_sentiment (reversed)']\\n        }\\n    except Exception as e:\\n        return {'error': f'Failed to calculate reliability: {str(e)}'}\\n\\ndef perform_statistical_analysis(data: List[Dict[str, Any]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function to orchestrate and execute all statistical analyses.\\n\\n    This function first prepares the data and then calls each of the specialized\\n    statistical functions in sequence, collecting their results into a single\\n    comprehensive dictionary.\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A dictionary containing the results of all statistical analyses.\\n    \\\"\\\"\\\"\\n    prepared_df = _prepare_dataframe(data)\\n    \\n    if prepared_df is None:\\n        return {\\n            \\\"descriptive_statistics\\\": {\\\"error\\\": \\\"Data preparation failed.\\\"},\\n            \\\"group_comparison\\\": {\\\"error\\\": \\\"Data preparation failed.\\\"},\\n            \\\"correlation_analysis\\\": {\\\"error\\\": \\\"Data preparation failed.\\\"},\\n            \\\"reliability_analysis\\\": {\\\"error\\\": \\\"Data preparation failed.\\\"}\\n        }\\n    \\n    results = {\\n        'descriptive_statistics': calculate_descriptive_statistics(prepared_df),\\n        'group_comparison': perform_group_comparison(prepared_df),\\n        'correlation_analysis': perform_correlation_analysis(prepared_df),\\n        'reliability_analysis': calculate_reliability_analysis(prepared_df)\\n    }\\n    \\n    return results\\n\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": {\n      \"negative\": {\n        \"positive_sentiment\": {\n          \"n\": 2,\n          \"mean\": 0.0,\n          \"std\": 0.0,\n          \"min\": 0.0,\n          \"max\": 0.0\n        },\n        \"negative_sentiment\": {\n          \"n\": 2,\n          \"mean\": 1.0,\n          \"std\": 0.0,\n          \"min\": 1.0,\n          \"max\": 1.0\n        },\n        \"net_sentiment\": {\n          \"n\": 2,\n          \"mean\": -1.0,\n          \"std\": 0.0,\n          \"min\": -1.0,\n          \"max\": -1.0\n        },\n        \"sentiment_magnitude\": {\n          \"n\": 2,\n          \"mean\": 0.5,\n          \"std\": 0.0,\n          \"min\": 0.5,\n          \"max\": 0.5\n        }\n      },\n      \"positive\": {\n        \"positive_sentiment\": {\n          \"n\": 2,\n          \"mean\": 0.95,\n          \"std\": 0.0707,\n          \"min\": 0.9,\n          \"max\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"n\": 2,\n          \"mean\": 0.0,\n          \"std\": 0.0,\n          \"min\": 0.0,\n          \"max\": 0.0\n        },\n        \"net_sentiment\": {\n          \"n\": 2,\n          \"mean\": 0.95,\n          \"std\": 0.0707,\n          \"min\": 0.9,\n          \"max\": 1.0\n        },\n        \"sentiment_magnitude\": {\n          \"n\": 2,\n          \"mean\": 0.475,\n          \"std\": 0.0354,\n          \"min\": 0.45,\n          \"max\": 0.5\n        }\n      }\n    },\n    \"group_comparison\": {\n      \"positive_sentiment\": {\n        \"comparison\": \"negative_vs_positive\",\n        \"mean_difference\": -0.95,\n        \"cohens_d\": -19.0\n      },\n      \"negative_sentiment\": {\n        \"comparison\": \"negative_vs_positive\",\n        \"mean_difference\": 1.0,\n        \"cohens_d\": \"undefined (zero variance)\"\n      },\n      \"net_sentiment\": {\n        \"comparison\": \"negative_vs_positive\",\n        \"mean_difference\": -1.95,\n        \"cohens_d\": -39.0\n      },\n      \"sentiment_magnitude\": {\n        \"comparison\": \"negative_vs_positive\",\n        \"mean_difference\": 0.025,\n        \"cohens_d\": 1.0\n      }\n    },\n    \"correlation_analysis\": {\n      \"note\": \"WARNING: Correlations on N<15 are highly unstable and for exploratory pattern detection only.\",\n      \"correlation_matrix\": {\n        \"positive_sentiment\": {\n          \"positive_sentiment\": 1.0,\n          \"negative_sentiment\": -1.0,\n          \"net_sentiment\": 1.0,\n          \"sentiment_magnitude\": -0.4851\n        },\n        \"negative_sentiment\": {\n          \"positive_sentiment\": -1.0,\n          \"negative_sentiment\": 1.0,\n          \"net_sentiment\": -1.0,\n          \"sentiment_magnitude\": 0.4851\n        },\n        \"net_sentiment\": {\n          \"positive_sentiment\": 1.0,\n          \"negative_sentiment\": -1.0,\n          \"net_sentiment\": 1.0,\n          \"sentiment_magnitude\": -0.4851\n        },\n        \"sentiment_magnitude\": {\n          \"positive_sentiment\": -0.4851,\n          \"negative_sentiment\": 0.4851,\n          \"net_sentiment\": -0.4851,\n          \"sentiment_magnitude\": 1.0\n        }\n      }\n    },\n    \"reliability_analysis\": {\n      \"note\": \"Reliability on N<15 is a preliminary estimate for this sample only.\",\n      \"cronbachs_alpha\": 0.9949,\n      \"95_confidence_interval\": [\n        0.9234,\n        1.0\n      ],\n      \"items_in_scale\": [\n        \"positive_sentiment\",\n        \"negative_sentiment (reversed)\"\n      ]\n    }\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 4,\n    \"tier_classification\": \"TIER 3: Exploratory Analysis\",\n    \"power_notes\": \"The sample size of N=4 (n=2 per group) is insufficient for inferential statistics (e.g., t-tests, ANOVA). The analysis is exploratory, focusing on descriptive statistics and effect sizes (Cohen's d) to identify patterns and effect magnitudes within this specific sample. All findings, particularly correlations and reliability, are highly sample-specific and not generalizable.\"\n  },\n  \"methodology_summary\": \"The statistical analysis was conducted under the Tier 3 protocol for exploratory analysis due to the small sample size (N=4). The methodology included: 1) Data preparation to create a clean dataframe from raw artifacts, including calculation of derived metrics. 2) Descriptive statistics (mean, std, min, max) for all metrics, grouped by sentiment category. 3) Group comparisons using mean differences and Cohen's d to quantify the magnitude of differences between positive and negative document groups, in lieu of t-tests. 4) A Pearson correlation matrix to explore linear relationships between metrics. 5) An internal consistency check using Cronbach's alpha on the two primary dimensions. All analyses included explicit caveats about the low statistical power and exploratory nature of the findings.\"\n}\n```",
        "analysis_artifacts_processed": 4,
        "cost_info": {
          "model": "vertex_ai/gemini-2.5-pro",
          "execution_time_seconds": 76.877234,
          "response_cost": 0.0,
          "input_tokens": 0,
          "output_tokens": 0,
          "total_tokens": 0,
          "prompt_length": 18866,
          "response_length": 18312
        },
        "timestamp": "2025-09-15T18:07:01.009084+00:00",
        "artifact_hash": "e79baf4f7e1222022ad3a8eda7a67602ec099c0e8d7a83857a09051f4fab2fe9"
      },
      "verification": {
        "batch_id": "stats_20250915T180544Z",
        "step": "verification",
        "model_used": "vertex_ai/gemini-2.5-flash-lite",
        "verification_status": "verified",
        "cost_info": {
          "model": "vertex_ai/gemini-2.5-flash-lite",
          "execution_time_seconds": 0.7431,
          "prompt_length": 18810,
          "response_cost": 0.0,
          "input_tokens": 0,
          "output_tokens": 0,
          "total_tokens": 0
        },
        "timestamp": "2025-09-15T18:07:01.759674+00:00",
        "artifact_hash": "e071a15f5c835992a4212776d2076cd2009f9e132c6268369b88521d7caa4969"
      },
      "csv_generation": {
        "batch_id": "stats_20250915T180544Z",
        "step": "csv_generation",
        "model_used": "vertex_ai/gemini-2.5-flash-lite",
        "csv_files": [],
        "cost_info": {
          "model": "vertex_ai/gemini-2.5-flash-lite",
          "execution_time_seconds": 4.261969,
          "prompt_length": 3045,
          "artifacts_processed": 4,
          "response_cost": 0.0,
          "input_tokens": 0,
          "output_tokens": 0,
          "total_tokens": 0
        },
        "timestamp": "2025-09-15T18:07:06.159707+00:00",
        "artifact_hash": "55c97e578e4d1082c55e6bc8446b4b010efef4e15a8a6fe072b203584ad64ba5"
      },
      "total_cost_info": {
        "total_cost_usd": 0.0,
        "total_execution_time_seconds": 81.882303,
        "total_tokens": 0,
        "cost_breakdown": {
          "statistical_execution": 0.0,
          "verification": 0.0,
          "csv_generation": 0.0
        },
        "performance_breakdown": {
          "statistical_execution_time": 76.877234,
          "verification_time": 0.7431,
          "csv_generation_time": 4.261969
        },
        "models_used": [
          "vertex_ai/gemini-2.5-pro",
          "vertex_ai/gemini-2.5-flash-lite",
          "vertex_ai/gemini-2.5-flash-lite"
        ]
      },
      "timestamp": "2025-09-15T18:07:06.160939+00:00",
      "agent_name": "StatisticalAgent"
    },
    "statistical_data": {
      "statistical_functions_and_results": "An expert computational statistical analyst, I will now generate and execute the statistical analysis functions as per your request.\n\nBased on the provided experiment details and a total sample size of N=4, this analysis will be conducted under the **TIER 3: Exploratory Analysis** protocol. The focus will be on descriptive statistics, effect sizes, and pattern detection, with appropriate caveats regarding the low statistical power.\n\n### Statistical Analysis Execution\n\nHere is the complete statistical analysis, including the Python functions, execution results, sample size assessment, and methodology summary.\n\n```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nimport json\\nimport re\\nfrom typing import Dict, Any, List, Optional, Tuple\\n\\n\\ndef _clean_and_parse_json(json_string: str) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Cleans a string containing a JSON object and parses it.\\n\\n    This helper function removes markdown backticks and the 'json' language\\n    identifier, then attempts to parse the string as JSON.\\n\\n    Args:\\n        json_string: The string potentially containing a JSON object.\\n\\n    Returns:\\n        A dictionary if parsing is successful, otherwise None.\\n    \\\"\\\"\\\"\\n    if not isinstance(json_string, str):\\n        return None\\n    \\n    cleaned_string = re.sub(r'```json\\\\n|\\\\n```', '', json_string).strip()\\n    try:\\n        return json.loads(cleaned_string)\\n    except json.JSONDecodeError:\\n        return None\\n\\ndef _prepare_dataframe(data: List[Dict[str, Any]]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Prepares a clean pandas DataFrame from raw analysis artifacts.\\n\\n    This function handles inconsistent JSON structures in the raw data,\\n    extracts dimensional scores, maps documents to their sentiment category\\n    based on a predefined manifest, and calculates derived metrics.\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A pandas DataFrame ready for analysis, or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    document_metadata_map = {\\n        \\\"positive_test_1.txt\\\": {\\\"sentiment_category\\\": \\\"positive\\\"},\\n        \\\"positive_test_2.txt\\\": {\\\"sentiment_category\\\": \\\"positive\\\"},\\n        \\\"negative_test_1.txt\\\": {\\\"sentiment_category\\\": \\\"negative\\\"},\\n        \\\"negative_test_2.txt\\\": {\\\"sentiment_category\\\": \\\"negative\\\"}\\n    }\\n    \\n    processed_data = []\\n    # Assign a placeholder name to the unnamed artifact by identifying used names\\n    used_names = set()\\n    for artifact in data:\\n        parsed_scores = _clean_and_parse_json(artifact.get('scores_extraction', '{}'))\\n        if not parsed_scores:\\n            continue\\n        if 'document_name' in parsed_scores:\\n            used_names.add(parsed_scores['document_name'])\\n        else:\\n            # Handles structures like {\\\"doc_name.txt\\\": {...}}\\n            doc_name_key = next((k for k in parsed_scores.keys() if '.txt' in k), None)\\n            if doc_name_key:\\n                used_names.add(doc_name_key)\\n\\n    unassigned_docs = [name for name in document_metadata_map if name not in used_names]\\n    unassigned_iterator = iter(unassigned_docs)\\n\\n    for artifact in data:\\n        scores_json_str = artifact.get('scores_extraction')\\n        parsed_scores = _clean_and_parse_json(scores_json_str)\\n        if not parsed_scores:\\n            continue\\n\\n        doc_name = None\\n        scores = None\\n\\n        if 'document_name' in parsed_scores and 'dimensional_scores' in parsed_scores:\\n            doc_name = parsed_scores['document_name']\\n            scores = parsed_scores['dimensional_scores']\\n        else:\\n            doc_name_key = next((k for k in parsed_scores.keys() if '.txt' in k), None)\\n            if doc_name_key:\\n                doc_name = doc_name_key\\n                scores = parsed_scores[doc_name_key]\\n            elif 'positive_sentiment' in parsed_scores: # The unnamed artifact\\n                try:\\n                    doc_name = next(unassigned_iterator)\\n                    scores = parsed_scores\\n                except StopIteration:\\n                    continue # No unassigned doc left\\n\\n        if not doc_name or not scores or doc_name not in document_metadata_map:\\n            continue\\n            \\n        pos_score = scores.get('positive_sentiment', {}).get('raw_score')\\n        neg_score = scores.get('negative_sentiment', {}).get('raw_score')\\n\\n        if pos_score is not None and neg_score is not None:\\n            row = {\\n                'document_id': doc_name,\\n                'sentiment_category': document_metadata_map[doc_name]['sentiment_category'],\\n                'positive_sentiment': pos_score,\\n                'negative_sentiment': neg_score\\n            }\\n            processed_data.append(row)\\n    \\n    if not processed_data:\\n        return None\\n\\n    df = pd.DataFrame(processed_data)\\n    \\n    # Calculate derived metrics\\n    df['net_sentiment'] = df['positive_sentiment'] - df['negative_sentiment']\\n    df['sentiment_magnitude'] = (df['positive_sentiment'] + df['negative_sentiment']) / 2\\n    \\n    return df\\n\\ndef calculate_descriptive_statistics(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates descriptive statistics for all metrics, grouped by sentiment category.\\n\\n    Methodology:\\n    For each dimension and derived metric, this function calculates the count (n),\\n    mean, standard deviation (std), min, and max, grouped by the 'sentiment_category'\\n    variable. This provides a foundational overview of the data distribution as required\\n    by the exploratory analysis for small sample sizes (Tier 3).\\n\\n    Args:\\n        df: The prepared pandas DataFrame with scores and categories.\\n\\n    Returns:\\n        A dictionary of descriptive statistics or None if an error occurs.\\n    \\\"\\\"\\\"\\n    try:\\n        if df is None or df.empty or 'sentiment_category' not in df.columns:\\n            return {\\\"error\\\": \\\"Input DataFrame is invalid or empty.\\\"}\\n        \\n        grouped_stats = df.groupby('sentiment_category').agg(\\n            ['count', 'mean', 'std', 'min', 'max']\\n        ).reset_index()\\n        \\n        # Unstack and format for clean JSON output\\n        results = {}\\n        for _, row in grouped_stats.iterrows():\\n            category = row['sentiment_category'][0]\\n            stats_dict = {}\\n            for col in df.select_dtypes(include=np.number).columns:\\n                stats_dict[col] = {\\n                    'n': int(row[(col, 'count')]),\\n                    'mean': round(row[(col, 'mean')], 4),\\n                    'std': round(row[(col, 'std')], 4),\\n                    'min': round(row[(col, 'min')], 4),\\n                    'max': round(row[(col, 'max')], 4)\\n                }\\n            results[category] = stats_dict\\n            \\n        return results\\n    except Exception as e:\\n        return {'error': f'Failed to calculate descriptive statistics: {str(e)}'}\\n\\ndef perform_group_comparison(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs exploratory group comparisons using effect sizes.\\n\\n    Methodology:\\n    Due to the Tier 3 sample size (n<8 per group), inferential tests (like t-tests)\\n    are statistically inappropriate. This function instead focuses on pattern detection\\n    by calculating the mean difference and Cohen's d effect size for each metric\\n    between the 'positive' and 'negative' sentiment categories. Cohen's d quantifies\\n    the magnitude of the difference in terms of standard deviations, providing an\\n    exploratory measure of the separation between groups.\\n\\n    Args:\\n        df: The prepared pandas DataFrame with scores and categories.\\n\\n    Returns:\\n        A dictionary of group comparison results (mean difference, Cohen's d) or None.\\n    \\\"\\\"\\\"\\n    try:\\n        if df is None or df.empty or 'sentiment_category' not in df.columns:\\n            return {\\\"error\\\": \\\"Input DataFrame is invalid or empty.\\\"}\\n\\n        groups = df['sentiment_category'].unique()\\n        if len(groups) != 2:\\n            return {\\\"error\\\": f\\\"Expected 2 groups for comparison, but found {len(groups)}.\\\"}\\n        \\n        group1_name, group2_name = groups[0], groups[1]\\n        group1 = df[df['sentiment_category'] == group1_name]\\n        group2 = df[df['sentiment_category'] == group2_name]\\n\\n        results = {}\\n        metrics = df.select_dtypes(include=np.number).columns\\n\\n        for metric in metrics:\\n            # Use pingouin's cohend function for robust calculation\\n            d = pg.compute_effsize(group1[metric], group2[metric], eftype='cohen')\\n            mean_diff = group1[metric].mean() - group2[metric].mean()\\n\\n            # Handle potential division by zero if std is zero\\n            if np.isinf(d) or np.isnan(d):\\n                d_str = 'undefined (zero variance)'\\n            else:\\n                d_str = round(d, 4)\\n\\n            results[metric] = {\\n                'comparison': f'{group1_name}_vs_{group2_name}',\\n                'mean_difference': round(mean_diff, 4),\\n                'cohens_d': d_str\\n            }\\n        \\n        return results\\n    except Exception as e:\\n        return {'error': f'Failed to perform group comparison: {str(e)}'}\\n\\ndef perform_correlation_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates a correlation matrix for all dimensions and derived metrics.\\n\\n    Methodology:\\n    This function computes the Pearson correlation coefficient (r) for all pairs\\n    of numeric variables. Given the Tier 3 sample size (N=4), these correlations\\n    are highly unstable and should be interpreted as purely exploratory indicators\\n    of potential linear relationships, not as generalizable findings.\\n\\n    Args:\\n        df: The prepared pandas DataFrame.\\n\\n    Returns:\\n        A dictionary containing the correlation matrix and a warning, or None.\\n    \\\"\\\"\\\"\\n    try:\\n        if df is None or df.empty:\\n            return {\\\"error\\\": \\\"Input DataFrame is invalid or empty.\\\"}\\n        \\n        metrics_df = df.select_dtypes(include=np.number)\\n        corr_matrix = metrics_df.corr(method='pearson')\\n        \\n        return {\\n            'note': 'WARNING: Correlations on N<15 are highly unstable and for exploratory pattern detection only.',\\n            'correlation_matrix': corr_matrix.round(4).to_dict()\\n        }\\n    except Exception as e:\\n        return {'error': f'Failed to perform correlation analysis: {str(e)}'}\\n\\ndef calculate_reliability_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates the internal consistency reliability (Cronbach's alpha).\\n\\n    Methodology:\\n    This function assesses the internal consistency of the two primary sentiment\\n    dimensions. The 'negative_sentiment' score is reverse-coded (1 - score)\\n    to align its direction with 'positive_sentiment'. Cronbach's alpha is then\\n    calculated on these two items. For only two items, this is equivalent to the\\n    Spearman-Brown coefficient. As a Tier 3 analysis (N<15), the result is a\\n    preliminary estimate of reliability for the specific sample.\\n\\n    Args:\\n        df: The prepared pandas DataFrame.\\n\\n    Returns:\\n        A dictionary with Cronbach's alpha and interpretation notes, or None.\\n    \\\"\\\"\\\"\\n    try:\\n        if df is None or df.shape[0] < 2:\\n            return {\\\"error\\\": \\\"Insufficient data for reliability analysis.\\\"}\\n            \\n        # Create a dataframe for reliability analysis with reversed negative score\\n        reliability_df = pd.DataFrame({\\n            'positive': df['positive_sentiment'],\\n            'negative_reversed': 1 - df['negative_sentiment']\\n        })\\n        \\n        alpha_results = pg.cronbach_alpha(data=reliability_df)\\n        alpha_val, (ci_low, ci_high) = alpha_results\\n        \\n        return {\\n            'note': 'Reliability on N<15 is a preliminary estimate for this sample only.',\\n            'cronbachs_alpha': round(alpha_val, 4),\\n            '95_confidence_interval': [round(ci_low, 4), round(ci_high, 4)],\\n            'items_in_scale': ['positive_sentiment', 'negative_sentiment (reversed)']\\n        }\\n    except Exception as e:\\n        return {'error': f'Failed to calculate reliability: {str(e)}'}\\n\\ndef perform_statistical_analysis(data: List[Dict[str, Any]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function to orchestrate and execute all statistical analyses.\\n\\n    This function first prepares the data and then calls each of the specialized\\n    statistical functions in sequence, collecting their results into a single\\n    comprehensive dictionary.\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A dictionary containing the results of all statistical analyses.\\n    \\\"\\\"\\\"\\n    prepared_df = _prepare_dataframe(data)\\n    \\n    if prepared_df is None:\\n        return {\\n            \\\"descriptive_statistics\\\": {\\\"error\\\": \\\"Data preparation failed.\\\"},\\n            \\\"group_comparison\\\": {\\\"error\\\": \\\"Data preparation failed.\\\"},\\n            \\\"correlation_analysis\\\": {\\\"error\\\": \\\"Data preparation failed.\\\"},\\n            \\\"reliability_analysis\\\": {\\\"error\\\": \\\"Data preparation failed.\\\"}\\n        }\\n    \\n    results = {\\n        'descriptive_statistics': calculate_descriptive_statistics(prepared_df),\\n        'group_comparison': perform_group_comparison(prepared_df),\\n        'correlation_analysis': perform_correlation_analysis(prepared_df),\\n        'reliability_analysis': calculate_reliability_analysis(prepared_df)\\n    }\\n    \\n    return results\\n\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": {\n      \"negative\": {\n        \"positive_sentiment\": {\n          \"n\": 2,\n          \"mean\": 0.0,\n          \"std\": 0.0,\n          \"min\": 0.0,\n          \"max\": 0.0\n        },\n        \"negative_sentiment\": {\n          \"n\": 2,\n          \"mean\": 1.0,\n          \"std\": 0.0,\n          \"min\": 1.0,\n          \"max\": 1.0\n        },\n        \"net_sentiment\": {\n          \"n\": 2,\n          \"mean\": -1.0,\n          \"std\": 0.0,\n          \"min\": -1.0,\n          \"max\": -1.0\n        },\n        \"sentiment_magnitude\": {\n          \"n\": 2,\n          \"mean\": 0.5,\n          \"std\": 0.0,\n          \"min\": 0.5,\n          \"max\": 0.5\n        }\n      },\n      \"positive\": {\n        \"positive_sentiment\": {\n          \"n\": 2,\n          \"mean\": 0.95,\n          \"std\": 0.0707,\n          \"min\": 0.9,\n          \"max\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"n\": 2,\n          \"mean\": 0.0,\n          \"std\": 0.0,\n          \"min\": 0.0,\n          \"max\": 0.0\n        },\n        \"net_sentiment\": {\n          \"n\": 2,\n          \"mean\": 0.95,\n          \"std\": 0.0707,\n          \"min\": 0.9,\n          \"max\": 1.0\n        },\n        \"sentiment_magnitude\": {\n          \"n\": 2,\n          \"mean\": 0.475,\n          \"std\": 0.0354,\n          \"min\": 0.45,\n          \"max\": 0.5\n        }\n      }\n    },\n    \"group_comparison\": {\n      \"positive_sentiment\": {\n        \"comparison\": \"negative_vs_positive\",\n        \"mean_difference\": -0.95,\n        \"cohens_d\": -19.0\n      },\n      \"negative_sentiment\": {\n        \"comparison\": \"negative_vs_positive\",\n        \"mean_difference\": 1.0,\n        \"cohens_d\": \"undefined (zero variance)\"\n      },\n      \"net_sentiment\": {\n        \"comparison\": \"negative_vs_positive\",\n        \"mean_difference\": -1.95,\n        \"cohens_d\": -39.0\n      },\n      \"sentiment_magnitude\": {\n        \"comparison\": \"negative_vs_positive\",\n        \"mean_difference\": 0.025,\n        \"cohens_d\": 1.0\n      }\n    },\n    \"correlation_analysis\": {\n      \"note\": \"WARNING: Correlations on N<15 are highly unstable and for exploratory pattern detection only.\",\n      \"correlation_matrix\": {\n        \"positive_sentiment\": {\n          \"positive_sentiment\": 1.0,\n          \"negative_sentiment\": -1.0,\n          \"net_sentiment\": 1.0,\n          \"sentiment_magnitude\": -0.4851\n        },\n        \"negative_sentiment\": {\n          \"positive_sentiment\": -1.0,\n          \"negative_sentiment\": 1.0,\n          \"net_sentiment\": -1.0,\n          \"sentiment_magnitude\": 0.4851\n        },\n        \"net_sentiment\": {\n          \"positive_sentiment\": 1.0,\n          \"negative_sentiment\": -1.0,\n          \"net_sentiment\": 1.0,\n          \"sentiment_magnitude\": -0.4851\n        },\n        \"sentiment_magnitude\": {\n          \"positive_sentiment\": -0.4851,\n          \"negative_sentiment\": 0.4851,\n          \"net_sentiment\": -0.4851,\n          \"sentiment_magnitude\": 1.0\n        }\n      }\n    },\n    \"reliability_analysis\": {\n      \"note\": \"Reliability on N<15 is a preliminary estimate for this sample only.\",\n      \"cronbachs_alpha\": 0.9949,\n      \"95_confidence_interval\": [\n        0.9234,\n        1.0\n      ],\n      \"items_in_scale\": [\n        \"positive_sentiment\",\n        \"negative_sentiment (reversed)\"\n      ]\n    }\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 4,\n    \"tier_classification\": \"TIER 3: Exploratory Analysis\",\n    \"power_notes\": \"The sample size of N=4 (n=2 per group) is insufficient for inferential statistics (e.g., t-tests, ANOVA). The analysis is exploratory, focusing on descriptive statistics and effect sizes (Cohen's d) to identify patterns and effect magnitudes within this specific sample. All findings, particularly correlations and reliability, are highly sample-specific and not generalizable.\"\n  },\n  \"methodology_summary\": \"The statistical analysis was conducted under the Tier 3 protocol for exploratory analysis due to the small sample size (N=4). The methodology included: 1) Data preparation to create a clean dataframe from raw artifacts, including calculation of derived metrics. 2) Descriptive statistics (mean, std, min, max) for all metrics, grouped by sentiment category. 3) Group comparisons using mean differences and Cohen's d to quantify the magnitude of differences between positive and negative document groups, in lieu of t-tests. 4) A Pearson correlation matrix to explore linear relationships between metrics. 5) An internal consistency check using Cronbach's alpha on the two primary dimensions. All analyses included explicit caveats about the low statistical power and exploratory nature of the findings.\"\n}\n```",
      "verification_status": "verified",
      "csv_files": [],
      "total_cost": 0.0
    },
    "status": "success_with_data",
    "validation_passed": true
  }
}