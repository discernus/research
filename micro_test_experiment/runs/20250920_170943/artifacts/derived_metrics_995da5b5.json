{
  "analysis_id": "analysis_0602d735",
  "step": "derived_metrics_generation",
  "model_used": "vertex_ai/gemini-2.5-flash-lite",
  "raw_metrics_response": "```python\nimport json\n\n# Input scores for the document\ndocument_scores = {\n  \"document_0\": {\n    \"positive_sentiment\": {\n      \"raw_score\": 1.0,\n      \"salience\": 1.0,\n      \"confidence\": 1.0,\n      \"evidence\": \"This is a fantastic document with excellent content.\"\n    },\n    \"negative_sentiment\": {\n      \"raw_score\": 0.0,\n      \"salience\": 0.0,\n      \"confidence\": 1.0,\n      \"evidence\": \"\"\n    }\n  }\n}\n\n# Framework definitions for derived metrics\nframework_definition = {\n    \"derived_metrics\": [\n        {\n            \"name\": \"net_sentiment\",\n            \"formula\": \"dimensions.positive_sentiment.raw_score - dimensions.negative_sentiment.raw_score\",\n            \"interpretation\": \"> 0: Net positive sentiment\\n= 0: Balanced sentiment\\n< 0: Net negative sentiment\"\n        },\n        {\n            \"name\": \"sentiment_magnitude\",\n            \"formula\": \"(dimensions.positive_sentiment.raw_score + dimensions.negative_sentiment.raw_score) / 2\",\n            \"interpretation\": \"> 0.5: High emotional intensity\\n0.25-0.5: Moderate emotional intensity\\n< 0.25: Low emotional intensity\"\n        }\n    ]\n}\n\n# Calculate derived metrics\nresults = {}\nfor doc_id, scores in document_scores.items():\n    derived_metrics = {}\n    positive_sentiment = scores.get(\"positive_sentiment\", {}).get(\"raw_score\", 0.0)\n    negative_sentiment = scores.get(\"negative_sentiment\", {}).get(\"raw_score\", 0.0)\n\n    for metric in framework_definition[\"derived_metrics\"]:\n        formula = metric[\"formula\"]\n        try:\n            # Evaluate the formula using a safe environment\n            # We are explicitly providing the variables that can be used in the formula\n            calculated_value = eval(formula, {\"__builtins__\": None}, {\n                \"dimensions\": {\n                    \"positive_sentiment\": {\"raw_score\": positive_sentiment},\n                    \"negative_sentiment\": {\"raw_score\": negative_sentiment}\n                }\n            })\n            derived_metrics[metric[\"name\"]] = calculated_value\n        except Exception as e:\n            derived_metrics[metric[\"name\"]] = f\"Error calculating metric: {e}\"\n\n    results[doc_id] = {\n        \"dimensional_scores\": scores,\n        \"derived_metrics\": derived_metrics\n    }\n\n# Print the results in a structured format\nprint(json.dumps(results, indent=2))\n```\n\n```json\n{\n  \"document_0\": {\n    \"dimensional_scores\": {\n      \"positive_sentiment\": {\n        \"raw_score\": 1.0,\n        \"salience\": 1.0,\n        \"confidence\": 1.0,\n        \"evidence\": \"This is a fantastic document with excellent content.\"\n      },\n      \"negative_sentiment\": {\n        \"raw_score\": 0.0,\n        \"salience\": 0.0,\n        \"confidence\": 1.0,\n        \"evidence\": \"\"\n      }\n    },\n    \"derived_metrics\": {\n      \"net_sentiment\": 1.0,\n      \"sentiment_magnitude\": 0.5\n    }\n  }\n}\n```",
  "document_index": 0,
  "document_name": "document_0",
  "timestamp": "2025-09-20T17:10:35.041392+00:00"
}