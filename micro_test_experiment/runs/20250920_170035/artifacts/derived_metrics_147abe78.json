{
  "analysis_id": "analysis_76f6f40b",
  "step": "derived_metrics_generation",
  "model_used": "vertex_ai/gemini-2.5-flash-lite",
  "raw_metrics_response": "```python\nimport json\n\n# Provided dimensional scores for document_1\ndocument_scores = {\n  \"document_1\": {\n    \"positive_sentiment\": {\n      \"raw_score\": 1.0,\n      \"salience\": 1.0,\n      \"confidence\": 1.0\n    },\n    \"negative_sentiment\": {\n      \"raw_score\": 0.0,\n      \"salience\": 0.0,\n      \"confidence\": 1.0\n    }\n  }\n}\n\n# Framework definitions for derived metrics\nframework_definitions = {\n    \"derived_metrics\": [\n        {\n            \"name\": \"net_sentiment\",\n            \"formula\": \"dimensions.positive_sentiment.raw_score - dimensions.negative_sentiment.raw_score\",\n            \"interpretation\": \"> 0: Net positive sentiment\\n= 0: Balanced sentiment\\n< 0: Net negative sentiment\"\n        },\n        {\n            \"name\": \"sentiment_magnitude\",\n            \"formula\": \"(dimensions.positive_sentiment.raw_score + dimensions.negative_sentiment.raw_score) / 2\",\n            \"interpretation\": \"> 0.5: High emotional intensity\\n0.25-0.5: Moderate emotional intensity\\n< 0.25: Low emotional intensity\"\n        }\n    ]\n}\n\n# Function to calculate derived metrics\ndef calculate_derived_metrics(scores, definitions):\n    results = {}\n    for doc_id, doc_data in scores.items():\n        doc_results = {}\n        for metric in definitions[\"derived_metrics\"]:\n            # Evaluate the formula using the dimensional scores\n            # We are directly accessing raw_score for calculation as per the framework\n            positive_score = doc_data[\"positive_sentiment\"][\"raw_score\"]\n            negative_score = doc_data[\"negative_sentiment\"][\"raw_score\"]\n\n            # Safely evaluate the formula\n            try:\n                if metric[\"name\"] == \"net_sentiment\":\n                    calculated_value = positive_score - negative_score\n                elif metric[\"name\"] == \"sentiment_magnitude\":\n                    calculated_value = (positive_score + negative_score) / 2\n                else:\n                    calculated_value = None # Handle unsupported metrics if any\n\n                if calculated_value is not None:\n                    doc_results[metric[\"name\"]] = {\n                        \"calculated_value\": calculated_value,\n                        \"interpretation\": metric[\"interpretation\"]\n                    }\n            except Exception as e:\n                doc_results[metric[\"name\"]] = {\"error\": f\"Could not calculate: {e}\"}\n        results[doc_id] = doc_results\n    return results\n\n# Calculate the derived metrics\nderived_metrics_results = calculate_derived_metrics(document_scores, framework_definitions)\n\n# Print the results\nprint(\"Calculated Derived Metrics:\")\nprint(json.dumps(derived_metrics_results, indent=2))\n```\n\n```json\n{\n  \"document_1\": {\n    \"net_sentiment\": {\n      \"calculated_value\": 1.0,\n      \"interpretation\": \"> 0: Net positive sentiment\\n= 0: Balanced sentiment\\n< 0: Net negative sentiment\"\n    },\n    \"sentiment_magnitude\": {\n      \"calculated_value\": 0.5,\n      \"interpretation\": \"> 0.5: High emotional intensity\\n0.25-0.5: Moderate emotional intensity\\n< 0.25: Low emotional intensity\"\n    }\n  }\n}\n```",
  "document_index": 1,
  "document_name": "document_1",
  "timestamp": "2025-09-20T17:01:31.049154+00:00"
}