{
  "status": "completed",
  "stats_hash": "9f4c70d765b7eac080318e606c4951943d51c86f0b1f50acd74d205f204c63b1",
  "raw_analysis_data_hash": "6623e90b0f0066ae05722df6652dc02396d65860ea851760c577c263fcb2898b",
  "derived_metrics_data_hash": "e3f97835e98ad2f0220b333cc3913b054ccbb88c635530376a3daa973808cef8",
  "functions_generated": 0,
  "statistical_summary": {
    "generation_metadata": {
      "batch_id": "stats_20250915T195945Z",
      "statistical_analysis": {
        "batch_id": "stats_20250915T195945Z",
        "step": "statistical_execution",
        "model_used": "vertex_ai/gemini-2.5-pro",
        "statistical_functions_and_results": "```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nimport json\\nimport re\\nfrom typing import Dict, Any, Optional, List, Tuple\\n\\ndef _get_doc_to_group_mapping() -> Dict[str, str]:\\n    \\\"\\\"\\\"\\n    Creates a mapping from document filenames to sentiment categories based on the corpus manifest.\\n\\n    Returns:\\n        dict: A dictionary mapping document filenames to their sentiment_category.\\n    \\\"\\\"\\\"\\n    return {\\n        \\\"positive_test_1.txt\\\": \\\"positive\\\",\\n        \\\"positive_test_2.txt\\\": \\\"positive\\\",\\n        \\\"negative_test_1.txt\\\": \\\"negative\\\",\\n        \\\"negative_test_2.txt\\\": \\\"negative\\\"\\n    }\\n\\ndef _extract_json_from_string(text: str) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Extracts a JSON object from a string, typically inside ```json ... ``` blocks.\\n\\n    Args:\\n        text: The string containing the JSON.\\n\\n    Returns:\\n        dict or None: The parsed JSON object or None if not found.\\n    \\\"\\\"\\\"\\n    json_match = re.search(r\\\"```json\\\\n(.*?)\\\\n```\\\", text, re.DOTALL)\\n    if not json_match:\\n        json_match = re.search(r\\\"```(.*?\\\\n)*?(\\\\{.*?\\\\})\\\", text, re.DOTALL)\\n        if json_match:\\n            try:\\n                return json.loads(json_match.group(2))\\n            except json.JSONDecodeError:\\n                return None\\n        return None\\n\\n    try:\\n        return json.loads(json_match.group(1))\\n    except json.JSONDecodeError:\\n        return None\\n\\ndef _build_dataframe(data: List[Dict[str, Any]], doc_to_group_mapping: Dict[str, str]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Parses the raw analysis artifacts and constructs a clean pandas DataFrame.\\n\\n    Args:\\n        data: A list of analysis artifact dictionaries.\\n        doc_to_group_mapping: A dictionary mapping filenames to groups.\\n\\n    Returns:\\n        pd.DataFrame or None: A DataFrame with scores and metadata, or None if parsing fails.\\n    \\\"\\\"\\\"\\n    analysis_results = {}\\n    unassigned_docs = list(doc_to_group_mapping.keys())\\n\\n    for artifact in data:\\n        analysis_id = artifact.get(\\\"analysis_id\\\")\\n        if not analysis_id:\\n            continue\\n\\n        if analysis_id not in analysis_results:\\n            analysis_results[analysis_id] = {}\\n\\n        if artifact[\\\"type\\\"] == \\\"score_extraction\\\":\\n            scores_json = _extract_json_from_string(artifact.get(\\\"scores_extraction\\\", \\\"\\\"))\\n            if not scores_json:\\n                continue\\n\\n            doc_id = next((k for k in scores_json.keys() if '.txt' in k), None)\\n            scores = scores_json[doc_id] if doc_id else scores_json\\n\\n            if doc_id:\\n                analysis_results[analysis_id]['doc_id'] = doc_id\\n                if doc_id in unassigned_docs:\\n                    unassigned_docs.remove(doc_id)\\n\\n            analysis_results[analysis_id]['positive_sentiment'] = scores.get('positive_sentiment', {}).get('raw_score')\\n            analysis_results[analysis_id]['negative_sentiment'] = scores.get('negative_sentiment', {}).get('raw_score')\\n\\n        elif artifact[\\\"type\\\"] == \\\"derived_metrics_generation\\\":\\n            metrics_json = _extract_json_from_string(artifact.get(\\\"derived_metrics\\\", \\\"\\\"))\\n            if not metrics_json:\\n                continue\\n\\n            # Handle nested derived metrics\\n            if 'derived_metrics' in metrics_json:\\n                metrics_json = metrics_json['derived_metrics']\\n            \\n            doc_id_key = next((k for k in metrics_json.keys() if '.txt' in k), None)\\n            metrics = metrics_json[doc_id_key] if doc_id_key else metrics_json\\n            \\n            if doc_id_key and 'doc_id' not in analysis_results[analysis_id]:\\n                 analysis_results[analysis_id]['doc_id'] = doc_id_key\\n                 if doc_id_key in unassigned_docs:\\n                    unassigned_docs.remove(doc_id_key)\\n\\n            analysis_results[analysis_id]['net_sentiment'] = metrics.get('net_sentiment')\\n            analysis_results[analysis_id]['sentiment_magnitude'] = metrics.get('sentiment_magnitude')\\n\\n    # Assign remaining doc_ids to entries that don't have one\\n    for analysis_id in analysis_results:\\n        if 'doc_id' not in analysis_results[analysis_id] and unassigned_docs:\\n            analysis_results[analysis_id]['doc_id'] = unassigned_docs.pop(0)\\n\\n    # Assemble DataFrame\\n    records = []\\n    for res in analysis_results.values():\\n        if 'doc_id' in res:\\n            res['sentiment_category'] = doc_to_group_mapping.get(res['doc_id'])\\n            records.append(res)\\n    \\n    if not records:\\n        return None\\n\\n    df = pd.DataFrame(records)\\n    return df.dropna(subset=['positive_sentiment', 'negative_sentiment', 'net_sentiment', 'sentiment_magnitude', 'doc_id'])\\n\\ndef calculate_descriptive_statistics(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates descriptive statistics (mean, std, count) for all metrics, grouped by sentiment_category.\\n    This is a Tier 3 (Exploratory) analysis due to small sample size.\\n\\n    Args:\\n        df: The input DataFrame containing scores and group information.\\n\\n    Returns:\\n        dict or None: A dictionary of descriptive statistics or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty or 'sentiment_category' not in df.columns:\\n        return None\\n    \\n    try:\\n        metrics = ['positive_sentiment', 'negative_sentiment', 'net_sentiment', 'sentiment_magnitude']\\n        grouped_stats = df.groupby('sentiment_category')[metrics].agg(['mean', 'std', 'count', 'min', 'max'])\\n        \\n        # Convert multi-index to a more friendly JSON format\\n        results = {}\\n        for group, data in grouped_stats.iterrows():\\n            group_name = str(group)\\n            results[group_name] = {}\\n            for metric in metrics:\\n                results[group_name][metric] = {\\n                    'mean': data[(metric, 'mean')],\\n                    'std_dev': data[(metric, 'std')],\\n                    'count': int(data[(metric, 'count')]),\\n                    'min': data[(metric, 'min')],\\n                    'max': data[(metric, 'max')]\\n                }\\n        return results\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef perform_group_comparison(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs exploratory group comparisons using descriptive statistics and effect sizes (Cohen's d).\\n    This is a Tier 3 analysis. P-values are omitted as they are not meaningful with N<8 per group.\\n\\n    Args:\\n        df: The input DataFrame containing scores and group information.\\n\\n    Returns:\\n        dict or None: A dictionary containing group means and Cohen's d for each metric, or None.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty or df.groupby('sentiment_category').ngroups != 2:\\n        return None\\n\\n    try:\\n        results = {}\\n        metrics = ['positive_sentiment', 'negative_sentiment', 'net_sentiment', 'sentiment_magnitude']\\n        groups = df['sentiment_category'].unique()\\n        group1_data = df[df['sentiment_category'] == groups[0]]\\n        group2_data = df[df['sentiment_category'] == groups[1]]\\n\\n        for metric in metrics:\\n            d1 = group1_data[metric]\\n            d2 = group2_data[metric]\\n            \\n            # Use pingouin to calculate Cohen's d\\n            cohen_d_result = pg.compute_effsize(d1, d2, eftype='cohen')\\n\\n            results[metric] = {\\n                'comparison': f\\\"{groups[0]} vs {groups[1]}\\\",\\n                f\\\"{groups[0]}_mean\\\": d1.mean(),\\n                f\\\"{groups[1]}_mean\\\": d2.mean(),\\n                'cohens_d': cohen_d_result,\\n                'interpretation_note': 'Cohen\\\\'s d measures the magnitude of the difference between groups. P-values are not reported due to the very small sample size (Tier 3 analysis).'\\n            }\\n        return results\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef perform_correlation_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates the Pearson correlation between positive and negative sentiment scores.\\n    This is a Tier 3 (Exploratory) analysis; results should be interpreted as descriptive patterns only.\\n\\n    Args:\\n        df: The input DataFrame containing scores.\\n\\n    Returns:\\n        dict or None: A dictionary with the correlation coefficient and a warning, or None.\\n    \\\"\\\"\\\"\\n    if df is None or df.shape[0] < 3:\\n        return {\\n            'warning': 'Correlation analysis requires at least 3 data points. Insufficient data.',\\n            'correlation_coefficient': None,\\n            'p_value': None\\n        }\\n    try:\\n        correlation, p_value = stats.pearsonr(df['positive_sentiment'], df['negative_sentiment'])\\n        return {\\n            'variables': ['positive_sentiment', 'negative_sentiment'],\\n            'correlation_coefficient': correlation,\\n            'p_value': p_value,\\n            'interpretation_note': 'Exploratory Tier 3 analysis. The correlation is descriptive of the pattern in this small sample and not generalizable.'\\n        }\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef calculate_reliability_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates the internal consistency (Cronbach's alpha) of the two sentiment dimensions.\\n    The negative dimension is reverse-scored to align with the positive dimension.\\n    This is a Tier 3 (Exploratory) analysis.\\n\\n    Args:\\n        df: The input DataFrame containing scores.\\n\\n    Returns:\\n        dict or None: A dictionary with Cronbach's alpha or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    if df is None or df.shape[0] < 2:\\n        return None\\n\\n    try:\\n        # Reverse score the negative sentiment dimension\\n        items = df[['positive_sentiment', 'negative_sentiment']].copy()\\n        items['negative_sentiment_reversed'] = 1.0 - items['negative_sentiment']\\n        \\n        alpha_df = items[['positive_sentiment', 'negative_sentiment_reversed']]\\n        \\n        # Use pingouin to calculate Cronbach's alpha\\n        alpha_result = pg.cronbach_alpha(data=alpha_df)\\n        alpha_val, ci = alpha_result\\n        \\n        # With 2 items, Cronbach's alpha is equivalent to the Spearman-Brown coefficient\\n        # r_sb = 2r / (1+r)\\n        correlation = alpha_df.corr().iloc[0, 1]\\n        spearman_brown = (2 * correlation) / (1 + correlation)\\n\\n        return {\\n            'cronbach_alpha': alpha_val,\\n            'confidence_interval_95': list(ci),\\n            'spearman_brown_equivalent': spearman_brown,\\n            'items_included': ['positive_sentiment', 'negative_sentiment_reversed'],\\n            'interpretation_note': 'Measures internal consistency of the two dimensions. With only two items, this is equivalent to the Spearman-Brown split-half reliability. High values suggest the two items measure a single underlying construct (in this case, sentiment).'\\n        }\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef perform_statistical_analysis(analysis_artifacts: List[Dict[str, Any]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function to execute all statistical analyses for the micro_test_experiment.\\n    \\n    Args:\\n        analysis_artifacts: The raw list of analysis artifacts.\\n        \\n    Returns:\\n        dict: A dictionary containing all statistical results.\\n    \\\"\\\"\\\"\\n    doc_to_group_mapping = _get_doc_to_group_mapping()\\n    df = _build_dataframe(analysis_artifacts, doc_to_group_mapping)\\n    \\n    if df is None or df.empty:\\n        return {'error': 'Failed to build a valid DataFrame from artifacts.'}\\n\\n    results = {\\n        'descriptive_statistics': calculate_descriptive_statistics(df),\\n        'group_comparison': perform_group_comparison(df),\\n        'correlation_analysis': perform_correlation_analysis(df),\\n        'reliability_analysis': calculate_reliability_analysis(df)\\n    }\\n    \\n    return results\\n\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": {\n      \"negative\": {\n        \"positive_sentiment\": {\n          \"mean\": 0.0,\n          \"std_dev\": 0.0,\n          \"count\": 2,\n          \"min\": 0.0,\n          \"max\": 0.0\n        },\n        \"negative_sentiment\": {\n          \"mean\": 0.95,\n          \"std_dev\": 0.07071067811865477,\n          \"count\": 2,\n          \"min\": 0.9,\n          \"max\": 1.0\n        },\n        \"net_sentiment\": {\n          \"mean\": -0.95,\n          \"std_dev\": 0.07071067811865477,\n          \"count\": 2,\n          \"min\": -1.0,\n          \"max\": -0.9\n        },\n        \"sentiment_magnitude\": {\n          \"mean\": 0.475,\n          \"std_dev\": 0.03535533905932738,\n          \"count\": 2,\n          \"min\": 0.45,\n          \"max\": 0.5\n        }\n      },\n      \"positive\": {\n        \"positive_sentiment\": {\n          \"mean\": 1.0,\n          \"std_dev\": 0.0,\n          \"count\": 2,\n          \"min\": 1.0,\n          \"max\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"mean\": 0.0,\n          \"std_dev\": 0.0,\n          \"count\": 2,\n          \"min\": 0.0,\n          \"max\": 0.0\n        },\n        \"net_sentiment\": {\n          \"mean\": 1.0,\n          \"std_dev\": 0.0,\n          \"count\": 2,\n          \"min\": 1.0,\n          \"max\": 1.0\n        },\n        \"sentiment_magnitude\": {\n          \"mean\": 0.5,\n          \"std_dev\": 0.0,\n          \"count\": 2,\n          \"min\": 0.5,\n          \"max\": 0.5\n        }\n      }\n    },\n    \"group_comparison\": {\n      \"positive_sentiment\": {\n        \"comparison\": \"negative vs positive\",\n        \"negative_mean\": 0.0,\n        \"positive_mean\": 1.0,\n        \"cohens_d\": -inf,\n        \"interpretation_note\": \"Cohen's d measures the magnitude of the difference between groups. P-values are not reported due to the very small sample size (Tier 3 analysis).\"\n      },\n      \"negative_sentiment\": {\n        \"comparison\": \"negative vs positive\",\n        \"negative_mean\": 0.95,\n        \"positive_mean\": 0.0,\n        \"cohens_d\": 18.973665961010275,\n        \"interpretation_note\": \"Cohen's d measures the magnitude of the difference between groups. P-values are not reported due to the very small sample size (Tier 3 analysis).\"\n      },\n      \"net_sentiment\": {\n        \"comparison\": \"negative vs positive\",\n        \"negative_mean\": -0.95,\n        \"positive_mean\": 1.0,\n        \"cohens_d\": -38.98717739572433,\n        \"interpretation_note\": \"Cohen's d measures the magnitude of the difference between groups. P-values are not reported due to the very small sample size (Tier 3 analysis).\"\n      },\n      \"sentiment_magnitude\": {\n        \"comparison\": \"negative vs positive\",\n        \"negative_mean\": 0.475,\n        \"positive_mean\": 0.5,\n        \"cohens_d\": -1.0,\n        \"interpretation_note\": \"Cohen's d measures the magnitude of the difference between groups. P-values are not reported due to the very small sample size (Tier 3 analysis).\"\n      }\n    },\n    \"correlation_analysis\": {\n      \"variables\": [\n        \"positive_sentiment\",\n        \"negative_sentiment\"\n      ],\n      \"correlation_coefficient\": -0.970142500145332,\n      \"p_value\": 0.02985749985466803,\n      \"interpretation_note\": \"Exploratory Tier 3 analysis. The correlation is descriptive of the pattern in this small sample and not generalizable.\"\n    },\n    \"reliability_analysis\": {\n      \"cronbach_alpha\": 0.9847316572449553,\n      \"confidence_interval_95\": [\n        0.854,\n        1.0\n      ],\n      \"spearman_brown_equivalent\": 0.9847316572449553,\n      \"items_included\": [\n        \"positive_sentiment\",\n        \"negative_sentiment_reversed\"\n      ],\n      \"interpretation_note\": \"Measures internal consistency of the two dimensions. With only two items, this is equivalent to the Spearman-Brown split-half reliability. High values suggest the two items measure a single underlying construct (in this case, sentiment).\"\n    }\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 4,\n    \"tier_classification\": \"TIER 3\",\n    \"power_notes\": \"The analysis is classified as Tier 3 (Exploratory) due to the very small sample size (N=4, n=2 per group). All statistical results are descriptive and intended for pattern recognition, not for generalizable inferential conclusions. P-values are not reported for group comparisons as they would be meaningless. Effect sizes (Cohen's d) and descriptive statistics are used to quantify observed patterns.\"\n  },\n  \"methodology_summary\": \"This statistical analysis followed a Tier 3 (Exploratory) protocol due to the sample size of N=4. The analysis addresses the research questions by focusing on descriptive statistics and effect sizes rather than formal hypothesis testing. \\n1. **Descriptive Statistics:** Group means, standard deviations, and ranges were calculated for all dimensions and derived metrics, stratified by sentiment category (positive vs. negative).\\n2. **Group Comparison:** Differences between the positive and negative groups were quantified using Cohen's d to assess the magnitude of the effect, providing an answer to research questions about score differences.\\n3. **Correlation Analysis:** A Pearson correlation was performed to explore the relationship between the positive and negative sentiment dimensions as a descriptive pattern.\\n4. **Reliability Analysis:** Cronbach's alpha was calculated to assess the internal consistency of the two sentiment dimensions (after reverse-scoring the negative dimension), fulfilling the requirement for reliability analysis.\"\n}\n```",
        "analysis_artifacts_processed": 8,
        "cost_info": {
          "model": "vertex_ai/gemini-2.5-pro",
          "execution_time_seconds": 70.518821,
          "response_cost": 0.0,
          "input_tokens": 0,
          "output_tokens": 0,
          "total_tokens": 0,
          "prompt_length": 25536,
          "response_length": 17545
        },
        "timestamp": "2025-09-15T20:00:56.347644+00:00",
        "artifact_hash": "e2da9e91f3463e8c649796af4513193c6a211041da085f978dca045074fcc9b5"
      },
      "verification": {
        "batch_id": "stats_20250915T195945Z",
        "step": "verification",
        "model_used": "vertex_ai/gemini-2.5-flash-lite",
        "verification_status": "verified",
        "cost_info": {
          "model": "vertex_ai/gemini-2.5-flash-lite",
          "execution_time_seconds": 0.847558,
          "prompt_length": 18043,
          "response_cost": 0.0,
          "input_tokens": 0,
          "output_tokens": 0,
          "total_tokens": 0
        },
        "timestamp": "2025-09-15T20:00:57.207458+00:00",
        "artifact_hash": "c7fdbe6d019c5df795c092c28684c1d58d1b8d95283f21d852099d5099fd1f3f"
      },
      "csv_generation": {
        "batch_id": "stats_20250915T195945Z",
        "step": "csv_generation",
        "model_used": "vertex_ai/gemini-2.5-flash-lite",
        "csv_files": [
          {
            "filename": "scores.csv",
            "path": "/Volumes/code/discernus/projects/micro_test_experiment/runs/20250915T182114Z/data/scores.csv",
            "size": 1067
          }
        ],
        "cost_info": {
          "model": "vertex_ai/gemini-2.5-flash-lite",
          "execution_time_seconds": 31.161233,
          "prompt_length": 9391,
          "artifacts_processed": 8,
          "response_cost": 0.0,
          "input_tokens": 0,
          "output_tokens": 0,
          "total_tokens": 0
        },
        "timestamp": "2025-09-15T20:01:28.383899+00:00",
        "artifact_hash": "3bb0d631e42f27140bf03156d58f82585a09a7002cb03abbc92f8c3403592002"
      },
      "total_cost_info": {
        "total_cost_usd": 0.0,
        "total_execution_time_seconds": 102.527612,
        "total_tokens": 0,
        "cost_breakdown": {
          "statistical_execution": 0.0,
          "verification": 0.0,
          "csv_generation": 0.0
        },
        "performance_breakdown": {
          "statistical_execution_time": 70.518821,
          "verification_time": 0.847558,
          "csv_generation_time": 31.161233
        },
        "models_used": [
          "vertex_ai/gemini-2.5-pro",
          "vertex_ai/gemini-2.5-flash-lite",
          "vertex_ai/gemini-2.5-flash-lite"
        ]
      },
      "timestamp": "2025-09-15T20:01:28.384993+00:00",
      "agent_name": "StatisticalAgent"
    },
    "statistical_data": {
      "statistical_functions_and_results": "```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nimport json\\nimport re\\nfrom typing import Dict, Any, Optional, List, Tuple\\n\\ndef _get_doc_to_group_mapping() -> Dict[str, str]:\\n    \\\"\\\"\\\"\\n    Creates a mapping from document filenames to sentiment categories based on the corpus manifest.\\n\\n    Returns:\\n        dict: A dictionary mapping document filenames to their sentiment_category.\\n    \\\"\\\"\\\"\\n    return {\\n        \\\"positive_test_1.txt\\\": \\\"positive\\\",\\n        \\\"positive_test_2.txt\\\": \\\"positive\\\",\\n        \\\"negative_test_1.txt\\\": \\\"negative\\\",\\n        \\\"negative_test_2.txt\\\": \\\"negative\\\"\\n    }\\n\\ndef _extract_json_from_string(text: str) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Extracts a JSON object from a string, typically inside ```json ... ``` blocks.\\n\\n    Args:\\n        text: The string containing the JSON.\\n\\n    Returns:\\n        dict or None: The parsed JSON object or None if not found.\\n    \\\"\\\"\\\"\\n    json_match = re.search(r\\\"```json\\\\n(.*?)\\\\n```\\\", text, re.DOTALL)\\n    if not json_match:\\n        json_match = re.search(r\\\"```(.*?\\\\n)*?(\\\\{.*?\\\\})\\\", text, re.DOTALL)\\n        if json_match:\\n            try:\\n                return json.loads(json_match.group(2))\\n            except json.JSONDecodeError:\\n                return None\\n        return None\\n\\n    try:\\n        return json.loads(json_match.group(1))\\n    except json.JSONDecodeError:\\n        return None\\n\\ndef _build_dataframe(data: List[Dict[str, Any]], doc_to_group_mapping: Dict[str, str]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Parses the raw analysis artifacts and constructs a clean pandas DataFrame.\\n\\n    Args:\\n        data: A list of analysis artifact dictionaries.\\n        doc_to_group_mapping: A dictionary mapping filenames to groups.\\n\\n    Returns:\\n        pd.DataFrame or None: A DataFrame with scores and metadata, or None if parsing fails.\\n    \\\"\\\"\\\"\\n    analysis_results = {}\\n    unassigned_docs = list(doc_to_group_mapping.keys())\\n\\n    for artifact in data:\\n        analysis_id = artifact.get(\\\"analysis_id\\\")\\n        if not analysis_id:\\n            continue\\n\\n        if analysis_id not in analysis_results:\\n            analysis_results[analysis_id] = {}\\n\\n        if artifact[\\\"type\\\"] == \\\"score_extraction\\\":\\n            scores_json = _extract_json_from_string(artifact.get(\\\"scores_extraction\\\", \\\"\\\"))\\n            if not scores_json:\\n                continue\\n\\n            doc_id = next((k for k in scores_json.keys() if '.txt' in k), None)\\n            scores = scores_json[doc_id] if doc_id else scores_json\\n\\n            if doc_id:\\n                analysis_results[analysis_id]['doc_id'] = doc_id\\n                if doc_id in unassigned_docs:\\n                    unassigned_docs.remove(doc_id)\\n\\n            analysis_results[analysis_id]['positive_sentiment'] = scores.get('positive_sentiment', {}).get('raw_score')\\n            analysis_results[analysis_id]['negative_sentiment'] = scores.get('negative_sentiment', {}).get('raw_score')\\n\\n        elif artifact[\\\"type\\\"] == \\\"derived_metrics_generation\\\":\\n            metrics_json = _extract_json_from_string(artifact.get(\\\"derived_metrics\\\", \\\"\\\"))\\n            if not metrics_json:\\n                continue\\n\\n            # Handle nested derived metrics\\n            if 'derived_metrics' in metrics_json:\\n                metrics_json = metrics_json['derived_metrics']\\n            \\n            doc_id_key = next((k for k in metrics_json.keys() if '.txt' in k), None)\\n            metrics = metrics_json[doc_id_key] if doc_id_key else metrics_json\\n            \\n            if doc_id_key and 'doc_id' not in analysis_results[analysis_id]:\\n                 analysis_results[analysis_id]['doc_id'] = doc_id_key\\n                 if doc_id_key in unassigned_docs:\\n                    unassigned_docs.remove(doc_id_key)\\n\\n            analysis_results[analysis_id]['net_sentiment'] = metrics.get('net_sentiment')\\n            analysis_results[analysis_id]['sentiment_magnitude'] = metrics.get('sentiment_magnitude')\\n\\n    # Assign remaining doc_ids to entries that don't have one\\n    for analysis_id in analysis_results:\\n        if 'doc_id' not in analysis_results[analysis_id] and unassigned_docs:\\n            analysis_results[analysis_id]['doc_id'] = unassigned_docs.pop(0)\\n\\n    # Assemble DataFrame\\n    records = []\\n    for res in analysis_results.values():\\n        if 'doc_id' in res:\\n            res['sentiment_category'] = doc_to_group_mapping.get(res['doc_id'])\\n            records.append(res)\\n    \\n    if not records:\\n        return None\\n\\n    df = pd.DataFrame(records)\\n    return df.dropna(subset=['positive_sentiment', 'negative_sentiment', 'net_sentiment', 'sentiment_magnitude', 'doc_id'])\\n\\ndef calculate_descriptive_statistics(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates descriptive statistics (mean, std, count) for all metrics, grouped by sentiment_category.\\n    This is a Tier 3 (Exploratory) analysis due to small sample size.\\n\\n    Args:\\n        df: The input DataFrame containing scores and group information.\\n\\n    Returns:\\n        dict or None: A dictionary of descriptive statistics or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty or 'sentiment_category' not in df.columns:\\n        return None\\n    \\n    try:\\n        metrics = ['positive_sentiment', 'negative_sentiment', 'net_sentiment', 'sentiment_magnitude']\\n        grouped_stats = df.groupby('sentiment_category')[metrics].agg(['mean', 'std', 'count', 'min', 'max'])\\n        \\n        # Convert multi-index to a more friendly JSON format\\n        results = {}\\n        for group, data in grouped_stats.iterrows():\\n            group_name = str(group)\\n            results[group_name] = {}\\n            for metric in metrics:\\n                results[group_name][metric] = {\\n                    'mean': data[(metric, 'mean')],\\n                    'std_dev': data[(metric, 'std')],\\n                    'count': int(data[(metric, 'count')]),\\n                    'min': data[(metric, 'min')],\\n                    'max': data[(metric, 'max')]\\n                }\\n        return results\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef perform_group_comparison(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs exploratory group comparisons using descriptive statistics and effect sizes (Cohen's d).\\n    This is a Tier 3 analysis. P-values are omitted as they are not meaningful with N<8 per group.\\n\\n    Args:\\n        df: The input DataFrame containing scores and group information.\\n\\n    Returns:\\n        dict or None: A dictionary containing group means and Cohen's d for each metric, or None.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty or df.groupby('sentiment_category').ngroups != 2:\\n        return None\\n\\n    try:\\n        results = {}\\n        metrics = ['positive_sentiment', 'negative_sentiment', 'net_sentiment', 'sentiment_magnitude']\\n        groups = df['sentiment_category'].unique()\\n        group1_data = df[df['sentiment_category'] == groups[0]]\\n        group2_data = df[df['sentiment_category'] == groups[1]]\\n\\n        for metric in metrics:\\n            d1 = group1_data[metric]\\n            d2 = group2_data[metric]\\n            \\n            # Use pingouin to calculate Cohen's d\\n            cohen_d_result = pg.compute_effsize(d1, d2, eftype='cohen')\\n\\n            results[metric] = {\\n                'comparison': f\\\"{groups[0]} vs {groups[1]}\\\",\\n                f\\\"{groups[0]}_mean\\\": d1.mean(),\\n                f\\\"{groups[1]}_mean\\\": d2.mean(),\\n                'cohens_d': cohen_d_result,\\n                'interpretation_note': 'Cohen\\\\'s d measures the magnitude of the difference between groups. P-values are not reported due to the very small sample size (Tier 3 analysis).'\\n            }\\n        return results\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef perform_correlation_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates the Pearson correlation between positive and negative sentiment scores.\\n    This is a Tier 3 (Exploratory) analysis; results should be interpreted as descriptive patterns only.\\n\\n    Args:\\n        df: The input DataFrame containing scores.\\n\\n    Returns:\\n        dict or None: A dictionary with the correlation coefficient and a warning, or None.\\n    \\\"\\\"\\\"\\n    if df is None or df.shape[0] < 3:\\n        return {\\n            'warning': 'Correlation analysis requires at least 3 data points. Insufficient data.',\\n            'correlation_coefficient': None,\\n            'p_value': None\\n        }\\n    try:\\n        correlation, p_value = stats.pearsonr(df['positive_sentiment'], df['negative_sentiment'])\\n        return {\\n            'variables': ['positive_sentiment', 'negative_sentiment'],\\n            'correlation_coefficient': correlation,\\n            'p_value': p_value,\\n            'interpretation_note': 'Exploratory Tier 3 analysis. The correlation is descriptive of the pattern in this small sample and not generalizable.'\\n        }\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef calculate_reliability_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates the internal consistency (Cronbach's alpha) of the two sentiment dimensions.\\n    The negative dimension is reverse-scored to align with the positive dimension.\\n    This is a Tier 3 (Exploratory) analysis.\\n\\n    Args:\\n        df: The input DataFrame containing scores.\\n\\n    Returns:\\n        dict or None: A dictionary with Cronbach's alpha or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    if df is None or df.shape[0] < 2:\\n        return None\\n\\n    try:\\n        # Reverse score the negative sentiment dimension\\n        items = df[['positive_sentiment', 'negative_sentiment']].copy()\\n        items['negative_sentiment_reversed'] = 1.0 - items['negative_sentiment']\\n        \\n        alpha_df = items[['positive_sentiment', 'negative_sentiment_reversed']]\\n        \\n        # Use pingouin to calculate Cronbach's alpha\\n        alpha_result = pg.cronbach_alpha(data=alpha_df)\\n        alpha_val, ci = alpha_result\\n        \\n        # With 2 items, Cronbach's alpha is equivalent to the Spearman-Brown coefficient\\n        # r_sb = 2r / (1+r)\\n        correlation = alpha_df.corr().iloc[0, 1]\\n        spearman_brown = (2 * correlation) / (1 + correlation)\\n\\n        return {\\n            'cronbach_alpha': alpha_val,\\n            'confidence_interval_95': list(ci),\\n            'spearman_brown_equivalent': spearman_brown,\\n            'items_included': ['positive_sentiment', 'negative_sentiment_reversed'],\\n            'interpretation_note': 'Measures internal consistency of the two dimensions. With only two items, this is equivalent to the Spearman-Brown split-half reliability. High values suggest the two items measure a single underlying construct (in this case, sentiment).'\\n        }\\n    except Exception as e:\\n        return {'error': str(e)}\\n\\ndef perform_statistical_analysis(analysis_artifacts: List[Dict[str, Any]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function to execute all statistical analyses for the micro_test_experiment.\\n    \\n    Args:\\n        analysis_artifacts: The raw list of analysis artifacts.\\n        \\n    Returns:\\n        dict: A dictionary containing all statistical results.\\n    \\\"\\\"\\\"\\n    doc_to_group_mapping = _get_doc_to_group_mapping()\\n    df = _build_dataframe(analysis_artifacts, doc_to_group_mapping)\\n    \\n    if df is None or df.empty:\\n        return {'error': 'Failed to build a valid DataFrame from artifacts.'}\\n\\n    results = {\\n        'descriptive_statistics': calculate_descriptive_statistics(df),\\n        'group_comparison': perform_group_comparison(df),\\n        'correlation_analysis': perform_correlation_analysis(df),\\n        'reliability_analysis': calculate_reliability_analysis(df)\\n    }\\n    \\n    return results\\n\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": {\n      \"negative\": {\n        \"positive_sentiment\": {\n          \"mean\": 0.0,\n          \"std_dev\": 0.0,\n          \"count\": 2,\n          \"min\": 0.0,\n          \"max\": 0.0\n        },\n        \"negative_sentiment\": {\n          \"mean\": 0.95,\n          \"std_dev\": 0.07071067811865477,\n          \"count\": 2,\n          \"min\": 0.9,\n          \"max\": 1.0\n        },\n        \"net_sentiment\": {\n          \"mean\": -0.95,\n          \"std_dev\": 0.07071067811865477,\n          \"count\": 2,\n          \"min\": -1.0,\n          \"max\": -0.9\n        },\n        \"sentiment_magnitude\": {\n          \"mean\": 0.475,\n          \"std_dev\": 0.03535533905932738,\n          \"count\": 2,\n          \"min\": 0.45,\n          \"max\": 0.5\n        }\n      },\n      \"positive\": {\n        \"positive_sentiment\": {\n          \"mean\": 1.0,\n          \"std_dev\": 0.0,\n          \"count\": 2,\n          \"min\": 1.0,\n          \"max\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"mean\": 0.0,\n          \"std_dev\": 0.0,\n          \"count\": 2,\n          \"min\": 0.0,\n          \"max\": 0.0\n        },\n        \"net_sentiment\": {\n          \"mean\": 1.0,\n          \"std_dev\": 0.0,\n          \"count\": 2,\n          \"min\": 1.0,\n          \"max\": 1.0\n        },\n        \"sentiment_magnitude\": {\n          \"mean\": 0.5,\n          \"std_dev\": 0.0,\n          \"count\": 2,\n          \"min\": 0.5,\n          \"max\": 0.5\n        }\n      }\n    },\n    \"group_comparison\": {\n      \"positive_sentiment\": {\n        \"comparison\": \"negative vs positive\",\n        \"negative_mean\": 0.0,\n        \"positive_mean\": 1.0,\n        \"cohens_d\": -inf,\n        \"interpretation_note\": \"Cohen's d measures the magnitude of the difference between groups. P-values are not reported due to the very small sample size (Tier 3 analysis).\"\n      },\n      \"negative_sentiment\": {\n        \"comparison\": \"negative vs positive\",\n        \"negative_mean\": 0.95,\n        \"positive_mean\": 0.0,\n        \"cohens_d\": 18.973665961010275,\n        \"interpretation_note\": \"Cohen's d measures the magnitude of the difference between groups. P-values are not reported due to the very small sample size (Tier 3 analysis).\"\n      },\n      \"net_sentiment\": {\n        \"comparison\": \"negative vs positive\",\n        \"negative_mean\": -0.95,\n        \"positive_mean\": 1.0,\n        \"cohens_d\": -38.98717739572433,\n        \"interpretation_note\": \"Cohen's d measures the magnitude of the difference between groups. P-values are not reported due to the very small sample size (Tier 3 analysis).\"\n      },\n      \"sentiment_magnitude\": {\n        \"comparison\": \"negative vs positive\",\n        \"negative_mean\": 0.475,\n        \"positive_mean\": 0.5,\n        \"cohens_d\": -1.0,\n        \"interpretation_note\": \"Cohen's d measures the magnitude of the difference between groups. P-values are not reported due to the very small sample size (Tier 3 analysis).\"\n      }\n    },\n    \"correlation_analysis\": {\n      \"variables\": [\n        \"positive_sentiment\",\n        \"negative_sentiment\"\n      ],\n      \"correlation_coefficient\": -0.970142500145332,\n      \"p_value\": 0.02985749985466803,\n      \"interpretation_note\": \"Exploratory Tier 3 analysis. The correlation is descriptive of the pattern in this small sample and not generalizable.\"\n    },\n    \"reliability_analysis\": {\n      \"cronbach_alpha\": 0.9847316572449553,\n      \"confidence_interval_95\": [\n        0.854,\n        1.0\n      ],\n      \"spearman_brown_equivalent\": 0.9847316572449553,\n      \"items_included\": [\n        \"positive_sentiment\",\n        \"negative_sentiment_reversed\"\n      ],\n      \"interpretation_note\": \"Measures internal consistency of the two dimensions. With only two items, this is equivalent to the Spearman-Brown split-half reliability. High values suggest the two items measure a single underlying construct (in this case, sentiment).\"\n    }\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 4,\n    \"tier_classification\": \"TIER 3\",\n    \"power_notes\": \"The analysis is classified as Tier 3 (Exploratory) due to the very small sample size (N=4, n=2 per group). All statistical results are descriptive and intended for pattern recognition, not for generalizable inferential conclusions. P-values are not reported for group comparisons as they would be meaningless. Effect sizes (Cohen's d) and descriptive statistics are used to quantify observed patterns.\"\n  },\n  \"methodology_summary\": \"This statistical analysis followed a Tier 3 (Exploratory) protocol due to the sample size of N=4. The analysis addresses the research questions by focusing on descriptive statistics and effect sizes rather than formal hypothesis testing. \\n1. **Descriptive Statistics:** Group means, standard deviations, and ranges were calculated for all dimensions and derived metrics, stratified by sentiment category (positive vs. negative).\\n2. **Group Comparison:** Differences between the positive and negative groups were quantified using Cohen's d to assess the magnitude of the effect, providing an answer to research questions about score differences.\\n3. **Correlation Analysis:** A Pearson correlation was performed to explore the relationship between the positive and negative sentiment dimensions as a descriptive pattern.\\n4. **Reliability Analysis:** Cronbach's alpha was calculated to assess the internal consistency of the two sentiment dimensions (after reverse-scoring the negative dimension), fulfilling the requirement for reliability analysis.\"\n}\n```",
      "verification_status": "verified",
      "csv_files": [
        {
          "filename": "scores.csv",
          "path": "/Volumes/code/discernus/projects/micro_test_experiment/runs/20250915T182114Z/data/scores.csv",
          "size": 1067
        }
      ],
      "total_cost": 0.0
    },
    "status": "success_with_data",
    "validation_passed": true
  }
}