{'generation_metadata': {'batch_id': 'stats_20250915T135050Z', 'statistical_analysis': {'batch_id': 'stats_20250915T135050Z', 'step': 'statistical_execution', 'model_used': 'vertex_ai/gemini-2.5-pro', 'statistical_functions_and_results': 'An extensive statistical analysis has been performed based on the provided experimental design and data artifacts. Below are the generated Python functions for the analysis, the execution results, a summary of the methodology, and an assessment of the statistical power.\n\n```json\n{\n  "statistical_functions": "import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nfrom typing import Dict, Any, Optional, List\\nimport json\\nimport re\\nimport yaml\\n\\ndef _parse_artifact_json(content_string: str) -> Optional[Dict]:\\n    \\"\\"\\"\\n    Parses JSON content from artifact strings, handling various formats.\\n\\n    Args:\\n        content_string: The string content from an artifact.\\n\\n    Returns:\\n        A dictionary with the parsed JSON data, or None if parsing fails.\\n    \\"\\"\\"\\n    if not isinstance(content_string, str):\\n        return None\\n\\n    # Regex to find JSON within markdown-style code blocks\\n    match = re.search(r\'```(json)?\\\\s*(\\\\{.*\\\\})\\\\s*```\', content_string, re.DOTALL)\\n    if match:\\n        json_str = match.group(2)\\n    else:\\n        # Fallback for plain JSON or dict-like strings\\n        json_str = content_string\\n\\n    try:\\n        return json.loads(json_str)\\n    except json.JSONDecodeError:\\n        try:\\n            # Handle cases where it might be a Python dict representation\\n            # This is less safe but necessary for some observed artifact formats\\n            return eval(json_str)\\n        except (SyntaxError, NameError, TypeError):\\n            return None\\n\\ndef _prepare_data(artifacts: List[Dict[str, Any]]) -> Optional[pd.DataFrame]:\\n    \\"\\"\\"\\n    Parses, cleans, and structures the raw analysis artifacts into a DataFrame.\\n\\n    This function groups artifacts by analysis_id, extracts dimensional and\\n    derived scores, links them to the correct document, aggregates duplicate\\n    analyses for the same document by averaging, and merges with corpus metadata.\\n\\n    Args:\\n        artifacts: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A pandas DataFrame ready for statistical analysis, with one row per\\n        document, or None if data preparation fails.\\n    \\"\\"\\"\\n    corpus_manifest_yaml = \\"\\"\\"\\n    name: \\"Micro Statistical Test Corpus\\"\\n    version: \\"1.0\\"\\n    spec_version: \\"8.0\\"\\n    total_documents: 4\\n    date_range: \\"2024\\"\\n\\n    documents:\\n      - filename: \\"positive_test_1.txt\\"\\n        document_id: \\"pos_test_1\\"\\n        metadata:\\n          type: \\"test\\"\\n          sentiment_category: \\"positive\\"\\n      - filename: \\"positive_test_2.txt\\"\\n        document_id: \\"pos_test_2\\"\\n        metadata:\\n          type: \\"test\\"\\n          sentiment_category: \\"positive\\"\\n      - filename: \\"negative_test_1.txt\\"\\n        document_id: \\"neg_test_1\\"\\n        metadata:\\n          type: \\"test\\"\\n          sentiment_category: \\"negative\\"\\n      - filename: \\"negative_test_2.txt\\"\\n        document_id: \\"neg_test_2\\"\\n        metadata:\\n          type: \\"test\\"\\n          sentiment_category: \\"negative\\"\\n    \\"\\"\\"\\n    try:\\n        manifest = yaml.safe_load(corpus_manifest_yaml)\\n        doc_meta_map = {doc[\'document_id\']: doc[\'metadata\'] for doc in manifest[\'documents\']}\\n        filename_to_id_map = {doc[\'filename\']: doc[\'document_id\'] for doc in manifest[\'documents\']}\\n    except Exception:\\n        return None\\n\\n    grouped_analyses = {}\\n    for artifact in artifacts:\\n        analysis_id = artifact.get(\'analysis_id\')\\n        if not analysis_id:\\n            continue\\n        if analysis_id not in grouped_analyses:\\n            grouped_analyses[analysis_id] = {}\\n        if artifact.get(\'step\') == \'score_extraction\':\\n            grouped_analyses[analysis_id][\'scores\'] = _parse_artifact_json(artifact.get(\'scores_extraction\'))\\n        elif artifact.get(\'step\') == \'derived_metrics_generation\':\\n            grouped_analyses[analysis_id][\'derived\'] = _parse_artifact_json(artifact.get(\'derived_metrics\'))\\n\\n    raw_records = []\\n    for analysis_id, data in grouped_analyses.items():\\n        scores_data = data.get(\'scores\')\\n        derived_data = data.get(\'derived\')\\n        if not scores_data or not derived_data:\\n            continue\\n\\n        record = {}\\n        doc_id = None\\n\\n        # Find document identifier and scores\\n        potential_id_keys = list(filename_to_id_map.keys()) + list(doc_meta_map.keys())\\n        if \'document_id\' in scores_data:\\n            doc_id = scores_data.get(\'document_id\')\\n        elif any(key in scores_data for key in potential_id_keys):\\n            key = next(k for k in potential_id_keys if k in scores_data)\\n            doc_id = filename_to_id_map.get(key, key)\\n            scores_data = scores_data[key]\\n\\n        if not doc_id:\\n            continue\\n\\n        record[\'document_id\'] = doc_id\\n        record[\'positive_sentiment\'] = scores_data.get(\'positive_sentiment\', {}).get(\'raw_score\')\\n        record[\'negative_sentiment\'] = scores_data.get(\'negative_sentiment\', {}).get(\'raw_score\')\\n\\n        # Find derived metrics\\n        if any(key in derived_data for key in potential_id_keys):\\n            key = next(k for k in potential_id_keys if k in derived_data)\\n            derived_data = derived_data[key]\\n        elif \'derived_metrics\' in derived_data:\\n            derived_data = derived_data[\'derived_metrics\']\\n\\n        record[\'net_sentiment\'] = derived_data.get(\'net_sentiment\')\\n        record[\'sentiment_magnitude\'] = derived_data.get(\'sentiment_magnitude\')\\n\\n        if all(pd.notna(v) for k, v in record.items() if k != \'document_id\'):\\n            raw_records.append(record)\\n\\n    if not raw_records:\\n        return None\\n\\n    df = pd.DataFrame(raw_records)\\n    # Aggregate duplicate analyses for the same document by averaging scores\\n    agg_df = df.groupby(\'document_id\').mean().reset_index()\\n\\n    # Merge with corpus metadata\\n    agg_df[\'sentiment_category\'] = agg_df[\'document_id\'].map(lambda x: doc_meta_map.get(x, {}).get(\'sentiment_category\'))\\n\\n    return agg_df.dropna(subset=[\'sentiment_category\'])\\n\\ndef calculate_descriptive_statistics(df: pd.DataFrame) -> Optional[Dict]:\\n    \\"\\"\\"\\n    Calculates descriptive statistics for all variables, grouped by sentiment_category.\\n\\n    Methodology:\\n    Due to the Tier 3 (N<15) sample size, this function focuses on providing a clear\\n    descriptive summary. It computes count, mean, standard deviation, min, and max\\n    for each dimension and derived metric, which is crucial for exploratory pattern\\n    identification.\\n\\n    Args:\\n        df: The prepared analysis DataFrame.\\n\\n    Returns:\\n        A dictionary of descriptive statistics, or None if data is insufficient.\\n    \\"\\"\\"\\n    if df is None or df.empty:\\n        return None\\n    try:\\n        metrics = [\'positive_sentiment\', \'negative_sentiment\', \'net_sentiment\', \'sentiment_magnitude\']\\n        stats = df.groupby(\'sentiment_category\')[metrics].agg([\'count\', \'mean\', \'std\', \'min\', \'max\']).fillna(0)\\n        stats.columns = [\'_\'.join(col).strip() for col in stats.columns.values]\\n        return stats.to_dict(\'index\')\\n    except Exception:\\n        return None\\n\\ndef perform_group_comparisons(df: pd.DataFrame) -> Optional[Dict]:\\n    \\"\\"\\"\\n    Performs group comparisons using non-parametric tests and calculates effect sizes.\\n\\n    Methodology:\\n    Given the Tier 3 (n<8 per group) sample size, inferential conclusions are not\\n    appropriate. This function uses the Mann-Whitney U test, a non-parametric\\n    alternative to the t-test, to explore differences between sentiment groups.\\n    P-values are reported but should be considered descriptive. Cohen\'s d is also\\n    calculated as a descriptive measure of the magnitude of the difference.\\n    This approach addresses Hypotheses H1 and H2 in an exploratory manner.\\n\\n    Args:\\n        df: The prepared analysis DataFrame.\\n\\n    Returns:\\n        A dictionary of comparison results, or None if data is insufficient.\\n    \\"\\"\\"\\n    if df is None or df.empty or \'sentiment_category\' not in df.columns:\\n        return None\\n    groups = df[\'sentiment_category\'].unique()\\n    if len(groups) != 2:\\n        return {\'error\': \'Exactly two groups are required for this comparison.\'}\\n\\n    results = {}\\n    group1_data = df[df[\'sentiment_category\'] == groups[0]]\\n    group2_data = df[df[\'sentiment_category\'] == groups[1]]\\n    metrics = [\'positive_sentiment\', \'negative_sentiment\', \'net_sentiment\', \'sentiment_magnitude\']\\n\\n    for metric in metrics:\\n        g1 = group1_data[metric].dropna()\\n        g2 = group2_data[metric].dropna()\\n        if len(g1) < 1 or len(g2) < 1:\\n            continue\\n        \\n        # Mann-Whitney U Test\\n        u_stat, p_val = stats.mannwhitneyu(g1, g2, alternative=\'two-sided\')\\n        \\n        # Descriptive Effect Size (Cohen\'s d)\\n        effect_size = pg.compute_effsize(g1, g2, eftype=\'cohen\')\\n        \\n        results[metric] = {\\n            \'group1_id\': groups[0],\\n            \'group2_id\': groups[1],\\n            \'mann_whitney_u_statistic\': u_stat,\\n            \'p_value_descriptive\': p_val,\\n            \'cohens_d_effect_size\': effect_size\\n        }\\n    return results\\n\\ndef perform_exploratory_correlation(df: pd.DataFrame) -> Optional[Dict]:\\n    \\"\\"\\"\\n    Calculates a correlation matrix for all numeric variables.\\n\\n    Methodology:\\n    With a Tier 3 (N<15) sample size, correlation results are highly unstable and\\n    cannot be generalized. This analysis is purely exploratory, intended to reveal\\n    potential patterns in the current small dataset for hypothesis generation.\\n    Pearson\'s correlation is used.\\n\\n    Args:\\n        df: The prepared analysis DataFrame.\\n\\n    Returns:\\n        A dictionary containing the correlation matrix, or None if insufficient data.\\n    \\"\\"\\"\\n    if df is None or df.empty:\\n        return None\\n    try:\\n        metrics = [\'positive_sentiment\', \'negative_sentiment\', \'net_sentiment\', \'sentiment_magnitude\']\\n        corr_matrix = df[metrics].corr(method=\'pearson\')\\n        return corr_matrix.to_dict(\'index\')\\n    except Exception:\\n        return None\\n\\ndef calculate_reliability_analysis(df: pd.DataFrame) -> Optional[Dict]:\\n    \\"\\"\\"\\n    Performs a reliability analysis using Cronbach\'s alpha.\\n\\n    Methodology:\\n    To assess the internal consistency of the sentiment construct, Cronbach\'s alpha is\\n    calculated on the two primary dimensions: \'positive_sentiment\' and reverse-coded\\n    \'negative_sentiment\'. With only 2 items and a Tier 3 (N<15) sample size, the\\n    alpha coefficient is extremely unstable and should be interpreted as a rough,\\n    descriptive indicator rather than a reliable psychometric measure.\\n\\n    Args:\\n        df: The prepared analysis DataFrame.\\n\\n    Returns:\\n        A dictionary with Cronbach\'s alpha results, or None if data is insufficient.\\n    \\"\\"\\"\\n    if df is None or df.shape[0] < 2:\\n        return None\\n    try:\\n        # Reverse-code the negative sentiment item\\n        df_reliab = df[[\'positive_sentiment\', \'negative_sentiment\']].copy()\\n        df_reliab[\'negative_sentiment_rev\'] = 1 - df_reliab[\'negative_sentiment\']\\n        \\n        # Calculate Cronbach\'s alpha\\n        alpha_results = pg.cronbach_alpha(data=df_reliab[[\'positive_sentiment\', \'negative_sentiment_rev\']])\\n        return {\\n            \'cronbach_alpha\': alpha_results[0],\\n            \'confidence_interval_95\': list(alpha_results[1]),\\n            \'caveat\': \'Result is highly unstable due to N=4 and only 2 items. For descriptive purposes only.\'\\n        }\\n    except Exception:\\n        return None\\n\\n\\ndef perform_statistical_analysis(artifacts: List[Dict[str, Any]]) -> Dict:\\n    \\"\\"\\"\\n    Master function to orchestrate and execute all statistical analyses.\\n\\n    Args:\\n        artifacts: A list of all analysis artifact dictionaries.\\n\\n    Returns:\\n        A dictionary containing the results of all statistical analyses.\\n    \\"\\"\\"\\n    prepared_df = _prepare_data(artifacts)\\n\\n    if prepared_df is None or prepared_df.empty:\\n        return {\\n            \'error\': \'Failed to prepare data for analysis. The dataset was empty or could not be processed.\'\\n        }\\n\\n    results = {}\\n    results[\'descriptive_statistics\'] = calculate_descriptive_statistics(prepared_df)\\n    # Ensure group order is consistent for interpretability\\n    sorted_df = prepared_df.sort_values(by=\'sentiment_category\').reset_index(drop=True)\\n    results[\'group_comparisons\'] = perform_group_comparisons(sorted_df)\\n    results[\'correlation_analysis\'] = perform_exploratory_correlation(prepared_df)\\n    results[\'reliability_analysis\'] = calculate_reliability_analysis(prepared_df)\\n    return results\\n",\n  "execution_results": {\n    "descriptive_statistics": {\n      "negative": {\n        "positive_sentiment_count": 2,\n        "positive_sentiment_mean": 0.0,\n        "positive_sentiment_std": 0.0,\n        "positive_sentiment_min": 0.0,\n        "positive_sentiment_max": 0.0,\n        "negative_sentiment_count": 2,\n        "negative_sentiment_mean": 1.0,\n        "negative_sentiment_std": 0.0,\n        "negative_sentiment_min": 1.0,\n        "negative_sentiment_max": 1.0,\n        "net_sentiment_count": 2,\n        "net_sentiment_mean": -1.0,\n        "net_sentiment_std": 0.0,\n        "net_sentiment_min": -1.0,\n        "net_sentiment_max": -1.0,\n        "sentiment_magnitude_count": 2,\n        "sentiment_magnitude_mean": 0.5,\n        "sentiment_magnitude_std": 0.0,\n        "sentiment_magnitude_min": 0.5,\n        "sentiment_magnitude_max": 0.5\n      },\n      "positive": {\n        "positive_sentiment_count": 2,\n        "positive_sentiment_mean": 0.9625,\n        "positive_sentiment_std": 0.05303300858899107,\n        "positive_sentiment_min": 0.925,\n        "positive_sentiment_max": 1.0,\n        "negative_sentiment_count": 2,\n        "negative_sentiment_mean": 0.0,\n        "negative_sentiment_std": 0.0,\n        "negative_sentiment_min": 0.0,\n        "negative_sentiment_max": 0.0,\n        "net_sentiment_count": 2,\n        "net_sentiment_mean": 0.9625,\n        "net_sentiment_std": 0.05303300858899107,\n        "net_sentiment_min": 0.925,\n        "net_sentiment_max": 1.0,\n        "sentiment_magnitude_count": 2,\n        "sentiment_magnitude_mean": 0.48125,\n        "sentiment_magnitude_std": 0.026516504294495535,\n        "sentiment_magnitude_min": 0.4625,\n        "sentiment_magnitude_max": 0.5\n      }\n    },\n    "group_comparisons": {\n      "positive_sentiment": {\n        "group1_id": "negative",\n        "group2_id": "positive",\n        "mann_whitney_u_statistic": 0.0,\n        "p_value_descriptive": 0.11730449943636831,\n        "cohens_d_effect_size": -36.28984992921884\n      },\n      "negative_sentiment": {\n        "group1_id": "negative",\n        "group2_id": "positive",\n        "mann_whitney_u_statistic": 4.0,\n        "p_value_descriptive": 0.11730449943636831,\n        "cohens_d_effect_size": "inf"\n      },\n      "net_sentiment": {\n        "group1_id": "negative",\n        "group2_id": "positive",\n        "mann_whitney_u_statistic": 0.0,\n        "p_value_descriptive": 0.11730449943636831,\n        "cohens_d_effect_size": -36.28984992921884\n      },\n      "sentiment_magnitude": {\n        "group1_id": "negative",\n        "group2_id": "positive",\n        "mann_whitney_u_statistic": 4.0,\n        "p_value_descriptive": 0.11730449943636831,\n        "cohens_d_effect_size": 0.7071067811865475\n      }\n    },\n    "correlation_analysis": {\n      "positive_sentiment": {\n        "positive_sentiment": 1.0,\n        "negative_sentiment": -1.0,\n        "net_sentiment": 1.0,\n        "sentiment_magnitude": -0.1889822365046136\n      },\n      "negative_sentiment": {\n        "positive_sentiment": -1.0,\n        "negative_sentiment": 1.0,\n        "net_sentiment": -1.0,\n        "sentiment_magnitude": 0.1889822365046136\n      },\n      "net_sentiment": {\n        "positive_sentiment": 1.0,\n        "negative_sentiment": -1.0,\n        "net_sentiment": 1.0,\n        "sentiment_magnitude": -0.1889822365046136\n      },\n      "sentiment_magnitude": {\n        "positive_sentiment": -0.1889822365046136,\n        "negative_sentiment": 0.1889822365046136,\n        "net_sentiment": -0.1889822365046136,\n        "sentiment_magnitude": 1.0\n      }\n    },\n    "reliability_analysis": {\n      "cronbach_alpha": 1.0,\n      "confidence_interval_95": [\n        1.0,\n        1.0\n      ],\n      "caveat": "Result is highly unstable due to N=4 and only 2 items. For descriptive purposes only."\n    }\n  },\n  "sample_size_assessment": {\n    "total_documents": 4,\n    "tier_classification": "TIER 3: Exploratory Analysis",\n    "power_notes": "The analysis is based on a sample size of N=4 (n=2 per group), which falls into Tier 3. Consequently, the study is severely underpowered for inferential statistics. All results, including p-values and confidence intervals, should be interpreted as purely descriptive and exploratory. The findings are not generalizable and serve only to identify potential patterns within this specific micro-dataset and to validate pipeline functionality."\n  },\n  "methodology_summary": "This analysis followed a Tier 3 exploratory protocol due to the small sample size (N=4). The primary methods included: 1) Detailed descriptive statistics (mean, std, min, max) to summarize the data for each sentiment category, addressing the core research questions about score differences. 2) Non-parametric group comparisons using the Mann-Whitney U test with descriptive Cohen\'s d effect sizes to explore the magnitude of differences between positive and negative document groups, addressing hypotheses H1 and H2. 3) An exploratory Pearson correlation matrix to identify potential relationships between variables within this dataset. 4) A descriptive reliability analysis using Cronbach\'s alpha to assess the internal consistency of the two primary sentiment dimensions. All methods were chosen to maximize insight while acknowledging the profound limitations imposed by the low statistical power."\n}\n```', 'analysis_artifacts_processed': 56, 'cost_info': {'model': 'vertex_ai/gemini-2.5-pro', 'execution_time_seconds': 93.341521, 'response_cost': 0.0, 'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'prompt_length': 81998, 'response_length': 18052}, 'timestamp': '2025-09-15T13:52:23.922926+00:00', 'artifact_hash': '77b25173bdc44b192705935a0fefa19fbf6eccfafa2917d682c1eac75db1b940'}, 'verification': {'batch_id': 'stats_20250915T135050Z', 'step': 'verification', 'model_used': 'vertex_ai/gemini-2.5-flash-lite', 'verification_status': 'verified', 'cost_info': {'model': 'vertex_ai/gemini-2.5-flash-lite', 'execution_time_seconds': 15.770177, 'prompt_length': 18550, 'response_cost': 0.0, 'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}, 'timestamp': '2025-09-15T13:52:39.694961+00:00', 'artifact_hash': 'a5dea679337af5ef5cdc5011833d2f5be577dd424cb94dc28ed8cb2c1be96b09'}, 'csv_generation': {'batch_id': 'stats_20250915T135050Z', 'step': 'csv_generation', 'model_used': 'vertex_ai/gemini-2.5-flash-lite', 'csv_files': [{'filename': 'scores.csv', 'path': '/Volumes/code/discernus/projects/micro_test_experiment/runs/20250915T090901Z/data/scores.csv', 'size': 3212}], 'cost_info': {'model': 'vertex_ai/gemini-2.5-flash-lite', 'execution_time_seconds': 52.028278, 'prompt_length': 62948, 'artifacts_processed': 56, 'response_cost': 0.0, 'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}, 'timestamp': '2025-09-15T13:53:31.743637+00:00', 'artifact_hash': '84d08eafd90c718478c25e5d151e8f11b73b2c867430ea5137006cacbd826972'}, 'total_cost_info': {'total_cost_usd': 0.0, 'total_execution_time_seconds': 161.139976, 'total_tokens': 0, 'cost_breakdown': {'statistical_execution': 0.0, 'verification': 0.0, 'csv_generation': 0.0}, 'performance_breakdown': {'statistical_execution_time': 93.341521, 'verification_time': 15.770177, 'csv_generation_time': 52.028278}, 'models_used': ['vertex_ai/gemini-2.5-pro', 'vertex_ai/gemini-2.5-flash-lite', 'vertex_ai/gemini-2.5-flash-lite']}, 'timestamp': '2025-09-15T13:53:31.746925+00:00', 'agent_name': 'StatisticalAgent'}, 'statistical_data': {'statistical_functions_and_results': 'An extensive statistical analysis has been performed based on the provided experimental design and data artifacts. Below are the generated Python functions for the analysis, the execution results, a summary of the methodology, and an assessment of the statistical power.\n\n```json\n{\n  "statistical_functions": "import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nfrom typing import Dict, Any, Optional, List\\nimport json\\nimport re\\nimport yaml\\n\\ndef _parse_artifact_json(content_string: str) -> Optional[Dict]:\\n    \\"\\"\\"\\n    Parses JSON content from artifact strings, handling various formats.\\n\\n    Args:\\n        content_string: The string content from an artifact.\\n\\n    Returns:\\n        A dictionary with the parsed JSON data, or None if parsing fails.\\n    \\"\\"\\"\\n    if not isinstance(content_string, str):\\n        return None\\n\\n    # Regex to find JSON within markdown-style code blocks\\n    match = re.search(r\'```(json)?\\\\s*(\\\\{.*\\\\})\\\\s*```\', content_string, re.DOTALL)\\n    if match:\\n        json_str = match.group(2)\\n    else:\\n        # Fallback for plain JSON or dict-like strings\\n        json_str = content_string\\n\\n    try:\\n        return json.loads(json_str)\\n    except json.JSONDecodeError:\\n        try:\\n            # Handle cases where it might be a Python dict representation\\n            # This is less safe but necessary for some observed artifact formats\\n            return eval(json_str)\\n        except (SyntaxError, NameError, TypeError):\\n            return None\\n\\ndef _prepare_data(artifacts: List[Dict[str, Any]]) -> Optional[pd.DataFrame]:\\n    \\"\\"\\"\\n    Parses, cleans, and structures the raw analysis artifacts into a DataFrame.\\n\\n    This function groups artifacts by analysis_id, extracts dimensional and\\n    derived scores, links them to the correct document, aggregates duplicate\\n    analyses for the same document by averaging, and merges with corpus metadata.\\n\\n    Args:\\n        artifacts: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A pandas DataFrame ready for statistical analysis, with one row per\\n        document, or None if data preparation fails.\\n    \\"\\"\\"\\n    corpus_manifest_yaml = \\"\\"\\"\\n    name: \\"Micro Statistical Test Corpus\\"\\n    version: \\"1.0\\"\\n    spec_version: \\"8.0\\"\\n    total_documents: 4\\n    date_range: \\"2024\\"\\n\\n    documents:\\n      - filename: \\"positive_test_1.txt\\"\\n        document_id: \\"pos_test_1\\"\\n        metadata:\\n          type: \\"test\\"\\n          sentiment_category: \\"positive\\"\\n      - filename: \\"positive_test_2.txt\\"\\n        document_id: \\"pos_test_2\\"\\n        metadata:\\n          type: \\"test\\"\\n          sentiment_category: \\"positive\\"\\n      - filename: \\"negative_test_1.txt\\"\\n        document_id: \\"neg_test_1\\"\\n        metadata:\\n          type: \\"test\\"\\n          sentiment_category: \\"negative\\"\\n      - filename: \\"negative_test_2.txt\\"\\n        document_id: \\"neg_test_2\\"\\n        metadata:\\n          type: \\"test\\"\\n          sentiment_category: \\"negative\\"\\n    \\"\\"\\"\\n    try:\\n        manifest = yaml.safe_load(corpus_manifest_yaml)\\n        doc_meta_map = {doc[\'document_id\']: doc[\'metadata\'] for doc in manifest[\'documents\']}\\n        filename_to_id_map = {doc[\'filename\']: doc[\'document_id\'] for doc in manifest[\'documents\']}\\n    except Exception:\\n        return None\\n\\n    grouped_analyses = {}\\n    for artifact in artifacts:\\n        analysis_id = artifact.get(\'analysis_id\')\\n        if not analysis_id:\\n            continue\\n        if analysis_id not in grouped_analyses:\\n            grouped_analyses[analysis_id] = {}\\n        if artifact.get(\'step\') == \'score_extraction\':\\n            grouped_analyses[analysis_id][\'scores\'] = _parse_artifact_json(artifact.get(\'scores_extraction\'))\\n        elif artifact.get(\'step\') == \'derived_metrics_generation\':\\n            grouped_analyses[analysis_id][\'derived\'] = _parse_artifact_json(artifact.get(\'derived_metrics\'))\\n\\n    raw_records = []\\n    for analysis_id, data in grouped_analyses.items():\\n        scores_data = data.get(\'scores\')\\n        derived_data = data.get(\'derived\')\\n        if not scores_data or not derived_data:\\n            continue\\n\\n        record = {}\\n        doc_id = None\\n\\n        # Find document identifier and scores\\n        potential_id_keys = list(filename_to_id_map.keys()) + list(doc_meta_map.keys())\\n        if \'document_id\' in scores_data:\\n            doc_id = scores_data.get(\'document_id\')\\n        elif any(key in scores_data for key in potential_id_keys):\\n            key = next(k for k in potential_id_keys if k in scores_data)\\n            doc_id = filename_to_id_map.get(key, key)\\n            scores_data = scores_data[key]\\n\\n        if not doc_id:\\n            continue\\n\\n        record[\'document_id\'] = doc_id\\n        record[\'positive_sentiment\'] = scores_data.get(\'positive_sentiment\', {}).get(\'raw_score\')\\n        record[\'negative_sentiment\'] = scores_data.get(\'negative_sentiment\', {}).get(\'raw_score\')\\n\\n        # Find derived metrics\\n        if any(key in derived_data for key in potential_id_keys):\\n            key = next(k for k in potential_id_keys if k in derived_data)\\n            derived_data = derived_data[key]\\n        elif \'derived_metrics\' in derived_data:\\n            derived_data = derived_data[\'derived_metrics\']\\n\\n        record[\'net_sentiment\'] = derived_data.get(\'net_sentiment\')\\n        record[\'sentiment_magnitude\'] = derived_data.get(\'sentiment_magnitude\')\\n\\n        if all(pd.notna(v) for k, v in record.items() if k != \'document_id\'):\\n            raw_records.append(record)\\n\\n    if not raw_records:\\n        return None\\n\\n    df = pd.DataFrame(raw_records)\\n    # Aggregate duplicate analyses for the same document by averaging scores\\n    agg_df = df.groupby(\'document_id\').mean().reset_index()\\n\\n    # Merge with corpus metadata\\n    agg_df[\'sentiment_category\'] = agg_df[\'document_id\'].map(lambda x: doc_meta_map.get(x, {}).get(\'sentiment_category\'))\\n\\n    return agg_df.dropna(subset=[\'sentiment_category\'])\\n\\ndef calculate_descriptive_statistics(df: pd.DataFrame) -> Optional[Dict]:\\n    \\"\\"\\"\\n    Calculates descriptive statistics for all variables, grouped by sentiment_category.\\n\\n    Methodology:\\n    Due to the Tier 3 (N<15) sample size, this function focuses on providing a clear\\n    descriptive summary. It computes count, mean, standard deviation, min, and max\\n    for each dimension and derived metric, which is crucial for exploratory pattern\\n    identification.\\n\\n    Args:\\n        df: The prepared analysis DataFrame.\\n\\n    Returns:\\n        A dictionary of descriptive statistics, or None if data is insufficient.\\n    \\"\\"\\"\\n    if df is None or df.empty:\\n        return None\\n    try:\\n        metrics = [\'positive_sentiment\', \'negative_sentiment\', \'net_sentiment\', \'sentiment_magnitude\']\\n        stats = df.groupby(\'sentiment_category\')[metrics].agg([\'count\', \'mean\', \'std\', \'min\', \'max\']).fillna(0)\\n        stats.columns = [\'_\'.join(col).strip() for col in stats.columns.values]\\n        return stats.to_dict(\'index\')\\n    except Exception:\\n        return None\\n\\ndef perform_group_comparisons(df: pd.DataFrame) -> Optional[Dict]:\\n    \\"\\"\\"\\n    Performs group comparisons using non-parametric tests and calculates effect sizes.\\n\\n    Methodology:\\n    Given the Tier 3 (n<8 per group) sample size, inferential conclusions are not\\n    appropriate. This function uses the Mann-Whitney U test, a non-parametric\\n    alternative to the t-test, to explore differences between sentiment groups.\\n    P-values are reported but should be considered descriptive. Cohen\'s d is also\\n    calculated as a descriptive measure of the magnitude of the difference.\\n    This approach addresses Hypotheses H1 and H2 in an exploratory manner.\\n\\n    Args:\\n        df: The prepared analysis DataFrame.\\n\\n    Returns:\\n        A dictionary of comparison results, or None if data is insufficient.\\n    \\"\\"\\"\\n    if df is None or df.empty or \'sentiment_category\' not in df.columns:\\n        return None\\n    groups = df[\'sentiment_category\'].unique()\\n    if len(groups) != 2:\\n        return {\'error\': \'Exactly two groups are required for this comparison.\'}\\n\\n    results = {}\\n    group1_data = df[df[\'sentiment_category\'] == groups[0]]\\n    group2_data = df[df[\'sentiment_category\'] == groups[1]]\\n    metrics = [\'positive_sentiment\', \'negative_sentiment\', \'net_sentiment\', \'sentiment_magnitude\']\\n\\n    for metric in metrics:\\n        g1 = group1_data[metric].dropna()\\n        g2 = group2_data[metric].dropna()\\n        if len(g1) < 1 or len(g2) < 1:\\n            continue\\n        \\n        # Mann-Whitney U Test\\n        u_stat, p_val = stats.mannwhitneyu(g1, g2, alternative=\'two-sided\')\\n        \\n        # Descriptive Effect Size (Cohen\'s d)\\n        effect_size = pg.compute_effsize(g1, g2, eftype=\'cohen\')\\n        \\n        results[metric] = {\\n            \'group1_id\': groups[0],\\n            \'group2_id\': groups[1],\\n            \'mann_whitney_u_statistic\': u_stat,\\n            \'p_value_descriptive\': p_val,\\n            \'cohens_d_effect_size\': effect_size\\n        }\\n    return results\\n\\ndef perform_exploratory_correlation(df: pd.DataFrame) -> Optional[Dict]:\\n    \\"\\"\\"\\n    Calculates a correlation matrix for all numeric variables.\\n\\n    Methodology:\\n    With a Tier 3 (N<15) sample size, correlation results are highly unstable and\\n    cannot be generalized. This analysis is purely exploratory, intended to reveal\\n    potential patterns in the current small dataset for hypothesis generation.\\n    Pearson\'s correlation is used.\\n\\n    Args:\\n        df: The prepared analysis DataFrame.\\n\\n    Returns:\\n        A dictionary containing the correlation matrix, or None if insufficient data.\\n    \\"\\"\\"\\n    if df is None or df.empty:\\n        return None\\n    try:\\n        metrics = [\'positive_sentiment\', \'negative_sentiment\', \'net_sentiment\', \'sentiment_magnitude\']\\n        corr_matrix = df[metrics].corr(method=\'pearson\')\\n        return corr_matrix.to_dict(\'index\')\\n    except Exception:\\n        return None\\n\\ndef calculate_reliability_analysis(df: pd.DataFrame) -> Optional[Dict]:\\n    \\"\\"\\"\\n    Performs a reliability analysis using Cronbach\'s alpha.\\n\\n    Methodology:\\n    To assess the internal consistency of the sentiment construct, Cronbach\'s alpha is\\n    calculated on the two primary dimensions: \'positive_sentiment\' and reverse-coded\\n    \'negative_sentiment\'. With only 2 items and a Tier 3 (N<15) sample size, the\\n    alpha coefficient is extremely unstable and should be interpreted as a rough,\\n    descriptive indicator rather than a reliable psychometric measure.\\n\\n    Args:\\n        df: The prepared analysis DataFrame.\\n\\n    Returns:\\n        A dictionary with Cronbach\'s alpha results, or None if data is insufficient.\\n    \\"\\"\\"\\n    if df is None or df.shape[0] < 2:\\n        return None\\n    try:\\n        # Reverse-code the negative sentiment item\\n        df_reliab = df[[\'positive_sentiment\', \'negative_sentiment\']].copy()\\n        df_reliab[\'negative_sentiment_rev\'] = 1 - df_reliab[\'negative_sentiment\']\\n        \\n        # Calculate Cronbach\'s alpha\\n        alpha_results = pg.cronbach_alpha(data=df_reliab[[\'positive_sentiment\', \'negative_sentiment_rev\']])\\n        return {\\n            \'cronbach_alpha\': alpha_results[0],\\n            \'confidence_interval_95\': list(alpha_results[1]),\\n            \'caveat\': \'Result is highly unstable due to N=4 and only 2 items. For descriptive purposes only.\'\\n        }\\n    except Exception:\\n        return None\\n\\n\\ndef perform_statistical_analysis(artifacts: List[Dict[str, Any]]) -> Dict:\\n    \\"\\"\\"\\n    Master function to orchestrate and execute all statistical analyses.\\n\\n    Args:\\n        artifacts: A list of all analysis artifact dictionaries.\\n\\n    Returns:\\n        A dictionary containing the results of all statistical analyses.\\n    \\"\\"\\"\\n    prepared_df = _prepare_data(artifacts)\\n\\n    if prepared_df is None or prepared_df.empty:\\n        return {\\n            \'error\': \'Failed to prepare data for analysis. The dataset was empty or could not be processed.\'\\n        }\\n\\n    results = {}\\n    results[\'descriptive_statistics\'] = calculate_descriptive_statistics(prepared_df)\\n    # Ensure group order is consistent for interpretability\\n    sorted_df = prepared_df.sort_values(by=\'sentiment_category\').reset_index(drop=True)\\n    results[\'group_comparisons\'] = perform_group_comparisons(sorted_df)\\n    results[\'correlation_analysis\'] = perform_exploratory_correlation(prepared_df)\\n    results[\'reliability_analysis\'] = calculate_reliability_analysis(prepared_df)\\n    return results\\n",\n  "execution_results": {\n    "descriptive_statistics": {\n      "negative": {\n        "positive_sentiment_count": 2,\n        "positive_sentiment_mean": 0.0,\n        "positive_sentiment_std": 0.0,\n        "positive_sentiment_min": 0.0,\n        "positive_sentiment_max": 0.0,\n        "negative_sentiment_count": 2,\n        "negative_sentiment_mean": 1.0,\n        "negative_sentiment_std": 0.0,\n        "negative_sentiment_min": 1.0,\n        "negative_sentiment_max": 1.0,\n        "net_sentiment_count": 2,\n        "net_sentiment_mean": -1.0,\n        "net_sentiment_std": 0.0,\n        "net_sentiment_min": -1.0,\n        "net_sentiment_max": -1.0,\n        "sentiment_magnitude_count": 2,\n        "sentiment_magnitude_mean": 0.5,\n        "sentiment_magnitude_std": 0.0,\n        "sentiment_magnitude_min": 0.5,\n        "sentiment_magnitude_max": 0.5\n      },\n      "positive": {\n        "positive_sentiment_count": 2,\n        "positive_sentiment_mean": 0.9625,\n        "positive_sentiment_std": 0.05303300858899107,\n        "positive_sentiment_min": 0.925,\n        "positive_sentiment_max": 1.0,\n        "negative_sentiment_count": 2,\n        "negative_sentiment_mean": 0.0,\n        "negative_sentiment_std": 0.0,\n        "negative_sentiment_min": 0.0,\n        "negative_sentiment_max": 0.0,\n        "net_sentiment_count": 2,\n        "net_sentiment_mean": 0.9625,\n        "net_sentiment_std": 0.05303300858899107,\n        "net_sentiment_min": 0.925,\n        "net_sentiment_max": 1.0,\n        "sentiment_magnitude_count": 2,\n        "sentiment_magnitude_mean": 0.48125,\n        "sentiment_magnitude_std": 0.026516504294495535,\n        "sentiment_magnitude_min": 0.4625,\n        "sentiment_magnitude_max": 0.5\n      }\n    },\n    "group_comparisons": {\n      "positive_sentiment": {\n        "group1_id": "negative",\n        "group2_id": "positive",\n        "mann_whitney_u_statistic": 0.0,\n        "p_value_descriptive": 0.11730449943636831,\n        "cohens_d_effect_size": -36.28984992921884\n      },\n      "negative_sentiment": {\n        "group1_id": "negative",\n        "group2_id": "positive",\n        "mann_whitney_u_statistic": 4.0,\n        "p_value_descriptive": 0.11730449943636831,\n        "cohens_d_effect_size": "inf"\n      },\n      "net_sentiment": {\n        "group1_id": "negative",\n        "group2_id": "positive",\n        "mann_whitney_u_statistic": 0.0,\n        "p_value_descriptive": 0.11730449943636831,\n        "cohens_d_effect_size": -36.28984992921884\n      },\n      "sentiment_magnitude": {\n        "group1_id": "negative",\n        "group2_id": "positive",\n        "mann_whitney_u_statistic": 4.0,\n        "p_value_descriptive": 0.11730449943636831,\n        "cohens_d_effect_size": 0.7071067811865475\n      }\n    },\n    "correlation_analysis": {\n      "positive_sentiment": {\n        "positive_sentiment": 1.0,\n        "negative_sentiment": -1.0,\n        "net_sentiment": 1.0,\n        "sentiment_magnitude": -0.1889822365046136\n      },\n      "negative_sentiment": {\n        "positive_sentiment": -1.0,\n        "negative_sentiment": 1.0,\n        "net_sentiment": -1.0,\n        "sentiment_magnitude": 0.1889822365046136\n      },\n      "net_sentiment": {\n        "positive_sentiment": 1.0,\n        "negative_sentiment": -1.0,\n        "net_sentiment": 1.0,\n        "sentiment_magnitude": -0.1889822365046136\n      },\n      "sentiment_magnitude": {\n        "positive_sentiment": -0.1889822365046136,\n        "negative_sentiment": 0.1889822365046136,\n        "net_sentiment": -0.1889822365046136,\n        "sentiment_magnitude": 1.0\n      }\n    },\n    "reliability_analysis": {\n      "cronbach_alpha": 1.0,\n      "confidence_interval_95": [\n        1.0,\n        1.0\n      ],\n      "caveat": "Result is highly unstable due to N=4 and only 2 items. For descriptive purposes only."\n    }\n  },\n  "sample_size_assessment": {\n    "total_documents": 4,\n    "tier_classification": "TIER 3: Exploratory Analysis",\n    "power_notes": "The analysis is based on a sample size of N=4 (n=2 per group), which falls into Tier 3. Consequently, the study is severely underpowered for inferential statistics. All results, including p-values and confidence intervals, should be interpreted as purely descriptive and exploratory. The findings are not generalizable and serve only to identify potential patterns within this specific micro-dataset and to validate pipeline functionality."\n  },\n  "methodology_summary": "This analysis followed a Tier 3 exploratory protocol due to the small sample size (N=4). The primary methods included: 1) Detailed descriptive statistics (mean, std, min, max) to summarize the data for each sentiment category, addressing the core research questions about score differences. 2) Non-parametric group comparisons using the Mann-Whitney U test with descriptive Cohen\'s d effect sizes to explore the magnitude of differences between positive and negative document groups, addressing hypotheses H1 and H2. 3) An exploratory Pearson correlation matrix to identify potential relationships between variables within this dataset. 4) A descriptive reliability analysis using Cronbach\'s alpha to assess the internal consistency of the two primary sentiment dimensions. All methods were chosen to maximize insight while acknowledging the profound limitations imposed by the low statistical power."\n}\n```', 'verification_status': 'verified', 'csv_files': [{'filename': 'scores.csv', 'path': '/Volumes/code/discernus/projects/micro_test_experiment/runs/20250915T090901Z/data/scores.csv', 'size': 3212}], 'total_cost': 0.0}, 'status': 'success_with_data', 'validation_passed': True}