{
  "status": "success",
  "functions_generated": 6,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 24540,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: micro_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-08-30T02:06:45.571038+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_stats(data, dimensions=None, derived_metrics=None):\n    \"\"\"\n    Calculates descriptive statistics (mean, std, min, max, count) for specified\n    sentiment dimensions and derived metrics.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis results.\n                             Expected columns: 'positive_sentiment_raw',\n                             'negative_sentiment_raw', 'net_sentiment',\n                             'sentiment_magnitude'.\n        dimensions (list, optional): List of dimension names to calculate stats for.\n                                     Defaults to ['positive_sentiment_raw', 'negative_sentiment_raw'].\n        derived_metrics (list, optional): List of derived metric names to calculate stats for.\n                                          Defaults to ['net_sentiment', 'sentiment_magnitude'].\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each specified\n              dimension and derived metric, or None if input data is invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if data is None or data.empty:\n        return None\n\n    if dimensions is None:\n        dimensions = ['positive_sentiment_raw', 'negative_sentiment_raw']\n    if derived_metrics is None:\n        derived_metrics = ['net_sentiment', 'sentiment_magnitude']\n\n    # Ensure all required columns exist, if not, return None\n    required_cols = dimensions + derived_metrics\n    if not all(col in data.columns for col in required_cols):\n        missing = [col for col in required_cols if col not in data.columns]\n        print(f\"Missing required columns for descriptive statistics: {missing}\")\n        return None\n\n    # Calculate derived metrics if they are not already present\n    if 'net_sentiment' not in data.columns and 'positive_sentiment_raw' in data.columns and 'negative_sentiment_raw' in data.columns:\n        data['net_sentiment'] = data['positive_sentiment_raw'] - data['negative_sentiment_raw']\n    if 'sentiment_magnitude' not in data.columns and 'positive_sentiment_raw' in data.columns and 'negative_sentiment_raw' in data.columns:\n        data['sentiment_magnitude'] = (data['positive_sentiment_raw'] + data['negative_sentiment_raw']) / 2\n\n    # Filter data to only include specified dimensions and derived metrics\n    cols_to_describe = [col for col in dimensions + derived_metrics if col in data.columns]\n    if not cols_to_describe:\n        return None\n\n    subset_data = data[cols_to_describe]\n\n    # Calculate descriptive statistics\n    descriptive_stats = subset_data.describe().to_dict()\n\n    return descriptive_stats\n\ndef perform_anova_test(data, group_col='sentiment_category', dependent_vars=None):\n    \"\"\"\n    Performs a one-way ANOVA test to compare the means of sentiment scores\n    across different sentiment categories.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis results.\n                             Must include columns for sentiment scores and a\n                             grouping column.\n        group_col (str): The name of the column that defines the groups\n                         (e.g., 'sentiment_category').\n        dependent_vars (list, optional): List of dependent variable column names\n                                         to perform ANOVA on.\n                                         Defaults to ['positive_sentiment_raw', 'negative_sentiment_raw'].\n\n    Returns:\n        dict: A dictionary where keys are dependent variable names and values\n              are dictionaries containing ANOVA results (F-statistic, p-value),\n              or None if insufficient data or requirements are not met.\n    \"\"\"\n    import pandas as pd\n    import statsmodels.api as sm\n    from statsmodels.formula.api import ols\n    from scipy import stats\n\n    if data is None or data.empty:\n        return None\n\n    if group_col not in data.columns:\n        print(f\"Grouping column '{group_col}' not found in data.\")\n        return None\n\n    if dependent_vars is None:\n        dependent_vars = ['positive_sentiment_raw', 'negative_sentiment_raw']\n\n    # Check if all dependent variables exist\n    if not all(dv in data.columns for dv in dependent_vars):\n        missing_dv = [dv for dv in dependent_vars if dv not in data.columns]\n        print(f\"Missing dependent variables for ANOVA: {missing_dv}\")\n        return None\n\n    # Ensure there are at least two groups and at least two observations per group\n    unique_groups = data[group_col].nunique()\n    if unique_groups < 2:\n        print(\"ANOVA requires at least two groups.\")\n        return None\n\n    group_counts = data[group_col].value_counts()\n    if not all(count >= 2 for count in group_counts):\n        print(\"ANOVA requires at least two observations per group.\")\n        return None\n\n    # --- Create derived metrics if they don't exist ---\n    if 'net_sentiment' not in data.columns and 'positive_sentiment_raw' in data.columns and 'negative_sentiment_raw' in data.columns:\n        data['net_sentiment'] = data['positive_sentiment_raw'] - data['negative_sentiment_raw']\n    if 'sentiment_magnitude' not in data.columns and 'positive_sentiment_raw' in data.columns and 'negative_sentiment_raw' in data.columns:\n        data['sentiment_magnitude'] = (data['positive_sentiment_raw'] + data['negative_sentiment_raw']) / 2\n    \n    # Add derived metrics to dependent_vars if they exist and are requested\n    if 'net_sentiment' in data.columns and 'net_sentiment' not in dependent_vars:\n        dependent_vars.append('net_sentiment')\n    if 'sentiment_magnitude' in data.columns and 'sentiment_magnitude' not in dependent_vars:\n        dependent_vars.append('sentiment_magnitude')\n\n    # Filter out dependent variables that are not in the dataframe\n    dependent_vars = [dv for dv in dependent_vars if dv in data.columns]\n\n    anova_results = {}\n\n    for dv in dependent_vars:\n        try:\n            # Create the formula for OLS\n            formula = f'{dv} ~ C({group_col})'\n            \n            # Perform ANOVA using statsmodels\n            model = ols(formula, data=data).fit()\n            anova_table = sm.stats.anova_lm(model, typ=2)\n\n            # Extract F-statistic and p-value\n            f_statistic = anova_table['F'][0]\n            p_value = anova_table['PR(>F)'][0]\n\n            anova_results[dv] = {\n                'f_statistic': float(f_statistic),\n                'p_value': float(p_value)\n            }\n        except Exception as e:\n            print(f\"Error performing ANOVA for {dv}: {e}\")\n            anova_results[dv] = {'error': str(e)}\n\n    return anova_results\n\ndef perform_posthoc_tukey_hsd(data, group_col='sentiment_category', dependent_vars=None):\n    \"\"\"\n    Performs Tukey's Honestly Significant Difference (HSD) post-hoc test\n    following a significant ANOVA result. This function assumes ANOVA has\n    already been performed and found significant differences.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis results.\n                             Must include columns for sentiment scores and a\n                             grouping column.\n        group_col (str): The name of the column that defines the groups\n                         (e.g., 'sentiment_category').\n        dependent_vars (list, optional): List of dependent variable column names\n                                         to perform Tukey's HSD on.\n                                         Defaults to ['positive_sentiment_raw', 'negative_sentiment_raw'].\n\n    Returns:\n        dict: A dictionary where keys are dependent variable names and values\n              are dictionaries containing Tukey's HSD results (pairwise comparisons),\n              or None if insufficient data or requirements are not met.\n    \"\"\"\n    import pandas as pd\n    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n    from scipy import stats\n\n    if data is None or data.empty:\n        return None\n\n    if group_col not in data.columns:\n        print(f\"Grouping column '{group_col}' not found in data.\")\n        return None\n\n    if dependent_vars is None:\n        dependent_vars = ['positive_sentiment_raw', 'negative_sentiment_raw']\n\n    # Ensure all dependent variables exist\n    if not all(dv in data.columns for dv in dependent_vars):\n        missing_dv = [dv for dv in dependent_vars if dv not in data.columns]\n        print(f\"Missing dependent variables for Tukey HSD: {missing_dv}\")\n        return None\n\n    # Ensure there are at least two groups and at least two observations per group\n    unique_groups = data[group_col].nunique()\n    if unique_groups < 2:\n        print(\"Tukey HSD requires at least two groups.\")\n        return None\n\n    group_counts = data[group_col].value_counts()\n    if not all(count >= 2 for count in group_counts):\n        print(\"Tukey HSD requires at least two observations per group.\")\n        return None\n\n    # --- Create derived metrics if they don't exist ---\n    if 'net_sentiment' not in data.columns and 'positive_sentiment_raw' in data.columns and 'negative_sentiment_raw' in data.columns:\n        data['net_sentiment'] = data['positive_sentiment_raw'] - data['negative_sentiment_raw']\n    if 'sentiment_magnitude' not in data.columns and 'positive_sentiment_raw' in data.columns and 'negative_sentiment_raw' in data.columns:\n        data['sentiment_magnitude'] = (data['positive_sentiment_raw'] + data['negative_sentiment_raw']) / 2\n\n    # Add derived metrics to dependent_vars if they exist and are requested\n    if 'net_sentiment' in data.columns and 'net_sentiment' not in dependent_vars:\n        dependent_vars.append('net_sentiment')\n    if 'sentiment_magnitude' in data.columns and 'sentiment_magnitude' not in dependent_vars:\n        dependent_vars.append('sentiment_magnitude')\n\n    # Filter out dependent variables that are not in the dataframe\n    dependent_vars = [dv for dv in dependent_vars if dv in data.columns]\n\n    tukey_results = {}\n\n    for dv in dependent_vars:\n        try:\n            # Perform Tukey's HSD test\n            # Ensure data is sorted by group for pairwise_tukeyhsd\n            data_sorted = data.sort_values(by=group_col)\n            \n            # Check for NaN values in the dependent variable or group column\n            if data_sorted[dv].isnull().any() or data_sorted[group_col].isnull().any():\n                print(f\"Skipping Tukey HSD for {dv} due to missing values.\")\n                continue\n\n            tukey_result = pairwise_tukeyhsd(endog=data_sorted[dv], groups=data_sorted[group_col], alpha=0.05)\n            \n            # Convert results to a dictionary for easier parsing\n            result_dict = {}\n            for i in range(len(tukey_result.groupsunique)):\n                for j in range(i + 1, len(tukey_result.groupsunique)):\n                    group1 = tukey_result.groupsunique[i]\n                    group2 = tukey_result.groupsunique[j]\n                    \n                    # Find the corresponding row in the summary table\n                    summary_row = tukey_result.summary().data[1:] # Skip header\n                    \n                    comparison_data = None\n                    for row in summary_row:\n                        if (row[0] == group1 and row[1] == group2) or (row[0] == group2 and row[1] == group1):\n                            comparison_data = row\n                            break\n                    \n                    if comparison_data:\n                        result_dict[f\"{group1} vs {group2}\"] = {\n                            'meandiff': float(comparison_data[2]),\n                            'lower_ci': float(comparison_data[3]),\n                            'upper_ci': float(comparison_data[4]),\n                            'reject_null': bool(comparison_data[5])\n                        }\n            \n            tukey_results[dv] = result_dict\n\n        except Exception as e:\n            print(f\"Error performing Tukey HSD for {dv}: {e}\")\n            tukey_results[dv] = {'error': str(e)}\n\n    return tukey_results\n\ndef calculate_sentiment_balance_and_magnitude(data):\n    \"\"\"\n    Calculates the 'net_sentiment' and 'sentiment_magnitude' derived metrics\n    from the raw positive and negative sentiment scores.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis results.\n                             Must include 'positive_sentiment_raw' and\n                             'negative_sentiment_raw' columns.\n\n    Returns:\n        pd.DataFrame: The original DataFrame with added 'net_sentiment' and\n                      'sentiment_magnitude' columns, or None if input is invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if data is None or data.empty:\n        return None\n\n    required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n    if not all(col in data.columns for col in required_cols):\n        missing = [col for col in required_cols if col not in data.columns]\n        print(f\"Missing required columns for derived metrics calculation: {missing}\")\n        return None\n\n    # Calculate Net Sentiment: positive_sentiment - negative_sentiment\n    data['net_sentiment'] = data['positive_sentiment_raw'] - data['negative_sentiment_raw']\n\n    # Calculate Sentiment Magnitude: (positive_sentiment + negative_sentiment) / 2\n    data['sentiment_magnitude'] = (data['positive_sentiment_raw'] + data['negative_sentiment_raw']) / 2\n\n    return data\n\ndef create_sentiment_category_grouping(data):\n    \"\"\"\n    Creates a 'sentiment_category' column in the DataFrame based on the\n    'positive_sentiment_raw' score to group documents into 'positive' or 'negative'\n    categories. This mapping is based on the experiment's design.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis results.\n                             Must include 'positive_sentiment_raw' column.\n\n    Returns:\n        pd.DataFrame: The original DataFrame with an added 'sentiment_category' column,\n                      or None if input is invalid or grouping cannot be performed.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if data is None or data.empty:\n        return None\n\n    if 'positive_sentiment_raw' not in data.columns:\n        print(\"Column 'positive_sentiment_raw' not found for creating sentiment categories.\")\n        return None\n\n    # Define the threshold for sentiment category.\n    # Based on the experiment description, we assume a simple split.\n    # A common approach is to consider scores > 0.5 as positive, <= 0.5 as negative.\n    # Given the sample data, positive_sentiment_raw is either 0.0 or 0.9,\n    # suggesting a clear binary split. We'll use a threshold that separates these.\n    # A threshold of 0.5 is a standard choice for binary classification.\n    \n    threshold = 0.5\n\n    # Assign categories: 'positive' if positive_sentiment_raw > threshold, 'negative' otherwise.\n    data['sentiment_category'] = np.where(data['positive_sentiment_raw'] > threshold, 'positive', 'negative')\n\n    # Ensure the category is a string type\n    data['sentiment_category'] = data['sentiment_category'].astype(str)\n\n    # Check if both categories are present\n    if data['sentiment_category'].nunique() < 2:\n        print(\"Warning: Only one sentiment category found after grouping. ANOVA and post-hoc tests may not be meaningful.\")\n\n    return data\n\ndef analyze_sentiment_dimensions_and_derived_metrics(data):\n    \"\"\"\n    Performs a comprehensive analysis of sentiment dimensions and derived metrics\n    as per the experiment's research questions. This includes:\n    1. Calculating descriptive statistics for all relevant metrics.\n    2. Creating sentiment categories for grouping.\n    3. Performing ANOVA to test for significant differences between categories.\n    4. Performing Tukey's HSD post-hoc tests if ANOVA is significant.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis results.\n                             Expected columns: 'positive_sentiment_raw',\n                             'negative_sentiment_raw'.\n\n    Returns:\n        dict: A dictionary containing the results of all analyses performed,\n              or None if input data is invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if data is None or data.empty:\n        return None\n\n    # Ensure necessary columns are present for initial calculations\n    if not all(col in data.columns for col in ['positive_sentiment_raw', 'negative_sentiment_raw']):\n        print(\"Input data is missing required sentiment score columns.\")\n        return None\n\n    # Step 1: Calculate derived metrics (net_sentiment, sentiment_magnitude)\n    data_with_derived = calculate_sentiment_balance_and_magnitude(data.copy())\n    if data_with_derived is None:\n        return None\n\n    # Step 2: Create sentiment category grouping\n    data_grouped = create_sentiment_category_grouping(data_with_derived.copy())\n    if data_grouped is None:\n        return None\n\n    # Define the variables to analyze\n    sentiment_dimensions = ['positive_sentiment_raw', 'negative_sentiment_raw']\n    derived_metrics = ['net_sentiment', 'sentiment_magnitude']\n    all_metrics = sentiment_dimensions + derived_metrics\n    grouping_variable = 'sentiment_category'\n\n    analysis_results = {}\n\n    # Step 3: Calculate descriptive statistics for all metrics\n    descriptive_stats = calculate_descriptive_stats(data_grouped.copy(), dimensions=sentiment_dimensions, derived_metrics=derived_metrics)\n    if descriptive_stats:\n        analysis_results['descriptive_statistics'] = descriptive_stats\n\n    # Step 4: Perform ANOVA test\n    # Check if there are at least two groups for ANOVA\n    if data_grouped[grouping_variable].nunique() >= 2:\n        anova_results = perform_anova_test(data_grouped.copy(), group_col=grouping_variable, dependent_vars=all_metrics)\n        if anova_results:\n            analysis_results['anova_results'] = anova_results\n\n            # Step 5: Perform Tukey's HSD post-hoc test for significant ANOVA results\n            for metric, stats in anova_results.items():\n                if 'p_value' in stats and stats['p_value'] < 0.05:\n                    print(f\"ANOVA for {metric} is significant (p={stats['p_value']:.4f}). Performing Tukey's HSD.\")\n                    # Ensure the metric is in the list for post-hoc if it wasn't originally\n                    if metric not in all_metrics:\n                        all_metrics.append(metric)\n                    \n                    # Perform Tukey's HSD only for this specific metric\n                    posthoc_results = perform_posthoc_tukey_hsd(data_grouped.copy(), group_col=grouping_variable, dependent_vars=[metric])\n                    if posthoc_results and metric in posthoc_results:\n                        if 'posthoc_tukey_hsd' not in analysis_results:\n                            analysis_results['posthoc_tukey_hsd'] = {}\n                        analysis_results['posthoc_tukey_hsd'][metric] = posthoc_results[metric]\n                else:\n                    print(f\"ANOVA for {metric} is not significant (p={stats.get('p_value', 'N/A'):.4f}). Skipping Tukey's HSD.\")\n    else:\n        print(\"Not enough sentiment categories (less than 2) to perform ANOVA.\")\n        analysis_results['anova_results'] = {'error': 'Insufficient number of sentiment categories.'}\n\n    return analysis_results\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}