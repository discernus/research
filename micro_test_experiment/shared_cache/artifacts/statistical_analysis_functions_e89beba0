{
  "status": "success",
  "functions_generated": 5,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 19385,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: micro_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-09T18:01:54.352827+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates derived sentiment metrics based on the framework specification.\n\n    This function computes 'net_sentiment' and 'sentiment_magnitude' from the raw\n    dimensional scores. It is a necessary preprocessing step for subsequent analyses\n    that rely on these composite measures.\n\n    Methodology:\n    - Net Sentiment: Calculated as the difference between positive and negative sentiment\n      scores (positive_sentiment_raw - negative_sentiment_raw). This metric reflects\n      the overall valence of the text.\n    - Sentiment Magnitude: Calculated as the averaged sum of positive and negative\n      sentiment scores ((positive_sentiment_raw + negative_sentiment_raw) / 2). This\n      metric reflects the overall emotional intensity of the text, irrespective of\n      valence. The division by 2 normalizes the metric back to the 0.0-1.0 range.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis scores. Must include\n                             'positive_sentiment_raw' and 'negative_sentiment_raw' columns.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        pd.DataFrame: The input DataFrame augmented with 'net_sentiment' and\n                      'sentiment_magnitude' columns, or None if required columns\n                      are missing or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if not all(col in data.columns for col in ['positive_sentiment_raw', 'negative_sentiment_raw']):\n            # As a seasoned practitioner, I must log this, but for the framework, returning None is sufficient.\n            return None\n\n        df = data.copy()\n\n        # Net Sentiment: Balance between positive and negative sentiment\n        df['net_sentiment'] = df['positive_sentiment_raw'] - df['negative_sentiment_raw']\n\n        # Sentiment Magnitude: Combined intensity of emotional language\n        df['sentiment_magnitude'] = (df['positive_sentiment_raw'] + df['negative_sentiment_raw']) / 2\n\n        return df\n\n    except Exception:\n        # A robust function must handle unexpected errors gracefully.\n        return None\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Generates descriptive statistics for sentiment scores, grouped by sentiment category.\n\n    Methodology:\n    This function first calculates derived metrics. It then assigns each document to a\n    'sentiment_category' (positive/negative) based on its filename. Finally, it computes\n    descriptive statistics (mean, standard deviation, count, min, max) for each primary\n    dimension and derived metric, grouped by the sentiment category.\n\n    Power Assessment:\n    Exploratory analysis - results are suggestive rather than conclusive (N < 15).\n    With extremely small sample sizes (n<5 per group), descriptive statistics provide a\n    basic summary of the observed data but are highly sensitive to individual data points\n    and should not be used to infer population characteristics. The focus is on pattern\n    recognition within the sample.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing analysis data with a 'document_name' column.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary of descriptive statistics for each variable, grouped by\n              sentiment category, or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if 'document_name' not in data.columns or data.shape[0] < 1:\n            return None\n\n        # Step 1: Calculate derived metrics\n        data_with_derived = calculate_derived_metrics(data)\n        if data_with_derived is None:\n            return None\n\n        # Step 2: Create grouping variable from document name\n        data_with_derived['sentiment_category'] = np.where(\n            data_with_derived['document_name'].str.startswith('positive'), 'positive', 'negative'\n        )\n\n        if data_with_derived['sentiment_category'].nunique() < 2:\n            # Analysis requires at least two groups to compare.\n            return None\n\n        # Step 3: Define variables for analysis\n        analysis_vars = [\n            'positive_sentiment_raw',\n            'negative_sentiment_raw',\n            'net_sentiment',\n            'sentiment_magnitude'\n        ]\n        \n        # Ensure all analysis variables are present\n        if not all(col in data_with_derived.columns for col in analysis_vars):\n            return None\n\n        # Step 4: Calculate descriptive statistics\n        descriptives = data_with_derived.groupby('sentiment_category')[analysis_vars].describe()\n\n        # Format output for JSON serialization\n        results = descriptives.to_dict('index')\n        for group, stats in results.items():\n            # Unstack the multi-level columns\n            results[group] = {var: {stat: val for stat, val in stats[var].items()} for var in stats}\n\n        return {\n            \"analysis_type\": \"Descriptive Statistics\",\n            \"power_assessment\": \"Tier 3 (Exploratory): N < 15. Results are suggestive, not conclusive.\",\n            \"statistics\": results\n        }\n\n    except Exception:\n        return None\n\ndef perform_anova_analysis(data, **kwargs):\n    \"\"\"\n    Performs a one-way ANOVA to compare sentiment scores between sentiment categories.\n\n    Methodology:\n    This function compares the means of two groups ('positive' vs. 'negative' sentiment\n    documents) on several dependent variables: positive_sentiment_raw, negative_sentiment_raw,\n    net_sentiment, and sentiment_magnitude. It calculates the F-statistic, p-value, and\n    the effect size (eta-squared, \u03b7\u00b2). Eta-squared measures the proportion of variance in the\n    dependent variable that is associated with the group membership.\n\n    Power Assessment:\n    Exploratory analysis - results are suggestive rather than conclusive (N < 15, n < 5 per group).\n    ANOVA is not statistically valid with group sizes this small (n=2). Assumptions of normality\n    and homogeneity of variances cannot be tested. The p-value is uninterpretable. The analysis\n    is performed to fulfill the experimental specification, but the output should be treated as\n    purely descriptive. The effect size (eta-squared) is reported as it describes the magnitude\n    of the difference within this specific sample, but it is likely an unstable and inflated estimate.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis data, including 'document_name'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing ANOVA results (F-statistic, p-value, eta-squared)\n              for each dependent variable, or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n    from scipy.stats import f_oneway\n\n    try:\n        if 'document_name' not in data.columns or data.shape[0] < 4:\n            return None\n\n        # Step 1: Calculate derived metrics\n        df = calculate_derived_metrics(data)\n        if df is None:\n            return None\n\n        # Step 2: Create grouping variable\n        df['sentiment_category'] = np.where(df['document_name'].str.startswith('positive'), 'positive', 'negative')\n        \n        groups = df['sentiment_category'].unique()\n        if len(groups) < 2:\n            return None # ANOVA requires at least 2 groups\n\n        # Step 3: Define variables and prepare for analysis\n        analysis_vars = ['positive_sentiment_raw', 'negative_sentiment_raw', 'net_sentiment', 'sentiment_magnitude']\n        results = {}\n\n        for var in analysis_vars:\n            if var not in df.columns:\n                continue\n\n            # Check for sufficient data per group (at least 2 for variance calculation)\n            group_data = [df[var][df['sentiment_category'] == g].dropna() for g in groups]\n            if any(len(d) < 2 for d in group_data):\n                continue\n\n            # Step 4: Perform ANOVA\n            f_stat, p_val = f_oneway(*group_data)\n\n            # Step 5: Calculate Eta-Squared (\u03b7\u00b2)\n            ss_between = sum(len(g) * (g.mean() - df[var].mean())**2 for g in group_data)\n            ss_total = sum((df[var] - df[var].mean())**2)\n            eta_squared = ss_between / ss_total if ss_total > 0 else 0\n\n            results[var] = {\n                'f_statistic': f_stat,\n                'p_value': p_val,\n                'eta_squared': eta_squared,\n                'interpretation_caveat': \"p-value is uninterpretable due to extremely small sample size.\"\n            }\n\n        if not results:\n            return None\n\n        return {\n            \"analysis_type\": \"One-Way ANOVA\",\n            \"power_assessment\": \"Tier 3 (Exploratory): N < 15, n < 5 per group. Results are not statistically valid.\",\n            \"results\": results\n        }\n\n    except Exception:\n        return None\n\ndef calculate_reliability_analysis(data, **kwargs):\n    \"\"\"\n    Calculates internal consistency (Cronbach's alpha) for the primary sentiment dimensions.\n\n    Methodology:\n    This function assesses the reliability of the 'positive_sentiment_raw' and\n    'negative_sentiment_raw' dimensions as if they were items on a single scale.\n    Cronbach's alpha is a measure of internal consistency, indicating how closely\n    related a set of items are as a group. For two items, alpha is a direct function\n    of their correlation. A negative alpha suggests that the items are negatively\n    correlated, which in this case is expected (higher positive should correlate with\n    lower negative). For interpretation, the negative sentiment score is typically\n    reverse-coded, but here we calculate it on the raw scores to check their direct\n    relationship as specified by the framework.\n\n    Power Assessment:\n    Exploratory analysis - results are suggestive rather than conclusive (N < 15).\n    Reliability estimates from very small samples are highly unstable and should not be\n    considered a true measure of the instrument's reliability. This analysis is provided\n    for pipeline validation purposes only.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis data.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the Cronbach's alpha value, or None if data\n              is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n    import pingouin as pg\n\n    try:\n        items = ['positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in items) or data.shape[0] < 2:\n            return None\n\n        # Drop rows with missing values in the items\n        alpha_data = data[items].dropna()\n\n        if alpha_data.shape[0] < 2:\n            return None\n\n        # Calculate Cronbach's alpha\n        alpha_results = pg.cronbach_alpha(data=alpha_data)\n        cronbach_alpha = alpha_results[0]\n\n        return {\n            \"analysis_type\": \"Reliability Analysis (Cronbach's Alpha)\",\n            \"power_assessment\": \"Tier 3 (Exploratory): N < 15. Estimate is unstable and not generalizable.\",\n            \"items\": items,\n            \"cronbach_alpha\": cronbach_alpha,\n            \"n_items\": alpha_results[2],\n            \"n_samples\": alpha_data.shape[0]\n        }\n\n    except Exception:\n        return None\n\ndef perform_correlation_analysis(data, **kwargs):\n    \"\"\"\n    Computes the Pearson correlation matrix for all sentiment scores.\n\n    Methodology:\n    This function calculates the pairwise correlation between all primary and derived\n    sentiment metrics using the Pearson correlation coefficient (r). This assesses the\n    strength and direction of the linear relationship between each pair of variables.\n    The resulting matrix shows how each variable relates to every other variable.\n\n    Power Assessment:\n    Exploratory analysis - results are suggestive rather than conclusive (N < 15).\n    Correlation coefficients derived from small samples are highly volatile and may not\n    accurately reflect the true population correlation. P-values are not reported as they\n    would be statistically meaningless. The analysis serves to identify the magnitude and\n    direction of relationships within this specific dataset only.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis data.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the correlation matrix, or None if data is\n              insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        # Step 1: Calculate derived metrics\n        df = calculate_derived_metrics(data)\n        if df is None:\n            return None\n\n        # Step 2: Define variables for correlation\n        correlation_vars = [\n            'positive_sentiment_raw',\n            'negative_sentiment_raw',\n            'net_sentiment',\n            'sentiment_magnitude',\n            'positive_sentiment_salience',\n            'negative_sentiment_salience',\n            'positive_sentiment_confidence',\n            'negative_sentiment_confidence'\n        ]\n        \n        # Filter for available columns to avoid errors\n        available_vars = [var for var in correlation_vars if var in df.columns]\n        \n        if len(available_vars) < 2:\n            return None\n\n        # Step 3: Compute correlation matrix\n        correlation_matrix = df[available_vars].corr(method='pearson')\n\n        # Convert to dictionary for JSON output\n        results = correlation_matrix.to_dict('index')\n\n        return {\n            \"analysis_type\": \"Pearson Correlation Matrix\",\n            \"power_assessment\": \"Tier 3 (Exploratory): N < 15. Coefficients are unstable and not generalizable.\",\n            \"correlation_matrix\": results\n        }\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}