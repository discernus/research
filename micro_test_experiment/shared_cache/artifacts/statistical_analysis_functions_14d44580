{
  "status": "success",
  "functions_generated": 3,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 19093,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: micro_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-12T00:56:38.729407+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for sentiment dimensions and derived metrics, grouped by sentiment category.\n\n    Methodology:\n    This function operates under the Tier 3 (Exploratory Analysis) protocol due to the small sample size (N<15).\n    The results are intended for pattern recognition and are suggestive rather than conclusive.\n    1.  Input data is validated to ensure necessary columns ('document_name', 'positive_sentiment_raw', 'negative_sentiment_raw') are present.\n    2.  A 'sentiment_category' grouping variable is created by mapping document filenames (e.g., 'positive_test_1.txt' -> 'positive').\n    3.  Derived metrics 'net_sentiment' (positive - negative) and 'sentiment_magnitude' ((positive + negative) / 2) are calculated as per the framework specification.\n    4.  The DataFrame is grouped by 'sentiment_category'.\n    5.  Descriptive statistics (mean, standard deviation, count, min, max) are computed for each primary dimension and derived metric within each group.\n    6.  The total sample size (N) is reported in the output.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis scores with columns including 'document_name',\n                             'positive_sentiment_raw', and 'negative_sentiment_raw'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each metric, grouped by sentiment category.\n              Returns None if the input data is invalid or insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n        \n        df = data.copy()\n        \n        # Tier 3 Power Assessment\n        n_total = len(df)\n        power_caveat = f\"Exploratory analysis - results are suggestive rather than conclusive (N={n_total}).\"\n\n        # 1. Create grouping variable from document name\n        def get_sentiment_category(filename):\n            if isinstance(filename, str):\n                if filename.lower().startswith('positive'):\n                    return 'positive'\n                elif filename.lower().startswith('negative'):\n                    return 'negative'\n            return 'unknown'\n\n        df['sentiment_category'] = df['document_name'].apply(get_sentiment_category)\n\n        if 'unknown' in df['sentiment_category'].unique():\n            # Handle cases where grouping fails for some rows\n            df = df[df['sentiment_category'] != 'unknown']\n        \n        if df.empty or df['sentiment_category'].nunique() < 1:\n            return None\n\n        # 2. Calculate derived metrics\n        df['net_sentiment'] = df['positive_sentiment_raw'] - df['negative_sentiment_raw']\n        df['sentiment_magnitude'] = (df['positive_sentiment_raw'] + df['negative_sentiment_raw']) / 2\n\n        # 3. Calculate descriptive statistics\n        metrics = ['positive_sentiment_raw', 'negative_sentiment_raw', 'net_sentiment', 'sentiment_magnitude']\n        \n        # Use .agg to prevent potential FutureWarning with multiple aggregations\n        descriptives = df.groupby('sentiment_category')[metrics].agg(['mean', 'std', 'count', 'min', 'max'])\n        \n        # Reformat the output into a nested dictionary for better JSON serialization\n        results = {\n            'power_assessment': {\n                'tier': 'Tier 3 (Exploratory)',\n                'sample_size': n_total,\n                'caveat': power_caveat\n            },\n            'descriptive_statistics': descriptives.to_dict(orient='index')\n        }\n        \n        # Convert numpy types to native Python types for JSON compatibility\n        def convert_numpy_types(obj):\n            if isinstance(obj, dict):\n                return {k: convert_numpy_types(v) for k, v in obj.items()}\n            if isinstance(obj, np.integer):\n                return int(obj)\n            if isinstance(obj, np.floating):\n                return float(obj)\n            if isinstance(obj, (list, tuple)):\n                return [convert_numpy_types(i) for i in obj]\n            return obj\n\n        return convert_numpy_types(results)\n\n    except Exception:\n        return None\n\ndef perform_exploratory_anova(data, **kwargs):\n    \"\"\"\n    Performs an exploratory one-way ANOVA to compare sentiment scores between sentiment categories.\n\n    Methodology:\n    This function is classified as Tier 3 (Exploratory Analysis) due to the extremely small sample size (N<5 per group),\n    as specified in the experimental design.\n\n    Statistical Conservatism and Caveats:\n    - The results are purely descriptive and should NOT be used for inferential claims.\n    - The p-value is not interpretable in the traditional sense of statistical significance due to the violation of sample size assumptions for ANOVA.\n    - Assumptions of ANOVA (normality of residuals, homogeneity of variances) cannot be meaningfully tested with such small N and are almost certainly violated.\n    - The analysis is performed because it was explicitly requested by the research design for pipeline validation.\n    - Effect size (Eta-squared) is reported as it can be more informative than p-values in underpowered contexts, but should still be interpreted with extreme caution.\n\n    Process:\n    1.  Prepares the data by creating the 'sentiment_category' group and calculating derived metrics.\n    2.  Checks for the minimum data requirements (at least two groups with at least n=2).\n    3.  For each dependent variable, a one-way ANOVA is performed using `scipy.stats.f_oneway`.\n    4.  Eta-squared (\u03b7\u00b2) effect size is calculated to estimate the proportion of variance attributable to the group.\n\n    Args:\n        data (pd.DataFrame): DataFrame with columns 'document_name', 'positive_sentiment_raw', 'negative_sentiment_raw'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the F-statistic, p-value, and eta-squared for each metric.\n              Returns None if data is insufficient for ANOVA.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy import stats\n\n    try:\n        required_cols = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        df = data.copy()\n        n_total = len(df)\n\n        def get_sentiment_category(filename):\n            if isinstance(filename, str):\n                if filename.lower().startswith('positive'):\n                    return 'positive'\n                elif filename.lower().startswith('negative'):\n                    return 'negative'\n            return 'unknown'\n\n        df['sentiment_category'] = df['document_name'].apply(get_sentiment_category)\n        df = df[df['sentiment_category'] != 'unknown']\n\n        group_counts = df['sentiment_category'].value_counts()\n        if len(group_counts) < 2 or not all(count >= 2 for count in group_counts):\n            return {\n                'status': 'failed',\n                'reason': 'Insufficient data for ANOVA (requires at least 2 groups with n>=2).',\n                'group_counts': group_counts.to_dict()\n            }\n\n        df['net_sentiment'] = df['positive_sentiment_raw'] - df['negative_sentiment_raw']\n        df['sentiment_magnitude'] = (df['positive_sentiment_raw'] + df['negative_sentiment_raw']) / 2\n\n        metrics = ['positive_sentiment_raw', 'negative_sentiment_raw', 'net_sentiment', 'sentiment_magnitude']\n        results = {}\n\n        for metric in metrics:\n            groups = [df[metric][df['sentiment_category'] == category] for category in df['sentiment_category'].unique()]\n            \n            # Check for zero variance within groups, which can cause errors\n            if any(g.var() == 0 for g in groups if len(g) > 1):\n                f_stat, p_value = (np.nan, np.nan) # Cannot compute ANOVA\n            else:\n                f_stat, p_value = stats.f_oneway(*groups)\n\n            # Calculate Eta-squared (\u03b7\u00b2)\n            ss_total = ((df[metric] - df[metric].mean())**2).sum()\n            if ss_total > 0:\n                # SS_between = F * (k-1) * MS_within\n                # A more direct calculation:\n                grand_mean = df[metric].mean()\n                ss_between = sum(len(g) * (g.mean() - grand_mean)**2 for g in groups)\n                eta_squared = ss_between / ss_total\n            else:\n                eta_squared = np.nan\n\n            results[metric] = {\n                'f_statistic': f_stat if not np.isnan(f_stat) else 'NaN',\n                'p_value': p_value if not np.isnan(p_value) else 'NaN',\n                'eta_squared': eta_squared if not np.isnan(eta_squared) else 'NaN'\n            }\n\n        return {\n            'power_assessment': {\n                'tier': 'Tier 3 (Exploratory)',\n                'sample_size': n_total,\n                'group_counts': group_counts.to_dict(),\n                'caveat': f\"Exploratory analysis - results are suggestive, not conclusive (N={n_total}). P-values are not interpretable due to small sample size.\"\n            },\n            'anova_results': results\n        }\n\n    except Exception:\n        return None\n\ndef calculate_internal_consistency(data, **kwargs):\n    \"\"\"\n    Calculates the internal consistency (Cronbach's alpha) between positive and negative sentiment scores.\n\n    Methodology:\n    This function operates under the Tier 3 (Exploratory Analysis) protocol due to the small sample size (N<15).\n    The reliability estimate is highly unstable and should be interpreted with extreme caution.\n\n    Statistical Conservatism and Caveats:\n    - Cronbach's alpha is calculated on two \"items\": (1) `positive_sentiment_raw` and (2) the inverted `negative_sentiment_raw` (1 - score).\n    - This assesses the extent to which these two dimensions measure the same underlying construct (sentiment valence) in opposite directions.\n    - With only two items, Cronbach's alpha is mathematically equivalent to the Spearman-Brown prophecy formula for a two-item test, which simplifies to `2 * r / (1 + r)`, where `r` is the Pearson correlation between the items.\n    - Given the small N, this value is highly sensitive to individual data points and is provided for exploratory purposes and pipeline validation only.\n\n    Process:\n    1.  Validates input data and sample size.\n    2.  Creates a new item by inverting the negative sentiment score (`1 - negative_sentiment_raw`).\n    3.  Calculates the Pearson correlation between `positive_sentiment_raw` and the inverted negative score.\n    4.  Computes Cronbach's alpha using the simplified two-item formula.\n\n    Args:\n        data (pd.DataFrame): DataFrame with columns 'positive_sentiment_raw' and 'negative_sentiment_raw'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the number of items, sample size, Pearson correlation, and Cronbach's alpha.\n              Returns None if data is insufficient for calculation.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        df = data[required_cols].copy().dropna()\n        n_total = len(df)\n\n        if n_total < 2:\n            return None\n\n        power_caveat = f\"Exploratory analysis - reliability estimate is unstable and suggestive, not conclusive (N={n_total}).\"\n\n        # Create the two items for the scale.\n        # Item 1: positive_sentiment_raw\n        # Item 2: The inverse of negative sentiment, to align it with the positive scale.\n        df['negative_sentiment_inverted'] = 1 - df['negative_sentiment_raw']\n        \n        items = df[['positive_sentiment_raw', 'negative_sentiment_inverted']]\n        \n        # With only 2 items, alpha can be calculated from their correlation.\n        # alpha = (k * r_bar) / (1 + (k-1) * r_bar)\n        # For k=2, this is 2*r / (1+r)\n        \n        # Check for zero variance, which makes correlation undefined\n        if items.iloc[:, 0].var() == 0 or items.iloc[:, 1].var() == 0:\n            return {\n                'power_assessment': {\n                    'tier': 'Tier 3 (Exploratory)',\n                    'sample_size': n_total,\n                    'caveat': power_caveat\n                },\n                'status': 'failed',\n                'reason': 'Cannot compute correlation due to zero variance in one or both items.'\n            }\n\n        correlation = items.iloc[:, 0].corr(items.iloc[:, 1])\n        \n        if pd.isna(correlation):\n             return {\n                'power_assessment': {\n                    'tier': 'Tier 3 (Exploratory)',\n                    'sample_size': n_total,\n                    'caveat': power_caveat\n                },\n                'status': 'failed',\n                'reason': 'Correlation resulted in NaN, cannot compute alpha.'\n            }\n\n        # Calculate Cronbach's alpha for two items\n        if correlation > -1: # Denominator would be zero if r = -1\n            cronbach_alpha = (2 * correlation) / (1 + correlation)\n        else:\n            cronbach_alpha = -np.inf # Or handle as undefined\n\n        return {\n            'power_assessment': {\n                'tier': 'Tier 3 (Exploratory)',\n                'sample_size': n_total,\n                'caveat': power_caveat\n            },\n            'internal_consistency': {\n                'cronbach_alpha': cronbach_alpha,\n                'num_items': 2,\n                'item_pearson_correlation': correlation\n            }\n        }\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}