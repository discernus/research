{
  "generation_metadata": {
    "status": "success",
    "functions_generated": 8,
    "output_file": "automatedderivedmetricsagent_functions.py",
    "module_size": 8127,
    "function_code_content": "[\n  {\n    \"filename\": \"sentiment_binary_v1_calculations.py\",\n    \"content\": \"import pandas as pd\\nimport numpy as np\\nimport json\\nfrom typing import Optional, Dict, Any\\n\\ndef _get_scores_from_row(row: pd.Series) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Parses the 'raw_analysis_response' column of a DataFrame row to extract dimensional scores.\\n\\n    This helper function is designed to be robust against various data inconsistencies,\\n    including missing columns, non-string data, different JSON delimiters, malformed JSON,\\n    and variations in the nested data structure.\\n\\n    Args:\\n        row: A pandas Series representing a row of a DataFrame.\\n\\n    Returns:\\n        A dictionary containing the dimensional scores if found, otherwise None.\\n    \\\"\\\"\\\"\\n    if 'raw_analysis_response' not in row:\\n        return None\\n\\n    raw_response = row['raw_analysis_response']\\n    if not isinstance(raw_response, str):\\n        return None\\n\\n    json_content = None\\n    start_marker = '<<<DISCERNUS_ANALYSIS_JSON_v6>>>'\\n    end_marker = '<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>'\\n\\n    # Primary parsing method based on explicit instructions\\n    if start_marker in raw_response and end_marker in raw_response:\\n        start_idx = raw_response.find(start_marker)\\n        end_idx = raw_response.find(end_marker)\\n        json_content = raw_response[start_idx + len(start_marker):end_idx].strip()\\n    # Fallback for markdown code fences\\n    elif raw_response.strip().startswith(\\\"```json\\\"):\\n        json_part = raw_response.strip()\\n        json_content = json_part[len(\\\"```json\\\"):].strip().rstrip('`')\\n    # Fallback for raw JSON strings\\n    elif raw_response.strip().startswith(\\\"{\\\"):\\n        json_content = raw_response.strip()\\n\\n    if not json_content:\\n        return None\\n\\n    try:\\n        analysis = json.loads(json_content)\\n        # Navigate the structure based on the sample data\\n        if 'document_analyses' in analysis and isinstance(analysis['document_analyses'], list) and len(analysis['document_analyses']) > 0:\\n            # The framework is designed for single document analysis per call\\n            doc_analysis = analysis['document_analyses'][0]\\n            if 'dimensional_scores' in doc_analysis:\\n                return doc_analysis['dimensional_scores']\\n    except (json.JSONDecodeError, KeyError, IndexError, TypeError):\\n        return None\\n    \\n    return None\\n\\ndef calculate_net_sentiment(data: pd.DataFrame, **kwargs) -> Optional[float]:\\n    \\\"\\\"\\\"\\n    Calculates the average Net Sentiment across all documents in the DataFrame.\\n    Net Sentiment is the balance between positive and negative sentiment.\\n\\n    Formula: mean(dimensions.positive_sentiment.raw_score - dimensions.negative_sentiment.raw_score)\\n\\n    Args:\\n        data: pandas DataFrame with a 'raw_analysis_response' column.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        A single float representing the average net sentiment, or None if it cannot be calculated.\\n    \\\"\\\"\\\"\\n    def _calculate(row):\\n        scores = _get_scores_from_row(row)\\n        if scores:\\n            try:\\n                pos_score = scores.get('positive_sentiment', {}).get('raw_score')\\n                neg_score = scores.get('negative_sentiment', {}).get('raw_score')\\n                if pos_score is not None and neg_score is not None:\\n                    return float(pos_score) - float(neg_score)\\n            except (AttributeError, TypeError, ValueError):\\n                return None\\n        return None\\n\\n    if 'raw_analysis_response' not in data.columns:\\n        return None\\n\\n    net_sentiments = data.apply(_calculate, axis=1).dropna()\\n\\n    if net_sentiments.empty:\\n        return None\\n    \\n    return float(net_sentiments.mean())\\n\\ndef calculate_sentiment_magnitude(data: pd.DataFrame, **kwargs) -> Optional[float]:\\n    \\\"\\\"\\\"\\n    Calculates the average Sentiment Magnitude across all documents in the DataFrame.\\n    Sentiment Magnitude is the combined intensity of emotional language.\\n\\n    Formula: mean((dimensions.positive_sentiment.raw_score + dimensions.negative_sentiment.raw_score) / 2)\\n\\n    Args:\\n        data: pandas DataFrame with a 'raw_analysis_response' column.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        A single float representing the average sentiment magnitude, or None if it cannot be calculated.\\n    \\\"\\\"\\\"\\n    def _calculate(row):\\n        scores = _get_scores_from_row(row)\\n        if scores:\\n            try:\\n                pos_score = scores.get('positive_sentiment', {}).get('raw_score')\\n                neg_score = scores.get('negative_sentiment', {}).get('raw_score')\\n                if pos_score is not None and neg_score is not None:\\n                    return (float(pos_score) + float(neg_score)) / 2.0\\n            except (AttributeError, TypeError, ValueError):\\n                return None\\n        return None\\n\\n    if 'raw_analysis_response' not in data.columns:\\n        return None\\n\\n    magnitudes = data.apply(_calculate, axis=1).dropna()\\n\\n    if magnitudes.empty:\\n        return None\\n        \\n    return float(magnitudes.mean())\\n\\ndef calculate_all_derived_metrics(data: pd.DataFrame, **kwargs) -> Dict[str, Optional[float]]:\\n    \\\"\\\"\\\"\\n    Calculates all derived metrics for the given DataFrame, returning a dictionary of aggregate scores.\\n    This function calls each individual metric calculation function directly by name.\\n\\n    Args:\\n        data: pandas DataFrame with analysis data.\\n        **kwargs: Additional parameters to be passed to calculation functions.\\n\\n    Returns:\\n        A dictionary where keys are metric names and values are the calculated aggregate scores.\\n    \\\"\\\"\\\"\\n    metrics = {\\n        \\\"net_sentiment\\\": calculate_net_sentiment(data, **kwargs),\\n        \\\"sentiment_magnitude\\\": calculate_sentiment_magnitude(data, **kwargs),\\n    }\\n    return metrics\\n\\ndef calculate_derived_metrics(data: pd.DataFrame, **kwargs) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates all derived metrics for each row and adds them as new columns to the DataFrame.\\n\\n    This function processes each row, calculates the derived metrics based on its\\n    'raw_analysis_response' data, and appends the results as new columns named\\n    'net_sentiment' and 'sentiment_magnitude'.\\n\\n    Args:\\n        data: pandas DataFrame with a 'raw_analysis_response' column.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        A new pandas DataFrame with the original data plus the new derived metric columns.\\n        Missing values in the new columns are filled with 0.0.\\n    \\\"\\\"\\\"\\n    df = data.copy()\\n    \\n    if 'raw_analysis_response' not in df.columns:\\n        df['net_sentiment'] = 0.0\\n        df['sentiment_magnitude'] = 0.0\\n        return df\\n\\n    def _calculate_metrics_for_row(row: pd.Series) -> pd.Series:\\n        \\\"\\\"\\\"Helper to calculate all metrics for a single row.\\\"\\\"\\\"\\n        scores = _get_scores_from_row(row)\\n        net_sentiment = None\\n        sentiment_magnitude = None\\n\\n        if scores:\\n            try:\\n                pos_score = scores.get('positive_sentiment', {}).get('raw_score')\\n                neg_score = scores.get('negative_sentiment', {}).get('raw_score')\\n\\n                if pos_score is not None and neg_score is not None:\\n                    pos_score_f = float(pos_score)\\n                    neg_score_f = float(neg_score)\\n                    net_sentiment = pos_score_f - neg_score_f\\n                    sentiment_magnitude = (pos_score_f + neg_score_f) / 2.0\\n            except (AttributeError, TypeError, ValueError):\\n                pass\\n\\n        return pd.Series({\\n            'net_sentiment': net_sentiment,\\n            'sentiment_magnitude': sentiment_magnitude\\n        })\\n\\n    derived_metrics_df = df.apply(_calculate_metrics_for_row, axis=1)\\n    \\n    df = df.join(derived_metrics_df)\\n\\n    df['net_sentiment'] = df['net_sentiment'].fillna(0.0)\\n    df['sentiment_magnitude'] = df['sentiment_magnitude'].fillna(0.0)\\n\\n    return df\\n\"\n  }\n]",
    "cached_with_code": true
  },
  "derived_metrics_data": {
    "status": "success",
    "original_count": 4,
    "derived_count": 4,
    "derived_metrics": [
      {
        "raw_analysis_response": "```json\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.95,\n    \"analysis_notes\": \"Applied three independent analytical approaches with median aggregation. Document is explicitly designed as a positive sentiment test case, leading to high scores for positive and zero for negative sentiment.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"positive_test_1.txt\",\n      \"document_name\": \"Positive Test Document 1\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence_quotes\": {\n        \"positive_sentiment\": [\n          \"This is a wonderful day! Everything is going perfectly. I feel great about the future.\",\n          \"Success is everywhere. The team did an excellent job. We're achieving amazing results. Optimism fills the air. What a fantastic opportunity! I'm thrilled with the progress. Everything looks bright and promising.\"\n        ],\n        \"negative_sentiment\": []\n      },\n      \"marked_up_document\": \"# Document Analysis - Marked Up Text\\n\\n# Positive Test Document 1\\n\\n**Context**: Sample text with positive sentiment\\n\\n[POSITIVE_SENTIMENT: \\\"This is a wonderful day!\\\"] [POSITIVE_SENTIMENT: \\\"Everything is going perfectly.\\\"] [POSITIVE_SENTIMENT: \\\"I feel great about the future.\\\"] [POSITIVE_SENTIMENT: \\\"Success is everywhere.\\\"] [POSITIVE_SENTIMENT: \\\"The team did an excellent job.\\\"] [POSITIVE_SENTIMENT: \\\"We're achieving amazing results.\\\"] [POSITIVE_SENTIMENT: \\\"Optimism fills the air.\\\"] [POSITIVE_SENTIMENT: \\\"What a fantastic opportunity!\\\"] [POSITIVE_SENTIMENT: \\\"I'm thrilled with the progress.\\\"] [POSITIVE_SENTIMENT: \\\"Everything looks bright and promising.\\\"]\"\n    }\n  ]\n}\n```",
        "net_sentiment": 1.0,
        "sentiment_magnitude": 0.5
      },
      {
        "raw_analysis_response": "```json\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"Sentiment Binary Framework v1.0\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.98,\n    \"analysis_notes\": \"Applied three independent analytical approaches with median aggregation. The document is explicitly and entirely positive, making scoring straightforward. No negative sentiment was present.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"positive_test_2.txt\",\n      \"document_name\": \"Positive Test Document 2\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence_quotes\": {\n        \"positive_sentiment\": [\n          \"What a superb morning! All systems are operating flawlessly. I'm excited about what's coming next. Achievement surrounds us. The group performed outstandingly. We're reaching incredible goals.\",\n          \"Hopefulness permeates everything. Such a marvelous chance! I'm delighted by the advancement. Everything appears glowing and encouraging.\"\n        ],\n        \"negative_sentiment\": []\n      },\n      \"marked_up_document\": \"# Document Analysis - Marked Up Text\\n\\n# Positive Test Document 2\\n\\n**Context**: [positive_sentiment: \\\"Sample text with positive sentiment\\\"]\\n\\n[positive_sentiment: \\\"What a superb morning!\\\"] [positive_sentiment: \\\"All systems are operating flawlessly.\\\"] [positive_sentiment: \\\"I'm excited about what's coming next.\\\"] [positive_sentiment: \\\"Achievement surrounds us.\\\"] [positive_sentiment: \\\"The group performed outstandingly.\\\"] [positive_sentiment: \\\"We're reaching incredible goals.\\\"] [positive_sentiment: \\\"Hopefulness permeates everything.\\\"] [positive_sentiment: \\\"Such a marvelous chance!\\\"] [positive_sentiment: \\\"I'm delighted by the advancement.\\\"] [positive_sentiment: \\\"Everything appears glowing and encouraging.\\\"]\"\n    }\n  ]\n}\n```",
        "net_sentiment": 1.0,
        "sentiment_magnitude": 0.5
      },
      {
        "raw_analysis_response": "```json\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.98,\n    \"analysis_notes\": \"Applied three independent analytical approaches with median aggregation. The document explicitly signals negative sentiment and contains an overwhelming density of negative language, with a complete absence of positive language.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"negative_test_1.txt\",\n      \"document_name\": \"negative_test_1.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence_quotes\": {\n        \"positive_sentiment\": [\n          \"No positive language found in the document.\"\n        ],\n        \"negative_sentiment\": [\n          \"This is a terrible situation. Everything is going wrong. I feel awful about the future. Failure surrounds us.\",\n          \"We're facing disaster. Pessimism fills the air. What a disastrous outcome! I'm devastated by the results. Everything looks dark and hopeless.\"\n        ]\n      },\n      \"marked_up_document\": \"# Document Analysis - Marked Up Text\\n\\n# Negative Test Document 1\\n\\n**Context**: Sample text with negative sentiment\\n\\nThis is a [NEGATIVE_SENTIMENT: terrible situation]. [NEGATIVE_SENTIMENT: Everything is going wrong]. I feel [NEGATIVE_SENTIMENT: awful about the future]. [NEGATIVE_SENTIMENT: Failure surrounds us]. The team did a [NEGATIVE_SENTIMENT: horrible job]. We're facing [NEGATIVE_SENTIMENT: disaster]. [NEGATIVE_SENTIMENT: Pessimism fills the air]. What a [NEGATIVE_SENTIMENT: disastrous outcome]! I'm [NEGATIVE_SENTIMENT: devastated by the results]. [NEGATIVE_SENTIMENT: Everything looks dark and hopeless].\"\n    }\n  ]\n}\n```",
        "net_sentiment": -1.0,
        "sentiment_magnitude": 0.5
      },
      {
        "raw_analysis_response": "```json\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.99,\n    \"analysis_notes\": \"Applied three independent analytical approaches with median aggregation. Document is overtly and consistently negative.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"negative_test_2.txt\",\n      \"document_name\": \"negative_test_2.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence_quotes\": {\n        \"positive_sentiment\": [],\n        \"negative_sentiment\": [\n          \"What an awful predicament. All plans are failing miserably. I'm dreading what's to come.\",\n          \"Defeat engulfs us. The group performed dreadfully. We're encountering catastrophe. Despair saturates everything. Such a calamitous result! I'm crushed by the setbacks. Everything appears bleak and discouraging.\"\n        ]\n      },\n      \"marked_up_document\": \"# Document Analysis - Marked Up Text\\n\\n# Negative Test Document 2\\n\\n**Context**: Sample text with negative sentiment\\n\\n[NEGATIVE_SENTIMENT: \\\"What an awful predicament.\\\"] [NEGATIVE_SENTIMENT: \\\"All plans are failing miserably.\\\"] [NEGATIVE_SENTIMENT: \\\"I'm dreading what's to come.\\\"] [NEGATIVE_SENTIMENT: \\\"Defeat engulfs us.\\\"] [NEGATIVE_SENTIMENT: \\\"The group performed dreadfully.\\\"] [NEGATIVE_SENTIMENT: \\\"We're encountering catastrophe.\\\"] [NEGATIVE_SENTIMENT: \\\"Despair saturates everything.\\\"] [NEGATIVE_SENTIMENT: \\\"Such a calamitous result!\\\"] [NEGATIVE_SENTIMENT: \\\"I'm crushed by the setbacks.\\\"] [NEGATIVE_SENTIMENT: \\\"Everything appears bleak and discouraging.\\\"]\\n\\n\"\n    }\n  ]\n}\n```",
        "net_sentiment": -1.0,
        "sentiment_magnitude": 0.5
      }
    ],
    "columns_added": [
      "sentiment_magnitude",
      "net_sentiment"
    ]
  },
  "status": "success_with_data",
  "validation_passed": true
}