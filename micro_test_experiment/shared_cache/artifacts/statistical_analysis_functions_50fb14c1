{
  "status": "success",
  "functions_generated": 5,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 20264,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: micro_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-12T01:05:34.035100+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates derived sentiment metrics based on the framework specification.\n\n    This function computes 'net_sentiment' and 'sentiment_magnitude' from the raw\n    sentiment scores. It is a necessary preprocessing step before running\n    further statistical analyses on these derived metrics.\n\n    Methodology:\n    - Net Sentiment: Calculated as (positive_sentiment_raw - negative_sentiment_raw).\n      This metric represents the overall balance of sentiment, with positive values\n      indicating more positive than negative sentiment, and vice versa. Its range\n      is [-1.0, 1.0].\n    - Sentiment Magnitude: Calculated as (positive_sentiment_raw + negative_sentiment_raw) / 2.\n      This metric represents the overall intensity of emotional language, regardless\n      of polarity. It is normalized to a [0.0, 1.0] scale.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data with columns\n                             'positive_sentiment_raw' and 'negative_sentiment_raw'.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pd.DataFrame: The input DataFrame with two new columns, 'net_sentiment' and\n                      'sentiment_magnitude', or None if required columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if not all(col in data.columns for col in ['positive_sentiment_raw', 'negative_sentiment_raw']):\n            # As a seasoned practitioner, I must ensure data integrity.\n            # Returning None is a clear signal of a data contract violation.\n            return None\n\n        df = data.copy()\n\n        # Net Sentiment: Balance between positive and negative sentiment\n        df['net_sentiment'] = df['positive_sentiment_raw'] - df['negative_sentiment_raw']\n\n        # Sentiment Magnitude: Combined intensity of emotional language\n        df['sentiment_magnitude'] = (df['positive_sentiment_raw'] + df['negative_sentiment_raw']) / 2\n\n        return df\n\n    except Exception:\n        # Catch-all for any unexpected pandas errors.\n        return None\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Generates descriptive statistics for sentiment scores, grouped by category.\n\n    This function provides a summary of the central tendency, dispersion, and range\n    for each primary and derived sentiment metric. It groups the data by the\n    'sentiment_category' variable, which is derived from document filenames.\n\n    Statistical Methodology:\n    - Grouping: Data is partitioned into 'positive' and 'negative' groups based on\n      the 'document_name' prefix.\n    - Metrics: For each group and each variable, the following are calculated:\n        - mean: The arithmetic average.\n        - std: The standard deviation, a measure of dispersion.\n        - min: The minimum observed value.\n        - max: The maximum observed value.\n        - count: The number of observations (N).\n\n    Power Assessment (Tier 3: Exploratory):\n    - With a total sample size of N=4 (n=2 per group), this analysis is purely\n      exploratory. The descriptive statistics are useful for pattern recognition\n      but are not stable estimates of population parameters. Results are suggestive\n      rather than conclusive.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data. Must include\n                             'document_name' and sentiment score columns.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A nested dictionary of descriptive statistics for each variable\n              and group, or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if 'document_name' not in data.columns:\n            return None\n\n        # Pre-calculate derived metrics to include them in the analysis\n        df = calculate_derived_metrics(data)\n        if df is None:\n            return None\n\n        # Create grouping variable based on filename convention\n        df['sentiment_category'] = np.where(\n            df['document_name'].str.startswith('positive'), 'positive',\n            np.where(df['document_name'].str.startswith('negative'), 'negative', 'unknown')\n        )\n\n        if df['sentiment_category'].nunique() < 2:\n            # Analysis requires at least two groups for comparison.\n            return None\n\n        # Identify all relevant numeric columns for analysis\n        score_cols = [\n            'positive_sentiment_raw', 'negative_sentiment_raw',\n            'net_sentiment', 'sentiment_magnitude'\n        ]\n        # Filter for columns that actually exist in the dataframe\n        valid_cols = [col for col in score_cols if col in df.columns]\n\n        if not valid_cols:\n            return None\n\n        # Group by the derived category and calculate descriptive stats\n        descriptives = df.groupby('sentiment_category')[valid_cols].agg(['mean', 'std', 'min', 'max', 'count'])\n\n        # Convert the multi-index DataFrame to a nested dictionary for clean output\n        return descriptives.to_dict('index')\n\n    except Exception:\n        return None\n\ndef perform_exploratory_anova(data, **kwargs):\n    \"\"\"\n    Performs a one-way ANOVA to compare sentiment scores between categories.\n\n    Statistical Methodology:\n    - Test: A one-way Analysis of Variance (ANOVA) is conducted for each dependent\n      variable to test for differences between the means of the 'positive' and\n      'negative' sentiment groups.\n    - Effect Size: Eta-squared (\u03b7\u00b2) is reported as a measure of effect size,\n      representing the proportion of variance in the dependent variable that is\n      attributable to the independent variable (sentiment_category).\n    - Grouping: The 'sentiment_category' is derived from document filenames.\n\n    Power Assessment (Tier 3: Exploratory):\n    - ANOVA requires N\u226510 per group for well-powered analysis. With n=2 per group,\n      this analysis falls into Tier 3.\n    - Exploratory analysis - results are suggestive rather than conclusive (N=4).\n    - The F-statistic and p-value are reported for completeness but should be\n      interpreted with extreme caution. They are highly unstable with this sample size.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing ANOVA results (F-statistic, p-value, eta-squared)\n              for each sentiment metric, or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import f_oneway\n\n    try:\n        df = calculate_derived_metrics(data)\n        if df is None:\n            return None\n\n        df['sentiment_category'] = np.where(\n            df['document_name'].str.startswith('positive'), 'positive',\n            np.where(df['document_name'].str.startswith('negative'), 'negative', 'unknown')\n        )\n\n        groups = df.groupby('sentiment_category')\n        if len(groups) < 2 or any(len(g) < 2 for _, g in groups):\n            # ANOVA requires at least 2 groups with at least 2 samples each.\n            return None\n\n        group_names = list(groups.groups.keys())\n        group_data = [groups.get_group(g) for g in group_names]\n\n        results = {}\n        score_cols = [\n            'positive_sentiment_raw', 'negative_sentiment_raw',\n            'net_sentiment', 'sentiment_magnitude'\n        ]\n        valid_cols = [col for col in score_cols if col in df.columns]\n\n        for col in valid_cols:\n            samples = [g[col].dropna().values for g in group_data]\n            \n            if any(len(s) < 2 for s in samples):\n                continue\n\n            f_val, p_val = f_oneway(*samples)\n\n            # Calculate Eta-squared (\u03b7\u00b2)\n            ss_between = sum(len(s) * (np.mean(s) - df[col].mean())**2 for s in samples)\n            ss_total = sum((x - df[col].mean())**2 for x in df[col])\n            eta_squared = ss_between / ss_total if ss_total > 0 else 0\n\n            results[col] = {\n                'f_statistic': f_val,\n                'p_value': p_val,\n                'eta_squared': eta_squared,\n                'N_total': len(df),\n                'groups': {name: len(g) for name, g in zip(group_names, samples)},\n                'notes': \"Exploratory analysis due to extremely small sample size (Tier 3).\"\n            }\n\n        return results if results else None\n\n    except Exception:\n        return None\n\ndef perform_exploratory_t_tests(data, **kwargs):\n    \"\"\"\n    Performs t-tests and non-parametric alternatives between sentiment groups.\n\n    Statistical Methodology:\n    - Test 1 (Parametric): Welch's independent two-sample t-test is used. It does\n      not assume equal variances, making it more robust for small, unequal groups.\n    - Test 2 (Non-Parametric): The Mann-Whitney U test is provided as a non-parametric\n      alternative, suitable for small sample sizes or when normality assumptions\n      are violated. It tests for differences in distribution location.\n    - Effect Size: Cohen's d is calculated as a standardized measure of the mean\n      difference between the two groups.\n    - Grouping: Data is split into 'positive' and 'negative' groups based on filename.\n\n    Power Assessment (Tier 3: Exploratory):\n    - T-tests require N\u226515 per group for well-powered analysis. With n=2 per group,\n      this analysis is firmly in Tier 3.\n    - Exploratory analysis - results are suggestive rather than conclusive (N=4).\n    - P-values are highly unreliable. Focus should be on the direction and magnitude\n      of the effect size (Cohen's d).\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary of test results for each metric, or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import ttest_ind, mannwhitneyu\n\n    try:\n        df = calculate_derived_metrics(data)\n        if df is None:\n            return None\n\n        df['sentiment_category'] = np.where(\n            df['document_name'].str.startswith('positive'), 'positive',\n            np.where(df['document_name'].str.startswith('negative'), 'negative', 'unknown')\n        )\n\n        if 'positive' not in df['sentiment_category'].values or 'negative' not in df['sentiment_category'].values:\n            return None\n\n        group1 = df[df['sentiment_category'] == 'positive']\n        group2 = df[df['sentiment_category'] == 'negative']\n\n        if len(group1) < 2 or len(group2) < 2:\n            # Meaningful comparison requires at least two data points per group.\n            return None\n\n        results = {}\n        score_cols = [\n            'positive_sentiment_raw', 'negative_sentiment_raw',\n            'net_sentiment', 'sentiment_magnitude'\n        ]\n        valid_cols = [col for col in score_cols if col in df.columns]\n\n        for col in valid_cols:\n            g1_data = group1[col].dropna()\n            g2_data = group2[col].dropna()\n\n            if len(g1_data) < 1 or len(g2_data) < 1:\n                continue\n\n            # Welch's T-test\n            t_stat, t_p = ttest_ind(g1_data, g2_data, equal_var=False, nan_policy='omit')\n\n            # Mann-Whitney U Test\n            u_stat, u_p = mannwhitneyu(g1_data, g2_data, alternative='two-sided')\n\n            # Cohen's d\n            s_pooled = np.sqrt(((len(g1_data) - 1) * np.var(g1_data, ddof=1) + (len(g2_data) - 1) * np.var(g2_data, ddof=1)) / (len(g1_data) + len(g2_data) - 2))\n            cohen_d = (np.mean(g1_data) - np.mean(g2_data)) / s_pooled if s_pooled > 0 else 0\n\n            results[col] = {\n                'welch_t_test': {'t_statistic': t_stat, 'p_value': t_p},\n                'mann_whitney_u': {'u_statistic': u_stat, 'p_value': u_p},\n                'cohen_d': cohen_d,\n                'group_means': {'positive': np.mean(g1_data), 'negative': np.mean(g2_data)},\n                'notes': \"Exploratory analysis (Tier 3). P-values are unreliable; focus on effect size.\"\n            }\n\n        return results if results else None\n\n    except Exception:\n        return None\n\ndef calculate_reliability_analysis(data, **kwargs):\n    \"\"\"\n    Calculates internal consistency reliability (Cronbach's alpha).\n\n    Statistical Methodology:\n    - Test: Cronbach's alpha is a measure of internal consistency for a set of scale\n      or test items. Here, we treat 'positive_sentiment' and a reverse-coded\n      'negative_sentiment' as two items of a single \"Valence\" scale.\n    - Formula: For two items, alpha is calculated as 2r / (1+r), where r is the\n      Pearson correlation between the two items.\n    - Interpretation: Alpha values range from 0 to 1. Higher values suggest that\n      the items measure the same underlying construct.\n\n    Power Assessment (Tier 3: Exploratory):\n    - Reliability analysis requires N\u226530 for stable estimates. With N=4, this\n      analysis is in Tier 3.\n    - Exploratory analysis - results are suggestive rather than conclusive (N=4).\n    - The alpha coefficient is highly sensitive to the specific data points in such\n      a small sample and should not be generalized.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing the Cronbach's alpha value and interpretation\n              notes, or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if not all(c in data.columns for c in ['positive_sentiment_raw', 'negative_sentiment_raw']):\n            return None\n\n        df = data.copy()\n        if len(df) < 3:\n            # Correlation, and thus alpha, is not meaningful with < 3 points.\n            return None\n\n        # Reverse-code the negative item so it aligns with the positive item's direction.\n        # A high score on this new item means \"low negative sentiment\".\n        df['negative_sentiment_reversed'] = 1.0 - df['negative_sentiment_raw']\n\n        items = df[['positive_sentiment_raw', 'negative_sentiment_reversed']].dropna()\n\n        if len(items) < 3:\n            return None\n\n        # Calculate Pearson correlation between the two items\n        correlation_matrix = items.corr()\n        if correlation_matrix.shape != (2, 2):\n            return None\n        \n        r = correlation_matrix.iloc[0, 1]\n\n        # Calculate Cronbach's alpha for two items\n        # Formula: alpha = (k * r_bar) / (1 + (k-1) * r_bar) where k=2\n        # Simplifies to: alpha = 2*r / (1+r)\n        if (1 + r) == 0:\n            # Avoid division by zero if correlation is -1\n            alpha = np.nan\n        else:\n            alpha = (2 * r) / (1 + r)\n\n        return {\n            'cronbach_alpha': alpha,\n            'n_items': 2,\n            'n_observations': len(items),\n            'notes': f\"Exploratory analysis (Tier 3, N={len(items)}). Alpha is unstable with small N.\"\n        }\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}