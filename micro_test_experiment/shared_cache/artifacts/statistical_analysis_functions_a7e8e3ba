{
  "status": "success",
  "functions_generated": 5,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 22195,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: micro_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-12T01:40:23.366045+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef run_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates and returns descriptive statistics for primary and derived sentiment metrics, grouped by sentiment category.\n\n    Methodology:\n    This function first calculates the derived metrics 'net_sentiment' and 'sentiment_magnitude' based on the framework specification.\n    It then assigns each document to a 'sentiment_category' ('positive' or 'negative') based on its filename prefix.\n    Finally, it computes descriptive statistics (count, mean, standard deviation, min, max) for the primary dimensions\n    (positive_sentiment_raw, negative_sentiment_raw) and the derived metrics, grouped by the sentiment category.\n\n    Power Assessment (Tier 3: Exploratory Analysis):\n    The analysis is exploratory due to a small sample size (N < 15). The descriptive statistics provide a snapshot of the data's\n    central tendency and dispersion, but these findings are suggestive rather than conclusive and may not generalize.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns including 'document_name',\n                             'positive_sentiment_raw', and 'negative_sentiment_raw'.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each metric, grouped by sentiment category.\n              Returns None if the required columns are missing or data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n        \n        df = data.copy()\n\n        # --- Data Preparation ---\n        # 1. Create grouping variable from document name\n        df['sentiment_category'] = np.where(df['document_name'].str.startswith('positive'), 'positive', 'negative')\n\n        # 2. Calculate derived metrics as per framework spec\n        df['net_sentiment'] = df['positive_sentiment_raw'] - df['negative_sentiment_raw']\n        df['sentiment_magnitude'] = (df['positive_sentiment_raw'] + df['negative_sentiment_raw']) / 2\n\n        if df['sentiment_category'].nunique() < 2:\n            return None # Not enough groups for a meaningful grouped summary\n\n        # --- Analysis ---\n        metrics_to_analyze = ['positive_sentiment_raw', 'negative_sentiment_raw', 'net_sentiment', 'sentiment_magnitude']\n        \n        descriptives = df.groupby('sentiment_category')[metrics_to_analyze].agg(['count', 'mean', 'std', 'min', 'max']).reset_index()\n        \n        # Format output for JSON serialization\n        descriptives.columns = ['_'.join(col).strip() for col in descriptives.columns.values]\n        descriptives.rename(columns={'sentiment_category_': 'sentiment_category'}, inplace=True)\n        \n        # Add a note about the analysis tier\n        results = {\n            \"analysis_name\": \"Descriptive Statistics\",\n            \"analysis_tier\": \"Tier 3 (Exploratory)\",\n            \"tier_caveat\": f\"Exploratory analysis - results are suggestive rather than conclusive (N={len(df)}).\",\n            \"statistics\": descriptives.to_dict(orient='records')\n        }\n        \n        return results\n\n    except Exception:\n        return None\n\ndef run_group_comparison_tests(data, **kwargs):\n    \"\"\"\n    Performs group comparison tests (t-test, Mann-Whitney U) between sentiment categories for all metrics.\n\n    Methodology:\n    This function compares the 'positive' and 'negative' sentiment groups on primary and derived metrics.\n    For each metric, it performs:\n    1. Independent Samples t-test: A parametric test assuming normality and equal variances.\n    2. Mann-Whitney U test: A non-parametric alternative that does not assume a normal distribution.\n    3. Cohen's d: An effect size measure quantifying the difference between the two group means.\n\n    Power Assessment (Tier 3: Exploratory Analysis):\n    With group sizes n < 8, this analysis is exploratory. P-values from t-tests are unreliable. The Mann-Whitney U test\n    is more appropriate for small, non-normal samples. The primary focus should be on the direction and magnitude of the\n    effect size (Cohen's d), which is less sensitive to sample size than p-values. Results are suggestive, not conclusive.\n\n    Args:\n        data (pd.DataFrame): DataFrame with 'document_name', 'positive_sentiment_raw', 'negative_sentiment_raw'.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary of test results for each metric, including statistics, p-values, and effect sizes.\n              Returns None if data is insufficient for comparison (e.g., fewer than two groups).\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import ttest_ind, mannwhitneyu\n\n    def cohen_d(x, y):\n        \"\"\"Helper function to calculate Cohen's d for independent samples.\"\"\"\n        nx, ny = len(x), len(y)\n        if nx < 2 or ny < 2: return 0.0\n        dof = nx + ny - 2\n        s_pooled = np.sqrt(((nx - 1) * np.var(x, ddof=1) + (ny - 1) * np.var(y, ddof=1)) / dof)\n        return (np.mean(x) - np.mean(y)) / s_pooled if s_pooled > 0 else 0.0\n\n    try:\n        required_cols = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        df = data.copy()\n        df['sentiment_category'] = np.where(df['document_name'].str.startswith('positive'), 'positive', 'negative')\n        df['net_sentiment'] = df['positive_sentiment_raw'] - df['negative_sentiment_raw']\n        df['sentiment_magnitude'] = (df['positive_sentiment_raw'] + df['negative_sentiment_raw']) / 2\n\n        groups = df['sentiment_category'].unique()\n        if len(groups) != 2:\n            return None # Requires exactly two groups for this test\n\n        group1_data = df[df['sentiment_category'] == groups[0]]\n        group2_data = df[df['sentiment_category'] == groups[1]]\n\n        if len(group1_data) < 2 or len(group2_data) < 2:\n            return None # Insufficient data in one or both groups\n\n        results = {\n            \"analysis_name\": \"Group Comparison Tests\",\n            \"analysis_tier\": \"Tier 3 (Exploratory)\",\n            \"tier_caveat\": f\"Exploratory analysis - results are suggestive rather than conclusive (N={len(df)}, Groups: n1={len(group1_data)}, n2={len(group2_data)}). Focus on effect sizes.\",\n            \"groups_compared\": list(groups),\n            \"comparisons\": {}\n        }\n\n        metrics_to_analyze = ['positive_sentiment_raw', 'negative_sentiment_raw', 'net_sentiment', 'sentiment_magnitude']\n        for metric in metrics_to_analyze:\n            g1 = group1_data[metric].dropna()\n            g2 = group2_data[metric].dropna()\n\n            if len(g1) < 2 or len(g2) < 2: continue\n\n            # T-test\n            t_stat, t_pvalue = ttest_ind(g1, g2, equal_var=False) # Welch's t-test is safer for small N\n\n            # Mann-Whitney U\n            try:\n                u_stat, u_pvalue = mannwhitneyu(g1, g2, alternative='two-sided')\n            except ValueError: # Can occur if all values in a group are identical\n                u_stat, u_pvalue = np.nan, np.nan\n\n            # Cohen's d\n            d_value = cohen_d(g1, g2)\n\n            results[\"comparisons\"][metric] = {\n                \"t_test\": {\"statistic\": t_stat, \"p_value\": t_pvalue},\n                \"mann_whitney_u\": {\"statistic\": u_stat, \"p_value\": u_pvalue},\n                \"cohens_d\": d_value\n            }\n        \n        return results\n\n    except Exception:\n        return None\n\ndef run_anova_analysis(data, **kwargs):\n    \"\"\"\n    Performs a one-way ANOVA to compare means across sentiment categories for all metrics.\n\n    Methodology:\n    This function conducts a one-way Analysis of Variance (ANOVA) for each primary and derived metric to test if there\n    are statistically significant differences between the means of the 'positive' and 'negative' sentiment groups.\n    It reports the F-statistic, the p-value, and the eta-squared (\u03b7\u00b2) effect size, which represents the proportion\n    of variance in the dependent variable that is attributable to the independent variable (sentiment category).\n\n    Power Assessment (Tier 3: Exploratory Analysis):\n    With group sizes n < 5, this analysis is severely underpowered and exploratory. The assumptions of ANOVA (especially\n    normality) are unlikely to be met. The p-value is not interpretable. The results, particularly the eta-squared\n    effect size, should be viewed as a preliminary indicator of potential group differences, requiring a larger sample\n    for any conclusive validation.\n\n    Args:\n        data (pd.DataFrame): DataFrame with 'document_name', 'positive_sentiment_raw', 'negative_sentiment_raw'.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary of ANOVA results (F-statistic, p-value, eta-squared) for each metric.\n              Returns None if data is insufficient for analysis.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import f_oneway\n\n    try:\n        required_cols = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        df = data.copy()\n        df['sentiment_category'] = np.where(df['document_name'].str.startswith('positive'), 'positive', 'negative')\n        df['net_sentiment'] = df['positive_sentiment_raw'] - df['negative_sentiment_raw']\n        df['sentiment_magnitude'] = (df['positive_sentiment_raw'] + df['negative_sentiment_raw']) / 2\n\n        if df['sentiment_category'].nunique() < 2:\n            return None\n\n        grouped_data = [group[1] for group in df.groupby('sentiment_category')]\n        if any(len(g) < 2 for g in grouped_data):\n            return None # ANOVA requires at least 2 observations per group\n\n        results = {\n            \"analysis_name\": \"One-Way ANOVA\",\n            \"analysis_tier\": \"Tier 3 (Exploratory)\",\n            \"tier_caveat\": f\"Exploratory analysis - results are suggestive rather than conclusive (N={len(df)}). ANOVA is not robust with this sample size.\",\n            \"analyses\": {}\n        }\n\n        metrics_to_analyze = ['positive_sentiment_raw', 'negative_sentiment_raw', 'net_sentiment', 'sentiment_magnitude']\n        for metric in metrics_to_analyze:\n            # Prepare data for f_oneway\n            samples = [group[metric].dropna().values for group in grouped_data]\n            \n            # Check if we have enough data to run the test\n            if any(len(s) < 1 for s in samples):\n                continue\n\n            f_stat, p_value = f_oneway(*samples)\n\n            # Calculate Eta-squared (\u03b7\u00b2)\n            ss_between = sum(len(s) * (np.mean(s) - df[metric].mean())**2 for s in samples)\n            ss_total = sum((x - df[metric].mean())**2 for x in df[metric])\n            eta_squared = ss_between / ss_total if ss_total > 0 else 0.0\n\n            results[\"analyses\"][metric] = {\n                \"f_statistic\": f_stat,\n                \"p_value\": p_value,\n                \"eta_squared_effect_size\": eta_squared\n            }\n        \n        return results\n\n    except Exception:\n        return None\n\ndef run_correlation_analysis(data, **kwargs):\n    \"\"\"\n    Calculates the correlation between positive and negative sentiment scores.\n\n    Methodology:\n    This function computes the Pearson correlation coefficient (r) between the 'positive_sentiment_raw' and\n    'negative_sentiment_raw' dimensions. This measures the strength and direction of the linear relationship\n    between the two primary sentiment scores across all documents. A negative correlation is typically expected,\n    indicating that as one sentiment increases, the other tends to decrease.\n\n    Power Assessment (Tier 3: Exploratory Analysis):\n    With a sample size N < 15, this correlation analysis is exploratory. The calculated correlation coefficient (r)\n    can be highly influenced by outliers and may not be stable. The p-value is not reliable for significance testing.\n    The result should be interpreted as a preliminary indication of an association, not a confirmed relationship.\n\n    Args:\n        data (pd.DataFrame): DataFrame with 'positive_sentiment_raw' and 'negative_sentiment_raw'.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing the Pearson correlation coefficient and p-value.\n              Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    from scipy.stats import pearsonr\n\n    try:\n        required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        df = data.copy().dropna(subset=required_cols)\n        \n        if len(df) < 3: # Pearson r is not meaningful with < 3 points\n            return {\n                \"analysis_name\": \"Correlation Analysis\",\n                \"analysis_tier\": \"Tier 3 (Exploratory)\",\n                \"tier_caveat\": f\"Insufficient data (N={len(df)}) for meaningful correlation analysis.\",\n                \"correlation_results\": None\n            }\n\n        pos_scores = df['positive_sentiment_raw']\n        neg_scores = df['negative_sentiment_raw']\n\n        corr, p_value = pearsonr(pos_scores, neg_scores)\n\n        results = {\n            \"analysis_name\": \"Correlation Analysis\",\n            \"analysis_tier\": \"Tier 3 (Exploratory)\",\n            \"tier_caveat\": f\"Exploratory analysis - results are suggestive rather than conclusive (N={len(df)}).\",\n            \"correlation_results\": {\n                \"variables\": [\"positive_sentiment_raw\", \"negative_sentiment_raw\"],\n                \"pearson_correlation_coefficient\": corr,\n                \"p_value\": p_value\n            }\n        }\n        \n        return results\n\n    except Exception:\n        return None\n\ndef run_reliability_analysis(data, **kwargs):\n    \"\"\"\n    Calculates internal consistency reliability (Cronbach's alpha) for the two primary sentiment dimensions.\n\n    Methodology:\n    This function treats the 'positive_sentiment_raw' and 'negative_sentiment_raw' dimensions as two \"items\" in a\n    scale and calculates Cronbach's alpha. This metric assesses the extent to which these two items measure a single,\n    unidimensional latent construct (e.g., overall \"sentimentality\").\n    **Methodological Caveat**: Cronbach's alpha is typically used for scales with 3+ items. With only two items, it is\n    mathematically equivalent to the Spearman-Brown prophecy formula for a two-item test, which simplifies to\n    2r / (1+r), where r is the Pearson correlation between the two items. Interpretation requires caution.\n\n    Power Assessment (Tier 3: Exploratory Analysis):\n    With a sample size N < 15, this reliability estimate is exploratory and potentially unstable. It provides a\n    preliminary look at the internal consistency of the two core dimensions but should not be considered a definitive\n    measure of reliability for the framework.\n\n    Args:\n        data (pd.DataFrame): DataFrame with 'positive_sentiment_raw' and 'negative_sentiment_raw'.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing the Cronbach's alpha value. Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        df = data.copy()[required_cols].dropna()\n        \n        if len(df) < 2:\n            return None # Not enough data\n\n        k = len(df.columns)\n        if k < 2:\n            return None # Need at least two dimensions\n\n        # Calculate total score variance\n        df['total'] = df.sum(axis=1)\n        total_var = df['total'].var(ddof=1)\n\n        # Calculate sum of item variances\n        item_vars_sum = df[required_cols].var(ddof=1).sum()\n\n        # Cronbach's alpha formula\n        if total_var > 0:\n            alpha = (k / (k - 1)) * (1 - (item_vars_sum / total_var))\n        else:\n            alpha = np.nan # Undefined if there is no variance in total scores\n\n        results = {\n            \"analysis_name\": \"Reliability Analysis (Cronbach's Alpha)\",\n            \"analysis_tier\": \"Tier 3 (Exploratory)\",\n            \"tier_caveat\": f\"Exploratory analysis - results are suggestive rather than conclusive (N={len(df)}). Alpha on two items has limited interpretability.\",\n            \"reliability_results\": {\n                \"cronbachs_alpha\": alpha,\n                \"num_items\": k,\n                \"note\": \"Alpha for two items is equivalent to the Spearman-Brown coefficient and should be interpreted with caution.\"\n            }\n        }\n        \n        return results\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}