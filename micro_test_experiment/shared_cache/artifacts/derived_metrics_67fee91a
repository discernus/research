{
  "analysis_id": "analysis_16c593f6",
  "step": "derived_metrics_generation",
  "model_used": "vertex_ai/gemini-2.5-flash-lite",
  "derived_metrics": "```python\nimport json\n\n# Input scores\nscores_data = {\n  \"negative_test_1.txt\": {\n    \"positive_sentiment\": {\n      \"raw_score\": 0.0,\n      \"salience\": 0.0,\n      \"confidence\": 1.0\n    },\n    \"negative_sentiment\": {\n      \"raw_score\": 1.0,\n      \"salience\": 1.0,\n      \"confidence\": 1.0\n    }\n  }\n}\n\n# Framework definitions for derived metrics\nframework_definitions = {\n    \"derived_metrics\": [\n        {\n            \"name\": \"net_sentiment\",\n            \"description\": \"Net sentiment balance (positive - negative)\",\n            \"formula\": \"dimensions.positive_sentiment.raw_score - dimensions.negative_sentiment.raw_score\"\n        },\n        {\n            \"name\": \"sentiment_magnitude\",\n            \"description\": \"Average emotional intensity (positive + negative) / 2\",\n            \"formula\": \"(dimensions.positive_sentiment.raw_score + dimensions.negative_sentiment.raw_score) / 2\"\n        }\n    ]\n}\n\ncalculated_results = {}\n\n# Iterate through each document in the scores data\nfor doc_name, doc_scores in scores_data.items():\n    positive_sentiment_score = doc_scores.get(\"positive_sentiment\", {}).get(\"raw_score\", 0.0)\n    negative_sentiment_score = doc_scores.get(\"negative_sentiment\", {}).get(\"raw_score\", 0.0)\n\n    derived_metrics_for_doc = {}\n\n    # Calculate derived metrics based on the framework formulas\n    for metric in framework_definitions[\"derived_metrics\"]:\n        formula = metric[\"formula\"]\n        try:\n            # Safely evaluate the formula using the available scores\n            # We're using a simple substitution here, assuming the formula only uses dimension names directly.\n            # For more complex formulas or to prevent arbitrary code execution, a dedicated expression parser would be better.\n            calculated_value = eval(formula.replace(\"dimensions.positive_sentiment.raw_score\", str(positive_sentiment_score)).replace(\"dimensions.negative_sentiment.raw_score\", str(negative_sentiment_score)))\n            derived_metrics_for_doc[metric[\"name\"]] = calculated_value\n        except Exception as e:\n            derived_metrics_for_doc[metric[\"name\"]] = f\"Error calculating: {e}\"\n\n    calculated_results[doc_name] = derived_metrics_for_doc\n\n# Output the calculated results\nprint(json.dumps(calculated_results, indent=2))\n```",
  "timestamp": "2025-09-15T13:05:16.999466+00:00"
}