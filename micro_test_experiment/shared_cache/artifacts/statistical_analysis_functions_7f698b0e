{
  "status": "success",
  "functions_generated": 6,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 11685,
  "function_code_content": "import pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List\nimport warnings\n\ndef _get_prepared_data(data: pd.DataFrame) -> Optional[pd.DataFrame]:\n    \"\"\"\n    Internal helper function to prepare the data for analysis.\n    \n    This function adds the 'sentiment_category' grouping variable based on filenames\n    and calculates the derived metrics 'net_sentiment' and 'sentiment_magnitude'.\n\n    Args:\n        data (pd.DataFrame): The input dataframe with at least 'document_name',\n                             'positive_sentiment_raw', and 'negative_sentiment_raw' columns.\n\n    Returns:\n        Optional[pd.DataFrame]: A new DataFrame with added columns, or None if input is invalid.\n    \"\"\"\n    if data is None or data.empty:\n        return None\n\n    required_cols = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n    if not all(col in data.columns for col in required_cols):\n        return None\n\n    df = data.copy()\n\n    # 1. Create grouping variable 'sentiment_category'\n    try:\n        df['sentiment_category'] = np.where(df['document_name'].str.startswith('positive'), 'positive', 'negative')\n    except Exception:\n        # Fallback if string operations fail\n        return None\n\n    # 2. Calculate derived metrics\n    df['net_sentiment'] = df['positive_sentiment_raw'] - df['negative_sentiment_raw']\n    df['sentiment_magnitude'] = (df['positive_sentiment_raw'] + df['negative_sentiment_raw']) / 2\n\n    return df\n\ndef calculate_descriptive_statistics(data: pd.DataFrame, **kwargs) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Calculates descriptive statistics for all dimensions and derived metrics, grouped by sentiment category.\n\n    Methodology:\n    This function computes the mean, standard deviation, count, min, and max for each numeric score.\n    Analysis is exploratory (Tier 3) due to the small sample size (N < 15).\n    Results are suggestive of patterns rather than conclusive findings.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        Optional[Dict[str, Any]]: A dictionary of descriptive statistics or None if data is insufficient.\n    \"\"\"\n    df = _get_prepared_data(data)\n    if df is None or df.empty:\n        return None\n\n    try:\n        metrics = ['positive_sentiment_raw', 'negative_sentiment_raw', 'net_sentiment', 'sentiment_magnitude']\n        if not all(metric in df.columns for metric in metrics):\n            return None\n\n        grouped_stats = df.groupby('sentiment_category')[metrics].agg(['mean', 'std', 'count', 'min', 'max'])\n\n        # Flatten the multi-index columns into a single-level dictionary\n        results = {}\n        for group in grouped_stats.index:\n            results[group] = {}\n            for metric in grouped_stats.columns.levels[0]:\n                for stat in grouped_stats.columns.levels[1]:\n                    key = f\"{metric}__{stat}\"\n                    value = grouped_stats.loc[group, (metric, stat)]\n                    results[group][key] = float(value) if pd.notna(value) else None\n\n        total_n = int(df.shape[0])\n        return {\n            'analysis_tier': f'Tier 3 (Exploratory)',\n            'power_assessment': f'Exploratory analysis - results are suggestive rather than conclusive (N={total_n})',\n            'statistics_by_group': results\n        }\n    except Exception:\n        return None\n\ndef perform_anova_analysis(data: pd.DataFrame, **kwargs) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Performs a one-way ANOVA to compare sentiment scores between sentiment categories.\n\n    Methodology:\n    A one-way ANOVA is conducted for each dependent variable. Given the extremely small sample size\n    (n < 5 per group), this analysis is strictly exploratory (Tier 3). Assumptions of normality and\n    homogeneity of variances are not tested and likely violated. The results, including the F-statistic,\n    p-value, and eta-squared effect size, should be interpreted as preliminary indicators of potential\n    differences and not as formal hypothesis tests.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        Optional[Dict[str, Any]]: A dictionary of ANOVA results for each metric or None if data is insufficient.\n    \"\"\"\n    df = _get_prepared_data(data)\n    if df is None or df.empty:\n        return None\n\n    try:\n        groups = df['sentiment_category'].unique()\n        if len(groups) < 2:\n            return None\n\n        group_data = [df[df['sentiment_category'] == g] for g in groups]\n        min_group_size = min(len(g) for g in group_data)\n        if min_group_size < 2:\n            return None\n\n        results = {}\n        metrics = ['positive_sentiment_raw', 'negative_sentiment_raw', 'net_sentiment', 'sentiment_magnitude']\n\n        for metric in metrics:\n            samples = [g[metric].values for g in group_data]\n            \n            # Calculate ANOVA\n            f_val, p_val = stats.f_oneway(*samples)\n\n            # Calculate Eta-squared (effect size)\n            ss_between = sum(len(s) * (np.mean(s) - df[metric].mean())**2 for s in samples)\n            ss_total = sum((x - df[metric].mean())**2 for x in df[metric])\n            eta_squared = ss_between / ss_total if ss_total > 0 else 0\n\n            results[metric] = {\n                'f_statistic': float(f_val) if pd.notna(f_val) else None,\n                'p_value': float(p_val) if pd.notna(p_val) else None,\n                'eta_squared': float(eta_squared) if pd.notna(eta_squared) else None\n            }\n        \n        total_n = int(df.shape[0])\n        return {\n            'analysis_tier': f'Tier 3 (Exploratory)',\n            'power_assessment': f'Exploratory analysis - results are suggestive rather than conclusive (N={total_n}, min group n={min_group_size})',\n            'anova_results': results\n        }\n    except Exception:\n        return None\n\ndef perform_t_tests(data: pd.DataFrame, **kwargs) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Performs independent t-tests and Mann-Whitney U tests between sentiment categories.\n\n    Methodology:\n    This function compares the two sentiment groups ('positive' vs. 'negative') on each metric.\n    Due to the very small sample size (n < 8 per group), this is a Tier 3 exploratory analysis.\n    - Welch's t-test is used as it does not assume equal variances.\n    - The non-parametric Mann-Whitney U test is included as a robust alternative.\n    - Cohen's d is reported as an effect size measure.\n    Results are suggestive and not statistically conclusive.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        Optional[Dict[str, Any]]: A dictionary of test results for each metric or None if data is insufficient.\n    \"\"\"\n    df = _get_prepared_data(data)\n    if df is None or df.empty:\n        return None\n\n    try:\n        group1 = df[df['sentiment_category'] == 'positive']\n        group2 = df[df['sentiment_category'] == 'negative']\n\n        if group1.empty or group2.empty:\n            return None\n\n        results = {}\n        metrics = ['positive_sentiment_raw', 'negative_sentiment_raw', 'net_sentiment', 'sentiment_magnitude']\n\n        for metric in metrics:\n            g1_data = group1[metric].dropna()\n            g2_data = group2[metric].dropna()\n\n            if len(g1_data) < 1 or len(g2_data) < 1:\n                continue\n\n            # Welch's T-test\n            t_stat, t_p = stats.ttest_ind(g1_data, g2_data, equal_var=False)\n\n            # Mann-Whitney U Test\n            u_stat, u_p = stats.mannwhitneyu(g1_data, g2_data, alternative='two-sided')\n\n            # Cohen's d\n            s_pool = np.sqrt(((len(g1_data) - 1) * np.var(g1_data, ddof=1) + (len(g2_data) - 1) * np.var(g2_data, ddof=1)) / (len(g1_data) + len(g2_data) - 2))\n            cohen_d = (np.mean(g1_data) - np.mean(g2_data)) / s_pool if s_pool > 0 else 0\n\n            results[metric] = {\n                'welch_t_statistic': float(t_stat) if pd.notna(t_stat) else None,\n                'welch_t_p_value': float(t_p) if pd.notna(t_p) else None,\n                'mann_whitney_u_statistic': float(u_stat) if pd.notna(u_stat) else None,\n                'mann_whitney_u_p_value': float(u_p) if pd.notna(u_p) else None,\n                'cohens_d': float(cohen_d) if pd.notna(cohen_d) else None\n            }\n\n        total_n = int(df.shape[0])\n        return {\n            'analysis_tier': f'Tier 3 (Exploratory)',\n            'power_assessment': f'Exploratory analysis - results are suggestive rather than conclusive (N={total_n})',\n            'comparison': 'positive_vs_negative',\n            'test_results': results\n        }\n    except Exception:\n        return None\n\ndef calculate_reliability_analysis(data: pd.DataFrame, **kwargs) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Calculates Cronbach's alpha for internal consistency of the sentiment dimensions.\n\n    Methodology:\n    Cronbach's alpha is calculated to assess the internal consistency between the positive and\n    (reverse-coded) negative sentiment dimensions. This treats them as two 'items' measuring a single\n    underlying sentiment construct. This is a Tier 3 (Exploratory) analysis due to the small sample size (N < 15).\n    Alpha values from small samples are highly unstable.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        Optional[Dict[str, Any]]: A dictionary with the Cronbach's alpha value or None if data is insufficient.\n    \"\"\"\n    df = _get_prepared_data(data)\n    if df is None or df.shape[0] < 2:\n        return None\n\n    try:\n        # Reverse code the negative sentiment item\n        items = df[['positive_sentiment_raw']].copy()\n        items['negative_sentiment_rev'] = 1.0 - df['negative_sentiment_raw']\n        \n        k = items.shape[1]\n        if k < 2:\n            return None\n\n        # Manual calculation of Cronbach's Alpha\n        items_var = items.var(axis=0, ddof=1).sum()\n        total_var = items.sum(axis=1).var(ddof=1)\n        \n        alpha = (k / (k - 1)) * (1 - items_var / total_var) if total_var > 0 else 0\n\n        total_n = int(df.shape[0])\n        return {\n            'analysis_tier': f'Tier 3 (Exploratory)',\n            'power_assessment': f'Exploratory analysis - results are suggestive rather than conclusive (N={total_n})',\n            'cronbachs_alpha': float(alpha) if pd.notna(alpha) else None,\n            'num_items': k,\n            'notes': 'Calculated on positive_sentiment and reverse-coded negative_sentiment.'\n        }\n    except Exception:\n        return None\n\n\ndef perform_statistical_analysis(data: pd.DataFrame, **kwargs) -> Dict[str, Any]:\n    \"\"\"\n    Master function that executes all statistical analyses and returns combined results.\n\n    Args:\n        data (pd.DataFrame): pandas DataFrame containing analysis data.\n        **kwargs: Additional parameters passed to individual functions.\n\n    Returns:\n        Dict[str, Any]: Combined results from all statistical analyses.\n    \"\"\"\n    results = {}\n\n    analysis_functions = {\n        'descriptive_statistics': calculate_descriptive_statistics,\n        'anova_analysis': perform_anova_analysis,\n        't_tests': perform_t_tests,\n        'reliability_analysis': calculate_reliability_analysis\n    }\n\n    for name, func in analysis_functions.items():\n        try:\n            results[name] = func(data, **kwargs)\n        except Exception as e:\n            results[name] = {'error': str(e)}\n\n    return results\n",
  "cached_with_code": true
}