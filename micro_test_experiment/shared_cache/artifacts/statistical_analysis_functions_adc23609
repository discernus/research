{
  "status": "success",
  "functions_generated": 4,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 19484,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: micro_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-10T23:16:33.927418+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for sentiment dimensions and derived metrics.\n\n    This function provides descriptive statistics (mean, standard deviation, min, max, count)\n    for the primary dimensions (positive_sentiment, negative_sentiment) and the derived\n    metrics (net_sentiment, sentiment_magnitude). It calculates these statistics for the\n    entire dataset and also provides a breakdown per sentiment category.\n\n    Statistical Methodology:\n    - The function first calculates derived metrics as specified in the framework.\n    - It then derives the 'sentiment_category' grouping variable from document names.\n    - Descriptive statistics are computed using standard pandas methods.\n    - Power Assessment (Tier 3: Exploratory Analysis): With a total sample size (N) of 4,\n      these statistics are purely exploratory. They describe the patterns within this specific\n      small sample but cannot be reliably generalized. Results are suggestive rather than\n      conclusive.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             including 'document_name', 'positive_sentiment_raw',\n                             and 'negative_sentiment_raw'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing two keys:\n              'overall_descriptives': A dict of descriptive stats for the whole dataset.\n              'grouped_descriptives': A dict of descriptive stats for each sentiment category.\n              Returns None if data is insufficient or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if not isinstance(data, pd.DataFrame) or data.empty:\n            return None\n\n        required_cols = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        df = data.copy()\n\n        # Calculate derived metrics\n        df['net_sentiment'] = df['positive_sentiment_raw'] - df['negative_sentiment_raw']\n        df['sentiment_magnitude'] = (df['positive_sentiment_raw'] + df['negative_sentiment_raw']) / 2\n\n        # Create grouping variable from document name\n        def get_category(name):\n            if 'positive' in name:\n                return 'positive'\n            if 'negative' in name:\n                return 'negative'\n            return 'unknown'\n        df['sentiment_category'] = df['document_name'].apply(get_category)\n        \n        df = df[df['sentiment_category'] != 'unknown']\n        if df.empty:\n            return None\n\n        analysis_cols = ['positive_sentiment_raw', 'negative_sentiment_raw', 'net_sentiment', 'sentiment_magnitude']\n        \n        # Overall descriptives\n        overall_descriptives = df[analysis_cols].describe().to_dict()\n\n        # Grouped descriptives\n        grouped_descriptives = df.groupby('sentiment_category')[analysis_cols].describe().unstack().to_dict()\n\n        results = {\n            'analysis_tier': 'Tier 3 (Exploratory)',\n            'sample_size': len(df),\n            'power_caveat': f\"Exploratory analysis - results are suggestive rather than conclusive (N={len(df)}).\",\n            'overall_descriptives': overall_descriptives,\n            'grouped_descriptives': grouped_descriptives\n        }\n        \n        return results\n\n    except Exception:\n        return None\n\ndef perform_group_comparison_analysis(data, **kwargs):\n    \"\"\"\n    Performs group comparison (ANOVA, Mann-Whitney U) between sentiment categories.\n\n    This function compares the means of sentiment scores across the 'positive' and\n    'negative' sentiment categories. It is designed to address research questions about\n    significant differences between groups.\n\n    Statistical Methodology:\n    - Groups are defined by the 'sentiment_category' variable, derived from document names.\n    - For each dependent variable, a one-way ANOVA is performed to test for differences in means.\n      Eta-squared is calculated as a measure of effect size (proportion of variance explained).\n    - As a non-parametric alternative suitable for small samples, the Mann-Whitney U test is also\n      performed.\n    - Power Assessment (Tier 3: Exploratory Analysis): The sample size (n=2 per group) is\n      far too small for inferential statistics. P-values are not meaningful and should be ignored.\n      The analysis is provided to fulfill the experimental specification, but results are purely\n      descriptive of this specific dataset. Focus should be on effect sizes as indicators of\n      the magnitude of differences, with the understanding that they are highly unstable.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             including 'document_name', 'positive_sentiment_raw',\n                             and 'negative_sentiment_raw'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary where keys are the dependent variables. Each value is another\n              dict containing the results of ANOVA and Mann-Whitney U tests.\n              Returns None if data is insufficient for comparison (e.g., <2 groups).\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy import stats\n\n    try:\n        if not isinstance(data, pd.DataFrame) or data.empty:\n            return None\n\n        required_cols = ['document_name', 'positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        df = data.copy()\n\n        # Calculate derived metrics\n        df['net_sentiment'] = df['positive_sentiment_raw'] - df['negative_sentiment_raw']\n        df['sentiment_magnitude'] = (df['positive_sentiment_raw'] + df['negative_sentiment_raw']) / 2\n\n        # Create grouping variable\n        def get_category(name):\n            if 'positive' in name:\n                return 'positive'\n            if 'negative' in name:\n                return 'negative'\n            return 'unknown'\n        df['sentiment_category'] = df['document_name'].apply(get_category)\n        \n        df = df[df['sentiment_category'] != 'unknown']\n        \n        groups = df['sentiment_category'].unique()\n        if len(groups) < 2:\n            return None # Cannot perform comparison with less than 2 groups\n\n        group_data = {group: df[df['sentiment_category'] == group] for group in groups}\n        \n        # Check for minimum sample size per group for ANOVA\n        if any(len(g_data) < 2 for g_data in group_data.values()):\n             return {\n                'error': 'Insufficient data for group comparison.',\n                'power_caveat': f\"At least one group has n<2. Total N={len(df)}.\"\n             }\n\n        analysis_vars = ['positive_sentiment_raw', 'negative_sentiment_raw', 'net_sentiment', 'sentiment_magnitude']\n        results = {\n            'analysis_tier': 'Tier 3 (Exploratory)',\n            'sample_size': len(df),\n            'power_caveat': f\"Exploratory analysis - p-values are not meaningful. Focus on effect sizes with caution (N={len(df)}).\",\n            'tests_performed': {}\n        }\n\n        for var in analysis_vars:\n            samples = [g_data[var].values for g_data in group_data.values()]\n            \n            # ANOVA\n            # Check for zero variance within groups, which makes ANOVA undefined\n            if any(np.var(s) == 0 for s in samples if len(s) > 1):\n                f_stat, p_val_anova = np.nan, np.nan\n                eta_squared = np.nan\n            else:\n                f_stat, p_val_anova = stats.f_oneway(*samples)\n                # Calculate Eta-squared\n                ss_between = sum(len(s) * (np.mean(s) - df[var].mean())**2 for s in samples)\n                ss_total = sum((x - df[var].mean())**2 for x in df[var])\n                eta_squared = ss_between / ss_total if ss_total > 0 else 0.0\n\n            # Mann-Whitney U (since we have exactly 2 groups)\n            if len(samples) == 2:\n                u_stat, p_val_mannu = stats.mannwhitneyu(samples[0], samples[1], alternative='two-sided')\n            else:\n                u_stat, p_val_mannu = np.nan, np.nan # Not applicable for >2 groups\n\n            results['tests_performed'][var] = {\n                'anova': {\n                    'f_statistic': f_stat,\n                    'p_value': p_val_anova,\n                    'eta_squared': eta_squared\n                },\n                'mann_whitney_u': {\n                    'u_statistic': u_stat,\n                    'p_value': p_val_mannu\n                }\n            }\n        \n        return results\n\n    except Exception:\n        return None\n\ndef calculate_reliability_analysis(data, **kwargs):\n    \"\"\"\n    Calculates the internal consistency (Cronbach's alpha) of the sentiment dimensions.\n\n    This function assesses the reliability of the two primary sentiment dimensions\n    (positive and negative) as measures of a single underlying sentiment construct.\n\n    Statistical Methodology:\n    - To measure internal consistency, the 'negative_sentiment_raw' score is first\n      reverse-coded (1 - score).\n    - Cronbach's alpha is then calculated on the 'positive_sentiment_raw' and the\n      reverse-coded negative sentiment score. Alpha values range from -inf to 1.\n      Higher values (typically > 0.7) suggest the items reliably measure the same construct.\n    - This function requires the 'pingouin' library.\n    - Power Assessment (Tier 3: Exploratory Analysis): With a total sample size (N) of 4,\n      the Cronbach's alpha estimate is extremely unstable and should be considered purely\n      illustrative. It does not provide a reliable estimate of the true internal consistency.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             'positive_sentiment_raw' and 'negative_sentiment_raw'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the Cronbach's alpha value and its confidence interval.\n              Returns None if data is insufficient, required columns are missing, or the\n              'pingouin' library is not installed.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        import pingouin as pg\n    except ImportError:\n        return {'error': \"The 'pingouin' library is required for reliability analysis.\"}\n\n    try:\n        if not isinstance(data, pd.DataFrame) or data.empty:\n            return None\n\n        required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n        \n        if len(data) < 2:\n            return None\n\n        df = data[required_cols].copy()\n        \n        # Reverse-code the negative item\n        df['negative_sentiment_rev'] = 1 - df['negative_sentiment_raw']\n        \n        items = df[['positive_sentiment_raw', 'negative_sentiment_rev']]\n        \n        # Calculate Cronbach's alpha\n        alpha_results = pg.cronbach_alpha(data=items)\n        alpha_val = alpha_results[0]\n        ci95 = alpha_results[1]\n\n        results = {\n            'analysis_tier': 'Tier 3 (Exploratory)',\n            'sample_size': len(df),\n            'power_caveat': f\"Exploratory analysis - reliability estimate is highly unstable and illustrative only (N={len(df)}).\",\n            'cronbach_alpha': alpha_val,\n            'confidence_interval_95': list(ci95)\n        }\n        \n        return results\n\n    except Exception:\n        return None\n\ndef calculate_correlation_analysis(data, **kwargs):\n    \"\"\"\n    Calculates the Pearson correlation matrix for sentiment scores.\n\n    This function examines the linear relationships between the primary sentiment dimensions\n    and the derived metrics.\n\n    Statistical Methodology:\n    - Pearson's correlation coefficient (r) is calculated for pairs of variables.\n      'r' ranges from -1 (perfect negative correlation) to +1 (perfect positive correlation),\n      with 0 indicating no linear correlation.\n    - The analysis includes 'positive_sentiment_raw', 'negative_sentiment_raw',\n      'net_sentiment', and 'sentiment_magnitude'.\n    - Power Assessment (Tier 3: Exploratory Analysis): With a total sample size (N) of 4,\n      any correlation coefficients are extremely sensitive to individual data points and\n      are not stable or generalizable. This analysis is for exploratory pattern detection only.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             'positive_sentiment_raw' and 'negative_sentiment_raw'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the correlation matrix.\n              Returns None if data is insufficient or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if not isinstance(data, pd.DataFrame) or data.empty:\n            return None\n\n        required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n        if not all(col in data.columns for col in required_cols):\n            return None\n        \n        if len(data) < 2:\n            return None\n\n        df = data.copy()\n\n        # Calculate derived metrics\n        df['net_sentiment'] = df['positive_sentiment_raw'] - df['negative_sentiment_raw']\n        df['sentiment_magnitude'] = (df['positive_sentiment_raw'] + df['negative_sentiment_raw']) / 2\n\n        analysis_cols = ['positive_sentiment_raw', 'negative_sentiment_raw', 'net_sentiment', 'sentiment_magnitude']\n        \n        # Calculate correlation matrix\n        correlation_matrix = df[analysis_cols].corr(method='pearson')\n\n        results = {\n            'analysis_tier': 'Tier 3 (Exploratory)',\n            'sample_size': len(df),\n            'power_caveat': f\"Exploratory analysis - correlation estimates are highly unstable and for pattern detection only (N={len(df)}).\",\n            'correlation_matrix': correlation_matrix.to_dict()\n        }\n        \n        return results\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}