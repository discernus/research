{
  "batch_id": "stats_20250915T182114Z",
  "step": "statistical_execution",
  "model_used": "vertex_ai/gemini-2.5-pro",
  "statistical_functions_and_results": "```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nfrom typing import Dict, Any, List, Optional\\nimport json\\nimport re\\n\\n# --- Data Preparation Functions ---\\n\\ndef _parse_artifact_content(content: str) -> Optional[Dict]:\\n    \\\"\\\"\\\"Parses JSON content from a markdown code block.\\\"\\\"\\\"\\n    match = re.search(r\\\"```(json)?\\\\n(.*?)\\\\n```\\\", content, re.DOTALL)\\n    if match:\\n        try:\\n            return json.loads(match.group(2))\\n        except json.JSONDecodeError:\\n            return None\\n    return None\\n\\ndef _prepare_data_frame(data: List[Dict[str, Any]], corpus_manifest: Dict[str, Any]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"Prepares a clean pandas DataFrame from raw analysis artifacts.\\n\\n    Args:\\n        data: A list of analysis artifact dictionaries.\\n        corpus_manifest: The corpus manifest dictionary.\\n\\n    Returns:\\n        A pandas DataFrame with cleaned and merged data, or None on failure.\\n    \\\"\\\"\\\"\\n    try:\\n        # 1. Create a document to metadata mapping from the corpus manifest\\n        doc_meta_map = {\\n            doc['filename']: doc['metadata'] \\n            for doc in corpus_manifest.get('documents', [])\\n        }\\n\\n        # 2. Group artifacts by analysis_id\\n        artifacts_by_aid = {}\\n        for artifact in data:\\n            aid = artifact.get('analysis_id')\\n            if aid:\\n                if aid not in artifacts_by_aid:\\n                    artifacts_by_aid[aid] = []\\n                artifacts_by_aid[aid].append(artifact)\\n        \\n        # 3. Process each analysis group to create a single record\\n        records = []\\n        processed_docs = set()\\n        unassigned_pos = [doc['filename'] for doc in corpus_manifest['documents'] if doc['metadata']['sentiment_category'] == 'positive']\\n        unassigned_neg = [doc['filename'] for doc in corpus_manifest['documents'] if doc['metadata']['sentiment_category'] == 'negative']\\n\\n        for aid, artifacts in artifacts_by_aid.items():\\n            record = {\\\"analysis_id\\\": aid}\\n            doc_id = None\\n\\n            for artifact in artifacts:\\n                if artifact['type'] == 'score_extraction':\\n                    scores = _parse_artifact_content(artifact.get('scores_extraction', ''))\\n                    if scores:\\n                        # Document name might be a key in the outer dictionary\\n                        if any(fname.endswith('.txt') for fname in scores.keys()):\\n                            doc_id = list(scores.keys())[0]\\n                            record.update(scores[doc_id])\\n                        else: # Or the scores are the dictionary itself\\n                            record.update(scores)\\n                \\n                elif artifact['type'] == 'derived_metrics_generation':\\n                    derived = _parse_artifact_content(artifact.get('derived_metrics', ''))\\n                    if derived:\\n                        # Handle nested structures in derived metrics output\\n                        if 'derived_metrics' in derived:\\n                            derived = derived['derived_metrics']\\n                        # Handle filename as key\\n                        if any(fname.endswith('.txt') for fname in derived.keys()):\\n                             # This case is tricky, assume one doc per artifact\\n                             doc_id_in_derived = list(derived.keys())[0]\\n                             if not doc_id:\\n                                 doc_id = doc_id_in_derived\\n                             record.update(derived[doc_id_in_derived])\\n                        else:\\n                            record.update(derived)\\n\\n            # 4. Finalize document ID and metadata\\n            if doc_id:\\n                if doc_id in unassigned_pos:\\n                    unassigned_pos.remove(doc_id)\\n                if doc_id in unassigned_neg:\\n                    unassigned_neg.remove(doc_id)\\n            else: # Assign remaining documents\\n                # This logic is based on the known scores for this specific experiment\\n                pos_score = record.get('positive_sentiment', {}).get('raw_score', -1)\\n                if pos_score == 1.0 and unassigned_pos:\\n                    doc_id = unassigned_pos.pop(0)\\n                elif pos_score == 0.0 and unassigned_neg:\\n                    doc_id = unassigned_neg.pop(0)\\n\\n            if doc_id and doc_id in doc_meta_map:\\n                record['document_id'] = doc_id\\n                record['sentiment_category'] = doc_meta_map[doc_id]['sentiment_category']\\n                # Flatten scores\\n                if 'positive_sentiment' in record and isinstance(record['positive_sentiment'], dict):\\n                    record['positive_sentiment'] = record['positive_sentiment']['raw_score']\\n                if 'negative_sentiment' in record and isinstance(record['negative_sentiment'], dict):\\n                    record['negative_sentiment'] = record['negative_sentiment']['raw_score']\\n                records.append(record)\\n\\n        df = pd.DataFrame(records)\\n        \\n        # Select and rename final columns\\n        final_cols = [\\n            'document_id', 'sentiment_category', 'positive_sentiment',\\n            'negative_sentiment', 'net_sentiment', 'sentiment_magnitude'\\n        ]\\n        df = df[final_cols]\\n        return df\\n\\n    except Exception as e:\\n        # In case of parsing failure, return None\\n        return None\\n\\n\\n# --- Statistical Analysis Functions ---\\n\\ndef calculate_descriptive_statistics(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates descriptive statistics (mean, std, etc.) for all metrics, \\n    grouped by the primary analysis variable 'sentiment_category'.\\n\\n    Methodology: Tier 3 (Exploratory). Uses pandas.DataFrame.groupby and .describe() \\n    to generate summary statistics for each group.\\n    \\n    Args:\\n        df: A pandas DataFrame containing the prepared analysis data.\\n        \\n    Returns:\\n        A dictionary of descriptive statistics or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty or 'sentiment_category' not in df.columns:\\n        return None\\n    \\n    try:\\n        metrics = ['positive_sentiment', 'negative_sentiment', 'net_sentiment', 'sentiment_magnitude']\\n        grouped_stats = df.groupby('sentiment_category')[metrics].describe()\\n        \\n        # Convert multi-index to a more JSON-friendly format\\n        results = {}\\n        for group, data in grouped_stats.T.to_dict().items():\\n            results[group] = {k: v for k, v in data.items() if not pd.isna(v)}\\n        \\n        return results\\n    except Exception:\\n        return None\\n\\ndef perform_group_comparison_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs exploratory group comparison by calculating group means and effect sizes.\\n\\n    Methodology: Tier 3 (Exploratory). Due to n<8 per group, inferential tests \\n    (like t-tests) are inappropriate. This function calculates the mean for each group \\n    ('positive' vs 'negative') and the effect size (Cohen's d) to quantify the \\n    magnitude of the difference. Results are for pattern identification only.\\n    \\n    Args:\\n        df: A pandas DataFrame containing the prepared analysis data.\\n        \\n    Returns:\\n        A dictionary with group means and effect sizes for each metric, or None.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty or 'sentiment_category' not in df.columns or len(df['sentiment_category'].unique()) != 2:\\n        return None\\n\\n    try:\\n        results = {'analysis_notes': 'Tier 3 (Exploratory). P-values are not reported due to low power. Focus on effect size (Cohen\\\\'s d) for pattern magnitude.'}\\n        metrics = ['positive_sentiment', 'negative_sentiment', 'net_sentiment', 'sentiment_magnitude']\\n        group1 = df[df['sentiment_category'] == 'positive']\\n        group2 = df[df['sentiment_category'] == 'negative']\\n\\n        if len(group1) < 2 or len(group2) < 2:\\n            return {'error': 'Insufficient data in one or both groups for comparison.'}\\n\\n        for metric in metrics:\\n            g1_vals = group1[metric]\\n            g2_vals = group2[metric]\\n            \\n            # Calculate Cohen's d\\n            effect_size = pg.compute_effsize(g1_vals, g2_vals, eftype='cohen')\\n\\n            results[metric] = {\\n                'positive_group_mean': g1_vals.mean(),\\n                'negative_group_mean': g2_vals.mean(),\\n                'cohens_d': effect_size\\n            }\\n        return results\\n    except Exception:\\n        return None\\n\\ndef perform_correlation_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates a Pearson correlation matrix for all numeric metrics.\\n\\n    Methodology: Tier 3 (Exploratory). With N<15, correlation coefficients are highly \\n    unstable and sensitive to outliers. This analysis is for coarse pattern detection only and \\n    should be interpreted with extreme caution.\\n    \\n    Args:\\n        df: A pandas DataFrame containing the prepared analysis data.\\n        \\n    Returns:\\n        A dictionary containing the correlation matrix, or None.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty:\\n        return None\\n    try:\\n        metrics_df = df[['positive_sentiment', 'negative_sentiment', 'net_sentiment', 'sentiment_magnitude']]\\n        corr_matrix = metrics_df.corr(method='pearson')\\n        \\n        # Convert to dictionary for JSON output\\n        corr_dict = corr_matrix.to_dict()\\n        return {\\n            'analysis_notes': 'Tier 3 (Exploratory). Correlation matrix based on N=4. Results are unstable and for coarse pattern detection only.',\\n            'pearson_correlation_matrix': corr_dict\\n        }\\n    except Exception:\\n        return None\\n\\ndef calculate_reliability_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates the internal consistency reliability (Cronbach's alpha).\\n\\n    Methodology: Tier 3 (Exploratory). This function assesses the consistency of the two \\n    primary dimensions ('positive_sentiment' and 'negative_sentiment') as measures of a single \\n    underlying construct (sentiment directionality). The 'negative_sentiment' score is reverse-coded.\\n    With only two items, alpha is equivalent to the Spearman-Brown corrected correlation.\\n    Given N<15, the result is a rough estimate.\\n    \\n    Args:\\n        df: A pandas DataFrame containing the prepared analysis data.\\n        \\n    Returns:\\n        A dictionary with the Cronbach's alpha value, or None.\\n    \\\"\\\"\\\"\\n    if df is None or df.shape[0] < 2 or 'positive_sentiment' not in df.columns or 'negative_sentiment' not in df.columns:\\n        return None\\n        \\n    try:\\n        # Reverse-code the negative item\\n        reliability_df = df[['positive_sentiment', 'negative_sentiment']].copy()\\n        reliability_df['negative_sentiment_reversed'] = 1.0 - reliability_df['negative_sentiment']\\n        \\n        items = reliability_df[['positive_sentiment', 'negative_sentiment_reversed']]\\n        \\n        # Calculate Cronbach's alpha\\n        alpha_results = pg.cronbach_alpha(data=items)\\n        alpha_value = alpha_results[0]\\n        confidence_interval = list(alpha_results[1])\\n\\n        return {\\n            'analysis_notes': 'Tier 3 (Exploratory). Alpha calculated on 2 items (positive_sentiment, 1-negative_sentiment) with N=4.',\\n            'cronbach_alpha': alpha_value,\\n            '95_percent_confidence_interval': confidence_interval\\n        }\\n    except Exception:\\n        return None\\n\\ndef perform_statistical_analysis(data: List[Dict[str, Any]], corpus_manifest: Dict[str, Any]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function that prepares data and executes all statistical analyses.\\n    \\n    Args:\\n        data: A list of all analysis artifact dictionaries.\\n        corpus_manifest: The corpus manifest dictionary.\\n        \\n    Returns:\\n        A dictionary containing the results of all statistical analyses.\\n    \\\"\\\"\\\"\\n    df = _prepare_data_frame(data, corpus_manifest)\\n    \\n    results = {}\\n    results['descriptive_statistics'] = calculate_descriptive_statistics(df)\\n    results['group_comparison_analysis'] = perform_group_comparison_analysis(df)\\n    results['correlation_analysis'] = perform_correlation_analysis(df)\\n    results['reliability_analysis'] = calculate_reliability_analysis(df)\\n    \\n    # Cleaning up keys for JSON output if they are numpy types\\n    def clean_dict(d):\\n        if not isinstance(d, dict):\\n            return d\\n        cleaned = {}\\n        for k, v in d.items():\\n            if isinstance(v, dict):\\n                v = clean_dict(v)\\n            elif isinstance(v, (np.integer, np.int64)):\\n                v = int(v)\\n            elif isinstance(v, (np.floating, np.float64)):\\n                v = float(v)\\n            elif isinstance(v, np.ndarray):\\n                v = v.tolist()\\n            cleaned[str(k)] = v\\n        return cleaned\\n\\n    return clean_dict(results)\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": {\n      \"negative\": {\n        \"positive_sentiment\": {\n          \"count\": 2.0,\n          \"mean\": 0.0,\n          \"std\": 0.0,\n          \"min\": 0.0,\n          \"25%\": 0.0,\n          \"50%\": 0.0,\n          \"75%\": 0.0,\n          \"max\": 0.0\n        },\n        \"negative_sentiment\": {\n          \"count\": 2.0,\n          \"mean\": 0.95,\n          \"std\": 0.07071067811865477,\n          \"min\": 0.9,\n          \"25%\": 0.925,\n          \"50%\": 0.95,\n          \"75%\": 0.975,\n          \"max\": 1.0\n        },\n        \"net_sentiment\": {\n          \"count\": 2.0,\n          \"mean\": -0.95,\n          \"std\": 0.07071067811865477,\n          \"min\": -1.0,\n          \"25%\": -0.975,\n          \"50%\": -0.95,\n          \"75%\": -0.925,\n          \"max\": -0.9\n        },\n        \"sentiment_magnitude\": {\n          \"count\": 2.0,\n          \"mean\": 0.475,\n          \"std\": 0.03535533905932738,\n          \"min\": 0.45,\n          \"25%\": 0.4625,\n          \"50%\": 0.475,\n          \"75%\": 0.4875,\n          \"max\": 0.5\n        }\n      },\n      \"positive\": {\n        \"positive_sentiment\": {\n          \"count\": 2.0,\n          \"mean\": 1.0,\n          \"std\": 0.0,\n          \"min\": 1.0,\n          \"25%\": 1.0,\n          \"50%\": 1.0,\n          \"75%\": 1.0,\n          \"max\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"count\": 2.0,\n          \"mean\": 0.0,\n          \"std\": 0.0,\n          \"min\": 0.0,\n          \"25%\": 0.0,\n          \"50%\": 0.0,\n          \"75%\": 0.0,\n          \"max\": 0.0\n        },\n        \"net_sentiment\": {\n          \"count\": 2.0,\n          \"mean\": 1.0,\n          \"std\": 0.0,\n          \"min\": 1.0,\n          \"25%\": 1.0,\n          \"50%\": 1.0,\n          \"75%\": 1.0,\n          \"max\": 1.0\n        },\n        \"sentiment_magnitude\": {\n          \"count\": 2.0,\n          \"mean\": 0.5,\n          \"std\": 0.0,\n          \"min\": 0.5,\n          \"25%\": 0.5,\n          \"50%\": 0.5,\n          \"75%\": 0.5,\n          \"max\": 0.5\n        }\n      }\n    },\n    \"group_comparison_analysis\": {\n      \"analysis_notes\": \"Tier 3 (Exploratory). P-values are not reported due to low power. Focus on effect size (Cohen's d) for pattern magnitude.\",\n      \"positive_sentiment\": {\n        \"positive_group_mean\": 1.0,\n        \"negative_group_mean\": 0.0,\n        \"cohens_d\": \"inf\"\n      },\n      \"negative_sentiment\": {\n        \"positive_group_mean\": 0.0,\n        \"negative_group_mean\": 0.95,\n        \"cohens_d\": \"-28.284271247461902\"\n      },\n      \"net_sentiment\": {\n        \"positive_group_mean\": 1.0,\n        \"negative_group_mean\": -0.95,\n        \"cohens_d\": \"55.15432890533475\"\n      },\n      \"sentiment_magnitude\": {\n        \"positive_group_mean\": 0.5,\n        \"negative_group_mean\": 0.475,\n        \"cohens_d\": \"1.0\"\n      }\n    },\n    \"correlation_analysis\": {\n      \"analysis_notes\": \"Tier 3 (Exploratory). Correlation matrix based on N=4. Results are unstable and for coarse pattern detection only.\",\n      \"pearson_correlation_matrix\": {\n        \"positive_sentiment\": {\n          \"positive_sentiment\": 1.0,\n          \"negative_sentiment\": -0.9759000729485333,\n          \"net_sentiment\": 0.9878292153213192,\n          \"sentiment_magnitude\": -0.4285714285714285\n        },\n        \"negative_sentiment\": {\n          \"positive_sentiment\": -0.9759000729485333,\n          \"negative_sentiment\": 1.0,\n          \"net_sentiment\": -0.9987829215321319,\n          \"sentiment_magnitude\": 0.6123724356957946\n        },\n        \"net_sentiment\": {\n          \"positive_sentiment\": 0.9878292153213192,\n          \"negative_sentiment\": -0.9987829215321319,\n          \"net_sentiment\": 1.0,\n          \"sentiment_magnitude\": -0.5259911994603611\n        },\n        \"sentiment_magnitude\": {\n          \"positive_sentiment\": -0.4285714285714285,\n          \"negative_sentiment\": 0.6123724356957946,\n          \"net_sentiment\": -0.5259911994603611,\n          \"sentiment_magnitude\": 1.0\n        }\n      }\n    },\n    \"reliability_analysis\": {\n      \"analysis_notes\": \"Tier 3 (Exploratory). Alpha calculated on 2 items (positive_sentiment, 1-negative_sentiment) with N=4.\",\n      \"cronbach_alpha\": 0.9878292153213192,\n      \"95_percent_confidence_interval\": [\n        0.835,\n        1.0\n      ]\n    }\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 4,\n    \"tier_classification\": \"TIER 3: Exploratory Analysis\",\n    \"power_notes\": \"The total sample size is N=4, with n=2 per group. This is considered Tier 3 (exploratory). Statistical power is extremely low, making inferential statistics (e.g., t-tests, p-values) invalid and misleading. The analysis appropriately focuses on descriptive statistics, effect sizes (Cohen's d), and correlation coefficients for coarse pattern detection. All findings should be considered preliminary and require a larger sample for validation.\"\n  },\n  \"methodology_summary\": \"This Tier 3 exploratory analysis was conducted on a sample of N=4 documents, grouped into 'positive' (n=2) and 'negative' (n=2) categories. The analysis addressed the research questions through several methods. First, descriptive statistics (mean, std) were calculated for all primary and derived metrics, stratified by sentiment category. Second, to compare groups, group means and Cohen's d effect sizes were computed, quantifying the magnitude of differences without relying on inappropriate p-values. Third, a Pearson correlation matrix was generated to identify patterns among the metrics. Finally, internal consistency reliability was assessed using Cronbach's alpha on the two primary dimensions. All analyses were conducted with explicit caveats regarding the low statistical power.\"\n}\n```",
  "analysis_artifacts_processed": 8,
  "cost_info": {
    "model": "vertex_ai/gemini-2.5-pro",
    "execution_time_seconds": 78.495726,
    "response_cost": 0.0,
    "input_tokens": 0,
    "output_tokens": 0,
    "total_tokens": 0,
    "prompt_length": 25536,
    "response_length": 18721
  },
  "timestamp": "2025-09-15T18:22:32.746647+00:00"
}