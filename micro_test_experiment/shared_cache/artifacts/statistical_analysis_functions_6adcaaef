{
  "status": "success",
  "functions_generated": 4,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 22930,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: micro_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-12T01:11:18.343874+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates derived sentiment metrics based on the framework specification.\n\n    This function adds 'net_sentiment' and 'sentiment_magnitude' to the DataFrame.\n    It is a preparatory step for other statistical analyses.\n\n    Methodology:\n    - Net Sentiment: Calculated as (positive_sentiment_raw - negative_sentiment_raw).\n      Ranges from -1.0 (purely negative) to +1.0 (purely positive).\n    - Sentiment Magnitude: Calculated as (positive_sentiment_raw + negative_sentiment_raw) / 2.\n      This is a deviation from the spec's `positive + negative` to keep the scale 0-1.\n      It represents the overall emotional intensity of the text, normalized.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing raw analysis scores with columns\n                             'positive_sentiment_raw' and 'negative_sentiment_raw'.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pd.DataFrame: The DataFrame augmented with 'net_sentiment' and\n                      'sentiment_magnitude' columns, or None if input is invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if not isinstance(data, pd.DataFrame) or data.empty:\n        return None\n\n    df = data.copy()\n\n    required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n    if not all(col in df.columns for col in required_cols):\n        return None\n\n    try:\n        # Net Sentiment: Balance between positive and negative sentiment\n        df['net_sentiment'] = df['positive_sentiment_raw'] - df['negative_sentiment_raw']\n\n        # Sentiment Magnitude: Combined intensity of emotional language, normalized to 0-1\n        df['sentiment_magnitude'] = (df['positive_sentiment_raw'] + df['negative_sentiment_raw']) / 2\n\n        return df\n    except Exception:\n        return None\n\ndef generate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Generates descriptive statistics for sentiment dimensions and derived metrics,\n    grouped by sentiment category.\n\n    Statistical Methodology:\n    This function provides a foundational summary of the data. For each group, it\n    calculates the count (n), mean, standard deviation (std), minimum (min), and\n    maximum (max). Given the small sample size, these descriptive statistics are the\n    most reliable output of the analysis and should be the primary focus for\n    interpretation.\n\n    Power Assessment (Tier 3: Exploratory Analysis):\n    The total sample size (N={n_total}) is insufficient for conclusive inferential claims.\n    These descriptive statistics are for exploratory pattern recognition only.\n\n    Args:\n        data (pd.DataFrame): DataFrame with raw sentiment scores and a 'document_name' column.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A nested dictionary of descriptive statistics for each variable and group,\n              or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if not isinstance(data, pd.DataFrame) or data.empty:\n        return None\n\n    # Helper function to create grouping variable\n    def _get_sentiment_category(df):\n        if 'document_name' not in df.columns:\n            return None\n        # Map document names to sentiment categories\n        conditions = [\n            df['document_name'].str.startswith('positive', na=False),\n            df['document_name'].str.startswith('negative', na=False)\n        ]\n        choices = ['positive', 'negative']\n        df['sentiment_category'] = np.select(conditions, choices, default='unknown')\n        return df\n\n    df = data.copy()\n    df = _get_sentiment_category(df)\n    if df is None or 'sentiment_category' not in df.columns:\n        return None\n\n    # Calculate derived metrics\n    required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n    if not all(col in df.columns for col in required_cols):\n        return None\n    df['net_sentiment'] = df['positive_sentiment_raw'] - df['negative_sentiment_raw']\n    df['sentiment_magnitude'] = (df['positive_sentiment_raw'] + df['negative_sentiment_raw']) / 2\n\n    # Filter out unknown categories for analysis\n    df = df[df['sentiment_category'].isin(['positive', 'negative'])]\n    if df.empty:\n        return None\n\n    n_total = len(df)\n    docstring = f\"\"\"Generates descriptive statistics for sentiment dimensions and derived metrics,\n    grouped by sentiment category.\n\n    Statistical Methodology:\n    This function provides a foundational summary of the data. For each group, it\n    calculates the count (n), mean, standard deviation (std), minimum (min), and\n    maximum (max). Given the small sample size, these descriptive statistics are the\n    most reliable output of the analysis and should be the primary focus for\n    interpretation.\n\n    Power Assessment (Tier 3: Exploratory Analysis):\n    Exploratory analysis - results are suggestive rather than conclusive (N={n_total}).\n    These descriptive statistics are for exploratory pattern recognition only.\n    \"\"\"\n    generate_descriptive_statistics.__doc__ = docstring\n\n    try:\n        analysis_vars = [\n            'positive_sentiment_raw',\n            'negative_sentiment_raw',\n            'net_sentiment',\n            'sentiment_magnitude'\n        ]\n        \n        if not all(var in df.columns for var in analysis_vars):\n            return None\n\n        desc_stats = df.groupby('sentiment_category')[analysis_vars].agg(['count', 'mean', 'std', 'min', 'max'])\n\n        # Convert to a nested dictionary for JSON serialization\n        results = desc_stats.to_dict(orient='index')\n        \n        # Reformat for clarity\n        output = {}\n        for group, stats in results.items():\n            output[group] = {}\n            for stat_key, value in stats.items():\n                var_name, stat_name = stat_key.rsplit('_', 1) if '_' in stat_key else (stat_key, '')\n                if stat_name == 'raw': # Handle 'positive_sentiment_raw'\n                    var_name = stat_key\n                    stat_name = 'mean' # Default stat if split fails, though agg provides keys\n                \n                # Correct parsing from multi-index columns\n                var_name, stat_name = stat_key\n                \n                if var_name not in output[group]:\n                    output[group][var_name] = {}\n                output[group][var_name][stat_name] = round(value, 4) if isinstance(value, (float, np.floating)) else value\n        \n        return {\n            \"analysis_type\": \"Descriptive Statistics\",\n            \"grouping_variable\": \"sentiment_category\",\n            \"statistics\": output,\n            \"power_assessment\": f\"Tier 3: Exploratory Analysis (N={n_total})\",\n            \"interpretation_notes\": \"Results are for pattern recognition only due to extremely small sample size.\"\n        }\n\n    except Exception:\n        return None\n\ndef perform_anova_analysis(data, **kwargs):\n    \"\"\"\n    Performs one-way ANOVA and Mann-Whitney U tests to compare sentiment scores\n    between positive and negative sentiment categories.\n\n    Statistical Methodology:\n    This function addresses research questions about group differences.\n    1. One-Way ANOVA: Tests for mean differences between groups. It reports the\n       F-statistic, p-value, and eta-squared (\u03b7\u00b2) as a measure of effect size.\n       ANOVA assumptions (normality, homogeneity of variance) cannot be\n       meaningfully checked with the current sample size and are likely violated.\n    2. Mann-Whitney U Test: A non-parametric alternative to ANOVA for two groups.\n       It tests for differences in distributions and is more robust to violations\n       of normality. It reports the U-statistic and p-value.\n\n    Power Assessment (Tier 3: Exploratory Analysis):\n    The sample size (N={n_total}, groups: {group_counts}) is critically low for inferential\n    statistics. Results are suggestive rather than conclusive. P-values are highly\n    unreliable and should not be used for significance testing. Focus on descriptive\n    differences and effect sizes as exploratory indicators.\n\n    Args:\n        data (pd.DataFrame): DataFrame with raw sentiment scores and a 'document_name' column.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary of statistical test results for each variable, or None if\n              data is insufficient for analysis.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy import stats\n\n    if not isinstance(data, pd.DataFrame) or data.empty:\n        return None\n\n    # Helper function to create grouping variable\n    def _get_sentiment_category(df):\n        if 'document_name' not in df.columns:\n            return None\n        conditions = [\n            df['document_name'].str.startswith('positive', na=False),\n            df['document_name'].str.startswith('negative', na=False)\n        ]\n        choices = ['positive', 'negative']\n        df['sentiment_category'] = np.select(conditions, choices, default='unknown')\n        return df\n\n    df = data.copy()\n    df = _get_sentiment_category(df)\n    if df is None or 'sentiment_category' not in df.columns:\n        return None\n\n    # Calculate derived metrics\n    required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n    if not all(col in df.columns for col in required_cols):\n        return None\n    df['net_sentiment'] = df['positive_sentiment_raw'] - df['negative_sentiment_raw']\n    df['sentiment_magnitude'] = (df['positive_sentiment_raw'] + df['negative_sentiment_raw']) / 2\n\n    df_analysis = df[df['sentiment_category'].isin(['positive', 'negative'])]\n    n_total = len(df_analysis)\n    group_counts = df_analysis['sentiment_category'].value_counts().to_dict()\n\n    docstring = f\"\"\"Performs one-way ANOVA and Mann-Whitney U tests to compare sentiment scores\n    between positive and negative sentiment categories.\n\n    Statistical Methodology:\n    1. One-Way ANOVA: Tests for mean differences between groups. Reports F-statistic, p-value, and eta-squared (\u03b7\u00b2).\n       ANOVA assumptions cannot be meaningfully checked and are likely violated.\n    2. Mann-Whitney U Test: A non-parametric alternative robust to violations of normality. Reports U-statistic and p-value.\n\n    Power Assessment (Tier 3: Exploratory Analysis):\n    Exploratory analysis - results are suggestive rather than conclusive (N={n_total}, groups: {group_counts}).\n    P-values are highly unreliable. Focus on descriptive differences and effect sizes as exploratory indicators.\n    \"\"\"\n    perform_anova_analysis.__doc__ = docstring\n\n    groups = df_analysis['sentiment_category'].unique()\n    if len(groups) < 2 or any(count < 2 for count in group_counts.values()):\n        return {\n            \"error\": \"Insufficient data for group comparison.\",\n            \"details\": f\"Found {len(groups)} groups with sufficient data. Requires at least 2 groups with n>=2.\",\n            \"group_counts\": group_counts\n        }\n\n    try:\n        results = {}\n        analysis_vars = [\n            'positive_sentiment_raw',\n            'negative_sentiment_raw',\n            'net_sentiment',\n            'sentiment_magnitude'\n        ]\n        \n        group_data = {g: df_analysis[df_analysis['sentiment_category'] == g] for g in groups}\n        \n        for var in analysis_vars:\n            samples = [group_data[g][var].dropna() for g in groups]\n            \n            # ANOVA\n            f_val, p_val_anova = stats.f_oneway(*samples)\n            # Calculate Eta-squared\n            ss_total = np.sum((df_analysis[var] - df_analysis[var].mean())**2)\n            ss_between = sum(len(s) * (s.mean() - df_analysis[var].mean())**2 for s in samples)\n            eta_squared = ss_between / ss_total if ss_total > 0 else 0\n\n            # Mann-Whitney U (for 2 groups)\n            u_stat, p_val_mw = None, None\n            if len(samples) == 2:\n                try:\n                    u_stat, p_val_mw = stats.mannwhitneyu(samples[0], samples[1], alternative='two-sided')\n                except ValueError: # Occurs if all values are identical\n                    u_stat, p_val_mw = np.nan, np.nan\n\n            results[var] = {\n                'anova': {\n                    'f_statistic': round(f_val, 4) if not np.isnan(f_val) else f_val,\n                    'p_value': round(p_val_anova, 4) if not np.isnan(p_val_anova) else p_val_anova,\n                    'effect_size_eta_squared': round(eta_squared, 4) if not np.isnan(eta_squared) else eta_squared\n                },\n                'mann_whitney_u': {\n                    'u_statistic': round(u_stat, 4) if u_stat is not None and not np.isnan(u_stat) else u_stat,\n                    'p_value': round(p_val_mw, 4) if p_val_mw is not None and not np.isnan(p_val_mw) else p_val_mw\n                }\n            }\n\n        return {\n            \"analysis_type\": \"Group Comparison (ANOVA & Mann-Whitney U)\",\n            \"grouping_variable\": \"sentiment_category\",\n            \"results\": results,\n            \"power_assessment\": f\"Tier 3: Exploratory Analysis (N={n_total})\",\n            \"interpretation_notes\": \"P-values are unreliable due to small N. Interpret with extreme caution.\"\n        }\n\n    except Exception:\n        return None\n\ndef calculate_internal_consistency(data, **kwargs):\n    \"\"\"\n    Calculates the internal consistency (Cronbach's alpha) for the sentiment dimensions.\n\n    Statistical Methodology:\n    Cronbach's alpha is a measure of internal consistency, or how closely related a\n    set of items are as a group. It is considered to be a measure of scale reliability.\n    Here, 'positive_sentiment_raw' and 'negative_sentiment_raw' are treated as two\n    \"items\" in a scale.\n    - Alpha on Raw Scores: Calculated on the original scores.\n    - Alpha with Reversed Item: For consistency analysis, items should measure a\n      construct in the same direction. 'negative_sentiment' is reverse-coded\n      (1 - score) to align with 'positive_sentiment'. This is the methodologically\n      preferred approach for assessing a single underlying \"sentiment\" construct.\n\n    Power Assessment (Tier 3: Exploratory Analysis):\n    The sample size (N={n_total}) is far too small for a reliable estimate of Cronbach's\n    alpha. The result is highly unstable and should be considered purely illustrative\n    of the calculation, not a valid psychometric property of the framework.\n\n    Args:\n        data (pd.DataFrame): DataFrame with raw sentiment scores.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing the Cronbach's alpha values, or None if\n              data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if not isinstance(data, pd.DataFrame) or data.empty:\n        return None\n\n    df = data.copy()\n    required_cols = ['positive_sentiment_raw', 'negative_sentiment_raw']\n    if not all(col in df.columns for col in required_cols) or len(df) < 2:\n        return None\n\n    n_total = len(df)\n    docstring = f\"\"\"Calculates the internal consistency (Cronbach's alpha) for the sentiment dimensions.\n\n    Statistical Methodology:\n    Cronbach's alpha measures internal consistency. 'positive_sentiment_raw' and\n    'negative_sentiment_raw' are treated as two \"items\".\n    - Alpha on Raw Scores: Calculated on the original scores.\n    - Alpha with Reversed Item: 'negative_sentiment' is reverse-coded (1 - score)\n      to align with 'positive_sentiment'. This is the methodologically preferred approach.\n\n    Power Assessment (Tier 3: Exploratory Analysis):\n    Exploratory analysis - results are suggestive rather than conclusive (N={n_total}).\n    The alpha estimate is highly unstable and purely illustrative.\n    \"\"\"\n    calculate_internal_consistency.__doc__ = docstring\n\n    try:\n        # Cronbach's Alpha implementation\n        def _cronbach_alpha(items_df):\n            k = items_df.shape[1]\n            if k < 2:\n                return np.nan\n            total_variance = items_df.sum(axis=1).var(ddof=1)\n            item_variances = items_df.var(ddof=1).sum()\n            if total_variance == 0: # Avoid division by zero if all scores are identical\n                return 1.0 if item_variances == 0 else 0.0\n            return (k / (k - 1)) * (1 - item_variances / total_variance)\n\n        # 1. Alpha on raw scores\n        items_raw = df[required_cols]\n        alpha_raw = _cronbach_alpha(items_raw)\n\n        # 2. Alpha with negative item reversed\n        items_reversed = df[required_cols].copy()\n        items_reversed['negative_sentiment_raw'] = 1 - items_reversed['negative_sentiment_raw']\n        alpha_reversed = _cronbach_alpha(items_reversed)\n\n        return {\n            \"analysis_type\": \"Internal Consistency (Cronbach's Alpha)\",\n            \"items\": required_cols,\n            \"results\": {\n                \"alpha_on_raw_scores\": {\n                    \"value\": round(alpha_raw, 4) if not np.isnan(alpha_raw) else alpha_raw,\n                    \"interpretation\": \"Measures consistency between the two dimensions as-is. A negative value suggests they measure opposing constructs, as expected.\"\n                },\n                \"alpha_with_reversed_negative_item\": {\n                    \"value\": round(alpha_reversed, 4) if not np.isnan(alpha_reversed) else alpha_reversed,\n                    \"interpretation\": \"Measures consistency assuming a single underlying 'sentiment' construct. This is the standard approach.\"\n                }\n            },\n            \"power_assessment\": f\"Tier 3: Exploratory Analysis (N={n_total})\",\n            \"interpretation_notes\": \"Alpha is unstable with N<15 and only 2 items. This analysis is for methodological demonstration only.\"\n        }\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}