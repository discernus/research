{
  "generation_metadata": {
    "status": "success",
    "functions_generated": 8,
    "output_file": "automatedderivedmetricsagent_functions.py",
    "module_size": 8127,
    "function_code_content": "[\n  {\n    \"filename\": \"sentiment_binary_v1_calculations.py\",\n    \"content\": \"import pandas as pd\\nimport numpy as np\\nimport json\\nfrom typing import Optional, Dict, Any\\n\\ndef _get_scores_from_row(row: pd.Series) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Parses the 'raw_analysis_response' column of a DataFrame row to extract dimensional scores.\\n\\n    This helper function is designed to be robust against various data inconsistencies,\\n    including missing columns, non-string data, different JSON delimiters, malformed JSON,\\n    and variations in the nested data structure.\\n\\n    Args:\\n        row: A pandas Series representing a row of a DataFrame.\\n\\n    Returns:\\n        A dictionary containing the dimensional scores if found, otherwise None.\\n    \\\"\\\"\\\"\\n    if 'raw_analysis_response' not in row:\\n        return None\\n\\n    raw_response = row['raw_analysis_response']\\n    if not isinstance(raw_response, str):\\n        return None\\n\\n    json_content = None\\n    start_marker = '<<<DISCERNUS_ANALYSIS_JSON_v6>>>'\\n    end_marker = '<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>'\\n\\n    # Primary parsing method based on explicit instructions\\n    if start_marker in raw_response and end_marker in raw_response:\\n        start_idx = raw_response.find(start_marker)\\n        end_idx = raw_response.find(end_marker)\\n        json_content = raw_response[start_idx + len(start_marker):end_idx].strip()\\n    # Fallback for markdown code fences\\n    elif raw_response.strip().startswith(\\\"```json\\\"):\\n        json_part = raw_response.strip()\\n        json_content = json_part[len(\\\"```json\\\"):].strip().rstrip('`')\\n    # Fallback for raw JSON strings\\n    elif raw_response.strip().startswith(\\\"{\\\"):\\n        json_content = raw_response.strip()\\n\\n    if not json_content:\\n        return None\\n\\n    try:\\n        analysis = json.loads(json_content)\\n        # Navigate the structure based on the sample data\\n        if 'document_analyses' in analysis and isinstance(analysis['document_analyses'], list) and len(analysis['document_analyses']) > 0:\\n            # The framework is designed for single document analysis per call\\n            doc_analysis = analysis['document_analyses'][0]\\n            if 'dimensional_scores' in doc_analysis:\\n                return doc_analysis['dimensional_scores']\\n    except (json.JSONDecodeError, KeyError, IndexError, TypeError):\\n        return None\\n    \\n    return None\\n\\ndef calculate_net_sentiment(data: pd.DataFrame, **kwargs) -> Optional[float]:\\n    \\\"\\\"\\\"\\n    Calculates the average Net Sentiment across all documents in the DataFrame.\\n    Net Sentiment is the balance between positive and negative sentiment.\\n\\n    Formula: mean(dimensions.positive_sentiment.raw_score - dimensions.negative_sentiment.raw_score)\\n\\n    Args:\\n        data: pandas DataFrame with a 'raw_analysis_response' column.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        A single float representing the average net sentiment, or None if it cannot be calculated.\\n    \\\"\\\"\\\"\\n    def _calculate(row):\\n        scores = _get_scores_from_row(row)\\n        if scores:\\n            try:\\n                pos_score = scores.get('positive_sentiment', {}).get('raw_score')\\n                neg_score = scores.get('negative_sentiment', {}).get('raw_score')\\n                if pos_score is not None and neg_score is not None:\\n                    return float(pos_score) - float(neg_score)\\n            except (AttributeError, TypeError, ValueError):\\n                return None\\n        return None\\n\\n    if 'raw_analysis_response' not in data.columns:\\n        return None\\n\\n    net_sentiments = data.apply(_calculate, axis=1).dropna()\\n\\n    if net_sentiments.empty:\\n        return None\\n    \\n    return float(net_sentiments.mean())\\n\\ndef calculate_sentiment_magnitude(data: pd.DataFrame, **kwargs) -> Optional[float]:\\n    \\\"\\\"\\\"\\n    Calculates the average Sentiment Magnitude across all documents in the DataFrame.\\n    Sentiment Magnitude is the combined intensity of emotional language.\\n\\n    Formula: mean((dimensions.positive_sentiment.raw_score + dimensions.negative_sentiment.raw_score) / 2)\\n\\n    Args:\\n        data: pandas DataFrame with a 'raw_analysis_response' column.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        A single float representing the average sentiment magnitude, or None if it cannot be calculated.\\n    \\\"\\\"\\\"\\n    def _calculate(row):\\n        scores = _get_scores_from_row(row)\\n        if scores:\\n            try:\\n                pos_score = scores.get('positive_sentiment', {}).get('raw_score')\\n                neg_score = scores.get('negative_sentiment', {}).get('raw_score')\\n                if pos_score is not None and neg_score is not None:\\n                    return (float(pos_score) + float(neg_score)) / 2.0\\n            except (AttributeError, TypeError, ValueError):\\n                return None\\n        return None\\n\\n    if 'raw_analysis_response' not in data.columns:\\n        return None\\n\\n    magnitudes = data.apply(_calculate, axis=1).dropna()\\n\\n    if magnitudes.empty:\\n        return None\\n        \\n    return float(magnitudes.mean())\\n\\ndef calculate_all_derived_metrics(data: pd.DataFrame, **kwargs) -> Dict[str, Optional[float]]:\\n    \\\"\\\"\\\"\\n    Calculates all derived metrics for the given DataFrame, returning a dictionary of aggregate scores.\\n    This function calls each individual metric calculation function directly by name.\\n\\n    Args:\\n        data: pandas DataFrame with analysis data.\\n        **kwargs: Additional parameters to be passed to calculation functions.\\n\\n    Returns:\\n        A dictionary where keys are metric names and values are the calculated aggregate scores.\\n    \\\"\\\"\\\"\\n    metrics = {\\n        \\\"net_sentiment\\\": calculate_net_sentiment(data, **kwargs),\\n        \\\"sentiment_magnitude\\\": calculate_sentiment_magnitude(data, **kwargs),\\n    }\\n    return metrics\\n\\ndef calculate_derived_metrics(data: pd.DataFrame, **kwargs) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Calculates all derived metrics for each row and adds them as new columns to the DataFrame.\\n\\n    This function processes each row, calculates the derived metrics based on its\\n    'raw_analysis_response' data, and appends the results as new columns named\\n    'net_sentiment' and 'sentiment_magnitude'.\\n\\n    Args:\\n        data: pandas DataFrame with a 'raw_analysis_response' column.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        A new pandas DataFrame with the original data plus the new derived metric columns.\\n        Missing values in the new columns are filled with 0.0.\\n    \\\"\\\"\\\"\\n    df = data.copy()\\n    \\n    if 'raw_analysis_response' not in df.columns:\\n        df['net_sentiment'] = 0.0\\n        df['sentiment_magnitude'] = 0.0\\n        return df\\n\\n    def _calculate_metrics_for_row(row: pd.Series) -> pd.Series:\\n        \\\"\\\"\\\"Helper to calculate all metrics for a single row.\\\"\\\"\\\"\\n        scores = _get_scores_from_row(row)\\n        net_sentiment = None\\n        sentiment_magnitude = None\\n\\n        if scores:\\n            try:\\n                pos_score = scores.get('positive_sentiment', {}).get('raw_score')\\n                neg_score = scores.get('negative_sentiment', {}).get('raw_score')\\n\\n                if pos_score is not None and neg_score is not None:\\n                    pos_score_f = float(pos_score)\\n                    neg_score_f = float(neg_score)\\n                    net_sentiment = pos_score_f - neg_score_f\\n                    sentiment_magnitude = (pos_score_f + neg_score_f) / 2.0\\n            except (AttributeError, TypeError, ValueError):\\n                pass\\n\\n        return pd.Series({\\n            'net_sentiment': net_sentiment,\\n            'sentiment_magnitude': sentiment_magnitude\\n        })\\n\\n    derived_metrics_df = df.apply(_calculate_metrics_for_row, axis=1)\\n    \\n    df = df.join(derived_metrics_df)\\n\\n    df['net_sentiment'] = df['net_sentiment'].fillna(0.0)\\n    df['sentiment_magnitude'] = df['sentiment_magnitude'].fillna(0.0)\\n\\n    return df\\n\"\n  }\n]",
    "cached_with_code": true
  },
  "derived_metrics_data": {
    "status": "success",
    "original_count": 4,
    "derived_count": 4,
    "derived_metrics": [
      {
        "raw_analysis_response": "```json\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.95,\n    \"analysis_notes\": \"Applied three independent analytical approaches with median aggregation. The document explicitly states its positive sentiment context and contains an overwhelming number of positive terms with no negative terms, making the scoring straightforward and highly confident.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"positive_test_1.txt\",\n      \"document_name\": \"positive_test_1.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence_quotes\": {\n        \"positive_sentiment\": [\n          \"This is a wonderful day! Everything is going perfectly. I feel great about the future. Success is everywhere.\",\n          \"We're achieving amazing results. Optimism fills the air. What a fantastic opportunity! I'm thrilled with the progress. Everything looks bright and promising.\"\n        ],\n        \"negative_sentiment\": [\n          \"No negative language or expressions were found in the document.\"\n        ]\n      },\n      \"marked_up_document\": \"# Document Analysis - Marked Up Text\\n\\n# Positive Test Document 1\\n\\n**Context**: Sample text with positive sentiment\\n\\nThis is a [positive_sentiment: \\\"wonderful\\\"] day! [positive_sentiment: \\\"Everything is going perfectly\\\"]. I feel [positive_sentiment: \\\"great\\\"] about the future. [positive_sentiment: \\\"Success is everywhere\\\"]. The team did an [positive_sentiment: \\\"excellent\\\"] job. We're achieving [positive_sentiment: \\\"amazing\\\"] results. [positive_sentiment: \\\"Optimism fills the air\\\"]. What a [positive_sentiment: \\\"fantastic\\\"] opportunity! I'm [positive_sentiment: \\\"thrilled\\\"] with the progress. [positive_sentiment: \\\"Everything looks bright and promising\\\"].\\n\"\n    }\n  ]\n}\n```",
        "net_sentiment": 1.0,
        "sentiment_magnitude": 0.5
      },
      {
        "raw_analysis_response": "```json\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.98,\n    \"analysis_notes\": \"Analysis performed using three independent approaches (Evidence-First, Context-Weighted, Pattern-Based). Median aggregation applied for scores, and most representative quotes selected. Document is explicitly designed as a positive test case, which strongly influenced extreme scores.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"positive_test_2.txt\",\n      \"document_name\": \"Positive Test Document 2\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence_quotes\": {\n        \"positive_sentiment\": [\n          \"What a superb morning! All systems are operating flawlessly. I'm excited about what's coming next.\",\n          \"Achievement surrounds us. The group performed outstandingly. We're reaching incredible goals. Hopefulness permeates everything.\"\n        ],\n        \"negative_sentiment\": []\n      },\n      \"marked_up_document\": \"# Document Analysis - Marked Up Text\\n\\n# Positive Test Document 2\\n\\n**Context**: [POSITIVE_SENTIMENT: \\\"Sample text with positive sentiment\\\"]\\n\\n[POSITIVE_SENTIMENT: \\\"What a superb morning!\\\"] [POSITIVE_SENTIMENT: \\\"All systems are operating flawlessly.\\\"] [POSITIVE_SENTIMENT: \\\"I'm excited about what's coming next.\\\"] [POSITIVE_SENTIMENT: \\\"Achievement surrounds us.\\\"] [POSITIVE_SENTIMENT: \\\"The group performed outstandingly.\\\"] [POSITIVE_SENTIMENT: \\\"We're reaching incredible goals.\\\"] [POSITIVE_SENTIMENT: \\\"Hopefulness permeates everything.\\\"] [POSITIVE_SENTIMENT: \\\"Such a marvelous chance!\\\"] [POSITIVE_SENTIMENT: \\\"I'm delighted by the advancement.\\\"] [POSITIVE_SENTIMENT: \\\"Everything appears glowing and encouraging.\\\"]\\n\"\n    }\n  ]\n}\n```",
        "net_sentiment": 1.0,
        "sentiment_magnitude": 0.5
      },
      {
        "raw_analysis_response": "```json\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.95,\n    \"analysis_notes\": \"Applied three independent analytical approaches with median aggregation\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"negative_test_1.txt\",\n      \"document_name\": \"negative_test_1.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence_quotes\": {\n        \"positive_sentiment\": [],\n        \"negative_sentiment\": [\n          \"This is a terrible situation. Everything is going wrong. I feel awful about the future.\",\n          \"Failure surrounds us. The team did a horrible job. We're facing disaster. Pessimism fills the air. What a disastrous outcome! I'm devastated by the results. Everything looks dark and hopeless.\"\n        ]\n      },\n      \"marked_up_document\": \"# Document Analysis - Marked Up Text\\n\\n# Negative Test Document 1\\n\\n**Context**: Sample text with negative sentiment\\n\\n[NEGATIVE_SENTIMENT: This is a terrible situation.] [NEGATIVE_SENTIMENT: Everything is going wrong.] [NEGATIVE_SENTIMENT: I feel awful about the future.] [NEGATIVE_SENTIMENT: Failure surrounds us.] [NEGATIVE_SENTIMENT: The team did a horrible job.] [NEGATIVE_SENTIMENT: We're facing disaster.] [NEGATIVE_SENTIMENT: Pessimism fills the air.] [NEGATIVE_SENTIMENT: What a disastrous outcome!] [NEGATIVE_SENTIMENT: I'm devastated by the results.] [NEGATIVE_SENTIMENT: Everything looks dark and hopeless.]\"\n    }\n  ]\n}\n```",
        "net_sentiment": -1.0,
        "sentiment_magnitude": 0.5
      },
      {
        "raw_analysis_response": "```json\n{\n  \"analysis_metadata\": {\n    \"framework_name\": \"sentiment_binary_v1\",\n    \"framework_version\": \"1.0.0\",\n    \"analyst_confidence\": 0.98,\n    \"analysis_notes\": \"Applied three independent analytical approaches (Evidence-First, Context-Weighted, Pattern-Based) and aggregated scores using the median. The document is explicitly and overwhelmingly negative, leaving no room for positive sentiment.\",\n    \"internal_consistency_approach\": \"3-run median aggregation\"\n  },\n  \"document_analyses\": [\n    {\n      \"document_id\": \"negative_test_2.txt\",\n      \"document_name\": \"negative_test_2.txt\",\n      \"dimensional_scores\": {\n        \"positive_sentiment\": {\n          \"raw_score\": 0.0,\n          \"salience\": 0.0,\n          \"confidence\": 1.0\n        },\n        \"negative_sentiment\": {\n          \"raw_score\": 1.0,\n          \"salience\": 1.0,\n          \"confidence\": 1.0\n        }\n      },\n      \"evidence_quotes\": {\n        \"positive_sentiment\": [],\n        \"negative_sentiment\": [\n          \"What an awful predicament. All plans are failing miserably. I'm dreading what's to come.\",\n          \"Defeat engulfs us. The group performed dreadfully. We're encountering catastrophe. Despair saturates everything. Such a calamitous result! I'm crushed by the setbacks. Everything appears bleak and discouraging.\"\n        ]\n      },\n      \"marked_up_document\": \"# Document Analysis - Marked Up Text\\n\\n# Negative Test Document 2\\n\\n**Context**: Sample text with negative sentiment\\n\\n[negative_sentiment: What an awful predicament.] [negative_sentiment: All plans are failing miserably.] [negative_sentiment: I'm dreading what's to come.] [negative_sentiment: Defeat engulfs us.] [negative_sentiment: The group performed dreadfully.] [negative_sentiment: We're encountering catastrophe.] [negative_sentiment: Despair saturates everything.] [negative_sentiment: Such a calamitous result!] [negative_sentiment: I'm crushed by the setbacks.] [negative_sentiment: Everything appears bleak and discouraging.]\"\n    }\n  ]\n}\n```",
        "net_sentiment": -1.0,
        "sentiment_magnitude": 0.5
      }
    ],
    "columns_added": [
      "net_sentiment",
      "sentiment_magnitude"
    ]
  },
  "status": "success_with_data",
  "validation_passed": true
}