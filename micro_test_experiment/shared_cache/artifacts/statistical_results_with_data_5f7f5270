{'generation_metadata': {'status': 'success', 'functions_generated': 4, 'output_file': 'automatedstatisticalanalysisagent_functions.py', 'module_size': 19158, 'function_code_content': '"""\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: micro_test_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-09-12T01:18:02.181465+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n"""\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings(\'ignore\', category=RuntimeWarning)\n\n\ndef run_descriptive_statistics(data, **kwargs):\n    """\n    Calculates and returns descriptive statistics for primary and derived sentiment metrics,\n    grouped by sentiment category.\n\n    Methodology:\n    This function first calculates the derived metrics \'net_sentiment\' and \'sentiment_magnitude\'\n    based on the framework specification. It then assigns each document to a \'sentiment_category\'\n    (\'positive\' or \'negative\') based on its filename prefix. Finally, it computes descriptive\n    statistics (count, mean, standard deviation, min, quartiles, max) for each metric,\n    grouped by the sentiment category.\n\n    Power Assessment (TIER 3: Exploratory Analysis):\n    The analysis is exploratory due to a small sample size (N=4). Results are suggestive\n    of patterns rather than conclusive findings. The descriptive statistics provide a\n    foundational summary of the data distribution.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing analysis data with columns including\n                             \'document_name\', \'positive_sentiment_raw\', and \'negative_sentiment_raw\'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each metric, grouped by\n              sentiment category. Returns None if data is insufficient or an error occurs.\n    """\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = [\'document_name\', \'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        if not all(col in data.columns for col in required_cols):\n            return None\n        if data.shape[0] < 2:\n            return None\n\n        df = data.copy()\n\n        # 1. Calculate derived metrics\n        df[\'net_sentiment\'] = df[\'positive_sentiment_raw\'] - df[\'negative_sentiment_raw\']\n        df[\'sentiment_magnitude\'] = (df[\'positive_sentiment_raw\'] + df[\'negative_sentiment_raw\']) / 2\n\n        # 2. Create grouping variable from document name\n        def get_category(name):\n            if name.lower().startswith(\'positive\'):\n                return \'positive\'\n            if name.lower().startswith(\'negative\'):\n                return \'negative\'\n            return \'unknown\'\n        df[\'sentiment_category\'] = df[\'document_name\'].apply(get_category)\n\n        if \'unknown\' in df[\'sentiment_category\'].unique():\n            # Handle cases where grouping is not clear\n            return None\n        \n        metrics_to_analyze = [\n            \'positive_sentiment_raw\',\n            \'negative_sentiment_raw\',\n            \'net_sentiment\',\n            \'sentiment_magnitude\'\n        ]\n\n        # 3. Calculate descriptive statistics\n        descriptives = df.groupby(\'sentiment_category\')[metrics_to_analyze].describe()\n\n        # Format output as a nested dictionary\n        results = descriptives.to_dict(\'index\')\n        for group, stats in results.items():\n            # Unstack the multi-level columns\n            results[group] = {\n                metric: {\n                    stat: val for stat, val in descriptives.loc[group, metric].to_dict().items()\n                } for metric in metrics_to_analyze\n            }\n        \n        return {\n            "analysis_name": "descriptive_statistics",\n            "grouping_variable": "sentiment_category",\n            "statistics": results,\n            "notes": "Exploratory analysis - results are suggestive rather than conclusive (N={}).".format(len(df))\n        }\n\n    except Exception:\n        return None\n\ndef run_exploratory_anova(data, **kwargs):\n    """\n    Performs a one-way ANOVA to compare sentiment metrics between sentiment categories.\n\n    Methodology:\n    This function compares the means of sentiment scores (\'positive_sentiment_raw\',\n    \'negative_sentiment_raw\', \'net_sentiment\', \'sentiment_magnitude\') across the two\n    sentiment categories (\'positive\', \'negative\'). It uses a one-way Analysis of Variance (ANOVA)\n    to test for differences. For each metric, it reports the F-statistic, the p-value, and\n    the eta-squared (η²) effect size. Eta-squared measures the proportion of variance in the\n    dependent variable that is associated with the grouping variable.\n\n    Power Assessment (TIER 3: Exploratory Analysis):\n    With group sizes of n=2, this analysis is severely underpowered. ANOVA assumptions\n    (normality, homogeneity of variances) cannot be meaningfully tested. The results,\n    especially the p-value, should not be used for inferential conclusions. The F-statistic\n    and eta-squared are reported as descriptive measures of the magnitude of difference\n    relative to within-group variance. Results are suggestive rather than conclusive (N=4).\n\n    Args:\n        data (pd.DataFrame): DataFrame containing analysis data with columns including\n                             \'document_name\', \'positive_sentiment_raw\', and \'negative_sentiment_raw\'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary of ANOVA results for each dependent variable, including F-statistic,\n              p-value, and eta-squared. Returns None if data is insufficient for the test.\n    """\n    import pandas as pd\n    import numpy as np\n    import statsmodels.api as sm\n    from statsmodels.formula.api import ols\n\n    try:\n        required_cols = [\'document_name\', \'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        if not all(col in data.columns for col in required_cols):\n            return None\n        if data.shape[0] < 4: # Need at least 2 per group for 2 groups\n            return None\n\n        df = data.copy()\n\n        # 1. Prepare data\n        df[\'net_sentiment\'] = df[\'positive_sentiment_raw\'] - df[\'negative_sentiment_raw\']\n        df[\'sentiment_magnitude\'] = (df[\'positive_sentiment_raw\'] + df[\'negative_sentiment_raw\']) / 2\n        \n        def get_category(name):\n            if name.lower().startswith(\'positive\'):\n                return \'positive\'\n            if name.lower().startswith(\'negative\'):\n                return \'negative\'\n            return \'unknown\'\n        df[\'sentiment_category\'] = df[\'document_name\'].apply(get_category)\n\n        # Check for valid grouping\n        group_counts = df[\'sentiment_category\'].value_counts()\n        if len(group_counts) < 2 or group_counts.min() < 2:\n            return None\n\n        # 2. Perform ANOVA for each metric\n        results = {}\n        metrics_to_analyze = [\n            \'positive_sentiment_raw\',\n            \'negative_sentiment_raw\',\n            \'net_sentiment\',\n            \'sentiment_magnitude\'\n        ]\n\n        for metric in metrics_to_analyze:\n            # statsmodels requires valid variable names for formulas\n            clean_metric_name = metric.replace(\'_\', \'\')\n            df.rename(columns={metric: clean_metric_name}, inplace=True)\n            \n            model = ols(f\'{clean_metric_name} ~ C(sentiment_category)\', data=df).fit()\n            anova_table = sm.stats.anova_lm(model, typ=2)\n            \n            # Calculate eta-squared\n            ss_between = anova_table[\'sum_sq\'][\'C(sentiment_category)\']\n            ss_total = anova_table[\'sum_sq\'].sum()\n            eta_squared = ss_between / ss_total if ss_total > 0 else 0\n\n            results[metric] = {\n                \'f_statistic\': anova_table[\'F\'][0],\n                \'p_value\': anova_table[\'PR(>F)\'][0],\n                \'eta_squared\': eta_squared,\n                \'df_between\': int(anova_table[\'df\'][0]),\n                \'df_within\': int(anova_table[\'df\'][1]),\n            }\n            # Revert column name for next iteration\n            df.rename(columns={clean_metric_name: metric}, inplace=True)\n\n        return {\n            "analysis_name": "exploratory_anova",\n            "grouping_variable": "sentiment_category",\n            "results": results,\n            "notes": "Exploratory analysis - results are suggestive rather than conclusive (N={}). P-values are unreliable due to small sample size.".format(len(df))\n        }\n\n    except Exception:\n        return None\n\ndef run_reliability_analysis(data, **kwargs):\n    """\n    Calculates Cronbach\'s alpha for the two primary sentiment dimensions.\n\n    Methodology:\n    This function assesses the relationship between the \'positive_sentiment_raw\' and\n    \'negative_sentiment_raw\' dimensions using Cronbach\'s alpha. In this context, the two\n    dimensions are theoretically opposing constructs. Therefore, this analysis serves more as a\n    test of discriminant validity than internal consistency. A low or negative alpha would\n    indicate that the dimensions are measuring distinct, opposing concepts as intended by the\n    framework, rather than a single underlying construct.\n\n    Power Assessment (TIER 3: Exploratory Analysis):\n    The analysis is exploratory due to a small sample size (N=4). The resulting alpha\n    value is highly unstable and should be interpreted with extreme caution. It provides a\n    preliminary, suggestive indication of the relationship between the two dimensions.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing analysis data with columns\n                             \'positive_sentiment_raw\' and \'negative_sentiment_raw\'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the Cronbach\'s alpha value and interpretation notes.\n              Returns None if data is insufficient or an error occurs.\n    """\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # Using a lightweight manual implementation for Cronbach\'s Alpha to avoid heavy dependencies.\n        # Formula: alpha = (k / (k-1)) * (1 - (sum(var_i) / var_total))\n        \n        required_cols = [\'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        if not all(col in data.columns for col in required_cols):\n            return None\n        \n        df_items = data[required_cols].dropna()\n        \n        if df_items.shape[0] < 2:\n            return None\n\n        k = df_items.shape[1]\n        if k < 2:\n            return None\n\n        # Sum of item variances\n        sum_item_variances = df_items.var(ddof=1).sum()\n\n        # Variance of total scores\n        total_scores = df_items.sum(axis=1)\n        variance_of_total_scores = total_scores.var(ddof=1)\n\n        if variance_of_total_scores == 0:\n            # If there\'s no variance in total scores, alpha is undefined or can be considered 1 if all items are identical constants.\n            # In this context, it means no information, so we return None.\n            alpha = np.nan\n        else:\n            alpha = (k / (k - 1)) * (1 - (sum_item_variances / variance_of_total_scores))\n\n        return {\n            "analysis_name": "reliability_analysis",\n            "metric": "Cronbach\'s Alpha",\n            "dimensions": required_cols,\n            "cronbachs_alpha": alpha if not np.isnan(alpha) else None,\n            "notes": [\n                "Exploratory analysis - results are suggestive rather than conclusive (N={}).".format(df_items.shape[0]),\n                "For opposing constructs like positive/negative sentiment, a low or negative alpha is expected and indicates discriminant validity."\n            ]\n        }\n\n    except Exception:\n        return None\n\ndef run_correlation_analysis(data, **kwargs):\n    """\n    Computes the Pearson correlation matrix for sentiment metrics.\n\n    Methodology:\n    This function first calculates the derived metrics \'net_sentiment\' and \'sentiment_magnitude\'.\n    It then computes a Pearson correlation matrix to examine the linear relationships between all\n    primary and derived sentiment metrics: \'positive_sentiment_raw\', \'negative_sentiment_raw\',\n    \'net_sentiment\', and \'sentiment_magnitude\'. The correlation coefficient (r) ranges from\n    -1 (perfect negative correlation) to +1 (perfect positive correlation), with 0 indicating\n    no linear correlation.\n\n    Power Assessment (TIER 3: Exploratory Analysis):\n    With a sample size of N=4, this analysis is exploratory. The correlation coefficients are\n    highly sensitive to individual data points and may not be stable. The results should be\n    viewed as preliminary indicators of association, not as confirmed relationships.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing analysis data with columns including\n                             \'positive_sentiment_raw\' and \'negative_sentiment_raw\'.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A dictionary containing the correlation matrix. Returns None if data is\n              insufficient or an error occurs.\n    """\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_cols = [\'positive_sentiment_raw\', \'negative_sentiment_raw\']\n        if not all(col in data.columns for col in required_cols):\n            return None\n        if data.shape[0] < 2:\n            return None\n\n        df = data.copy()\n\n        # 1. Calculate derived metrics\n        df[\'net_sentiment\'] = df[\'positive_sentiment_raw\'] - df[\'negative_sentiment_raw\']\n        df[\'sentiment_magnitude\'] = (df[\'positive_sentiment_raw\'] + df[\'negative_sentiment_raw\']) / 2\n\n        metrics_to_correlate = [\n            \'positive_sentiment_raw\',\n            \'negative_sentiment_raw\',\n            \'net_sentiment\',\n            \'sentiment_magnitude\'\n        ]\n        \n        # Ensure all columns exist and are numeric\n        df_corr = df[metrics_to_correlate].apply(pd.to_numeric, errors=\'coerce\').dropna()\n\n        if df_corr.shape[0] < 2:\n            return None\n\n        # 2. Compute correlation matrix\n        correlation_matrix = df_corr.corr(method=\'pearson\')\n\n        return {\n            "analysis_name": "correlation_analysis",\n            "method": "pearson",\n            "correlation_matrix": correlation_matrix.to_dict(),\n            "notes": "Exploratory analysis - results are suggestive rather than conclusive (N={}).".format(df_corr.shape[0])\n        }\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    """\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    """\n    results = {\n        \'analysis_metadata\': {\n            \'timestamp\': pd.Timestamp.now().isoformat(),\n            \'sample_size\': len(data),\n            \'alpha_level\': alpha,\n            \'variables_analyzed\': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith((\'calculate_\', \'perform_\', \'test_\')) and \n            name != \'run_complete_statistical_analysis\'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if \'alpha\' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {\'error\': f\'Analysis failed: {str(e)}\'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    """\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    """\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    """\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    """\n    report_lines = []\n    report_lines.append("STATISTICAL ANALYSIS SUMMARY REPORT")\n    report_lines.append("=" * 50)\n    \n    metadata = analysis_results.get(\'analysis_metadata\', {})\n    report_lines.append(f"Analysis Timestamp: {metadata.get(\'timestamp\', \'Unknown\')}")\n    report_lines.append(f"Sample Size: {metadata.get(\'sample_size\', \'Unknown\')}")\n    report_lines.append(f"Alpha Level: {metadata.get(\'alpha_level\', \'Unknown\')}")\n    report_lines.append(f"Variables: {len(metadata.get(\'variables_analyzed\', []))}")\n    report_lines.append("")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != \'analysis_metadata\' and isinstance(result, dict):\n            if \'error\' not in result:\n                report_lines.append(f"{analysis_name.replace(\'_\', \' \').title()}:")\n                \n                # Extract key statistics based on analysis type\n                if \'p_value\' in result:\n                    p_val = result[\'p_value\']\n                    significance = "significant" if p_val < metadata.get(\'alpha_level\', 0.05) else "not significant"\n                    report_lines.append(f"  - p-value: {p_val:.4f} ({significance})")\n                \n                if \'effect_size\' in result:\n                    report_lines.append(f"  - Effect size: {result[\'effect_size\']:.4f}")\n                \n                if \'correlation_matrix\' in result:\n                    report_lines.append(f"  - Correlation matrix generated with {len(result[\'correlation_matrix\'])} variables")\n                \n                if \'cronbach_alpha\' in result:\n                    alpha_val = result[\'cronbach_alpha\']\n                    reliability = "excellent" if alpha_val > 0.9 else "good" if alpha_val > 0.8 else "acceptable" if alpha_val > 0.7 else "questionable"\n                    report_lines.append(f"  - Cronbach\'s α: {alpha_val:.3f} ({reliability})")\n                \n                report_lines.append("")\n            else:\n                report_lines.append(f"{analysis_name}: ERROR - {result[\'error\']}")\n                report_lines.append("")\n    \n    return "\\n".join(report_lines)\n', 'cached_with_code': True}, 'statistical_data': {'generate_statistical_summary_report': 'STATISTICAL ANALYSIS SUMMARY REPORT\n==================================================\nAnalysis Timestamp: Unknown\nSample Size: Unknown\nAlpha Level: Unknown\nVariables: 0\n', 'perform_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-09-11T21:22:10.920547', 'sample_size': 4, 'alpha_level': 0.05, 'variables_analyzed': ['positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence', 'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence']}}, 'run_complete_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-09-11T21:22:10.928981', 'sample_size': 4, 'alpha_level': 0.05, 'variables_analyzed': ['positive_sentiment_raw', 'positive_sentiment_salience', 'positive_sentiment_confidence', 'negative_sentiment_raw', 'negative_sentiment_salience', 'negative_sentiment_confidence']}}, 'run_correlation_analysis': {'analysis_name': 'correlation_analysis', 'method': 'pearson', 'correlation_matrix': {'positive_sentiment_raw': {'positive_sentiment_raw': 1.0, 'negative_sentiment_raw': -0.9972413740548083, 'net_sentiment': 0.9992762131978784, 'sentiment_magnitude': -0.5151515151515155}, 'negative_sentiment_raw': {'positive_sentiment_raw': -0.9972413740548083, 'negative_sentiment_raw': 1.0, 'net_sentiment': -0.9993431854792206, 'sentiment_magnitude': 0.5773502691896262}, 'net_sentiment': {'positive_sentiment_raw': 0.9992762131978784, 'negative_sentiment_raw': -0.9993431854792206, 'net_sentiment': 1.0, 'sentiment_magnitude': -0.5473827978082595}, 'sentiment_magnitude': {'positive_sentiment_raw': -0.5151515151515155, 'negative_sentiment_raw': 0.5773502691896262, 'net_sentiment': -0.5473827978082595, 'sentiment_magnitude': 1.0}}, 'notes': 'Exploratory analysis - results are suggestive rather than conclusive (N=4).'}, 'run_descriptive_statistics': {'analysis_name': 'descriptive_statistics', 'grouping_variable': 'sentiment_category', 'statistics': {'negative': {'positive_sentiment_raw': {'count': 2.0, 'mean': 0.0, 'std': 0.0, 'min': 0.0, '25%': 0.0, '50%': 0.0, '75%': 0.0, 'max': 0.0}, 'negative_sentiment_raw': {'count': 2.0, 'mean': 1.0, 'std': 0.0, 'min': 1.0, '25%': 1.0, '50%': 1.0, '75%': 1.0, 'max': 1.0}, 'net_sentiment': {'count': 2.0, 'mean': -1.0, 'std': 0.0, 'min': -1.0, '25%': -1.0, '50%': -1.0, '75%': -1.0, 'max': -1.0}, 'sentiment_magnitude': {'count': 2.0, 'mean': 0.5, 'std': 0.0, 'min': 0.5, '25%': 0.5, '50%': 0.5, '75%': 0.5, 'max': 0.5}}, 'positive': {'positive_sentiment_raw': {'count': 2.0, 'mean': 0.95, 'std': 0.07071067811865474, 'min': 0.9, '25%': 0.925, '50%': 0.95, '75%': 0.975, 'max': 1.0}, 'negative_sentiment_raw': {'count': 2.0, 'mean': 0.0, 'std': 0.0, 'min': 0.0, '25%': 0.0, '50%': 0.0, '75%': 0.0, 'max': 0.0}, 'net_sentiment': {'count': 2.0, 'mean': 0.95, 'std': 0.07071067811865474, 'min': 0.9, '25%': 0.925, '50%': 0.95, '75%': 0.975, 'max': 1.0}, 'sentiment_magnitude': {'count': 2.0, 'mean': 0.475, 'std': 0.03535533905932737, 'min': 0.45, '25%': 0.4625, '50%': 0.475, '75%': 0.4875, 'max': 0.5}}}, 'notes': 'Exploratory analysis - results are suggestive rather than conclusive (N=4).'}, 'run_exploratory_anova': {'analysis_name': 'exploratory_anova', 'grouping_variable': 'sentiment_category', 'results': {'positive_sentiment_raw': {'f_statistic': 361.0000000000001, 'p_value': 0.002758625945191861, 'eta_squared': 0.9944903581267217, 'df_between': 1, 'df_within': 2}, 'negative_sentiment_raw': {'f_statistic': 2.028240960365167e+31, 'p_value': 4.930380657631325e-32, 'eta_squared': 1.0, 'df_between': 1, 'df_within': 2}, 'net_sentiment': {'f_statistic': 1521.0000000000005, 'p_value': 0.0006568145207794808, 'eta_squared': 0.9986868023637557, 'df_between': 1, 'df_within': 2}, 'sentiment_magnitude': {'f_statistic': 1.000000000000004, 'p_value': 0.42264973081037344, 'eta_squared': 0.3333333333333342, 'df_between': 1, 'df_within': 2}}, 'notes': 'Exploratory analysis - results are suggestive rather than conclusive (N=4). P-values are unreliable due to small sample size.'}, 'run_reliability_analysis': {'analysis_name': 'reliability_analysis', 'metric': "Cronbach's Alpha", 'dimensions': ['positive_sentiment_raw', 'negative_sentiment_raw'], 'cronbachs_alpha': -506.66666666666674, 'notes': ['Exploratory analysis - results are suggestive rather than conclusive (N=4).', 'For opposing constructs like positive/negative sentiment, a low or negative alpha is expected and indicates discriminant validity.']}}, 'status': 'success_with_data', 'validation_passed': True}