{
  "generation_metadata": {
    "status": "success",
    "functions_generated": 4,
    "output_file": "automatedstatisticalanalysisagent_functions.py",
    "module_size": 15796,
    "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: simple_test\nDescription: Statistical analysis experiment\nGenerated: 2025-08-23T17:57:15.275455+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates and returns descriptive statistics for all primary and derived numeric\n    metrics in the Cohesive Flourishing Framework analysis data.\n\n    This function provides a comprehensive statistical overview of the corpus, including\n    measures of central tendency (mean), dispersion (std), and range (min, max) for\n    each raw score, salience score, and calculated index. This is foundational for\n    understanding the overall distribution of rhetorical features in the dataset.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the flattened analysis results,\n                             with columns for each dimension's raw_score, salience, and\n                             all derived metrics.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        dict: A nested dictionary where keys are the metric names (e.g.,\n              'tribal_dominance_raw_score', 'full_cohesion_index') and values are\n              dictionaries of their descriptive statistics (mean, std, min, max, count).\n              Returns None if the input data is invalid or empty.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        results = {}\n        # Select all numeric columns which represent scores, salience, or indices\n        numeric_cols = data.select_dtypes(include=np.number).columns.tolist()\n\n        for col in numeric_cols:\n            # Ensure we only process relevant metrics\n            if col.endswith(('_score', '_salience', '_index', '_tension')):\n                # Drop NaN values for accurate calculations\n                valid_data = data[col].dropna()\n                if not valid_data.empty:\n                    results[col] = {\n                        'mean': float(valid_data.mean()),\n                        'std': float(valid_data.std()),\n                        'min': float(valid_data.min()),\n                        'p25': float(valid_data.quantile(0.25)),\n                        'median': float(valid_data.median()),\n                        'p75': float(valid_data.quantile(0.75)),\n                        'max': float(valid_data.max()),\n                        'count': int(valid_data.count())\n                    }\n\n        return results if results else None\n\n    except Exception:\n        return None\n\ndef calculate_baseline_scores(data, **kwargs):\n    \"\"\"\n    Calculates baseline descriptive statistics for the key cohesion and tension indices\n    defined in the Cohesive Flourishing Framework.\n\n    This function directly addresses the research question by focusing on the summary\n    metrics that evaluate overall discourse health and rhetorical contradiction. It\n    provides a clear, high-level statistical snapshot of the corpus's central tendencies\n    regarding cohesion and tension.\n\n    Methodology:\n    For each specified index, standard descriptive statistics (mean, median, std,\n    min, max, and quartiles) are computed to characterize the distribution of scores\n    across all documents in the corpus.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the analysis results,\n                             specifically the derived metric columns.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each key cohesion\n              and tension index. Returns None if the required columns are missing\n              or the data is empty.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        # These are the key summary metrics from the CFF specification\n        baseline_indices = [\n            'identity_tension', 'emotional_tension', 'success_tension',\n            'relational_tension', 'goal_tension',\n            'strategic_contradiction_index',\n            'descriptive_cohesion_index', 'motivational_cohesion_index',\n            'full_cohesion_index'\n        ]\n\n        results = {}\n        for index_col in baseline_indices:\n            if index_col in data.columns:\n                valid_data = data[index_col].dropna()\n                if not valid_data.empty:\n                    stats = valid_data.describe().to_dict()\n                    # Ensure standard JSON serializable types\n                    for key, value in stats.items():\n                        stats[key] = float(value)\n                    stats['count'] = int(stats['count'])\n                    results[index_col] = stats\n            else:\n                # Log or handle missing column if necessary, here we just skip\n                pass\n\n        return results if results else None\n\n    except Exception:\n        return None\n\ndef analyze_score_correlations(data, **kwargs):\n    \"\"\"\n    Computes a correlation matrix for the raw intensity scores of all CFF dimensions\n    and the main derived indices.\n\n    This analysis helps reveal underlying relationships between different rhetorical\n    strategies. For example, it can quantify whether appeals to 'fear' are commonly\n    associated with 'tribal_dominance' or if 'hope' correlates with 'cohesive_goals'.\n    The correlation coefficient (Pearson's r) measures the linear relationship\n    between two variables, ranging from -1 (perfect negative correlation) to +1\n    (perfect positive correlation), with 0 indicating no linear correlation.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame with columns for each dimension's\n                             raw_score and the key derived indices.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        dict: A dictionary representing the correlation matrix, where keys are\n              metric names and values are dictionaries of their correlations with\n              other metrics. Returns None if data is insufficient for correlation.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        # Define the core dimensions and key indices from the CFF spec\n        dimension_scores = [\n            'tribal_dominance_raw_score', 'individual_dignity_raw_score',\n            'fear_raw_score', 'hope_raw_score',\n            'envy_raw_score', 'compersion_raw_score',\n            'enmity_raw_score', 'amity_raw_score',\n            'fragmentative_goals_raw_score', 'cohesive_goals_raw_score'\n        ]\n        key_indices = [\n            'strategic_contradiction_index', 'full_cohesion_index'\n        ]\n\n        # Filter for columns that actually exist in the dataframe\n        cols_to_correlate = [col for col in dimension_scores + key_indices if col in data.columns]\n\n        if len(cols_to_correlate) < 2:\n            return None # Not enough data to correlate\n\n        # Create a subset of the data and drop rows with any missing values\n        # to ensure a valid correlation matrix\n        correlation_df = data[cols_to_correlate].dropna()\n\n        if correlation_df.shape[0] < 2:\n            return None # Not enough complete cases\n\n        # Calculate the Pearson correlation matrix\n        correlation_matrix = correlation_df.corr(method='pearson')\n\n        # Convert to a JSON-serializable dictionary\n        return correlation_matrix.to_dict()\n\n    except Exception:\n        return None\n\ndef identify_outlier_documents(data, **kwargs):\n    \"\"\"\n    Identifies documents with extreme scores on the primary CFF indices.\n\n    This function helps pinpoint documents that are exceptional examples of either\n    highly cohesive/fragmentative or highly coherent/incoherent rhetoric. It is useful\n    for qualitative deep dives and case study selection.\n\n    Methodology:\n    Outliers are identified using the Interquartile Range (IQR) method. A data point\n    is considered an outlier if it falls below Q1 - 1.5*IQR or above Q3 + 1.5*IQR.\n    This method is generally more robust to non-normally distributed data than\n    standard deviation-based approaches.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing analysis results. Must include\n                             a document identifier ('document_id' or 'filename') and the\n                             key index columns.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        dict: A dictionary where keys are the index names ('full_cohesion_index',\n              'strategic_contradiction_index'). Values are dictionaries categorizing\n              outliers into 'high' and 'low' lists, each containing tuples of\n              (document_identifier, score). Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        # Determine the document identifier column\n        if 'document_id' in data.columns:\n            doc_id_col = 'document_id'\n        elif 'filename' in data.columns:\n            doc_id_col = 'filename'\n        else:\n            return None # Cannot identify documents\n\n        indices_to_check = ['full_cohesion_index', 'strategic_contradiction_index']\n        outlier_results = {}\n\n        for col in indices_to_check:\n            if col in data.columns:\n                df_col = data[[doc_id_col, col]].dropna()\n                if df_col.empty:\n                    continue\n\n                Q1 = df_col[col].quantile(0.25)\n                Q3 = df_col[col].quantile(0.75)\n                IQR = Q3 - Q1\n                lower_bound = Q1 - 1.5 * IQR\n                upper_bound = Q3 + 1.5 * IQR\n\n                high_outliers = df_col[df_col[col] > upper_bound]\n                low_outliers = df_col[df_col[col] < lower_bound]\n\n                outlier_results[col] = {\n                    'high_outliers': [\n                        (row[doc_id_col], float(row[col])) for index, row in high_outliers.iterrows()\n                    ],\n                    'low_outliers': [\n                        (row[doc_id_col], float(row[col])) for index, row in low_outliers.iterrows()\n                    ]\n                }\n\n        return outlier_results if outlier_results else None\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
    "cached_with_code": true
  },
  "statistical_data": {
    "status": "success",
    "statistical_results": {
      "analysis_metadata": {
        "timestamp": "2025-08-23T14:35:32.245609",
        "sample_size": 4,
        "alpha_level": 0.05,
        "variables_analyzed": [
          "tribal_dominance_raw",
          "tribal_dominance_salience",
          "tribal_dominance_confidence",
          "individual_dignity_raw",
          "individual_dignity_salience",
          "individual_dignity_confidence",
          "fear_raw",
          "fear_salience",
          "fear_confidence",
          "hope_raw",
          "hope_salience",
          "hope_confidence",
          "envy_raw",
          "envy_salience",
          "envy_confidence",
          "compersion_raw",
          "compersion_salience",
          "compersion_confidence",
          "enmity_raw",
          "enmity_salience",
          "enmity_confidence",
          "amity_raw",
          "amity_salience",
          "amity_confidence",
          "fragmentative_goals_raw",
          "fragmentative_goals_salience",
          "fragmentative_goals_confidence",
          "cohesive_goals_raw",
          "cohesive_goals_salience",
          "cohesive_goals_confidence"
        ]
      }
    }
  },
  "status": "success_with_data",
  "validation_passed": true
}