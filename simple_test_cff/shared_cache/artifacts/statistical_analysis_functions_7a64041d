{
  "status": "success",
  "functions_generated": 3,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 18831,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: simple_test\nDescription: Statistical analysis experiment\nGenerated: 2025-08-23T19:25:48.867355+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for all raw and salience scores from the CFF analysis.\n\n    This function provides a baseline understanding of the corpus by summarizing the central\n    tendency and dispersion of each primary dimension's intensity (raw score) and\n    rhetorical prominence (salience). It computes the mean, standard deviation, count,\n    minimum, and maximum for each relevant column.\n\n    Args:\n        data (pd.DataFrame): A DataFrame containing the CFF analysis data, with columns\n                             following the specified `_raw` and `_salience` naming convention.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A nested dictionary where each key is a dimension's raw or salience score\n              and the value is a dictionary of its descriptive statistics. Returns None\n              if the input data is invalid or empty.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        results = {}\n        # Select only columns that represent raw scores or salience\n        score_columns = [col for col in data.columns if col.endswith('_raw') or col.endswith('_salience')]\n        \n        if not score_columns:\n            return {} # Return empty dict if no relevant columns found\n\n        numeric_df = data[score_columns].apply(pd.to_numeric, errors='coerce')\n\n        for col in numeric_df.columns:\n            if numeric_df[col].notna().sum() > 0:\n                results[col] = {\n                    'mean': float(numeric_df[col].mean()),\n                    'std': float(numeric_df[col].std()),\n                    'count': int(numeric_df[col].count()),\n                    'min': float(numeric_df[col].min()),\n                    '25%': float(numeric_df[col].quantile(0.25)),\n                    '50%': float(numeric_df[col].quantile(0.50)),\n                    '75%': float(numeric_df[col].quantile(0.75)),\n                    'max': float(numeric_df[col].max())\n                }\n        \n        return results\n\n    except Exception:\n        return None\n\ndef calculate_cff_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived metrics from the Cohesive Flourishing Framework v10.0.\n\n    This function implements the formulas for the framework's advanced metrics, including\n    Tension Indices, the Strategic Contradiction Index, and the three Salience-Weighted\n    Cohesion Indices. It processes the entire DataFrame to generate these metrics for each\n    document, providing a deeper layer of analysis beyond the raw scores.\n\n    Methodology:\n    - Tension Indices: Calculated using the formula `min(Score_A, Score_B) * |Salience_A - Salience_B|`\n      for each opposing pair, quantifying strategic rhetorical contradictions.\n    - Strategic Contradiction Index: The arithmetic mean of the five tension indices,\n      measuring overall message incoherence.\n    - Cohesion Indices: Salience-weighted scores normalized by the sum of relevant salience\n      values, ranging from -1.0 (fragmentative) to +1.0 (cohesive). An epsilon of 0.001\n      is added to the denominator to prevent division by zero.\n    - Note: The framework specifies 'compersion', but the provided data structure contains\n      'compassion'. This function uses the 'compassion' columns as a substitute to match\n      the actual data schema.\n\n    Args:\n        data (pd.DataFrame): A DataFrame with CFF raw and salience scores.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        pd.DataFrame: The original DataFrame with added columns for each derived metric.\n                      Returns None if required columns are missing or data is invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = data.copy()\n\n        required_cols = [\n            'tribal_dominance_raw', 'tribal_dominance_salience', 'individual_dignity_raw', 'individual_dignity_salience',\n            'fear_raw', 'fear_salience', 'hope_raw', 'hope_salience', 'envy_raw', 'envy_salience',\n            'compassion_raw', 'compassion_salience', 'enmity_raw', 'enmity_salience', 'amity_raw', 'amity_salience',\n            'fragmentative_goals_raw', 'fragmentative_goals_salience', 'cohesive_goals_raw', 'cohesive_goals_salience'\n        ]\n\n        if not all(col in df.columns for col in required_cols):\n            # Log or handle missing columns appropriately\n            return None\n        \n        # Convert relevant columns to numeric, coercing errors\n        for col in required_cols:\n            df[col] = pd.to_numeric(df[col], errors='coerce')\n\n        # --- 1. Tension Indices ---\n        df['identity_tension'] = np.minimum(df['tribal_dominance_raw'], df['individual_dignity_raw']) * \\\n                                 abs(df['tribal_dominance_salience'] - df['individual_dignity_salience'])\n        df['emotional_tension'] = np.minimum(df['fear_raw'], df['hope_raw']) * \\\n                                  abs(df['fear_salience'] - df['hope_salience'])\n        # NOTE: Using 'compassion' columns as a substitute for the framework's 'compersion'\n        df['success_tension'] = np.minimum(df['envy_raw'], df['compassion_raw']) * \\\n                                abs(df['envy_salience'] - df['compassion_salience'])\n        df['relational_tension'] = np.minimum(df['enmity_raw'], df['amity_raw']) * \\\n                                   abs(df['enmity_salience'] - df['amity_salience'])\n        df['goal_tension'] = np.minimum(df['fragmentative_goals_raw'], df['cohesive_goals_raw']) * \\\n                             abs(df['fragmentative_goals_salience'] - df['cohesive_goals_salience'])\n\n        # --- 2. Strategic Contradiction Index ---\n        tension_cols = ['identity_tension', 'emotional_tension', 'success_tension', 'relational_tension', 'goal_tension']\n        df['strategic_contradiction_index'] = df[tension_cols].mean(axis=1)\n\n        # --- 3. Salience-Weighted Cohesion Indices ---\n        # Intermediate cohesion components\n        identity_comp = (df['individual_dignity_raw'] * df['individual_dignity_salience'] - df['tribal_dominance_raw'] * df['tribal_dominance_salience'])\n        emotional_comp = (df['hope_raw'] * df['hope_salience'] - df['fear_raw'] * df['fear_salience'])\n        success_comp = (df['compassion_raw'] * df['compassion_salience'] - df['envy_raw'] * df['envy_salience'])\n        relational_comp = (df['amity_raw'] * df['amity_salience'] - df['enmity_raw'] * df['enmity_salience'])\n        goal_comp = (df['cohesive_goals_raw'] * df['cohesive_goals_salience'] - df['fragmentative_goals_raw'] * df['fragmentative_goals_salience'])\n\n        # Salience totals for normalization\n        descriptive_salience_total = (df['hope_salience'] + df['fear_salience'] + df['compassion_salience'] + df['envy_salience'] + df['amity_salience'] + df['enmity_salience'])\n        motivational_salience_total = descriptive_salience_total + df['cohesive_goals_salience'] + df['fragmentative_goals_salience']\n        full_salience_total = motivational_salience_total + df['individual_dignity_salience'] + df['tribal_dominance_salience']\n        \n        epsilon = 0.001\n\n        # Final Cohesion Indices\n        df['descriptive_cohesion_index'] = (emotional_comp + success_comp + relational_comp) / (descriptive_salience_total + epsilon)\n        df['motivational_cohesion_index'] = (emotional_comp + success_comp + relational_comp + goal_comp) / (motivational_salience_total + epsilon)\n        df['full_cohesion_index'] = (identity_comp + emotional_comp + success_comp + relational_comp + goal_comp) / (full_salience_total + epsilon)\n\n        # Clip values to be strictly within [-1.0, 1.0] range to handle potential floating point inaccuracies\n        cohesion_indices = ['descriptive_cohesion_index', 'motivational_cohesion_index', 'full_cohesion_index']\n        for col in cohesion_indices:\n            df[col] = df[col].clip(-1.0, 1.0)\n\n        return df\n\n    except Exception:\n        return None\n\ndef summarize_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates and summarizes the CFF derived metrics for the entire corpus.\n\n    This function first computes the derived metrics (tensions and cohesion indices) for\n    each document and then calculates descriptive statistics (mean, std, quartiles, etc.)\n    for these newly computed metrics. This provides a high-level overview of the corpus's\n    overall rhetorical patterns, answering questions about average cohesion, tension, and\n    strategic contradiction across all documents.\n\n    Args:\n        data (pd.DataFrame): A DataFrame with CFF raw and salience scores.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        dict: A nested dictionary of descriptive statistics for each derived metric.\n              Returns None if the input data is invalid or cannot be processed.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        # This function relies on the calculate_cff_derived_metrics logic.\n        # To maintain modularity, we define the calculation logic here again.\n        # In a real library, this would be a shared helper function.\n        \n        if data is None or data.empty:\n            return None\n\n        df = data.copy()\n\n        required_cols = [\n            'tribal_dominance_raw', 'tribal_dominance_salience', 'individual_dignity_raw', 'individual_dignity_salience',\n            'fear_raw', 'fear_salience', 'hope_raw', 'hope_salience', 'envy_raw', 'envy_salience',\n            'compassion_raw', 'compassion_salience', 'enmity_raw', 'enmity_salience', 'amity_raw', 'amity_salience',\n            'fragmentative_goals_raw', 'fragmentative_goals_salience', 'cohesive_goals_raw', 'cohesive_goals_salience'\n        ]\n\n        if not all(col in df.columns for col in required_cols):\n            return None\n        \n        for col in required_cols:\n            df[col] = pd.to_numeric(df[col], errors='coerce')\n\n        df['identity_tension'] = np.minimum(df['tribal_dominance_raw'], df['individual_dignity_raw']) * abs(df['tribal_dominance_salience'] - df['individual_dignity_salience'])\n        df['emotional_tension'] = np.minimum(df['fear_raw'], df['hope_raw']) * abs(df['fear_salience'] - df['hope_salience'])\n        df['success_tension'] = np.minimum(df['envy_raw'], df['compassion_raw']) * abs(df['envy_salience'] - df['compassion_salience'])\n        df['relational_tension'] = np.minimum(df['enmity_raw'], df['amity_raw']) * abs(df['enmity_salience'] - df['amity_salience'])\n        df['goal_tension'] = np.minimum(df['fragmentative_goals_raw'], df['cohesive_goals_raw']) * abs(df['fragmentative_goals_salience'] - df['cohesive_goals_salience'])\n        \n        tension_cols = ['identity_tension', 'emotional_tension', 'success_tension', 'relational_tension', 'goal_tension']\n        df['strategic_contradiction_index'] = df[tension_cols].mean(axis=1)\n\n        identity_comp = (df['individual_dignity_raw'] * df['individual_dignity_salience'] - df['tribal_dominance_raw'] * df['tribal_dominance_salience'])\n        emotional_comp = (df['hope_raw'] * df['hope_salience'] - df['fear_raw'] * df['fear_salience'])\n        success_comp = (df['compassion_raw'] * df['compassion_salience'] - df['envy_raw'] * df['envy_salience'])\n        relational_comp = (df['amity_raw'] * df['amity_salience'] - df['enmity_raw'] * df['enmity_salience'])\n        goal_comp = (df['cohesive_goals_raw'] * df['cohesive_goals_salience'] - df['fragmentative_goals_raw'] * df['fragmentative_goals_salience'])\n\n        descriptive_salience_total = (df['hope_salience'] + df['fear_salience'] + df['compassion_salience'] + df['envy_salience'] + df['amity_salience'] + df['enmity_salience'])\n        motivational_salience_total = descriptive_salience_total + df['cohesive_goals_salience'] + df['fragmentative_goals_salience']\n        full_salience_total = motivational_salience_total + df['individual_dignity_salience'] + df['tribal_dominance_salience']\n        \n        epsilon = 0.001\n        df['descriptive_cohesion_index'] = ((emotional_comp + success_comp + relational_comp) / (descriptive_salience_total + epsilon)).clip(-1.0, 1.0)\n        df['motivational_cohesion_index'] = ((emotional_comp + success_comp + relational_comp + goal_comp) / (motivational_salience_total + epsilon)).clip(-1.0, 1.0)\n        df['full_cohesion_index'] = ((identity_comp + emotional_comp + success_comp + relational_comp + goal_comp) / (full_salience_total + epsilon)).clip(-1.0, 1.0)\n\n        derived_metric_cols = tension_cols + ['strategic_contradiction_index', 'descriptive_cohesion_index', 'motivational_cohesion_index', 'full_cohesion_index']\n        \n        summary = {}\n        for col in derived_metric_cols:\n            if df[col].notna().sum() > 0:\n                summary[col] = {\n                    'mean': float(df[col].mean()),\n                    'std': float(df[col].std()),\n                    'count': int(df[col].count()),\n                    'min': float(df[col].min()),\n                    '25%': float(df[col].quantile(0.25)),\n                    '50%': float(df[col].quantile(0.50)),\n                    '75%': float(df[col].quantile(0.75)),\n                    'max': float(df[col].max())\n                }\n        return summary\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}