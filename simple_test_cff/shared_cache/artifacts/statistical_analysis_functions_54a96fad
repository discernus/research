{
  "status": "success",
  "functions_generated": 5,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 18807,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: simple_test\nDescription: Statistical analysis experiment\nGenerated: 2025-08-25T03:51:46.573083+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for all raw score, salience, and confidence dimensions.\n\n    This function provides a baseline statistical overview of the corpus by computing the mean,\n    standard deviation, count, minimum, and maximum for each numeric column provided in the\n    input data. This is the first step in understanding the distribution and central tendencies\n    of the analyzed rhetorical dimensions.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the analysis data with columns\n                             matching the CFF specification (e.g., 'tribal_dominance_raw',\n                             'tribal_dominance_salience').\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        dict: A dictionary where keys are column names and values are dictionaries\n              containing the descriptive statistics ('mean', 'std', 'count', 'min', 'max').\n              Returns None if the input data is invalid or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = data.copy()\n        numeric_columns = df.select_dtypes(include=np.number).columns\n        \n        if len(numeric_columns) == 0:\n            return {}\n\n        stats = df[numeric_columns].describe().transpose()\n        stats_dict = stats[['mean', 'std', 'count', 'min', 'max']].to_dict('index')\n\n        # Ensure all values are standard Python types for JSON serialization\n        for col, values in stats_dict.items():\n            for stat_name, value in values.items():\n                if pd.isna(value):\n                    stats_dict[col][stat_name] = None\n                elif stat_name == 'count':\n                    stats_dict[col][stat_name] = int(value)\n                else:\n                    stats_dict[col][stat_name] = float(value)\n        \n        return stats_dict\n\n    except Exception:\n        return None\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived CFF metrics for each document in the dataset.\n\n    This function implements the formulas specified in the Cohesive Flourishing Framework v10.1\n    to compute tension indices, the strategic contradiction index, and the three levels of\n    salience-weighted cohesion indices. It adds these new metrics as columns to the input\n    DataFrame, allowing for per-document analysis of rhetorical strategy and impact.\n\n    Methodology:\n    - Tension Indices: Calculated using the formula `min(Score_A, Score_B) * |Salience_A - Salience_B|`\n      for each opposing pair, quantifying strategic contradiction.\n    - Strategic Contradiction Index: The arithmetic mean of the five tension indices.\n    - Cohesion Indices: Calculated by summing the salience-weighted scores of cohesive dimensions\n      and subtracting the salience-weighted scores of fragmentative dimensions, then normalizing\n      by the sum of all relevant salience scores (plus a small epsilon to prevent division by zero).\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame with columns for raw scores and salience\n                             for all 10 CFF dimensions.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        pd.DataFrame: The original DataFrame with added columns for each derived metric.\n                      Returns None if required columns are missing or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = data.copy()\n\n        required_cols = [\n            'tribal_dominance_raw', 'tribal_dominance_salience', 'individual_dignity_raw', 'individual_dignity_salience',\n            'fear_raw', 'fear_salience', 'hope_raw', 'hope_salience',\n            'envy_raw', 'envy_salience', 'compersion_raw', 'compersion_salience',\n            'enmity_raw', 'enmity_salience', 'amity_raw', 'amity_salience',\n            'fragmentative_goals_raw', 'fragmentative_goals_salience', 'cohesive_goals_raw', 'cohesive_goals_salience'\n        ]\n        if not all(col in df.columns for col in required_cols):\n            return None\n\n        # --- 1. Tension Indices ---\n        df['identity_tension'] = np.minimum(df['tribal_dominance_raw'], df['individual_dignity_raw']) * np.abs(df['tribal_dominance_salience'] - df['individual_dignity_salience'])\n        df['emotional_tension'] = np.minimum(df['fear_raw'], df['hope_raw']) * np.abs(df['fear_salience'] - df['hope_salience'])\n        df['success_tension'] = np.minimum(df['envy_raw'], df['compersion_raw']) * np.abs(df['envy_salience'] - df['compersion_salience'])\n        df['relational_tension'] = np.minimum(df['enmity_raw'], df['amity_raw']) * np.abs(df['enmity_salience'] - df['amity_salience'])\n        df['goal_tension'] = np.minimum(df['fragmentative_goals_raw'], df['cohesive_goals_raw']) * np.abs(df['fragmentative_goals_salience'] - df['cohesive_goals_salience'])\n\n        # --- 2. Strategic Contradiction Index ---\n        tension_cols = ['identity_tension', 'emotional_tension', 'success_tension', 'relational_tension', 'goal_tension']\n        df['strategic_contradiction_index'] = df[tension_cols].mean(axis=1)\n\n        # --- 3. Salience-Weighted Cohesion Indices ---\n        epsilon = 0.001\n\n        # Descriptive Cohesion Index\n        descriptive_numerator = (df['hope_raw'] * df['hope_salience'] - df['fear_raw'] * df['fear_salience']) + \\\n                                (df['compersion_raw'] * df['compersion_salience'] - df['envy_raw'] * df['envy_salience']) + \\\n                                (df['amity_raw'] * df['amity_salience'] - df['enmity_raw'] * df['enmity_salience'])\n        descriptive_denominator = df['hope_salience'] + df['fear_salience'] + \\\n                                  df['compersion_salience'] + df['envy_salience'] + \\\n                                  df['amity_salience'] + df['enmity_salience']\n        df['descriptive_cohesion_index'] = descriptive_numerator / (descriptive_denominator + epsilon)\n\n        # Motivational Cohesion Index\n        motivational_numerator = descriptive_numerator + \\\n                                 (df['cohesive_goals_raw'] * df['cohesive_goals_salience'] - df['fragmentative_goals_raw'] * df['fragmentative_goals_salience'])\n        motivational_denominator = descriptive_denominator + \\\n                                   df['cohesive_goals_salience'] + df['fragmentative_goals_salience']\n        df['motivational_cohesion_index'] = motivational_numerator / (motivational_denominator + epsilon)\n\n        # Full Cohesion Index\n        full_numerator = motivational_numerator + \\\n                         (df['individual_dignity_raw'] * df['individual_dignity_salience'] - df['tribal_dominance_raw'] * df['tribal_dominance_salience'])\n        full_denominator = motivational_denominator + \\\n                           df['individual_dignity_salience'] + df['tribal_dominance_salience']\n        df['full_cohesion_index'] = full_numerator / (full_denominator + epsilon)\n\n        return df\n\n    except Exception:\n        return None\n\ndef summarize_corpus_metrics(data, **kwargs):\n    \"\"\"\n    Calculates and summarizes the derived CFF metrics for the entire corpus.\n\n    This function first computes all derived metrics (tensions and cohesion indices) for each\n    document using the `calculate_derived_metrics` function. It then aggregates these results\n    to provide a corpus-level summary, including mean, standard deviation, min, and max for\n    each derived metric. This directly addresses the research question about the baseline\n    cohesion and tension scores for the corpus.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame with columns for raw scores and salience\n                             for all 10 CFF dimensions.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each derived metric across\n              the corpus. Returns None if the data is insufficient or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        derived_df = calculate_derived_metrics(data)\n        if derived_df is None or derived_df.empty:\n            return None\n\n        metric_cols = [\n            'identity_tension', 'emotional_tension', 'success_tension', 'relational_tension', 'goal_tension',\n            'strategic_contradiction_index',\n            'descriptive_cohesion_index', 'motivational_cohesion_index', 'full_cohesion_index'\n        ]\n        \n        # Ensure all metric columns exist before proceeding\n        if not all(col in derived_df.columns for col in metric_cols):\n            return None\n\n        summary_stats = derived_df[metric_cols].describe().transpose()\n        summary_dict = summary_stats[['mean', 'std', 'count', 'min', 'max']].to_dict('index')\n        \n        # Ensure all values are standard Python types for JSON serialization\n        for col, values in summary_dict.items():\n            for stat_name, value in values.items():\n                if pd.isna(value):\n                    summary_dict[col][stat_name] = None\n                elif stat_name == 'count':\n                    summary_dict[col][stat_name] = int(value)\n                else:\n                    summary_dict[col][stat_name] = float(value)\n\n        return summary_dict\n\n    except Exception:\n        return None\n\ndef calculate_correlation_matrix(data, **kwargs):\n    \"\"\"\n    Computes the correlation matrix for CFF raw scores and salience values.\n\n    This analysis reveals the relationships between different rhetorical dimensions across the\n    corpus. For example, it can show whether the use of 'Fear' rhetoric is positively\n    correlated with 'Enmity' rhetoric. The function calculates Pearson correlation\n    coefficients for all pairs of raw score and salience columns.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the CFF analysis data.\n        **kwargs:\n            columns_to_correlate (list, optional): A list of column name suffixes to include.\n                                                  Defaults to ['_raw', '_salience'].\n\n    Returns:\n        dict: A dictionary representing the correlation matrix, suitable for JSON\n              serialization. Returns None if data is insufficient or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n        \n        suffixes_to_correlate = kwargs.get('columns_to_correlate', ['_raw', '_salience'])\n        \n        cols_to_correlate = [col for col in data.columns if any(col.endswith(suffix) for suffix in suffixes_to_correlate)]\n\n        if not cols_to_correlate:\n            return None\n\n        correlation_matrix = data[cols_to_correlate].corr(method='pearson')\n        \n        # Replace NaN with None for JSON compatibility\n        correlation_matrix = correlation_matrix.where(pd.notnull(correlation_matrix), None)\n\n        return correlation_matrix.to_dict('index')\n\n    except Exception:\n        return None\n\ndef identify_outlier_documents(data, **kwargs):\n    \"\"\"\n    Identifies documents with the highest and lowest scores for a given derived metric.\n\n    This function helps researchers pinpoint specific documents that are exemplary of\n    certain rhetorical strategies (e.g., most cohesive, most divisive, most contradictory).\n    It first calculates all derived CFF metrics and then sorts the documents based on a\n    specified metric to find the top and bottom N documents.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the CFF analysis data.\n        **kwargs:\n            metric (str): The name of the derived metric column to use for ranking.\n                          Defaults to 'full_cohesion_index'.\n            n (int): The number of top and bottom documents to return. Defaults to 5.\n\n    Returns:\n        dict: A dictionary with 'top' and 'bottom' keys, each containing a list of\n              outlier documents with their name and score. Returns None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        metric_column = kwargs.get('metric', 'full_cohesion_index')\n        n = kwargs.get('n', 5)\n\n        derived_df = calculate_derived_metrics(data)\n\n        if derived_df is None or derived_df.empty or metric_column not in derived_df.columns or 'document_name' not in derived_df.columns:\n            return None\n        \n        # Ensure n is not larger than the dataset size\n        n = min(n, len(derived_df))\n\n        # Sort and select top/bottom\n        df_sorted = derived_df.sort_values(by=metric_column, ascending=False)\n        \n        top_n = df_sorted.head(n)[['document_name', metric_column]]\n        bottom_n = df_sorted.tail(n).sort_values(by=metric_column, ascending=True)[['document_name', metric_column]]\n\n        results = {\n            'metric_analyzed': metric_column,\n            'top_performers': top_n.to_dict('records'),\n            'bottom_performers': bottom_n.to_dict('records')\n        }\n        \n        return results\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}