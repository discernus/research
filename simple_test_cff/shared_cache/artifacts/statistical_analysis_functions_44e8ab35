{
  "status": "success",
  "functions_generated": 4,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 17915,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: simple_test\nDescription: Statistical analysis experiment\nGenerated: 2025-08-24T03:46:17.184989+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for all raw score, salience, and confidence columns.\n\n    This function provides a baseline understanding of the data distribution for each primary\n    dimension measured by the framework. It computes the mean, standard deviation, count,\n    minimum, and maximum for each numeric column in the input DataFrame.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the analysis data with columns\n                             matching the specified CFF data structure.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        dict: A nested dictionary where each key is a column name and the value is a\n              dictionary of its descriptive statistics. Returns None if the input\n              data is invalid or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        # Ensure we are working with a copy\n        df = data.copy()\n\n        results = {}\n        numeric_columns = df.select_dtypes(include=[np.number]).columns\n\n        for col in numeric_columns:\n            # Filter for the primary measurement columns\n            if col.endswith(('_raw', '_salience', '_confidence')):\n                # Drop NaN values for accurate calculations\n                col_data = df[col].dropna()\n                if not col_data.empty:\n                    results[col] = {\n                        'mean': float(col_data.mean()),\n                        'std': float(col_data.std()),\n                        'count': int(col_data.count()),\n                        'min': float(col_data.min()),\n                        'max': float(col_data.max()),\n                        'median': float(col_data.median())\n                    }\n        \n        if not results:\n            return None\n\n        return results\n\n    except Exception:\n        return None\n\ndef calculate_cff_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived metrics as defined by the Cohesive Flourishing Framework.\n\n    This function implements the formulas for tension indices, the strategic contradiction\n    index, and the three salience-weighted cohesion indices. It takes the raw analysis\n    data and enriches it with these higher-order metrics, returning a new DataFrame.\n\n    Methodology:\n    - Tension Indices: Calculated using the formula `min(Score_A, Score_B) * |Salience_A - Salience_B|`\n      for each opposing pair. `np.minimum` is used for row-wise calculation.\n    - Strategic Contradiction Index: The arithmetic mean of the five tension indices.\n    - Cohesion Indices: Calculated using salience-weighted scores for relevant dimensions,\n      normalized by the sum of those dimensions' salience scores (plus a small epsilon\n      to prevent division by zero), as specified in the CFF documentation.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame with columns for raw scores and salience\n                             for all ten CFF dimensions.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        pd.DataFrame: A new DataFrame with added columns for each derived metric.\n                      Returns None if input data is invalid or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = data.copy()\n        \n        # Define required columns to ensure data integrity\n        required_cols = [\n            'tribal_dominance_raw', 'tribal_dominance_salience', 'individual_dignity_raw', 'individual_dignity_salience',\n            'fear_raw', 'fear_salience', 'hope_raw', 'hope_salience', 'envy_raw', 'envy_salience',\n            'compersion_raw', 'compersion_salience', 'enmity_raw', 'enmity_salience', 'amity_raw', 'amity_salience',\n            'fragmentative_goals_raw', 'fragmentative_goals_salience', 'cohesive_goals_raw', 'cohesive_goals_salience'\n        ]\n        if not all(col in df.columns for col in required_cols):\n            # Missing one or more required columns for calculation\n            return None\n\n        # --- 1. Calculate Tension Indices ---\n        df['identity_tension'] = np.minimum(df['tribal_dominance_raw'], df['individual_dignity_raw']) * \\\n                                 abs(df['tribal_dominance_salience'] - df['individual_dignity_salience'])\n        \n        df['emotional_tension'] = np.minimum(df['fear_raw'], df['hope_raw']) * \\\n                                  abs(df['fear_salience'] - df['hope_salience'])\n        \n        df['success_tension'] = np.minimum(df['envy_raw'], df['compersion_raw']) * \\\n                                abs(df['envy_salience'] - df['compersion_salience'])\n        \n        df['relational_tension'] = np.minimum(df['enmity_raw'], df['amity_raw']) * \\\n                                   abs(df['enmity_salience'] - df['amity_salience'])\n        \n        df['goal_tension'] = np.minimum(df['fragmentative_goals_raw'], df['cohesive_goals_raw']) * \\\n                             abs(df['fragmentative_goals_salience'] - df['cohesive_goals_salience'])\n\n        # --- 2. Calculate Strategic Contradiction Index ---\n        tension_cols = ['identity_tension', 'emotional_tension', 'success_tension', 'relational_tension', 'goal_tension']\n        df['strategic_contradiction_index'] = df[tension_cols].mean(axis=1)\n\n        # --- 3. Calculate Salience-Weighted Cohesion Indices ---\n        epsilon = 0.001\n\n        # Numerator components\n        identity_comp = (df['individual_dignity_raw'] * df['individual_dignity_salience']) - (df['tribal_dominance_raw'] * df['tribal_dominance_salience'])\n        emotional_comp = (df['hope_raw'] * df['hope_salience']) - (df['fear_raw'] * df['fear_salience'])\n        success_comp = (df['compersion_raw'] * df['compersion_salience']) - (df['envy_raw'] * df['envy_salience'])\n        relational_comp = (df['amity_raw'] * df['amity_salience']) - (df['enmity_raw'] * df['enmity_salience'])\n        goal_comp = (df['cohesive_goals_raw'] * df['cohesive_goals_salience']) - (df['fragmentative_goals_raw'] * df['fragmentative_goals_salience'])\n\n        # Denominator (total salience) components\n        descriptive_salience_total = df['hope_salience'] + df['fear_salience'] + df['compersion_salience'] + df['envy_salience'] + df['amity_salience'] + df['enmity_salience']\n        motivational_salience_total = descriptive_salience_total + df['cohesive_goals_salience'] + df['fragmentative_goals_salience']\n        full_salience_total = motivational_salience_total + df['individual_dignity_salience'] + df['tribal_dominance_salience']\n\n        # Descriptive Cohesion Index\n        numerator_desc = emotional_comp + success_comp + relational_comp\n        df['descriptive_cohesion_index'] = numerator_desc / (descriptive_salience_total + epsilon)\n\n        # Motivational Cohesion Index\n        numerator_motiv = emotional_comp + success_comp + relational_comp + goal_comp\n        df['motivational_cohesion_index'] = numerator_motiv / (motivational_salience_total + epsilon)\n\n        # Full Cohesion Index\n        numerator_full = identity_comp + emotional_comp + success_comp + relational_comp + goal_comp\n        df['full_cohesion_index'] = numerator_full / (full_salience_total + epsilon)\n        \n        # Clip results to be within the theoretical -1.0 to 1.0 range\n        cohesion_indices = ['descriptive_cohesion_index', 'motivational_cohesion_index', 'full_cohesion_index']\n        for col in cohesion_indices:\n            df[col] = df[col].clip(-1.0, 1.0)\n\n        return df\n\n    except Exception:\n        return None\n\ndef summarize_corpus_metrics(data, **kwargs):\n    \"\"\"\n    Calculates and summarizes the baseline cohesion and tension scores for the entire corpus.\n\n    This function first computes all derived CFF metrics (tensions and cohesion indices)\n    for each document. It then calculates descriptive statistics (mean, std, median, etc.)\n    for these derived metrics across the entire corpus, providing a high-level overview\n    of the rhetorical patterns in the dataset. This directly addresses the research question\n    regarding baseline scores.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the raw analysis data.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        dict: A nested dictionary with descriptive statistics for each derived metric,\n              or None if calculation fails.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        # First, calculate all derived metrics using the dedicated function\n        derived_data = calculate_cff_derived_metrics(data)\n\n        if derived_data is None or derived_data.empty:\n            return None\n\n        # List of derived metric columns to summarize\n        metric_columns = [\n            'identity_tension', 'emotional_tension', 'success_tension', 'relational_tension', 'goal_tension',\n            'strategic_contradiction_index', 'descriptive_cohesion_index',\n            'motivational_cohesion_index', 'full_cohesion_index'\n        ]\n        \n        # Check if derived columns were successfully created\n        if not all(col in derived_data.columns for col in metric_columns):\n            return None\n\n        summary = {}\n        for col in metric_columns:\n            col_data = derived_data[col].dropna()\n            if not col_data.empty:\n                summary[col] = {\n                    'mean': float(col_data.mean()),\n                    'std': float(col_data.std()),\n                    'min': float(col_data.min()),\n                    'max': float(col_data.max()),\n                    'median': float(col_data.median()),\n                    '25th_percentile': float(col_data.quantile(0.25)),\n                    '75th_percentile': float(col_data.quantile(0.75)),\n                    'count': int(col_data.count())\n                }\n        \n        if not summary:\n            return None\n            \n        return summary\n\n    except Exception:\n        return None\n\ndef calculate_correlation_matrix(data, **kwargs):\n    \"\"\"\n    Computes the Pearson correlation matrix for all CFF scores and derived metrics.\n\n    This analysis reveals the relationships between different rhetorical dimensions and\n    the higher-order indices. It first calculates all derived metrics and then computes\n    the correlation between all raw scores, salience scores, and the derived metrics.\n    The resulting matrix helps identify which rhetorical strategies tend to co-occur.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the raw analysis data.\n        **kwargs: Additional keyword arguments. Can include 'method' (e.g., 'pearson',\n                  'spearman') to specify the correlation type. Defaults to 'pearson'.\n\n    Returns:\n        dict: A dictionary representation of the correlation matrix, or None if\n              calculation fails.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        # Calculate derived metrics to include them in the correlation analysis\n        derived_data = calculate_cff_derived_metrics(data)\n\n        if derived_data is None or derived_data.empty:\n            return None\n        \n        # Select all numeric columns for correlation\n        numeric_cols = derived_data.select_dtypes(include=np.number).columns\n        \n        # Exclude confidence scores as they measure model certainty, not text properties\n        cols_to_correlate = [col for col in numeric_cols if not col.endswith('_confidence')]\n        \n        correlation_df = derived_data[cols_to_correlate]\n\n        # Remove columns with zero variance to prevent NaN results in correlation matrix\n        correlation_df = correlation_df.loc[:, correlation_df.std() > 0]\n\n        if correlation_df.empty:\n            return None\n\n        # Get correlation method from kwargs, default to 'pearson'\n        method = kwargs.get('method', 'pearson')\n        \n        # Calculate correlation matrix\n        corr_matrix = correlation_df.corr(method=method)\n\n        # Convert to a JSON-serializable format (nested dictionary)\n        return corr_matrix.to_dict()\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}