{
  "status": "success",
  "functions_generated": 5,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 19855,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: simple_test\nDescription: Statistical analysis experiment\nGenerated: 2025-08-27T04:58:45.014781+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived metrics as specified by the Cohesive Flourishing Framework v10.1 \n    and adds them as new columns to the DataFrame.\n\n    This function implements the formulas for:\n    - 5 Tension Indices (Identity, Emotional, Success, Relational, Goal)\n    - Strategic Contradiction Index (average of tensions)\n    - 3 Salience-Weighted Cohesion Indices (Descriptive, Motivational, Full)\n\n    Methodology:\n    The calculations are performed element-wise for each row in the DataFrame.\n    - Tension: `min(Score_A, Score_B) * |Salience_A - Salience_B|`. This quantifies\n      the rhetorical contradiction when opposing concepts are used with differing emphasis.\n    - Cohesion Indices: These are calculated as the sum of salience-weighted positive\n      dimensions minus the sum of salience-weighted negative dimensions, normalized by the\n      sum of all relevant salience scores. An epsilon of 0.001 is added to the denominator\n      to prevent division by zero. The result is a score from -1.0 (fragmentative) to \n      +1.0 (cohesive).\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data with columns\n                             matching the CFF specification (e.g., 'tribal_dominance_raw',\n                             'tribal_dominance_salience').\n        **kwargs: Not used.\n\n    Returns:\n        pd.DataFrame: The original DataFrame with added columns for each derived metric,\n                      or None if an error occurs or data is invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        if data is None or data.empty:\n            return None\n            \n        df = data.copy()\n\n        # --- Tension Indices ---\n        df['identity_tension'] = np.minimum(df['tribal_dominance_raw'], df['individual_dignity_raw']) * \\\n                                 abs(df['tribal_dominance_salience'] - df['individual_dignity_salience'])\n        \n        df['emotional_tension'] = np.minimum(df['fear_raw'], df['hope_raw']) * \\\n                                  abs(df['fear_salience'] - df['hope_salience'])\n        \n        df['success_tension'] = np.minimum(df['envy_raw'], df['compersion_raw']) * \\\n                                abs(df['envy_salience'] - df['compersion_salience'])\n        \n        df['relational_tension'] = np.minimum(df['enmity_raw'], df['amity_raw']) * \\\n                                   abs(df['enmity_salience'] - df['amity_salience'])\n        \n        df['goal_tension'] = np.minimum(df['fragmentative_goals_raw'], df['cohesive_goals_raw']) * \\\n                             abs(df['fragmentative_goals_salience'] - df['cohesive_goals_salience'])\n\n        # --- Strategic Contradiction Index ---\n        tension_cols = ['identity_tension', 'emotional_tension', 'success_tension', 'relational_tension', 'goal_tension']\n        df['strategic_contradiction_index'] = df[tension_cols].mean(axis=1)\n\n        # --- Salience-Weighted Cohesion Indices ---\n        epsilon = 0.001\n\n        # Descriptive Cohesion Index\n        descriptive_numerator = (df['hope_raw'] * df['hope_salience'] - df['fear_raw'] * df['fear_salience']) + \\\n                                (df['compersion_raw'] * df['compersion_salience'] - df['envy_raw'] * df['envy_salience']) + \\\n                                (df['amity_raw'] * df['amity_salience'] - df['enmity_raw'] * df['enmity_salience'])\n        descriptive_denominator = df['hope_salience'] + df['fear_salience'] + \\\n                                  df['compersion_salience'] + df['envy_salience'] + \\\n                                  df['amity_salience'] + df['enmity_salience'] + epsilon\n        df['descriptive_cohesion_index'] = descriptive_numerator / descriptive_denominator\n\n        # Motivational Cohesion Index\n        motivational_numerator = descriptive_numerator + \\\n                                 (df['cohesive_goals_raw'] * df['cohesive_goals_salience'] - df['fragmentative_goals_raw'] * df['fragmentative_goals_salience'])\n        motivational_denominator = descriptive_denominator + \\\n                                   df['cohesive_goals_salience'] + df['fragmentative_goals_salience']\n        df['motivational_cohesion_index'] = motivational_numerator / motivational_denominator\n\n        # Full Cohesion Index\n        full_numerator = motivational_numerator + \\\n                         (df['individual_dignity_raw'] * df['individual_dignity_salience'] - df['tribal_dominance_raw'] * df['tribal_dominance_salience'])\n        full_denominator = motivational_denominator + \\\n                           df['individual_dignity_salience'] + df['tribal_dominance_salience']\n        df['full_cohesion_index'] = full_numerator / full_denominator\n        \n        # Clip indices to ensure they are within the [-1.0, 1.0] range\n        cohesion_indices = ['descriptive_cohesion_index', 'motivational_cohesion_index', 'full_cohesion_index']\n        df[cohesion_indices] = df[cohesion_indices].clip(-1.0, 1.0)\n\n        return df\n\n    except (KeyError, TypeError, Exception):\n        return None\n\ndef get_corpus_summary_statistics(data, **kwargs):\n    \"\"\"\n    Provides descriptive statistics for all CFF dimensions and derived metrics across the corpus.\n\n    This function first calculates the derived metrics (tensions and cohesion indices)\n    and then computes summary statistics (mean, std, min, 25%, 50%, 75%, max) for all\n    numerical columns. This directly addresses the research question about baseline\n    cohesion and tension scores for the corpus.\n\n    Methodology:\n    1. Calls `calculate_derived_metrics` to ensure all indices are present.\n    2. Uses pandas' `.describe()` method to compute statistics for all numeric columns.\n    3. Filters the results to include only the derived metrics for a focused summary.\n    4. Returns the statistics as a dictionary for easy interpretation.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing two keys:\n              'derived_metrics_summary': Descriptive statistics for derived metrics.\n              'full_summary': Descriptive statistics for all numeric columns.\n              Returns None if data is insufficient or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        if data is None or data.empty:\n            return None\n\n        # First, calculate all derived metrics using the dedicated function\n        metrics_df = calculate_derived_metrics(data)\n        if metrics_df is None:\n            return None\n\n        # Define the columns of interest\n        derived_metric_cols = [\n            'identity_tension', 'emotional_tension', 'success_tension', \n            'relational_tension', 'goal_tension', 'strategic_contradiction_index',\n            'descriptive_cohesion_index', 'motivational_cohesion_index', 'full_cohesion_index'\n        ]\n        \n        # Generate summary for all numeric columns\n        full_summary = metrics_df.describe()\n\n        # Generate a focused summary for just the derived metrics\n        derived_summary = metrics_df[derived_metric_cols].describe()\n\n        results = {\n            'derived_metrics_summary': derived_summary.to_dict(),\n            'full_summary': full_summary.to_dict()\n        }\n        return results\n\n    except (KeyError, TypeError, Exception):\n        return None\n\ndef analyze_metric_correlations(data, **kwargs):\n    \"\"\"\n    Computes the Pearson correlation matrix for CFF dimensions and derived metrics.\n\n    This analysis helps reveal relationships between different rhetorical strategies.\n    For example, it can show whether appeals to 'Fear' are strongly correlated with\n    'Enmity' or 'Tribal Dominance' across the corpus.\n\n    Methodology:\n    1. Ensures all derived metrics are calculated.\n    2. Selects all numeric score, salience, and derived metric columns.\n    3. Computes the pairwise Pearson correlation coefficient for the selected columns.\n    4. Returns the correlation matrix as a dictionary. High absolute values (near 1.0\n       or -1.0) indicate a strong linear relationship.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary representing the correlation matrix, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        if data is None or data.empty:\n            return None\n\n        metrics_df = calculate_derived_metrics(data)\n        if metrics_df is None:\n            return None\n\n        # Select only numeric columns for correlation\n        numeric_cols = metrics_df.select_dtypes(include=np.number).columns.tolist()\n        \n        # Exclude confidence scores from correlation analysis if they exist\n        cols_to_correlate = [c for c in numeric_cols if not c.endswith('_confidence')]\n        \n        if not cols_to_correlate:\n            return None\n\n        correlation_matrix = metrics_df[cols_to_correlate].corr(method='pearson')\n        \n        # Replace NaN with None for JSON compatibility\n        correlation_matrix = correlation_matrix.where(pd.notnull(correlation_matrix), None)\n\n        return correlation_matrix.to_dict()\n\n    except (KeyError, TypeError, Exception):\n        return None\n\ndef identify_outlier_documents(data, **kwargs):\n    \"\"\"\n    Identifies documents with the highest and lowest scores for key CFF indices.\n\n    This function helps researchers pinpoint exemplary texts that are highly cohesive,\n    highly fragmentative, or exhibit high levels of rhetorical contradiction.\n\n    Methodology:\n    The function identifies the top N and bottom N documents for three key indices:\n    - `full_cohesion_index`: Overall measure of contribution to social cohesion.\n    - `strategic_contradiction_index`: Overall measure of rhetorical incoherence.\n    - `identity_tension`: Specific measure of contradiction in identity appeals.\n    \n    The number of documents to return (N) can be specified via the `top_n` kwarg.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs:\n            top_n (int): The number of top/bottom documents to identify. Defaults to 5.\n\n    Returns:\n        dict: A dictionary where keys are the metric names and values are dicts\n              containing 'highest_scoring' and 'lowest_scoring' documents,\n              or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        if data is None or data.empty:\n            return None\n\n        metrics_df = calculate_derived_metrics(data)\n        if metrics_df is None:\n            return None\n\n        top_n = kwargs.get('top_n', 5)\n        if not isinstance(top_n, int) or top_n < 1:\n            top_n = 5\n            \n        # Ensure n is not larger than the dataset\n        top_n = min(top_n, len(metrics_df))\n\n        outlier_metrics = [\n            'full_cohesion_index',\n            'strategic_contradiction_index',\n            'identity_tension'\n        ]\n        \n        results = {}\n\n        for metric in outlier_metrics:\n            if metric not in metrics_df.columns:\n                continue\n            \n            # Sort by the metric to find highest and lowest\n            sorted_df = metrics_df.sort_values(by=metric, ascending=False)\n            \n            highest = sorted_df.head(top_n)[['document_name', metric]]\n            lowest = sorted_df.tail(top_n)[['document_name', metric]].sort_values(by=metric, ascending=True)\n            \n            results[metric] = {\n                'highest_scoring': highest.to_dict('records'),\n                'lowest_scoring': lowest.to_dict('records')\n            }\n            \n        return results if results else None\n\n    except (KeyError, TypeError, Exception):\n        return None\n\ndef summarize_by_group(data, **kwargs):\n    \"\"\"\n    Calculates and summarizes CFF derived metrics, grouped by a specified column.\n\n    This function is designed for comparative analysis, such as comparing rhetorical\n    patterns across different speakers, parties, or document types. Since no corpus\n    manifest is available, the user must provide the name of a column to group by.\n    This function respects the \"no filename parsing\" rule by requiring explicit grouping info.\n\n    Methodology:\n    1. Requires a `grouping_col` to be specified in `kwargs`.\n    2. Calculates all derived CFF metrics.\n    3. Groups the DataFrame by the unique values in the `grouping_col`.\n    4. For each group, it calculates the mean for all derived metrics.\n    5. Returns a dictionary of these group-level summary statistics.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs:\n            grouping_col (str): The name of the column to group the data by.\n                                This is a REQUIRED kwarg for this function.\n\n    Returns:\n        dict: A dictionary where keys are the unique group identifiers and values are\n              dictionaries of the mean derived scores for that group. Returns None if\n              `grouping_col` is not provided or does not exist in the data.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        grouping_col = kwargs.get('grouping_col')\n        if not grouping_col:\n            # Gracefully return if no grouping column is specified.\n            return {'error': 'No grouping_col provided in kwargs.'}\n\n        if data is None or data.empty:\n            return None\n            \n        if grouping_col not in data.columns:\n            return {'error': f\"Grouping column '{grouping_col}' not found in data.\"}\n\n        metrics_df = calculate_derived_metrics(data)\n        if metrics_df is None:\n            return None\n\n        derived_metric_cols = [\n            'identity_tension', 'emotional_tension', 'success_tension', \n            'relational_tension', 'goal_tension', 'strategic_contradiction_index',\n            'descriptive_cohesion_index', 'motivational_cohesion_index', 'full_cohesion_index'\n        ]\n        \n        # Ensure all derived metric columns exist before grouping\n        cols_to_agg = [col for col in derived_metric_cols if col in metrics_df.columns]\n        if not cols_to_agg:\n            return None\n\n        # Group by the specified column and calculate the mean of derived metrics\n        grouped_summary = metrics_df.groupby(grouping_col)[cols_to_agg].mean()\n        \n        return grouped_summary.to_dict('index')\n\n    except (KeyError, TypeError, Exception):\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}