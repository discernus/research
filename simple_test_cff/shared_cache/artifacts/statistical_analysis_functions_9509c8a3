{
  "status": "success",
  "functions_generated": 2,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 12744,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: simple_test\nDescription: Statistical analysis experiment\nGenerated: 2025-08-24T19:47:50.799014+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for all raw score and salience dimensions.\n\n    This function provides a baseline understanding of the corpus by summarizing the central tendency,\n    dispersion, and range for each of the 20 primary dimensional scores (10 raw scores and 10\n    salience scores) from the Cohesive Flourishing Framework.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns matching the\n                             framework specification (e.g., 'tribal_dominance_raw', 'tribal_dominance_salience').\n        **kwargs: Additional parameters (unused).\n\n    Returns:\n        dict: A nested dictionary where each key is a dimension name (e.g., 'fear_raw') and the value\n              is another dictionary containing 'mean', 'std', 'min', 'max', and 'count'.\n              Returns None if the input data is invalid or empty.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        results = {}\n        # Identify all columns that are raw scores or salience scores.\n        score_columns = [col for col in data.columns if col.endswith('_raw') or col.endswith('_salience')]\n\n        if not score_columns:\n            return None\n\n        for col in score_columns:\n            if pd.api.types.is_numeric_dtype(data[col]):\n                # Drop NA values for accurate calculations\n                series = data[col].dropna()\n                if not series.empty:\n                    results[col] = {\n                        'mean': float(series.mean()),\n                        'std': float(series.std()),\n                        'min': float(series.min()),\n                        'max': float(series.max()),\n                        'count': int(series.count())\n                    }\n        \n        return results if results else None\n\n    except Exception:\n        return None\n\ndef summarize_cohesion_and_tension_scores(data, **kwargs):\n    \"\"\"\n    Calculates and summarizes all derived CFF metrics, including tension and cohesion indices.\n\n    This function implements the core logic of the Cohesive Flourishing Framework v10.1.\n    It first calculates the five tension indices, the overall Strategic Contradiction Index,\n    and the three salience-weighted Cohesion Indices (Descriptive, Motivational, Full).\n    It then computes descriptive statistics (mean, std, min, max, median) for these\n    newly calculated metrics, directly addressing the research question about baseline\n    cohesion and tension scores in the corpus.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns for all 20\n                             raw and salience dimensions.\n        **kwargs: Additional parameters (unused).\n\n    Returns:\n        dict: A nested dictionary of descriptive statistics for each derived metric.\n              Returns None if the input data is missing required columns or is otherwise invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        required_cols = [\n            'tribal_dominance_raw', 'tribal_dominance_salience', 'individual_dignity_raw', 'individual_dignity_salience',\n            'fear_raw', 'fear_salience', 'hope_raw', 'hope_salience',\n            'envy_raw', 'envy_salience', 'compersion_raw', 'compersion_salience',\n            'enmity_raw', 'enmity_salience', 'amity_raw', 'amity_salience',\n            'fragmentative_goals_raw', 'fragmentative_goals_salience', 'cohesive_goals_raw', 'cohesive_goals_salience'\n        ]\n\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        df = data.copy()\n\n        # --- 1. Calculate Tension Indices ---\n        df['identity_tension'] = np.minimum(df['tribal_dominance_raw'], df['individual_dignity_raw']) * abs(df['tribal_dominance_salience'] - df['individual_dignity_salience'])\n        df['emotional_tension'] = np.minimum(df['fear_raw'], df['hope_raw']) * abs(df['fear_salience'] - df['hope_salience'])\n        df['success_tension'] = np.minimum(df['envy_raw'], df['compersion_raw']) * abs(df['envy_salience'] - df['compersion_salience'])\n        df['relational_tension'] = np.minimum(df['enmity_raw'], df['amity_raw']) * abs(df['enmity_salience'] - df['amity_salience'])\n        df['goal_tension'] = np.minimum(df['fragmentative_goals_raw'], df['cohesive_goals_raw']) * abs(df['fragmentative_goals_salience'] - df['cohesive_goals_salience'])\n\n        # --- 2. Calculate Strategic Contradiction Index ---\n        tension_cols = ['identity_tension', 'emotional_tension', 'success_tension', 'relational_tension', 'goal_tension']\n        df['strategic_contradiction_index'] = df[tension_cols].mean(axis=1)\n\n        # --- 3. Calculate Salience-Weighted Cohesion Indices ---\n        epsilon = 0.001\n\n        # Calculate weighted components\n        identity_comp = (df['individual_dignity_raw'] * df['individual_dignity_salience']) - (df['tribal_dominance_raw'] * df['tribal_dominance_salience'])\n        emotional_comp = (df['hope_raw'] * df['hope_salience']) - (df['fear_raw'] * df['fear_salience'])\n        success_comp = (df['compersion_raw'] * df['compersion_salience']) - (df['envy_raw'] * df['envy_salience'])\n        relational_comp = (df['amity_raw'] * df['amity_salience']) - (df['enmity_raw'] * df['enmity_salience'])\n        goal_comp = (df['cohesive_goals_raw'] * df['cohesive_goals_salience']) - (df['fragmentative_goals_raw'] * df['fragmentative_goals_salience'])\n\n        # Calculate total salience denominators\n        descriptive_salience_total = df['hope_salience'] + df['fear_salience'] + df['compersion_salience'] + df['envy_salience'] + df['amity_salience'] + df['enmity_salience']\n        motivational_salience_total = descriptive_salience_total + df['cohesive_goals_salience'] + df['fragmentative_goals_salience']\n        full_salience_total = motivational_salience_total + df['individual_dignity_salience'] + df['tribal_dominance_salience']\n\n        # Calculate final indices\n        df['descriptive_cohesion_index'] = (emotional_comp + success_comp + relational_comp) / (descriptive_salience_total + epsilon)\n        df['motivational_cohesion_index'] = (emotional_comp + success_comp + relational_comp + goal_comp) / (motivational_salience_total + epsilon)\n        df['full_cohesion_index'] = (identity_comp + emotional_comp + success_comp + relational_comp + goal_comp) / (full_salience_total + epsilon)\n\n        # --- 4. Summarize Derived Metrics ---\n        derived_metrics_cols = tension_cols + [\n            'strategic_contradiction_index', 'descriptive_cohesion_index',\n            'motivational_cohesion_index', 'full_cohesion_index'\n        ]\n        \n        summary = {}\n        for col in derived_metrics_cols:\n            series = df[col].dropna()\n            if not series.empty:\n                summary[col] = {\n                    'mean': float(series.mean()),\n                    'std': float(series.std()),\n                    'min': float(series.min()),\n                    'max': float(series.max()),\n                    'median': float(series.median()),\n                    'count': int(series.count())\n                }\n        \n        return summary if summary else None\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}