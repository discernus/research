{'generation_metadata': {'status': 'success', 'functions_generated': 4, 'output_file': 'automatedstatisticalanalysisagent_functions.py', 'module_size': 19872, 'function_code_content': '"""\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: simple_test\nDescription: Statistical analysis experiment\nGenerated: 2025-08-29T15:34:44.929821+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n"""\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings(\'ignore\', category=RuntimeWarning)\n\n\ndef calculate_derived_metrics(data, **kwargs):\n    """\n    Calculates all derived metrics as specified in the Cohesive Flourishing Framework v10.\n\n    This function implements the formulas for tension indices, the strategic contradiction index,\n    and the three salience-weighted cohesion indices. It handles known discrepancies between\n    the framework specification (e.g., \'compersion\', \'enmity\') and the actual data column\n    names (e.g., \'competition_raw\', \'enity_raw\'). The calculated metrics are appended as new\n    columns to the input DataFrame.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data with columns for\n                             raw scores and salience for each of the 10 dimensions.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pd.DataFrame: The original DataFrame with added columns for each derived metric,\n                      or None if essential columns are missing.\n    """\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        df = data.copy()\n\n        # Define a mapping from framework concepts to actual column names\n        # This handles typos and discrepancies (e.g., enity vs enmity, competition vs compersion)\n        col_map = {\n            \'tribal_dominance_score\': \'tribal_dominance_raw\',\n            \'tribal_dominance_salience\': \'tribal_dominance_salience\',\n            \'individual_dignity_score\': \'individual_dignity_raw\',\n            \'individual_dignity_salience\': \'individual_dignity_salience\',\n            \'fear_score\': \'fear_raw\',\n            \'fear_salience\': \'fear_salience\',\n            \'hope_score\': \'hope_raw\',\n            \'hope_salience\': \'hope_salience\',\n            \'envy_score\': \'envy_raw\',\n            \'envy_salience\': \'envy_salience\',\n            \'compersion_score\': \'competition_raw\',  # Mismatch\n            \'compersion_salience\': \'competition_salience\', # Mismatch\n            \'enmity_score\': \'enity_raw\', # Typo\n            \'enmity_salience\': \'enity_salience\', # Typo\n            \'amity_score\': \'amity_raw\',\n            \'amity_salience\': \'amity_salience\',\n            \'fragmentative_goals_score\': \'fragmentative_goals_raw\',\n            \'fragmentative_goals_salience\': \'fragmentative_goals_salience\',\n            \'cohesive_goals_score\': \'cohesive_goals_raw\',\n            \'cohesive_goals_salience\': \'cohesive_goals_salience\'\n        }\n\n        # Verify all necessary columns exist\n        required_cols = list(col_map.values())\n        if not all(col in df.columns for col in required_cols):\n            # Log or print which columns are missing for debugging\n            missing = [col for col in required_cols if col not in df.columns]\n            # print(f"Error: Missing required columns: {missing}")\n            return None\n\n        # --- Tension Indices ---\n        df[\'identity_tension\'] = np.minimum(df[col_map[\'tribal_dominance_score\']], df[col_map[\'individual_dignity_score\']]) * \\\n                                 abs(df[col_map[\'tribal_dominance_salience\']] - df[col_map[\'individual_dignity_salience\']])\n\n        df[\'emotional_tension\'] = np.minimum(df[col_map[\'fear_score\']], df[col_map[\'hope_score\']]) * \\\n                                  abs(df[col_map[\'fear_salience\']] - df[col_map[\'hope_salience\']])\n\n        df[\'success_tension\'] = np.minimum(df[col_map[\'envy_score\']], df[col_map[\'compersion_score\']]) * \\\n                                abs(df[col_map[\'envy_salience\']] - df[col_map[\'compersion_salience\']])\n\n        df[\'relational_tension\'] = np.minimum(df[col_map[\'enmity_score\']], df[col_map[\'amity_score\']]) * \\\n                                   abs(df[col_map[\'enmity_salience\']] - df[col_map[\'amity_salience\']])\n\n        df[\'goal_tension\'] = np.minimum(df[col_map[\'fragmentative_goals_score\']], df[col_map[\'cohesive_goals_score\']]) * \\\n                             abs(df[col_map[\'fragmentative_goals_salience\']] - df[col_map[\'cohesive_goals_salience\']])\n\n        # --- Strategic Contradiction Index ---\n        tension_cols = [\'identity_tension\', \'emotional_tension\', \'success_tension\', \'relational_tension\', \'goal_tension\']\n        df[\'strategic_contradiction_index\'] = df[tension_cols].mean(axis=1)\n\n        # --- Salience-Weighted Cohesion Indices ---\n        epsilon = 0.001\n\n        # Intermediate Components\n        identity_comp = (df[col_map[\'individual_dignity_score\']] * df[col_map[\'individual_dignity_salience\']]) - \\\n                        (df[col_map[\'tribal_dominance_score\']] * df[col_map[\'tribal_dominance_salience\']])\n        emotional_comp = (df[col_map[\'hope_score\']] * df[col_map[\'hope_salience\']]) - \\\n                         (df[col_map[\'fear_score\']] * df[col_map[\'fear_salience\']])\n        success_comp = (df[col_map[\'compersion_score\']] * df[col_map[\'compersion_salience\']]) - \\\n                       (df[col_map[\'envy_score\']] * df[col_map[\'envy_salience\']])\n        relational_comp = (df[col_map[\'amity_score\']] * df[col_map[\'amity_salience\']]) - \\\n                          (df[col_map[\'enmity_score\']] * df[col_map[\'enmity_salience\']])\n        goal_comp = (df[col_map[\'cohesive_goals_score\']] * df[col_map[\'cohesive_goals_salience\']]) - \\\n                    (df[col_map[\'fragmentative_goals_score\']] * df[col_map[\'fragmentative_goals_salience\']])\n\n        # Salience Totals for Normalization\n        descriptive_salience_total = df[col_map[\'hope_salience\']] + df[col_map[\'fear_salience\']] + \\\n                                     df[col_map[\'compersion_salience\']] + df[col_map[\'envy_salience\']] + \\\n                                     df[col_map[\'amity_salience\']] + df[col_map[\'enmity_salience\']]\n\n        motivational_salience_total = descriptive_salience_total + \\\n                                      df[col_map[\'cohesive_goals_salience\']] + df[col_map[\'fragmentative_goals_salience\']]\n\n        full_salience_total = motivational_salience_total + \\\n                              df[col_map[\'individual_dignity_salience\']] + df[col_map[\'tribal_dominance_salience\']]\n\n        # Final Indices\n        df[\'descriptive_cohesion_index\'] = (emotional_comp + success_comp + relational_comp) / (descriptive_salience_total + epsilon)\n        df[\'motivational_cohesion_index\'] = (emotional_comp + success_comp + relational_comp + goal_comp) / (motivational_salience_total + epsilon)\n        df[\'full_cohesion_index\'] = (identity_comp + emotional_comp + success_comp + relational_comp + goal_comp) / (full_salience_total + epsilon)\n        \n        # Clip values to be within [-1.0, 1.0] range to handle any edge cases with epsilon\n        cohesion_indices = [\'descriptive_cohesion_index\', \'motivational_cohesion_index\', \'full_cohesion_index\']\n        for col in cohesion_indices:\n            df[col] = df[col].clip(-1.0, 1.0)\n\n        return df\n\n    except Exception as e:\n        # print(f"An error occurred in calculate_derived_metrics: {e}")\n        return None\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    """\n    Calculates and returns descriptive statistics for key CFF v10 metrics.\n\n    This function first computes the derived metrics (tensions and cohesion indices)\n    and then generates summary statistics (mean, std, min, 25%, 50%, 75%, max) for\n    both the original raw scores and the newly calculated derived metrics. This provides\n    a baseline overview of the corpus characteristics as per the research questions.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing the descriptive statistics, or None if an error occurs.\n    """\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        # First, ensure derived metrics are calculated\n        data_with_metrics = calculate_derived_metrics(data)\n        if data_with_metrics is None:\n            return None\n\n        # Select columns for descriptive statistics\n        raw_scores = [col for col in data.columns if \'_raw\' in col]\n        derived_metrics = [\n            \'identity_tension\', \'emotional_tension\', \'success_tension\', \'relational_tension\', \'goal_tension\',\n            \'strategic_contradiction_index\', \'descriptive_cohesion_index\', \'motivational_cohesion_index\',\n            \'full_cohesion_index\'\n        ]\n        \n        cols_to_describe = raw_scores + derived_metrics\n        \n        # Filter to only include columns that actually exist in the dataframe\n        cols_to_describe = [col for col in cols_to_describe if col in data_with_metrics.columns]\n\n        if not cols_to_describe:\n            return None\n\n        # Calculate descriptive statistics\n        descriptives = data_with_metrics[cols_to_describe].describe()\n\n        # Convert to a dictionary for a clean JSON-compatible output\n        return descriptives.to_dict()\n\n    except Exception as e:\n        # print(f"An error occurred in calculate_descriptive_statistics: {e}")\n        return None\n\ndef calculate_correlation_matrix(data, **kwargs):\n    """\n    Calculates the Pearson correlation matrix for key CFF v10 metrics.\n\n    This analysis explores the relationships between the primary dimensions (raw scores)\n    and the main composite indices (cohesion and tension). It helps to understand how\n    different rhetorical elements co-occur within the corpus.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary representing the correlation matrix, or None if an error occurs.\n    """\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        # First, ensure derived metrics are calculated\n        data_with_metrics = calculate_derived_metrics(data)\n        if data_with_metrics is None:\n            return None\n\n        # Select columns for correlation analysis\n        raw_scores = [col for col in data.columns if \'_raw\' in col]\n        key_indices = [\n            \'strategic_contradiction_index\',\n            \'full_cohesion_index\'\n        ]\n        \n        cols_to_correlate = raw_scores + key_indices\n        \n        # Filter to only include columns that actually exist in the dataframe\n        cols_to_correlate = [col for col in cols_to_correlate if col in data_with_metrics.columns]\n\n        if len(cols_to_correlate) < 2:\n            return None\n\n        # Calculate the correlation matrix\n        correlation_matrix = data_with_metrics[cols_to_correlate].corr(method=\'pearson\')\n\n        # Replace NaN values that can occur if a column has zero variance\n        correlation_matrix = correlation_matrix.fillna(0)\n\n        return correlation_matrix.to_dict()\n\n    except Exception as e:\n        # print(f"An error occurred in calculate_correlation_matrix: {e}")\n        return None\n\ndef compare_styles_on_cohesion(data, **kwargs):\n    """\n    Compares cohesion scores between \'Institutional\' and \'Populist\' rhetorical styles.\n\n    This function addresses the research question about whether institutional discourse\n    (represented by John McCain\'s concession speech) exhibits higher cohesion than\n    populist discourse. It uses an independent samples t-test to compare the means of\n    the three main cohesion indices between the two groups. Levene\'s test is used to\n    check for homogeneity of variances.\n\n    Methodology:\n    1. A \'rhetorical_style\' group is created based on document names.\n    2. Derived metrics are calculated.\n    3. For each cohesion index, an independent samples t-test is performed.\n    4. Results, including t-statistic and p-value, are returned.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary of t-test results for each cohesion index, or None if\n              groups are not viable for comparison.\n    """\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n    from scipy.stats import ttest_ind, levene\n\n    try:\n        # First, ensure derived metrics are calculated\n        df = calculate_derived_metrics(data)\n        if df is None:\n            return None\n\n        # Define grouping based on document name, as per prompt instructions\n        # This mapping is derived from the implicit corpus understanding.\n        def get_style(doc_name):\n            if \'john_mccain\' in doc_name:\n                return \'Institutional\'\n            # All others are categorized as populist for this comparison\n            elif \'steve_king\' in doc_name or \'bernie_sanders\' in doc_name or \'alexandria_ocasio_cortez\' in doc_name:\n                return \'Populist\'\n            return \'Other\'\n\n        df[\'rhetorical_style\'] = df[\'document_name\'].apply(get_style)\n\n        # Isolate the two groups for comparison\n        institutional_group = df[df[\'rhetorical_style\'] == \'Institutional\']\n        populist_group = df[df[\'rhetorical_style\'] == \'Populist\']\n\n        # Check if groups are large enough for a t-test\n        if len(institutional_group) < 2 or len(populist_group) < 2:\n            return {\n                "status": "Error: Insufficient data for one or both groups.",\n                "institutional_group_size": len(institutional_group),\n                "populist_group_size": len(populist_group)\n            }\n\n        results = {}\n        cohesion_indices = [\'descriptive_cohesion_index\', \'motivational_cohesion_index\', \'full_cohesion_index\']\n\n        for index in cohesion_indices:\n            group1 = institutional_group[index].dropna()\n            group2 = populist_group[index].dropna()\n\n            if len(group1) < 2 or len(group2) < 2:\n                results[index] = "Skipped: insufficient data for this index."\n                continue\n\n            # Levene\'s test for homogeneity of variances\n            levene_stat, levene_p = levene(group1, group2)\n            equal_var = levene_p > 0.05\n\n            # Independent samples t-test\n            t_stat, p_value = ttest_ind(group1, group2, equal_var=equal_var, nan_policy=\'omit\')\n\n            results[index] = {\n                \'institutional_mean\': group1.mean(),\n                \'populist_mean\': group2.mean(),\n                \'t_statistic\': t_stat,\n                \'p_value\': p_value,\n                \'levene_p_value\': levene_p,\n                \'equal_variances_assumed\': equal_var\n            }\n\n        return results\n\n    except Exception as e:\n        # print(f"An error occurred in compare_styles_on_cohesion: {e}")\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    """\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    """\n    results = {\n        \'analysis_metadata\': {\n            \'timestamp\': pd.Timestamp.now().isoformat(),\n            \'sample_size\': len(data),\n            \'alpha_level\': alpha,\n            \'variables_analyzed\': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith((\'calculate_\', \'perform_\', \'test_\')) and \n            name != \'run_complete_statistical_analysis\'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if \'alpha\' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {\'error\': f\'Analysis failed: {str(e)}\'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    """\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    """\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    """\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    """\n    report_lines = []\n    report_lines.append("STATISTICAL ANALYSIS SUMMARY REPORT")\n    report_lines.append("=" * 50)\n    \n    metadata = analysis_results.get(\'analysis_metadata\', {})\n    report_lines.append(f"Analysis Timestamp: {metadata.get(\'timestamp\', \'Unknown\')}")\n    report_lines.append(f"Sample Size: {metadata.get(\'sample_size\', \'Unknown\')}")\n    report_lines.append(f"Alpha Level: {metadata.get(\'alpha_level\', \'Unknown\')}")\n    report_lines.append(f"Variables: {len(metadata.get(\'variables_analyzed\', []))}")\n    report_lines.append("")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != \'analysis_metadata\' and isinstance(result, dict):\n            if \'error\' not in result:\n                report_lines.append(f"{analysis_name.replace(\'_\', \' \').title()}:")\n                \n                # Extract key statistics based on analysis type\n                if \'p_value\' in result:\n                    p_val = result[\'p_value\']\n                    significance = "significant" if p_val < metadata.get(\'alpha_level\', 0.05) else "not significant"\n                    report_lines.append(f"  - p-value: {p_val:.4f} ({significance})")\n                \n                if \'effect_size\' in result:\n                    report_lines.append(f"  - Effect size: {result[\'effect_size\']:.4f}")\n                \n                if \'correlation_matrix\' in result:\n                    report_lines.append(f"  - Correlation matrix generated with {len(result[\'correlation_matrix\'])} variables")\n                \n                if \'cronbach_alpha\' in result:\n                    alpha_val = result[\'cronbach_alpha\']\n                    reliability = "excellent" if alpha_val > 0.9 else "good" if alpha_val > 0.8 else "acceptable" if alpha_val > 0.7 else "questionable"\n                    report_lines.append(f"  - Cronbach\'s Î±: {alpha_val:.3f} ({reliability})")\n                \n                report_lines.append("")\n            else:\n                report_lines.append(f"{analysis_name}: ERROR - {result[\'error\']}")\n                report_lines.append("")\n    \n    return "\\n".join(report_lines)\n', 'cached_with_code': True}, 'statistical_data': {'calculate_correlation_matrix': None, 'calculate_derived_metrics': None, 'calculate_descriptive_statistics': None, 'compare_styles_on_cohesion': None, 'generate_statistical_summary_report': 'STATISTICAL ANALYSIS SUMMARY REPORT\n==================================================\nAnalysis Timestamp: Unknown\nSample Size: Unknown\nAlpha Level: Unknown\nVariables: 0\n', 'perform_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-08-29T11:50:05.355229', 'sample_size': 4, 'alpha_level': 0.05, 'variables_analyzed': ['tribal_dominance_raw', 'tribal_dominance_salience', 'tribal_dominance_confidence', 'individual_dignity_raw', 'individual_dignity_salience', 'individual_dignity_confidence', 'fear_raw', 'fear_salience', 'fear_confidence', 'hope_raw', 'hope_salience', 'hope_confidence', 'envy_raw', 'envy_salience', 'envy_confidence', 'compersion_raw', 'compersion_salience', 'compersion_confidence', 'enmity_raw', 'enmity_salience', 'enmity_confidence', 'amity_raw', 'amity_salience', 'amity_confidence', 'fragmentative_goals_raw', 'fragmentative_goals_salience', 'fragmentative_goals_confidence', 'cohesive_goals_raw', 'cohesive_goals_salience', 'cohesive_goals_confidence']}}, 'run_complete_statistical_analysis': {'analysis_metadata': {'timestamp': '2025-08-29T11:50:05.359587', 'sample_size': 4, 'alpha_level': 0.05, 'variables_analyzed': ['tribal_dominance_raw', 'tribal_dominance_salience', 'tribal_dominance_confidence', 'individual_dignity_raw', 'individual_dignity_salience', 'individual_dignity_confidence', 'fear_raw', 'fear_salience', 'fear_confidence', 'hope_raw', 'hope_salience', 'hope_confidence', 'envy_raw', 'envy_salience', 'envy_confidence', 'compersion_raw', 'compersion_salience', 'compersion_confidence', 'enmity_raw', 'enmity_salience', 'enmity_confidence', 'amity_raw', 'amity_salience', 'amity_confidence', 'fragmentative_goals_raw', 'fragmentative_goals_salience', 'fragmentative_goals_confidence', 'cohesive_goals_raw', 'cohesive_goals_salience', 'cohesive_goals_confidence']}}}, 'status': 'success_with_data', 'validation_passed': True}