{
  "status": "success",
  "functions_generated": 3,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 16993,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: simple_test\nDescription: Statistical analysis experiment\nGenerated: 2025-08-28T16:20:21.228687+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef _calculate_derived_metrics(data):\n    \"\"\"\n    Helper function to calculate all derived tension and cohesion indices\n    based on the raw and salience scores of the base dimensions.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the base dimension scores\n                             (e.g., 'tribal_dominance_raw', 'hope_salience').\n\n    Returns:\n        pd.DataFrame: The input DataFrame with added columns for all derived metrics.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    df = data.copy()\n\n    # Ensure all required base columns exist\n    required_cols = [\n        'tribal_dominance_raw', 'tribal_dominance_salience',\n        'individual_dignity_raw', 'individual_dignity_salience',\n        'fear_raw', 'fear_salience',\n        'hope_raw', 'hope_salience',\n        'envy_raw', 'envy_salience',\n        'compassion_raw', 'compassion_salience', # Note: 'compassion' column in data maps to 'compersion' concept in framework\n        'enmity_raw', 'enmity_salience',\n        'amity_raw', 'amity_salience',\n        'fragmentative_goals_raw', 'fragmentative_goals_salience',\n        'cohesive_goals_raw', 'cohesive_goals_salience'\n    ]\n    if not all(col in df.columns for col in required_cols):\n        missing = [col for col in required_cols if col not in df.columns]\n        raise ValueError(f\"Missing required columns for derived metrics calculation: {missing}\")\n\n    # Calculate Tension Indices\n    df['identity_tension'] = np.minimum(df['tribal_dominance_raw'], df['individual_dignity_raw']) * \\\n                             np.abs(df['tribal_dominance_salience'] - df['individual_dignity_salience'])\n    df['emotional_tension'] = np.minimum(df['fear_raw'], df['hope_raw']) * \\\n                              np.abs(df['fear_salience'] - df['hope_salience'])\n    df['success_tension'] = np.minimum(df['envy_raw'], df['compassion_raw']) * \\\n                            np.abs(df['envy_salience'] - df['compassion_salience'])\n    df['relational_tension'] = np.minimum(df['enmity_raw'], df['amity_raw']) * \\\n                               np.abs(df['enmity_salience'] - df['amity_salience'])\n    df['goal_tension'] = np.minimum(df['fragmentative_goals_raw'], df['cohesive_goals_raw']) * \\\n                         np.abs(df['fragmentative_goals_salience'] - df['cohesive_goals_salience'])\n\n    # Calculate Strategic Contradiction Index\n    df['strategic_contradiction_index'] = (\n        df['identity_tension'] + df['emotional_tension'] + df['success_tension'] +\n        df['relational_tension'] + df['goal_tension']\n    ) / 5\n\n    # Calculate Cohesion Components\n    df['emotional_cohesion_component'] = (df['hope_raw'] * df['hope_salience'] - df['fear_raw'] * df['fear_salience'])\n    df['success_cohesion_component'] = (df['compassion_raw'] * df['compassion_salience'] - df['envy_raw'] * df['envy_salience'])\n    df['relational_cohesion_component'] = (df['amity_raw'] * df['amity_salience'] - df['enmity_raw'] * df['enmity_salience'])\n    df['goal_cohesion_component'] = (df['cohesive_goals_raw'] * df['cohesive_goals_salience'] - df['fragmentative_goals_raw'] * df['fragmentative_goals_salience'])\n    df['identity_cohesion_component'] = (df['individual_dignity_raw'] * df['individual_dignity_salience'] - df['tribal_dominance_raw'] * df['tribal_dominance_salience'])\n\n    # Calculate Salience Totals for Normalization\n    epsilon = 0.001 # To prevent division by zero\n\n    df['descriptive_salience_total'] = (\n        df['hope_salience'] + df['fear_salience'] +\n        df['compassion_salience'] + df['envy_salience'] +\n        df['amity_salience'] + df['enmity_salience']\n    )\n    df['motivational_salience_total'] = (\n        df['descriptive_salience_total'] +\n        df['cohesive_goals_salience'] + df['fragmentative_goals_salience']\n    )\n    df['full_salience_total'] = (\n        df['motivational_salience_total'] +\n        df['individual_dignity_salience'] + df['tribal_dominance_salience']\n    )\n\n    # Calculate Cohesion Indices\n    df['descriptive_cohesion_index'] = df['emotional_cohesion_component'] + \\\n                                       df['success_cohesion_component'] + \\\n                                       df['relational_cohesion_component']\n    df['descriptive_cohesion_index'] = df['descriptive_cohesion_index'] / (df['descriptive_salience_total'] + epsilon)\n\n    df['motivational_cohesion_index'] = df['descriptive_cohesion_component'] + \\\n                                        df['goal_cohesion_component']\n    df['motivational_cohesion_index'] = df['motivational_cohesion_index'] / (df['motivational_salience_total'] + epsilon)\n\n    df['full_cohesion_index'] = df['identity_cohesion_component'] + \\\n                                df['emotional_cohesion_component'] + \\\n                                df['success_cohesion_component'] + \\\n                                df['relational_cohesion_component'] + \\\n                                df['goal_cohesion_component']\n    df['full_cohesion_index'] = df['full_cohesion_index'] / (df['full_salience_total'] + epsilon)\n\n    return df\n\ndef calculate_descriptive_stats(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics (mean, std, min, max, count) for all\n    raw scores, salience scores, confidence scores, and derived cohesion/tension indices.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with\n                             flattened dimension scores (e.g., 'tribal_dominance_raw').\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A dictionary where keys are metric names and values are dictionaries\n              of their descriptive statistics, or None if input data is empty.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if data.empty:\n        return None\n\n    try:\n        # Calculate derived metrics first\n        df_with_derived = _calculate_derived_metrics(data)\n\n        # Identify all relevant columns for descriptive stats\n        # Base dimensions\n        base_metrics = [\n            'tribal_dominance', 'individual_dignity', 'fear', 'hope',\n            'envy', 'compassion', 'enmity', 'amity',\n            'fragmentative_goals', 'cohesive_goals'\n        ]\n        base_cols = []\n        for metric in base_metrics:\n            base_cols.extend([f'{metric}_raw', f'{metric}_salience', f'{metric}_confidence'])\n\n        # Derived metrics\n        derived_metrics_cols = [\n            'identity_tension', 'emotional_tension', 'success_tension',\n            'relational_tension', 'goal_tension', 'strategic_contradiction_index',\n            'descriptive_cohesion_index', 'motivational_cohesion_index',\n            'full_cohesion_index'\n        ]\n\n        all_cols_for_stats = [col for col in base_cols + derived_metrics_cols if col in df_with_derived.columns]\n\n        if not all_cols_for_stats:\n            return None # No relevant columns found\n\n        descriptive_stats = {}\n        for col in all_cols_for_stats:\n            if pd.api.types.is_numeric_dtype(df_with_derived[col]):\n                series = df_with_derived[col].dropna()\n                if not series.empty:\n                    descriptive_stats[col] = {\n                        'mean': series.mean(),\n                        'std': series.std(),\n                        'min': series.min(),\n                        'max': series.max(),\n                        'count': series.count()\n                    }\n                else:\n                    descriptive_stats[col] = {\n                        'mean': np.nan, 'std': np.nan, 'min': np.nan,\n                        'max': np.nan, 'count': 0\n                    }\n            else:\n                descriptive_stats[col] = \"Non-numeric data, cannot calculate descriptive statistics.\"\n\n        return descriptive_stats\n\n    except Exception as e:\n        print(f\"Error calculating descriptive statistics: {e}\")\n        return None\n\ndef compare_cohesion_by_style(data, **kwargs):\n    \"\"\"\n    Compares cohesion and tension indices between 'institutional' and 'populist'\n    discourse styles using independent samples t-tests.\n\n    This function classifies documents based on their 'document_name' into\n    predefined 'institutional' or 'populist' styles. It then performs t-tests\n    on the 'full_cohesion_index', 'descriptive_cohesion_index',\n    'motivational_cohesion_index', and 'strategic_contradiction_index'.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with\n                             'document_name' and flattened dimension scores.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A dictionary containing t-test results (t-statistic, p-value)\n              for each compared metric, or None if insufficient data for comparison.\n    \"\"\"\n    import pandas as pd\n    from scipy import stats\n    import numpy as np\n\n    if data.empty:\n        return None\n\n    try:\n        # Define the mapping of document names to discourse styles\n        # This mapping is based on the sample data and research question provided.\n        # In a real scenario, this would come from a comprehensive corpus manifest.\n        speaker_style_map = {\n            \"john_mccain_2008_concession.txt\": \"institutional\",\n            \"bernie_sanders_2025_fighting_oligarchy.txt\": \"populist\",\n            \"alexandria_ocasio_cortez_2025_fighting_oligarchy.txt\": \"populist\",\n            \"steve_king_2017_house_floor.txt\": \"populist\",\n            # Add more mappings here if other documents are part of the corpus\n        }\n\n        # Add 'discourse_style' column to the DataFrame\n        data['discourse_style'] = data['document_name'].map(speaker_style_map)\n\n        # Filter out documents that are not classified\n        df_classified = data.dropna(subset=['discourse_style']).copy()\n\n        if df_classified.empty:\n            return {\"error\": \"No documents classified into discourse styles for comparison.\"}\n\n        # Calculate derived metrics\n        df_with_derived = _calculate_derived_metrics(df_classified)\n\n        # Define metrics to compare\n        metrics_to_compare = [\n            'full_cohesion_index',\n            'descriptive_cohesion_index',\n            'motivational_cohesion_index',\n            'strategic_contradiction_index'\n        ]\n\n        results = {}\n\n        institutional_data = df_with_derived[df_with_derived['discourse_style'] == 'institutional']\n        populist_data = df_with_derived[df_with_derived['discourse_style'] == 'populist']\n\n        if institutional_data.empty or populist_data.empty:\n            return {\"error\": \"Insufficient data for one or both discourse styles.\"}\n\n        for metric in metrics_to_compare:\n            group1 = institutional_data[metric].dropna()\n            group2 = populist_data[metric].dropna()\n\n            if len(group1) < 2 or len(group2) < 2: # Need at least 2 samples for t-test\n                results[metric] = {\"error\": \"Not enough data points for t-test in one or both groups.\"}\n                continue\n\n            # Perform independent samples t-test\n            # Assuming unequal variances (Welch's t-test) as a robust default\n            t_stat, p_value = stats.ttest_ind(group1, group2, equal_var=False)\n\n            results[metric] = {\n                't_statistic': t_stat,\n                'p_value': p_value,\n                'institutional_mean': group1.mean(),\n                'populist_mean': group2.mean(),\n                'institutional_std': group1.std(),\n                'populist_std': group2.std(),\n                'institutional_count': len(group1),\n                'populist_count': len(group2)\n            }\n\n        return results\n\n    except Exception as e:\n        print(f\"Error comparing cohesion by style: {e}\")\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}