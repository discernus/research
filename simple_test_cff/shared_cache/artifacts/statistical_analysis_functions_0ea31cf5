{
  "status": "success",
  "functions_generated": 5,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 19004,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: simple_test\nDescription: Statistical analysis experiment\nGenerated: 2025-08-24T21:09:36.433552+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics (mean, std, min, max, count) for all raw\n    score and salience dimensions in the dataset.\n\n    This function provides a foundational overview of the data distribution for each\n    primary dimension measured by the Cohesive Flourishing Framework, allowing\n    researchers to understand the central tendency and dispersion of scores across\n    the corpus before calculating composite metrics.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame containing the analysis data with\n                             columns matching the framework specification.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        dict: A nested dictionary where each key is a dimension name (e.g.,\n              'tribal_dominance_raw') and the value is a dictionary of its\n              descriptive statistics. Returns None if the input data is invalid\n              or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        results = {}\n        # Select only columns ending with _raw or _salience for analysis\n        score_columns = [col for col in data.columns if col.endswith('_raw') or col.endswith('_salience')]\n\n        if not score_columns:\n            return None\n\n        for col in score_columns:\n            if pd.api.types.is_numeric_dtype(data[col]):\n                # Ensure data is clean for JSON serialization\n                mean = data[col].mean()\n                std = data[col].std()\n                count = data[col].count()\n                min_val = data[col].min()\n                max_val = data[col].max()\n\n                results[col] = {\n                    'mean': float(mean) if pd.notna(mean) else None,\n                    'std': float(std) if pd.notna(std) else None,\n                    'count': int(count) if pd.notna(count) else 0,\n                    'min': float(min_val) if pd.notna(min_val) else None,\n                    'max': float(max_val) if pd.notna(max_val) else None\n                }\n\n        return results\n\n    except Exception:\n        return None\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived metrics as defined by the Cohesive Flourishing Framework v10.1.\n\n    This function implements the formulas for Tension Indices, the Strategic\n    Contradiction Index, and the three Salience-Weighted Cohesion Indices. It\n    appends these calculated metrics as new columns to the input DataFrame. This is\n    the core function for answering research questions about baseline cohesion and\n    tension scores.\n\n    Methodology:\n    - Tension Indices: Calculated using the formula `min(Score_A, Score_B) * |Salience_A - Salience_B|`\n      for each opposing pair of dimensions.\n    - Strategic Contradiction Index: The arithmetic mean of the five tension indices.\n    - Salience-Weighted Cohesion Indices: Calculated by summing the salience-weighted\n      scores of cohesive dimensions and subtracting the sum of salience-weighted scores\n      of fragmentative dimensions, then normalizing by the total salience of all\n      included dimensions (plus a small epsilon to prevent division by zero).\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame with columns for all raw and salience scores.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        pd.DataFrame: The original DataFrame with new columns for each derived metric.\n                      Returns None if required columns are missing or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = data.copy()\n\n        # --- 1. Calculate Tension Indices ---\n        df['identity_tension'] = np.minimum(df['tribal_dominance_raw'], df['individual_dignity_raw']) * np.abs(df['tribal_dominance_salience'] - df['individual_dignity_salience'])\n        df['emotional_tension'] = np.minimum(df['fear_raw'], df['hope_raw']) * np.abs(df['fear_salience'] - df['hope_salience'])\n        df['success_tension'] = np.minimum(df['envy_raw'], df['compersion_raw']) * np.abs(df['envy_salience'] - df['compersion_salience'])\n        df['relational_tension'] = np.minimum(df['enmity_raw'], df['amity_raw']) * np.abs(df['enmity_salience'] - df['amity_salience'])\n        df['goal_tension'] = np.minimum(df['fragmentative_goals_raw'], df['cohesive_goals_raw']) * np.abs(df['fragmentative_goals_salience'] - df['cohesive_goals_salience'])\n\n        # --- 2. Calculate Strategic Contradiction Index ---\n        tension_cols = ['identity_tension', 'emotional_tension', 'success_tension', 'relational_tension', 'goal_tension']\n        df['strategic_contradiction_index'] = df[tension_cols].mean(axis=1)\n\n        # --- 3. Calculate Salience-Weighted Cohesion Indices ---\n        epsilon = 0.001\n\n        # Numerator components\n        identity_comp = (df['individual_dignity_raw'] * df['individual_dignity_salience']) - (df['tribal_dominance_raw'] * df['tribal_dominance_salience'])\n        emotional_comp = (df['hope_raw'] * df['hope_salience']) - (df['fear_raw'] * df['fear_salience'])\n        success_comp = (df['compersion_raw'] * df['compersion_salience']) - (df['envy_raw'] * df['envy_salience'])\n        relational_comp = (df['amity_raw'] * df['amity_salience']) - (df['enmity_raw'] * df['enmity_salience'])\n        goal_comp = (df['cohesive_goals_raw'] * df['cohesive_goals_salience']) - (df['fragmentative_goals_raw'] * df['fragmentative_goals_salience'])\n\n        # Denominator (salience totals)\n        descriptive_salience_total = df['hope_salience'] + df['fear_salience'] + df['compersion_salience'] + df['envy_salience'] + df['amity_salience'] + df['enmity_salience']\n        motivational_salience_total = descriptive_salience_total + df['cohesive_goals_salience'] + df['fragmentative_goals_salience']\n        full_salience_total = motivational_salience_total + df['individual_dignity_salience'] + df['tribal_dominance_salience']\n\n        # Descriptive Cohesion Index\n        numerator_desc = emotional_comp + success_comp + relational_comp\n        df['descriptive_cohesion_index'] = numerator_desc / (descriptive_salience_total + epsilon)\n\n        # Motivational Cohesion Index\n        numerator_motiv = numerator_desc + goal_comp\n        df['motivational_cohesion_index'] = numerator_motiv / (motivational_salience_total + epsilon)\n\n        # Full Cohesion Index\n        numerator_full = numerator_motiv + identity_comp\n        df['full_cohesion_index'] = numerator_full / (full_salience_total + epsilon)\n        \n        # Clip results to the theoretical -1.0 to 1.0 range\n        cohesion_indices = ['descriptive_cohesion_index', 'motivational_cohesion_index', 'full_cohesion_index']\n        for col in cohesion_indices:\n            df[col] = df[col].clip(-1.0, 1.0)\n\n        return df\n\n    except (KeyError, Exception):\n        # KeyError if a required column is missing\n        return None\n\ndef summarize_corpus_metrics(data, **kwargs):\n    \"\"\"\n    Calculates and summarizes the key derived metrics for the entire corpus.\n\n    This function first computes the derived metrics (Tension, Cohesion, etc.)\n    for each document and then provides descriptive statistics (mean, std, min, max)\n    for these high-level indicators across the whole dataset. This directly addresses\n    the research question about baseline cohesion and tension scores for the corpus.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame with columns for all raw and salience scores.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        dict: A nested dictionary of descriptive statistics for each derived metric.\n              Returns None if the input data is invalid or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        # First, calculate the derived metrics using the dedicated function\n        metrics_df = calculate_derived_metrics(data)\n\n        if metrics_df is None:\n            # This happens if calculate_derived_metrics fails (e.g., missing columns)\n            return None\n\n        derived_metric_columns = [\n            'identity_tension', 'emotional_tension', 'success_tension',\n            'relational_tension', 'goal_tension', 'strategic_contradiction_index',\n            'descriptive_cohesion_index', 'motivational_cohesion_index', 'full_cohesion_index'\n        ]\n\n        # Calculate descriptive statistics for the derived metrics\n        summary = metrics_df[derived_metric_columns].describe().transpose()\n        summary.rename(columns={'25%': 'q1', '50%': 'median', '75%': 'q3'}, inplace=True)\n        \n        # Convert to a JSON-serializable dictionary\n        results = summary.to_dict('index')\n        \n        # Ensure all values are standard Python floats, not numpy types\n        for metric, stats in results.items():\n            for stat_name, value in stats.items():\n                stats[stat_name] = float(value) if pd.notna(value) else None\n        \n        return results\n\n    except Exception:\n        return None\n\ndef calculate_correlation_matrix(data, **kwargs):\n    \"\"\"\n    Computes the Pearson correlation matrix for raw scores and derived metrics.\n\n    This analysis helps reveal relationships between different rhetorical strategies.\n    For example, it can show if 'fear_raw' scores are positively correlated with\n    'enmity_raw' scores, or how the 'full_cohesion_index' relates to individual\n    dimensions. The function calculates derived metrics first to include them in\n    the correlation analysis.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame with raw and salience scores.\n        **kwargs: Additional keyword arguments (not used in this function).\n\n    Returns:\n        dict: A dictionary representing the correlation matrix, suitable for JSON\n              serialization. Returns None if data is insufficient or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty or len(data) < 2:\n            return None\n\n        # Calculate derived metrics to include them in the correlation\n        metrics_df = calculate_derived_metrics(data)\n        if metrics_df is None:\n            return None\n\n        # Select columns for correlation: all raw scores and all derived metrics\n        raw_score_cols = [col for col in metrics_df.columns if col.endswith('_raw')]\n        derived_metric_cols = [\n            'identity_tension', 'emotional_tension', 'success_tension',\n            'relational_tension', 'goal_tension', 'strategic_contradiction_index',\n            'descriptive_cohesion_index', 'motivational_cohesion_index', 'full_cohesion_index'\n        ]\n        \n        cols_to_correlate = raw_score_cols + derived_metric_cols\n        \n        # Ensure all selected columns exist in the dataframe\n        cols_to_correlate = [col for col in cols_to_correlate if col in metrics_df.columns]\n\n        correlation_matrix = metrics_df[cols_to_correlate].corr(method='pearson')\n\n        # Convert NaN to None for JSON compatibility\n        correlation_matrix = correlation_matrix.where(pd.notna(correlation_matrix), None)\n\n        return correlation_matrix.to_dict('index')\n\n    except Exception:\n        return None\n\ndef identify_outlier_documents(data, **kwargs):\n    \"\"\"\n    Identifies documents with the highest and lowest scores for key derived metrics.\n\n    This function is useful for qualitative analysis, allowing researchers to\n    easily find and examine the most extreme examples of cohesive, fragmentative,\n    or contradictory rhetoric within the corpus. It identifies the top and bottom N\n    documents for the 'full_cohesion_index' and 'strategic_contradiction_index'.\n\n    Args:\n        data (pd.DataFrame): A pandas DataFrame with raw and salience scores.\n        **kwargs:\n            n (int): The number of top/bottom documents to return. Defaults to 5.\n\n    Returns:\n        dict: A dictionary containing lists of the top and bottom N documents\n              for key metrics, including their scores. Returns None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        n = kwargs.get('n', 5)\n        if not isinstance(n, int) or n < 1:\n            n = 5\n\n        metrics_df = calculate_derived_metrics(data)\n        if metrics_df is None:\n            return None\n        \n        # Ensure n is not larger than the number of documents\n        n = min(n, len(metrics_df))\n\n        results = {}\n        metrics_to_analyze = ['full_cohesion_index', 'strategic_contradiction_index']\n\n        for metric in metrics_to_analyze:\n            if metric in metrics_df.columns:\n                # Sort ascending for 'lowest' and descending for 'highest'\n                lowest = metrics_df.nsmallest(n, metric)[['document_name', metric]]\n                highest = metrics_df.nlargest(n, metric)[['document_name', metric]]\n\n                results[metric] = {\n                    'lowest_scoring_documents': lowest.to_dict('records'),\n                    'highest_scoring_documents': highest.to_dict('records')\n                }\n\n        return results if results else None\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}