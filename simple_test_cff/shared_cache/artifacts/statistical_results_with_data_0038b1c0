{
  "generation_metadata": {
    "status": "success",
    "functions_generated": 5,
    "output_file": "automatedstatisticalanalysisagent_functions.py",
    "module_size": 19562,
    "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: simple_test\nDescription: Statistical analysis experiment\nGenerated: 2025-08-27T17:43:48.152896+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef _calculate_all_derived_metrics(data):\n    \"\"\"\n    Internal helper function to calculate all derived metrics from the CFF framework.\n    This function adds new columns to the DataFrame for each derived metric.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with raw and salience scores.\n\n    Returns:\n        pd.DataFrame: The DataFrame with added columns for derived metrics.\n                       Returns the original DataFrame if essential columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    df = data.copy()\n    \n    required_cols = [\n        'tribal_dominance_raw', 'tribal_dominance_salience',\n        'individual_dignity_raw', 'individual_dignity_salience',\n        'fear_raw', 'fear_salience', 'hope_raw', 'hope_salience',\n        'envy_raw', 'envy_salience', 'compersion_raw', 'compersion_salience',\n        'enmity_raw', 'enmity_salience', 'amity_raw', 'amity_salience',\n        'fragmentative_goals_raw', 'fragmentative_goals_salience',\n        'cohesive_goals_raw', 'cohesive_goals_salience'\n    ]\n    \n    if not all(col in df.columns for col in required_cols):\n        # Silently return original df if data is not as expected\n        return data\n\n    # Layer 1: Tension Indices\n    df['identity_tension'] = np.minimum(df['tribal_dominance_raw'], df['individual_dignity_raw']) * \\\n                             np.abs(df['tribal_dominance_salience'] - df['individual_dignity_salience'])\n    \n    df['emotional_tension'] = np.minimum(df['fear_raw'], df['hope_raw']) * \\\n                              np.abs(df['fear_salience'] - df['hope_salience'])\n                              \n    df['success_tension'] = np.minimum(df['envy_raw'], df['compersion_raw']) * \\\n                            np.abs(df['envy_salience'] - df['compersion_salience'])\n\n    df['relational_tension'] = np.minimum(df['enmity_raw'], df['amity_raw']) * \\\n                               np.abs(df['enmity_salience'] - df['amity_salience'])\n\n    df['goal_tension'] = np.minimum(df['fragmentative_goals_raw'], df['cohesive_goals_raw']) * \\\n                         np.abs(df['fragmentative_goals_salience'] - df['cohesive_goals_salience'])\n\n    # Layer 2: Strategic Contradiction Index\n    tension_cols = ['identity_tension', 'emotional_tension', 'success_tension', 'relational_tension', 'goal_tension']\n    df['strategic_contradiction_index'] = df[tension_cols].mean(axis=1)\n\n    # Layer 3: Salience-Weighted Cohesion Indices\n    epsilon = 0.001\n\n    # Descriptive Cohesion Index\n    descriptive_numerator = (df['hope_raw'] * df['hope_salience'] - df['fear_raw'] * df['fear_salience']) + \\\n                            (df['compersion_raw'] * df['compersion_salience'] - df['envy_raw'] * df['envy_salience']) + \\\n                            (df['amity_raw'] * df['amity_salience'] - df['enmity_raw'] * df['enmity_salience'])\n    descriptive_denominator = df['hope_salience'] + df['fear_salience'] + \\\n                              df['compersion_salience'] + df['envy_salience'] + \\\n                              df['amity_salience'] + df['enmity_salience']\n    df['descriptive_cohesion_index'] = descriptive_numerator / (descriptive_denominator + epsilon)\n\n    # Motivational Cohesion Index\n    motivational_numerator = descriptive_numerator + \\\n                             (df['cohesive_goals_raw'] * df['cohesive_goals_salience'] - df['fragmentative_goals_raw'] * df['fragmentative_goals_salience'])\n    motivational_denominator = descriptive_denominator + \\\n                               df['cohesive_goals_salience'] + df['fragmentative_goals_salience']\n    df['motivational_cohesion_index'] = motivational_numerator / (motivational_denominator + epsilon)\n\n    # Full Cohesion Index\n    full_numerator = motivational_numerator + \\\n                     (df['individual_dignity_raw'] * df['individual_dignity_salience'] - df['tribal_dominance_raw'] * df['tribal_dominance_salience'])\n    full_denominator = motivational_denominator + \\\n                       df['individual_dignity_salience'] + df['tribal_dominance_salience']\n    df['full_cohesion_index'] = full_numerator / (full_denominator + epsilon)\n    \n    # Clip results to be within the theoretical -1.0 to 1.0 range\n    cohesion_indices = ['descriptive_cohesion_index', 'motivational_cohesion_index', 'full_cohesion_index']\n    for col in cohesion_indices:\n        df[col] = df[col].clip(-1.0, 1.0)\n\n    return df\n\ndef get_corpus_summary_statistics(data, **kwargs):\n    \"\"\"\n    Calculates and returns baseline descriptive statistics for all raw scores, salience scores,\n    and derived CFF metrics (tensions and cohesion indices) across the entire corpus.\n\n    This function first calculates all derived metrics as defined in the Cohesive\n    Flourishing Framework v10 specification. It then computes the count, mean, standard\n    deviation, min, max, and quartile values for each primary and derived metric.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data with columns\n                                 matching the CFF specification.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each metric,\n              or None if the input data is empty or invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data.empty:\n            return None\n\n        # Calculate all derived metrics first\n        df_with_metrics = _calculate_all_derived_metrics(data)\n\n        # Define columns for summary\n        raw_scores = [col for col in df_with_metrics.columns if '_raw' in col]\n        salience_scores = [col for col in df_with_metrics.columns if '_salience' in col]\n        derived_metrics = [\n            'identity_tension', 'emotional_tension', 'success_tension', \n            'relational_tension', 'goal_tension', 'strategic_contradiction_index',\n            'descriptive_cohesion_index', 'motivational_cohesion_index', 'full_cohesion_index'\n        ]\n        \n        # Filter out derived metrics that might not have been calculated if columns were missing\n        derived_metrics = [m for m in derived_metrics if m in df_with_metrics.columns]\n\n        if not derived_metrics:\n             return {'error': 'Could not calculate derived metrics due to missing input columns.'}\n\n        summary_cols = raw_scores + salience_scores + derived_metrics\n        \n        # Generate descriptive statistics\n        stats = df_with_metrics[summary_cols].describe().round(4)\n        \n        # Convert to dictionary format for JSON compatibility\n        return stats.to_dict()\n\n    except Exception as e:\n        # In a real environment, one might log the error `e`\n        return None\n\ndef analyze_derived_metric_correlations(data, **kwargs):\n    \"\"\"\n    Computes the Pearson correlation matrix for the key derived CFF metrics.\n\n    This analysis reveals the relationships between different strategic aspects of the\n    discourse. For example, it can show whether higher rhetorical tension correlates\n    with lower cohesion scores. The function focuses on the three main Cohesion Indices\n    and the Strategic Contradiction Index.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A dictionary representing the correlation matrix, or None if\n              insufficient data is available.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data.empty or len(data) < 2:\n            return None\n\n        df_with_metrics = _calculate_all_derived_metrics(data)\n\n        correlation_cols = [\n            'descriptive_cohesion_index',\n            'motivational_cohesion_index',\n            'full_cohesion_index',\n            'strategic_contradiction_index',\n            'identity_tension',\n            'emotional_tension',\n            'success_tension',\n            'relational_tension',\n            'goal_tension'\n        ]\n        \n        # Ensure all columns exist before trying to calculate correlation\n        existing_cols = [col for col in correlation_cols if col in df_with_metrics.columns]\n        if len(existing_cols) < 2:\n            return None\n\n        correlation_matrix = df_with_metrics[existing_cols].corr(method='pearson').round(4)\n\n        return correlation_matrix.to_dict()\n\n    except Exception as e:\n        return None\n\ndef get_document_rankings(data, **kwargs):\n    \"\"\"\n    Ranks documents based on a specified derived metric from the CFF framework.\n\n    This function identifies the documents with the highest and lowest scores for a\n    given metric, such as 'full_cohesion_index' or 'strategic_contradiction_index'.\n    This is useful for identifying exemplary cases for qualitative analysis.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs:\n            metric (str): The name of the metric to rank by.\n                          Defaults to 'full_cohesion_index'.\n                          Valid options: 'full_cohesion_index', 'strategic_contradiction_index', etc.\n            top_n (int): The number of top and bottom documents to return. Defaults to 5.\n\n    Returns:\n        dict: A dictionary with 'top_ranked' and 'bottom_ranked' lists,\n              or None if the metric is invalid or data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        metric_to_rank = kwargs.get('metric', 'full_cohesion_index')\n        top_n = kwargs.get('top_n', 5)\n\n        if data.empty:\n            return None\n\n        df_with_metrics = _calculate_all_derived_metrics(data)\n\n        if metric_to_rank not in df_with_metrics.columns:\n            valid_metrics = [m for m in df_with_metrics.columns if 'index' in m or 'tension' in m]\n            return {\n                'error': f\"Invalid metric '{metric_to_rank}'.\",\n                'valid_metrics': valid_metrics\n            }\n        \n        # Ensure document_name column exists\n        if 'document_name' not in df_with_metrics.columns:\n             return {'error': '`document_name` column not found in data.'}\n\n        # Drop rows where the metric is NaN\n        df_sorted = df_with_metrics.dropna(subset=[metric_to_rank]).copy()\n        \n        if df_sorted.empty:\n            return None\n\n        # Top N (highest scores)\n        top_ranked = df_sorted.nlargest(top_n, metric_to_rank)[['document_name', metric_to_rank]]\n\n        # Bottom N (lowest scores)\n        bottom_ranked = df_sorted.nsmallest(top_n, metric_to_rank)[['document_name', metric_to_rank]]\n\n        return {\n            'metric_ranked': metric_to_rank,\n            'top_ranked': top_ranked.to_dict('records'),\n            'bottom_ranked': bottom_ranked.to_dict('records')\n        }\n\n    except Exception as e:\n        return None\n\ndef analyze_speaker_summary(data, **kwargs):\n    \"\"\"\n    Calculates summary statistics for CFF metrics, grouped by speaker.\n\n    This function attempts to load a 'corpus_manifest.json' file to map documents\n    to speakers. If the manifest is found, it aggregates the data by speaker and\n    computes the mean for all primary and derived CFF metrics. This allows for\n\n    comparison of rhetorical patterns across different speakers.\n\n    Per platform requirements, this function relies solely on the corpus manifest for\n    speaker identification and will NOT parse filenames. If the manifest is not found,\n    it will return a graceful error message.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs:\n            manifest_path (str): Optional path to the corpus manifest file.\n                                 Defaults to 'corpus_manifest.json'.\n\n    Returns:\n        dict: A dictionary of summary statistics grouped by speaker, or a status\n              message if the corpus manifest is not found or data is invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        manifest_path = Path(kwargs.get('manifest_path', 'corpus_manifest.json'))\n        \n        if not manifest_path.is_file():\n            return {\n                'status': 'error',\n                'message': 'Corpus manifest not found. Cannot perform speaker-level analysis.',\n                'details': f\"Expected manifest file at: {manifest_path.resolve()}\"\n            }\n\n        if data.empty:\n            return None\n            \n        if 'document_name' not in data.columns:\n             return {'error': '`document_name` column not found in data.'}\n\n        with open(manifest_path, 'r') as f:\n            manifest_data = json.load(f)\n        \n        # Assuming manifest is a list of dicts with 'filename' and 'speaker'\n        if not isinstance(manifest_data, list) or not all('filename' in d and 'speaker' in d for d in manifest_data):\n             return {'error': 'Corpus manifest is in an unrecognized format.'}\n\n        speaker_map = {item['filename']: item['speaker'] for item in manifest_data}\n        \n        df = data.copy()\n        df['speaker'] = df['document_name'].map(speaker_map)\n\n        if df['speaker'].isnull().all():\n            return {\n                'status': 'warning',\n                'message': 'No documents in the dataset could be matched to a speaker in the manifest.'\n            }\n        \n        df_with_metrics = _calculate_all_derived_metrics(df.dropna(subset=['speaker']))\n        \n        if df_with_metrics.empty:\n            return None\n\n        # Select all numeric columns for aggregation\n        numeric_cols = df_with_metrics.select_dtypes(include=np.number).columns.tolist()\n        \n        speaker_summary = df_with_metrics.groupby('speaker')[numeric_cols].mean().round(4)\n\n        return speaker_summary.to_dict('index')\n\n    except FileNotFoundError:\n         return {\n            'status': 'error',\n            'message': 'Corpus manifest not found. Cannot perform speaker-level analysis.',\n            'details': f\"Expected manifest file at: {manifest_path.resolve()}\"\n        }\n    except Exception as e:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
    "cached_with_code": true
  },
  "statistical_data": {
    "analyze_derived_metric_correlations": {
      "descriptive_cohesion_index": {
        "descriptive_cohesion_index": 1.0,
        "motivational_cohesion_index": 0.9988,
        "full_cohesion_index": 0.9986,
        "strategic_contradiction_index": -0.6154,
        "identity_tension": -0.2049,
        "emotional_tension": -0.7574,
        "success_tension": NaN,
        "relational_tension": -0.405,
        "goal_tension": -0.73
      },
      "motivational_cohesion_index": {
        "descriptive_cohesion_index": 0.9988,
        "motivational_cohesion_index": 1.0,
        "full_cohesion_index": 0.9989,
        "strategic_contradiction_index": -0.5997,
        "identity_tension": -0.1694,
        "emotional_tension": -0.7391,
        "success_tension": NaN,
        "relational_tension": -0.3928,
        "goal_tension": -0.7608
      },
      "full_cohesion_index": {
        "descriptive_cohesion_index": 0.9986,
        "motivational_cohesion_index": 0.9989,
        "full_cohesion_index": 1.0,
        "strategic_contradiction_index": -0.574,
        "identity_tension": -0.1544,
        "emotional_tension": -0.722,
        "success_tension": NaN,
        "relational_tension": -0.359,
        "goal_tension": -0.7581
      },
      "strategic_contradiction_index": {
        "descriptive_cohesion_index": -0.6154,
        "motivational_cohesion_index": -0.5997,
        "full_cohesion_index": -0.574,
        "strategic_contradiction_index": 1.0,
        "identity_tension": 0.8297,
        "emotional_tension": 0.9729,
        "success_tension": NaN,
        "relational_tension": 0.9636,
        "goal_tension": 0.0846
      },
      "identity_tension": {
        "descriptive_cohesion_index": -0.2049,
        "motivational_cohesion_index": -0.1694,
        "full_cohesion_index": -0.1544,
        "strategic_contradiction_index": 0.8297,
        "identity_tension": 1.0,
        "emotional_tension": 0.7749,
        "success_tension": NaN,
        "relational_tension": 0.8438,
        "goal_tension": -0.4653
      },
      "emotional_tension": {
        "descriptive_cohesion_index": -0.7574,
        "motivational_cohesion_index": -0.7391,
        "full_cohesion_index": -0.722,
        "strategic_contradiction_index": 0.9729,
        "identity_tension": 0.7749,
        "emotional_tension": 1.0,
        "success_tension": NaN,
        "relational_tension": 0.8758,
        "goal_tension": 0.1981
      },
      "success_tension": {
        "descriptive_cohesion_index": NaN,
        "motivational_cohesion_index": NaN,
        "full_cohesion_index": NaN,
        "strategic_contradiction_index": NaN,
        "identity_tension": NaN,
        "emotional_tension": NaN,
        "success_tension": NaN,
        "relational_tension": NaN,
        "goal_tension": NaN
      },
      "relational_tension": {
        "descriptive_cohesion_index": -0.405,
        "motivational_cohesion_index": -0.3928,
        "full_cohesion_index": -0.359,
        "strategic_contradiction_index": 0.9636,
        "identity_tension": 0.8438,
        "emotional_tension": 0.8758,
        "success_tension": NaN,
        "relational_tension": 1.0,
        "goal_tension": -0.0631
      },
      "goal_tension": {
        "descriptive_cohesion_index": -0.73,
        "motivational_cohesion_index": -0.7608,
        "full_cohesion_index": -0.7581,
        "strategic_contradiction_index": 0.0846,
        "identity_tension": -0.4653,
        "emotional_tension": 0.1981,
        "success_tension": NaN,
        "relational_tension": -0.0631,
        "goal_tension": 1.0
      }
    },
    "analyze_speaker_summary": {
      "status": "error",
      "message": "Corpus manifest not found. Cannot perform speaker-level analysis.",
      "details": "Expected manifest file at: /Volumes/code/discernus/corpus_manifest.json"
    },
    "generate_statistical_summary_report": "STATISTICAL ANALYSIS SUMMARY REPORT\n==================================================\nAnalysis Timestamp: Unknown\nSample Size: Unknown\nAlpha Level: Unknown\nVariables: 0\n",
    "get_corpus_summary_statistics": {
      "tribal_dominance_raw": {
        "count": 4.0,
        "mean": 0.475,
        "std": 0.3948,
        "min": 0.0,
        "25%": 0.225,
        "50%": 0.55,
        "75%": 0.8,
        "max": 0.8
      },
      "individual_dignity_raw": {
        "count": 4.0,
        "mean": 0.4,
        "std": 0.4082,
        "min": 0.0,
        "25%": 0.075,
        "50%": 0.4,
        "75%": 0.725,
        "max": 0.8
      },
      "fear_raw": {
        "count": 4.0,
        "mean": 0.65,
        "std": 0.3109,
        "min": 0.2,
        "25%": 0.575,
        "50%": 0.75,
        "75%": 0.825,
        "max": 0.9
      },
      "hope_raw": {
        "count": 4.0,
        "mean": 0.425,
        "std": 0.263,
        "min": 0.2,
        "25%": 0.275,
        "50%": 0.35,
        "75%": 0.5,
        "max": 0.8
      },
      "envy_raw": {
        "count": 4.0,
        "mean": 0.6,
        "std": 0.4243,
        "min": 0.0,
        "25%": 0.45,
        "50%": 0.75,
        "75%": 0.9,
        "max": 0.9
      },
      "compersion_raw": {
        "count": 4.0,
        "mean": 0.225,
        "std": 0.45,
        "min": 0.0,
        "25%": 0.0,
        "50%": 0.0,
        "75%": 0.225,
        "max": 0.9
      },
      "enmity_raw": {
        "count": 4.0,
        "mean": 0.675,
        "std": 0.45,
        "min": 0.0,
        "25%": 0.675,
        "50%": 0.9,
        "75%": 0.9,
        "max": 0.9
      },
      "amity_raw": {
        "count": 4.0,
        "mean": 0.475,
        "std": 0.4425,
        "min": 0.0,
        "25%": 0.15,
        "50%": 0.5,
        "75%": 0.825,
        "max": 0.9
      },
      "fragmentative_goals_raw": {
        "count": 4.0,
        "mean": 0.6,
        "std": 0.4,
        "min": 0.0,
        "25%": 0.6,
        "50%": 0.8,
        "75%": 0.8,
        "max": 0.8
      },
      "cohesive_goals_raw": {
        "count": 4.0,
        "mean": 0.5,
        "std": 0.3559,
        "min": 0.2,
        "25%": 0.2,
        "50%": 0.45,
        "75%": 0.75,
        "max": 0.9
      },
      "tribal_dominance_salience": {
        "count": 4.0,
        "mean": 0.575,
        "std": 0.4272,
        "min": 0.0,
        "25%": 0.375,
        "50%": 0.7,
        "75%": 0.9,
        "max": 0.9
      },
      "individual_dignity_salience": {
        "count": 4.0,
        "mean": 0.3875,
        "std": 0.4049,
        "min": 0.0,
        "25%": 0.075,
        "50%": 0.35,
        "75%": 0.6625,
        "max": 0.85
      },
      "fear_salience": {
        "count": 4.0,
        "mean": 0.65,
        "std": 0.2517,
        "min": 0.3,
        "25%": 0.6,
        "50%": 0.7,
        "75%": 0.75,
        "max": 0.9
      },
      "hope_salience": {
        "count": 4.0,
        "mean": 0.4,
        "std": 0.3367,
        "min": 0.2,
        "25%": 0.2,
        "50%": 0.25,
        "75%": 0.45,
        "max": 0.9
      },
      "envy_salience": {
        "count": 4.0,
        "mean": 0.55,
        "std": 0.4041,
        "min": 0.0,
        "25%": 0.375,
        "50%": 0.65,
        "75%": 0.825,
        "max": 0.9
      },
      "compersion_salience": {
        "count": 4.0,
        "mean": 0.225,
        "std": 0.45,
        "min": 0.0,
        "25%": 0.0,
        "50%": 0.0,
        "75%": 0.225,
        "max": 0.9
      },
      "enmity_salience": {
        "count": 4.0,
        "mean": 0.675,
        "std": 0.45,
        "min": 0.0,
        "25%": 0.675,
        "50%": 0.9,
        "75%": 0.9,
        "max": 0.9
      },
      "amity_salience": {
        "count": 4.0,
        "mean": 0.4875,
        "std": 0.4211,
        "min": 0.0,
        "25%": 0.225,
        "50%": 0.5,
        "75%": 0.7625,
        "max": 0.95
      },
      "fragmentative_goals_salience": {
        "count": 4.0,
        "mean": 0.575,
        "std": 0.3862,
        "min": 0.0,
        "25%": 0.525,
        "50%": 0.75,
        "75%": 0.8,
        "max": 0.8
      },
      "cohesive_goals_salience": {
        "count": 4.0,
        "mean": 0.5375,
        "std": 0.3497,
        "min": 0.2,
        "25%": 0.275,
        "50%": 0.5,
        "75%": 0.7625,
        "max": 0.95
      },
      "identity_tension": {
        "count": 4.0,
        "mean": 0.0625,
        "std": 0.1001,
        "min": 0.0,
        "25%": 0.0,
        "50%": 0.02,
        "75%": 0.0825,
        "max": 0.21
      },
      "emotional_tension": {
        "count": 4.0,
        "mean": 0.1425,
        "std": 0.0171,
        "min": 0.12,
        "25%": 0.135,
        "50%": 0.145,
        "75%": 0.1525,
        "max": 0.16
      },
      "success_tension": {
        "count": 4.0,
        "mean": 0.0,
        "std": 0.0,
        "min": 0.0,
        "25%": 0.0,
        "50%": 0.0,
        "75%": 0.0,
        "max": 0.0
      },
      "relational_tension": {
        "count": 4.0,
        "mean": 0.07,
        "std": 0.0825,
        "min": 0.0,
        "25%": 0.0,
        "50%": 0.06,
        "75%": 0.13,
        "max": 0.16
      },
      "goal_tension": {
        "count": 4.0,
        "mean": 0.055,
        "std": 0.064,
        "min": 0.0,
        "25%": 0.0,
        "50%": 0.05,
        "75%": 0.105,
        "max": 0.12
      },
      "strategic_contradiction_index": {
        "count": 4.0,
        "mean": 0.066,
        "std": 0.0369,
        "min": 0.024,
        "25%": 0.042,
        "50%": 0.067,
        "75%": 0.091,
        "max": 0.106
      },
      "descriptive_cohesion_index": {
        "count": 4.0,
        "mean": -0.2675,
        "std": 0.7038,
        "min": -0.7517,
        "25%": -0.7028,
        "50%": -0.5402,
        "75%": -0.105,
        "max": 0.762
      },
      "motivational_cohesion_index": {
        "count": 4.0,
        "mean": -0.2117,
        "std": 0.6946,
        "min": -0.6831,
        "25%": -0.6694,
        "50%": -0.4793,
        "75%": -0.0216,
        "max": 0.7948
      },
      "full_cohesion_index": {
        "count": 4.0,
        "mean": -0.1977,
        "std": 0.688,
        "min": -0.7065,
        "25%": -0.633,
        "50%": -0.44,
        "75%": -0.0046,
        "max": 0.7957
      }
    },
    "get_document_rankings": {
      "metric_ranked": "full_cohesion_index",
      "top_ranked": [
        {
          "document_name": "john_mccain_2008_concession.txt",
          "full_cohesion_index": 0.795712224283653
        },
        {
          "document_name": "alexandria_ocasio_cortez_2025_fighting_oligarchy.txt",
          "full_cohesion_index": -0.2713854943659737
        },
        {
          "document_name": "bernie_sanders_2025_fighting_oligarchy.txt",
          "full_cohesion_index": -0.6085633557922191
        },
        {
          "document_name": "steve_king_2017_house_floor.txt",
          "full_cohesion_index": -0.7065096645189958
        }
      ],
      "bottom_ranked": [
        {
          "document_name": "steve_king_2017_house_floor.txt",
          "full_cohesion_index": -0.7065096645189958
        },
        {
          "document_name": "bernie_sanders_2025_fighting_oligarchy.txt",
          "full_cohesion_index": -0.6085633557922191
        },
        {
          "document_name": "alexandria_ocasio_cortez_2025_fighting_oligarchy.txt",
          "full_cohesion_index": -0.2713854943659737
        },
        {
          "document_name": "john_mccain_2008_concession.txt",
          "full_cohesion_index": 0.795712224283653
        }
      ]
    },
    "perform_statistical_analysis": {
      "analysis_metadata": {
        "timestamp": "2025-08-27T16:16:19.677686",
        "sample_size": 4,
        "alpha_level": 0.05,
        "variables_analyzed": [
          "tribal_dominance_raw",
          "tribal_dominance_salience",
          "tribal_dominance_confidence",
          "individual_dignity_raw",
          "individual_dignity_salience",
          "individual_dignity_confidence",
          "fear_raw",
          "fear_salience",
          "fear_confidence",
          "hope_raw",
          "hope_salience",
          "hope_confidence",
          "envy_raw",
          "envy_salience",
          "envy_confidence",
          "compersion_raw",
          "compersion_salience",
          "compersion_confidence",
          "enmity_raw",
          "enmity_salience",
          "enmity_confidence",
          "amity_raw",
          "amity_salience",
          "amity_confidence",
          "fragmentative_goals_raw",
          "fragmentative_goals_salience",
          "fragmentative_goals_confidence",
          "cohesive_goals_raw",
          "cohesive_goals_salience",
          "cohesive_goals_confidence"
        ]
      }
    },
    "run_complete_statistical_analysis": {
      "analysis_metadata": {
        "timestamp": "2025-08-27T16:16:19.680709",
        "sample_size": 4,
        "alpha_level": 0.05,
        "variables_analyzed": [
          "tribal_dominance_raw",
          "tribal_dominance_salience",
          "tribal_dominance_confidence",
          "individual_dignity_raw",
          "individual_dignity_salience",
          "individual_dignity_confidence",
          "fear_raw",
          "fear_salience",
          "fear_confidence",
          "hope_raw",
          "hope_salience",
          "hope_confidence",
          "envy_raw",
          "envy_salience",
          "envy_confidence",
          "compersion_raw",
          "compersion_salience",
          "compersion_confidence",
          "enmity_raw",
          "enmity_salience",
          "enmity_confidence",
          "amity_raw",
          "amity_salience",
          "amity_confidence",
          "fragmentative_goals_raw",
          "fragmentative_goals_salience",
          "fragmentative_goals_confidence",
          "cohesive_goals_raw",
          "cohesive_goals_salience",
          "cohesive_goals_confidence"
        ]
      }
    }
  },
  "status": "success_with_data",
  "validation_passed": true
}