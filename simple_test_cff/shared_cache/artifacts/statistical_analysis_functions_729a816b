{
  "status": "success",
  "functions_generated": 6,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 21451,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: simple_test\nDescription: Statistical analysis experiment\nGenerated: 2025-08-26T01:14:09.611842+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived metrics as specified in the Cohesive Flourishing Framework and adds them as new columns to the DataFrame.\n\n    This function implements the formulas for tension indices, the strategic contradiction index,\n    and the three salience-weighted cohesion indices. It is a foundational function used by\n    many other analysis functions.\n\n    Methodology:\n    - Tension Indices: Calculated using the formula `min(Score_A, Score_B) * |Salience_A - Salience_B|` for each opposing pair.\n    - Strategic Contradiction Index: The arithmetic mean of the five tension indices.\n    - Cohesion Indices: Calculated by summing the salience-weighted scores of cohesive dimensions and subtracting the\n      salience-weighted scores of fragmentative dimensions, then normalizing by the sum of all relevant salience scores\n      (plus a small epsilon to prevent division by zero).\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw and salience scores for the 10 base dimensions.\n        **kwargs: Not used.\n\n    Returns:\n        pd.DataFrame: The original DataFrame with added columns for each derived metric, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = data.copy()\n        epsilon = 0.001\n\n        # Calculate Tension Indices\n        df['identity_tension'] = np.minimum(df['tribal_dominance_raw'], df['individual_dignity_raw']) * np.abs(df['tribal_dominance_salience'] - df['individual_dignity_salience'])\n        df['emotional_tension'] = np.minimum(df['fear_raw'], df['hope_raw']) * np.abs(df['fear_salience'] - df['hope_salience'])\n        df['success_tension'] = np.minimum(df['envy_raw'], df['compersion_raw']) * np.abs(df['envy_salience'] - df['compersion_salience'])\n        df['relational_tension'] = np.minimum(df['enmity_raw'], df['amity_raw']) * np.abs(df['enmity_salience'] - df['amity_salience'])\n        df['goal_tension'] = np.minimum(df['fragmentative_goals_raw'], df['cohesive_goals_raw']) * np.abs(df['fragmentative_goals_salience'] - df['cohesive_goals_salience'])\n\n        # Calculate Strategic Contradiction Index\n        tension_cols = ['identity_tension', 'emotional_tension', 'success_tension', 'relational_tension', 'goal_tension']\n        df['strategic_contradiction_index'] = df[tension_cols].mean(axis=1)\n\n        # Calculate Cohesion Index Components\n        df['identity_cohesion_component'] = (df['individual_dignity_raw'] * df['individual_dignity_salience']) - (df['tribal_dominance_raw'] * df['tribal_dominance_salience'])\n        df['emotional_cohesion_component'] = (df['hope_raw'] * df['hope_salience']) - (df['fear_raw'] * df['fear_salience'])\n        df['success_cohesion_component'] = (df['compersion_raw'] * df['compersion_salience']) - (df['envy_raw'] * df['envy_salience'])\n        df['relational_cohesion_component'] = (df['amity_raw'] * df['amity_salience']) - (df['enmity_raw'] * df['enmity_salience'])\n        df['goal_cohesion_component'] = (df['cohesive_goals_raw'] * df['cohesive_goals_salience']) - (df['fragmentative_goals_raw'] * df['fragmentative_goals_salience'])\n\n        # Calculate Salience Totals for Normalization\n        descriptive_salience_cols = ['hope_salience', 'fear_salience', 'compersion_salience', 'envy_salience', 'amity_salience', 'enmity_salience']\n        motivational_salience_cols = descriptive_salience_cols + ['cohesive_goals_salience', 'fragmentative_goals_salience']\n        full_salience_cols = motivational_salience_cols + ['individual_dignity_salience', 'tribal_dominance_salience']\n\n        df['descriptive_salience_total'] = df[descriptive_salience_cols].sum(axis=1)\n        df['motivational_salience_total'] = df[motivational_salience_cols].sum(axis=1)\n        df['full_salience_total'] = df[full_salience_cols].sum(axis=1)\n\n        # Calculate Final Cohesion Indices\n        df['descriptive_cohesion_index'] = (df['emotional_cohesion_component'] + df['success_cohesion_component'] + df['relational_cohesion_component']) / (df['descriptive_salience_total'] + epsilon)\n        df['motivational_cohesion_index'] = (df['emotional_cohesion_component'] + df['success_cohesion_component'] + df['relational_cohesion_component'] + df['goal_cohesion_component']) / (df['motivational_salience_total'] + epsilon)\n        df['full_cohesion_index'] = (df['identity_cohesion_component'] + df['emotional_cohesion_component'] + df['success_cohesion_component'] + df['relational_cohesion_component'] + df['goal_cohesion_component']) / (df['full_salience_total'] + epsilon)\n        \n        # Clean up intermediate columns\n        df = df.drop(columns=[\n            'identity_cohesion_component', 'emotional_cohesion_component', 'success_cohesion_component',\n            'relational_cohesion_component', 'goal_cohesion_component', 'descriptive_salience_total',\n            'motivational_salience_total', 'full_salience_total'\n        ])\n\n        return df\n\n    except Exception as e:\n        # In a real environment, log the error e\n        return None\n\ndef get_corpus_summary_statistics(data, **kwargs):\n    \"\"\"\n    Provides baseline descriptive statistics for the entire corpus.\n\n    This function first calculates all derived metrics (tensions and cohesion indices)\n    and then computes summary statistics (mean, std, min, 25%, 50%, 75%, max) for all\n    raw scores, salience scores, and derived metrics. This directly addresses the research\n    question regarding baseline scores for the corpus.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for key metrics, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return {\"error\": \"Input data is empty or None.\"}\n\n        # First, calculate all derived metrics\n        metrics_df = calculate_derived_metrics(data)\n        if metrics_df is None:\n            return {\"error\": \"Failed to calculate derived metrics.\"}\n\n        # Define the columns for which to generate statistics\n        raw_scores = [col for col in metrics_df.columns if col.endswith('_raw')]\n        salience_scores = [col for col in metrics_df.columns if col.endswith('_salience')]\n        derived_metrics = [\n            'identity_tension', 'emotional_tension', 'success_tension', 'relational_tension', 'goal_tension',\n            'strategic_contradiction_index', 'descriptive_cohesion_index', 'motivational_cohesion_index',\n            'full_cohesion_index'\n        ]\n        \n        columns_to_summarize = raw_scores + salience_scores + derived_metrics\n        \n        # Ensure all columns exist before summarizing\n        existing_columns = [col for col in columns_to_summarize if col in metrics_df.columns]\n        if not existing_columns:\n            return {\"error\": \"No valid columns found for summary.\"}\n\n        summary_stats = metrics_df[existing_columns].describe().to_dict()\n\n        return summary_stats\n\n    except Exception as e:\n        return None\n\ndef get_dimension_correlation_matrix(data, **kwargs):\n    \"\"\"\n    Calculates the Pearson correlation matrix for all raw and salience scores.\n\n    This analysis helps reveal relationships between the rhetorical dimensions. For example,\n    it can show if the use of 'fear' rhetoric is positively correlated with 'enmity' rhetoric\n    across the corpus. The matrix includes both raw intensity scores and salience scores to\n    provide a comprehensive view of co-occurring rhetorical strategies.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs:\n            method (str): The correlation method to use ('pearson', 'kendall', 'spearman'). Defaults to 'pearson'.\n\n    Returns:\n        dict: A dictionary representing the correlation matrix, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        method = kwargs.get('method', 'pearson')\n        \n        # Select all raw and salience columns for correlation analysis\n        score_columns = [col for col in data.columns if col.endswith('_raw') or col.endswith('_salience')]\n        \n        if len(score_columns) < 2:\n            return {\"error\": \"Not enough score columns to calculate correlation.\"}\n\n        correlation_matrix = data[score_columns].corr(method=method)\n        \n        # Replace NaN with None for JSON compatibility\n        correlation_matrix = correlation_matrix.where(pd.notnull(correlation_matrix), None)\n\n        return correlation_matrix.to_dict('index')\n\n    except Exception as e:\n        return None\n\ndef identify_high_contradiction_documents(data, **kwargs):\n    \"\"\"\n    Identifies documents with the highest levels of rhetorical contradiction.\n\n    This function calculates the 'strategic_contradiction_index' for each document and\n    returns the top documents based on this score. High scores indicate a mixed or\n    incoherent messaging strategy, where a speaker might be using competing appeals\n    simultaneously (e.g., appeals to both hope and fear with differing salience).\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs:\n            top_n (int): The number of top documents to return. Defaults to 10.\n            quantile (float): The quantile to use for defining \"high contradiction\".\n                              Overrides top_n if provided. E.g., 0.9 for top 10%.\n\n    Returns:\n        dict: A dictionary mapping document names to their contradiction scores, sorted descending, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        metrics_df = calculate_derived_metrics(data)\n        if metrics_df is None:\n            return {\"error\": \"Failed to calculate derived metrics.\"}\n\n        if 'strategic_contradiction_index' not in metrics_df.columns:\n            return {\"error\": \"Strategic contradiction index could not be calculated.\"}\n\n        quantile_threshold = kwargs.get('quantile')\n        if quantile_threshold:\n            threshold = metrics_df['strategic_contradiction_index'].quantile(quantile_threshold)\n            high_tension_docs = metrics_df[metrics_df['strategic_contradiction_index'] >= threshold]\n        else:\n            top_n = kwargs.get('top_n', 10)\n            high_tension_docs = metrics_df.nlargest(top_n, 'strategic_contradiction_index')\n\n        result = high_tension_docs[['document_name', 'strategic_contradiction_index', 'full_cohesion_index']].sort_values(\n            by='strategic_contradiction_index', ascending=False\n        ).to_dict('records')\n\n        return {\"high_contradiction_documents\": result}\n\n    except Exception as e:\n        return None\n\ndef analyze_rhetorical_profiles_by_speaker(data, **kwargs):\n    \"\"\"\n    Aggregates rhetorical scores by speaker to create average profiles.\n\n    This function attempts to load speaker metadata from a 'corpus_manifest.json' file in the\n    workspace. If the manifest is found, it maps speaker information to each document and\n    calculates the mean scores for all raw, salience, and derived metrics for each speaker.\n    If the manifest is not found, it handles this gracefully by returning a message, as per\n    system requirements that forbid filename parsing for speaker identification.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary of speaker profiles with their average scores, or a message indicating\n              the corpus manifest was not found, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n    import os\n    import sys\n\n    def _load_speaker_manifest():\n        \"\"\"Helper to load manifest, handling absence gracefully.\"\"\"\n        manifest_path = Path('corpus_manifest.json')\n        if not manifest_path.exists():\n            # As per requirements, handle missing manifest gracefully.\n            # A warning could be logged to stderr for the user.\n            print(\"Warning: corpus_manifest.json not found. Cannot perform speaker analysis.\", file=sys.stderr)\n            return None\n        try:\n            with open(manifest_path, 'r') as f:\n                manifest_data = json.load(f)\n            # Assuming manifest is a list of dicts with 'filename' and 'speaker' keys\n            return {item['filename']: item for item in manifest_data}\n        except (json.JSONDecodeError, KeyError, TypeError):\n            print(\"Error: corpus_manifest.json is malformed.\", file=sys.stderr)\n            return None\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        speaker_metadata = _load_speaker_manifest()\n        if speaker_metadata is None:\n            return {\n                \"status\": \"SKIPPED\",\n                \"reason\": \"Speaker analysis requires a 'corpus_manifest.json' file which was not found in the workspace.\"\n            }\n\n        metrics_df = calculate_derived_metrics(data)\n        if metrics_df is None:\n            return {\"error\": \"Failed to calculate derived metrics.\"}\n\n        # Map speaker data using the manifest\n        metrics_df['speaker'] = metrics_df['document_name'].map(lambda x: speaker_metadata.get(x, {}).get('speaker', 'Unknown'))\n        \n        if 'Unknown' in metrics_df['speaker'].unique():\n            print(f\"Warning: Some documents could not be mapped to a speaker via the manifest.\", file=sys.stderr)\n\n        # Exclude documents that couldn't be mapped to a known speaker\n        speaker_df = metrics_df[metrics_df['speaker'] != 'Unknown']\n        if speaker_df.empty:\n            return {\n                \"status\": \"FAILED\",\n                \"reason\": \"No documents could be mapped to speakers using the provided manifest.\"\n            }\n\n        # Group by speaker and calculate mean scores\n        profile_cols = [col for col in speaker_df.columns if '_raw' in col or '_salience' in col or '_index' in col or '_tension' in col]\n        speaker_profiles = speaker_df.groupby('speaker')[profile_cols].mean()\n\n        return speaker_profiles.to_dict('index')\n\n    except Exception as e:\n        return None\n\ndef get_cohesion_vs_contradiction_data(data, **kwargs):\n    \"\"\"\n    Generates data for plotting the Full Cohesion Index against the Strategic Contradiction Index.\n\n    This analysis is key to understanding the trade-offs in rhetorical strategies. It helps visualize\n    whether documents tend to be coherently cohesive (high cohesion, low contradiction), coherently\n    fragmentative (low cohesion, low contradiction), or strategically complex/incoherent (high contradiction).\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing a list of data points, each with document name, cohesion score,\n              and contradiction score. Returns None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        metrics_df = calculate_derived_metrics(data)\n        if metrics_df is None:\n            return {\"error\": \"Failed to calculate derived metrics.\"}\n\n        required_cols = ['document_name', 'full_cohesion_index', 'strategic_contradiction_index']\n        if not all(col in metrics_df.columns for col in required_cols):\n            return {\"error\": \"Required columns for analysis are missing.\"}\n\n        plot_data = metrics_df[required_cols].to_dict('records')\n\n        return {\"plot_data_points\": plot_data}\n\n    except Exception as e:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}