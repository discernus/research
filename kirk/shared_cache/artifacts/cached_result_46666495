{
  "status": "success",
  "functions_generated": 13,
  "output_file": "automatedderivedmetricsagent_functions.py",
  "module_size": 19822,
  "function_code_content": "[\n  {\n    \"filename\": \"cff_calculator.py\",\n    \"content\": \"import pandas as pd\\nimport numpy as np\\nimport json\\nfrom typing import Optional, Dict, Any, List\\n\\ndef _get_dimensional_scores(row: pd.Series) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Parses the JSON content from the 'raw_analysis_response' field of a DataFrame row.\\n\\n    It handles multiple possible formats for the JSON string:\\n    1. The proprietary format: <<<DISCERNUS_ANALYSIS_JSON_v6>>>...<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>\\n    2. A markdown code block: ```json...```\\n    3. A plain JSON string.\\n\\n    Args:\\n        row: A pandas Series representing a row of the DataFrame.\\n\\n    Returns:\\n        A dictionary of dimensional scores for the first document analysis, or None if parsing fails\\n        or the structure is invalid.\\n    \\\"\\\"\\\"\\n    try:\\n        # Access the nested field containing the raw response string\\n        raw_response = row.get('analysis_result', {}).get('result_content', {}).get('raw_analysis_response')\\n        if not isinstance(raw_response, str):\\n            return None\\n\\n        json_content = \\\"\\\"\\n        start_marker = '<<<DISCERNUS_ANALYSIS_JSON_v6>>>'\\n        end_marker = '<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>'\\n\\n        if start_marker in raw_response and end_marker in raw_response:\\n            start_idx = raw_response.find(start_marker) + len(start_marker)\\n            end_idx = raw_response.find(end_marker)\\n            json_content = raw_response[start_idx:end_idx].strip()\\n        elif raw_response.strip().startswith(\\\"```json\\\"):\\n            # Handles markdown code blocks like ```json\\\\n{...}\\\\n```\\n            json_content = '\\\\n'.join(raw_response.strip().split('\\\\n')[1:-1])\\n        elif raw_response.strip().startswith(\\\"{\\\"):\\n            json_content = raw_response.strip()\\n        else:\\n            return None\\n\\n        analysis = json.loads(json_content)\\n\\n        # Extract dimensional_scores from the nested structure\\n        if 'document_analyses' in analysis and isinstance(analysis['document_analyses'], list) and len(analysis['document_analyses']) > 0:\\n            scores = analysis['document_analyses'][0].get('dimensional_scores')\\n            if isinstance(scores, dict):\\n                return scores\\n\\n        return None\\n    except (KeyError, IndexError, TypeError, json.JSONDecodeError, AttributeError):\\n        return None\\n\\ndef calculate_identity_tension(dimensional_scores: Dict[str, Any], **kwargs) -> Optional[float]:\\n    \\\"\\\"\\\"\\n    Calculates the Identity Tension index.\\n    Measures contradiction between tribal dominance and individual dignity appeals.\\n    Formula: min(tribal_dominance_score, individual_dignity_score) * abs(tribal_dominance_salience - individual_dignity_salience)\\n\\n    Args:\\n        dimensional_scores: A dictionary containing the scores and saliences for all dimensions.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        The calculated tension score as a float, or None if required data is missing.\\n    \\\"\\\"\\\"\\n    try:\\n        td_score = float(dimensional_scores['tribal_dominance']['raw_score'])\\n        td_salience = float(dimensional_scores['tribal_dominance']['salience'])\\n        id_score = float(dimensional_scores['individual_dignity']['raw_score'])\\n        id_salience = float(dimensional_scores['individual_dignity']['salience'])\\n\\n        tension = min(td_score, id_score) * abs(td_salience - id_salience)\\n        return tension\\n    except (KeyError, TypeError, ValueError):\\n        return None\\n\\ndef calculate_emotional_tension(dimensional_scores: Dict[str, Any], **kwargs) -> Optional[float]:\\n    \\\"\\\"\\\"\\n    Calculates the Emotional Tension index.\\n    Measures contradiction between fear and hope messaging.\\n    Formula: min(fear_score, hope_score) * abs(fear_salience - hope_salience)\\n\\n    Args:\\n        dimensional_scores: A dictionary containing the scores and saliences for all dimensions.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        The calculated tension score as a float, or None if required data is missing.\\n    \\\"\\\"\\\"\\n    try:\\n        fear_score = float(dimensional_scores['fear']['raw_score'])\\n        fear_salience = float(dimensional_scores['fear']['salience'])\\n        hope_score = float(dimensional_scores['hope']['raw_score'])\\n        hope_salience = float(dimensional_scores['hope']['salience'])\\n\\n        tension = min(fear_score, hope_score) * abs(fear_salience - hope_salience)\\n        return tension\\n    except (KeyError, TypeError, ValueError):\\n        return None\\n\\ndef calculate_success_tension(dimensional_scores: Dict[str, Any], **kwargs) -> Optional[float]:\\n    \\\"\\\"\\\"\\n    Calculates the Success Tension index.\\n    Measures contradiction between envy and mudita rhetoric.\\n    Formula: min(envy_score, mudita_score) * abs(envy_salience - mudita_salience)\\n\\n    Args:\\n        dimensional_scores: A dictionary containing the scores and saliences for all dimensions.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        The calculated tension score as a float, or None if required data is missing.\\n    \\\"\\\"\\\"\\n    try:\\n        envy_score = float(dimensional_scores['envy']['raw_score'])\\n        envy_salience = float(dimensional_scores['envy']['salience'])\\n        mudita_score = float(dimensional_scores['mudita']['raw_score'])\\n        mudita_salience = float(dimensional_scores['mudita']['salience'])\\n\\n        tension = min(envy_score, mudita_score) * abs(envy_salience - mudita_salience)\\n        return tension\\n    except (KeyError, TypeError, ValueError):\\n        return None\\n\\ndef calculate_relational_tension(dimensional_scores: Dict[str, Any], **kwargs) -> Optional[float]:\\n    \\\"\\\"\\\"\\n    Calculates the Relational Tension index.\\n    Measures contradiction between enmity and amity positioning.\\n    Formula: min(enmity_score, amity_score) * abs(enmity_salience - amity_salience)\\n\\n    Args:\\n        dimensional_scores: A dictionary containing the scores and saliences for all dimensions.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        The calculated tension score as a float, or None if required data is missing.\\n    \\\"\\\"\\\"\\n    try:\\n        enmity_score = float(dimensional_scores['enmity']['raw_score'])\\n        enmity_salience = float(dimensional_scores['enmity']['salience'])\\n        amity_score = float(dimensional_scores['amity']['raw_score'])\\n        amity_salience = float(dimensional_scores['amity']['salience'])\\n\\n        tension = min(enmity_score, amity_score) * abs(enmity_salience - amity_salience)\\n        return tension\\n    except (KeyError, TypeError, ValueError):\\n        return None\\n\\ndef calculate_goal_tension(dimensional_scores: Dict[str, Any], **kwargs) -> Optional[float]:\\n    \\\"\\\"\\\"\\n    Calculates the Goal Tension index.\\n    Measures contradiction between fragmentative and cohesive objectives.\\n    Formula: min(fragmentative_goals_score, cohesive_goals_score) * abs(fragmentative_goals_salience - cohesive_goals_salience)\\n\\n    Args:\\n        dimensional_scores: A dictionary containing the scores and saliences for all dimensions.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        The calculated tension score as a float, or None if required data is missing.\\n    \\\"\\\"\\\"\\n    try:\\n        frag_score = float(dimensional_scores['fragmentative_goals']['raw_score'])\\n        frag_salience = float(dimensional_scores['fragmentative_goals']['salience'])\\n        coh_score = float(dimensional_scores['cohesive_goals']['raw_score'])\\n        coh_salience = float(dimensional_scores['cohesive_goals']['salience'])\\n\\n        tension = min(frag_score, coh_score) * abs(frag_salience - coh_salience)\\n        return tension\\n    except (KeyError, TypeError, ValueError):\\n        return None\\n\\ndef calculate_strategic_contradiction_index(dimensional_scores: Dict[str, Any], **kwargs) -> Optional[float]:\\n    \\\"\\\"\\\"\\n    Calculates the Strategic Contradiction Index.\\n    This is the average of all tension indices, measuring overall rhetorical coherence.\\n    Formula: (identity_tension + emotional_tension + success_tension + relational_tension + goal_tension) / 5\\n\\n    Args:\\n        dimensional_scores: A dictionary containing the scores and saliences for all dimensions.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        The calculated index as a float, or None if any component tension cannot be calculated.\\n    \\\"\\\"\\\"\\n    tensions = [\\n        calculate_identity_tension(dimensional_scores),\\n        calculate_emotional_tension(dimensional_scores),\\n        calculate_success_tension(dimensional_scores),\\n        calculate_relational_tension(dimensional_scores),\\n        calculate_goal_tension(dimensional_scores)\\n    ]\\n\\n    if any(t is None for t in tensions):\\n        return None\\n\\n    return sum(tensions) / 5.0\\n\\ndef calculate_descriptive_cohesion_index(dimensional_scores: Dict[str, Any], **kwargs) -> Optional[float]:\\n    \\\"\\\"\\\"\\n    Calculates the Descriptive Cohesion Index.\\n    Measures immediate emotional and relational climate using salience weighting.\\n    Formula: ((hope_score * hope_salience - fear_score * fear_salience) +\\n              (mudita_score * mudita_salience - envy_score * envy_salience) +\\n              (amity_score * amity_salience - enmity_score * enmity_salience)) /\\n             (sum of saliences for these 6 dimensions + 0.001)\\n\\n    Args:\\n        dimensional_scores: A dictionary containing the scores and saliences for all dimensions.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        The calculated index as a float, or None if required data is missing.\\n    \\\"\\\"\\\"\\n    try:\\n        hope_s = float(dimensional_scores['hope']['raw_score'])\\n        hope_sal = float(dimensional_scores['hope']['salience'])\\n        fear_s = float(dimensional_scores['fear']['raw_score'])\\n        fear_sal = float(dimensional_scores['fear']['salience'])\\n\\n        mudita_s = float(dimensional_scores['mudita']['raw_score'])\\n        mudita_sal = float(dimensional_scores['mudita']['salience'])\\n        envy_s = float(dimensional_scores['envy']['raw_score'])\\n        envy_sal = float(dimensional_scores['envy']['salience'])\\n\\n        amity_s = float(dimensional_scores['amity']['raw_score'])\\n        amity_sal = float(dimensional_scores['amity']['salience'])\\n        enmity_s = float(dimensional_scores['enmity']['raw_score'])\\n        enmity_sal = float(dimensional_scores['enmity']['salience'])\\n\\n        numerator = (hope_s * hope_sal - fear_s * fear_sal) + \\\\\\n                    (mudita_s * mudita_sal - envy_s * envy_sal) + \\\\\\n                    (amity_s * amity_sal - enmity_s * enmity_sal)\\n\\n        denominator = hope_sal + fear_sal + mudita_sal + envy_sal + amity_sal + enmity_sal + 0.001\\n\\n        if abs(denominator - 0.001) < 1e-9:  # All saliences were zero\\n            return 0.0\\n\\n        return numerator / denominator\\n    except (KeyError, TypeError, ValueError):\\n        return None\\n\\ndef calculate_motivational_cohesion_index(dimensional_scores: Dict[str, Any], **kwargs) -> Optional[float]:\\n    \\\"\\\"\\\"\\n    Calculates the Motivational Cohesion Index.\\n    Assesses likely behavioral consequences with salience-adjusted emphasis.\\n    Formula: (Descriptive Numerator + (cohesive_goals_score * cohesive_goals_salience - fragmentative_goals_score * fragmentative_goals_salience)) /\\n             (sum of saliences for these 8 dimensions + 0.001)\\n\\n    Args:\\n        dimensional_scores: A dictionary containing the scores and saliences for all dimensions.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        The calculated index as a float, or None if required data is missing.\\n    \\\"\\\"\\\"\\n    try:\\n        hope_s = float(dimensional_scores['hope']['raw_score'])\\n        hope_sal = float(dimensional_scores['hope']['salience'])\\n        fear_s = float(dimensional_scores['fear']['raw_score'])\\n        fear_sal = float(dimensional_scores['fear']['salience'])\\n\\n        mudita_s = float(dimensional_scores['mudita']['raw_score'])\\n        mudita_sal = float(dimensional_scores['mudita']['salience'])\\n        envy_s = float(dimensional_scores['envy']['raw_score'])\\n        envy_sal = float(dimensional_scores['envy']['salience'])\\n\\n        amity_s = float(dimensional_scores['amity']['raw_score'])\\n        amity_sal = float(dimensional_scores['amity']['salience'])\\n        enmity_s = float(dimensional_scores['enmity']['raw_score'])\\n        enmity_sal = float(dimensional_scores['enmity']['salience'])\\n\\n        coh_g_s = float(dimensional_scores['cohesive_goals']['raw_score'])\\n        coh_g_sal = float(dimensional_scores['cohesive_goals']['salience'])\\n        frag_g_s = float(dimensional_scores['fragmentative_goals']['raw_score'])\\n        frag_g_sal = float(dimensional_scores['fragmentative_goals']['salience'])\\n\\n        numerator = (hope_s * hope_sal - fear_s * fear_sal) + \\\\\\n                    (mudita_s * mudita_sal - envy_s * envy_sal) + \\\\\\n                    (amity_s * amity_sal - enmity_s * enmity_sal) + \\\\\\n                    (coh_g_s * coh_g_sal - frag_g_s * frag_g_sal)\\n\\n        denominator = hope_sal + fear_sal + mudita_sal + envy_sal + amity_sal + enmity_sal + coh_g_sal + frag_g_sal + 0.001\\n\\n        if abs(denominator - 0.001) < 1e-9: # All saliences were zero\\n            return 0.0\\n\\n        return numerator / denominator\\n    except (KeyError, TypeError, ValueError):\\n        return None\\n\\ndef calculate_full_cohesion_index(dimensional_scores: Dict[str, Any], **kwargs) -> Optional[float]:\\n    \\\"\\\"\\\"\\n    Calculates the Full Cohesion Index.\\n    Provides a comprehensive evaluation of democratic health and social cohesion.\\n    Formula: (Motivational Numerator + (individual_dignity_score * individual_dignity_salience - tribal_dominance_score * tribal_dominance_salience)) /\\n             (sum of saliences for all 10 dimensions + 0.001)\\n\\n    Args:\\n        dimensional_scores: A dictionary containing the scores and saliences for all dimensions.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        The calculated index as a float, or None if required data is missing.\\n    \\\"\\\"\\\"\\n    try:\\n        hope_s = float(dimensional_scores['hope']['raw_score'])\\n        hope_sal = float(dimensional_scores['hope']['salience'])\\n        fear_s = float(dimensional_scores['fear']['raw_score'])\\n        fear_sal = float(dimensional_scores['fear']['salience'])\\n\\n        mudita_s = float(dimensional_scores['mudita']['raw_score'])\\n        mudita_sal = float(dimensional_scores['mudita']['salience'])\\n        envy_s = float(dimensional_scores['envy']['raw_score'])\\n        envy_sal = float(dimensional_scores['envy']['salience'])\\n\\n        amity_s = float(dimensional_scores['amity']['raw_score'])\\n        amity_sal = float(dimensional_scores['amity']['salience'])\\n        enmity_s = float(dimensional_scores['enmity']['raw_score'])\\n        enmity_sal = float(dimensional_scores['enmity']['salience'])\\n\\n        coh_g_s = float(dimensional_scores['cohesive_goals']['raw_score'])\\n        coh_g_sal = float(dimensional_scores['cohesive_goals']['salience'])\\n        frag_g_s = float(dimensional_scores['fragmentative_goals']['raw_score'])\\n        frag_g_sal = float(dimensional_scores['fragmentative_goals']['salience'])\\n\\n        id_s = float(dimensional_scores['individual_dignity']['raw_score'])\\n        id_sal = float(dimensional_scores['individual_dignity']['salience'])\\n        td_s = float(dimensional_scores['tribal_dominance']['raw_score'])\\n        td_sal = float(dimensional_scores['tribal_dominance']['salience'])\\n\\n        numerator = (id_s * id_sal - td_s * td_sal) + \\\\\\n                    (hope_s * hope_sal - fear_s * fear_sal) + \\\\\\n                    (mudita_s * mudita_sal - envy_s * envy_sal) + \\\\\\n                    (amity_s * amity_sal - enmity_s * enmity_sal) + \\\\\\n                    (coh_g_s * coh_g_sal - frag_g_s * frag_g_sal)\\n\\n        denominator = id_sal + td_sal + hope_sal + fear_sal + mudita_sal + envy_sal + amity_sal + enmity_sal + coh_g_sal + frag_g_sal + 0.001\\n\\n        if abs(denominator - 0.001) < 1e-9: # All saliences were zero\\n            return 0.0\\n\\n        return numerator / denominator\\n    except (KeyError, TypeError, ValueError):\\n        return None\\n\\ndef calculate_all_derived_metrics(dimensional_scores: Dict[str, Any], **kwargs) -> Dict[str, Optional[float]]:\\n    \\\"\\\"\\\"\\n    Calculates all derived metrics from the Cohesive Flourishing Framework for a single data point.\\n\\n    This function calls each individual metric calculation function directly to avoid\\n    issues with dynamic module loading and function discovery via `inspect`.\\n\\n    Args:\\n        dimensional_scores: A dictionary containing the scores and saliences for all dimensions.\\n        **kwargs: Additional parameters (unused).\\n\\n    Returns:\\n        A dictionary mapping each derived metric's name to its calculated value.\\n    \\\"\\\"\\\"\\n    if not dimensional_scores:\\n        return {\\n            'identity_tension': None,\\n            'emotional_tension': None,\\n            'success_tension': None,\\n            'relational_tension': None,\\n            'goal_tension': None,\\n            'strategic_contradiction_index': None,\\n            'descriptive_cohesion_index': None,\\n            'motivational_cohesion_index': None,\\n            'full_cohesion_index': None\\n        }\\n\\n    results = {\\n        'identity_tension': calculate_identity_tension(dimensional_scores),\\n        'emotional_tension': calculate_emotional_tension(dimensional_scores),\\n        'success_tension': calculate_success_tension(dimensional_scores),\\n        'relational_tension': calculate_relational_tension(dimensional_scores),\\n        'goal_tension': calculate_goal_tension(dimensional_scores),\\n        'strategic_contradiction_index': calculate_strategic_contradiction_index(dimensional_scores),\\n        'descriptive_cohesion_index': calculate_descriptive_cohesion_index(dimensional_scores),\\n        'motivational_cohesion_index': calculate_motivational_cohesion_index(dimensional_scores),\\n        'full_cohesion_index': calculate_full_cohesion_index(dimensional_scores)\\n    }\\n    return results\\n\\ndef calculate_derived_metrics(data: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"\\n    Applies all CFF derived metric calculations to a DataFrame of analysis results.\\n\\n    This function iterates through each row of the input DataFrame, parses the\\n    analysis result, calculates all derived metrics, and appends them as new columns.\\n\\n    Args:\\n        data: A pandas DataFrame containing analysis results, with a column structure\\n              matching the framework specification (e.g., containing 'analysis_result').\\n\\n    Returns:\\n        A new pandas DataFrame with the original data plus new columns for each\\n        calculated derived metric.\\n    \\\"\\\"\\\"\\n    if 'analysis_result' not in data.columns:\\n        raise ValueError(\\\"Input DataFrame must contain 'analysis_result' column.\\\")\\n\\n    df = data.copy()\\n\\n    def process_row(row):\\n        dimensional_scores = _get_dimensional_scores(row)\\n        return calculate_all_derived_metrics(dimensional_scores)\\n\\n    # Apply the processing function to each row and create a temporary DataFrame from the results\\n    derived_metrics_series = df.apply(process_row, axis=1)\\n    derived_metrics_df = pd.json_normalize(derived_metrics_series)\\n\\n    # Ensure the new DataFrame has the same index as the original to join correctly\\n    derived_metrics_df.index = df.index\\n\\n    # Join the new metric columns to the copied DataFrame\\n    df = df.join(derived_metrics_df)\\n\\n    return df\\n\"\n  }\n]",
  "cached_with_code": true,
  "cache_metadata": {
    "cache_key": "derived_metrics_6e23402919a3",
    "cached_at": "2025-01-15T14:30:00Z",
    "agent_name": "DerivedMetricsPhase"
  }
}