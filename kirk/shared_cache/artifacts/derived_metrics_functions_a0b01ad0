{
  "status": "success",
  "functions_generated": 13,
  "output_file": "automatedderivedmetricsagent_functions.py",
  "module_size": 14479,
  "function_code_content": "import pandas as pd\nimport numpy as np\nimport json\nfrom typing import Optional, Dict, Any, List\n\ndef _extract_scores(row: pd.Series) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Parses the raw_analysis_response JSON string from a DataFrame row to extract dimensional scores.\n\n    This function navigates the nested data structure as specified in the problem description,\n    extracts the JSON content from the 'raw_analysis_response' field, and returns the\n    'dimensional_scores' dictionary from the first document analysis.\n\n    Args:\n        row: A pandas Series representing a single row of the DataFrame.\n\n    Returns:\n        A dictionary containing the dimensional scores, or None if parsing fails or data is missing.\n    \"\"\"\n    try:\n        raw_response = row['analysis_result']['result_content']['raw_analysis_response']\n        if not isinstance(raw_response, str):\n            return None\n    except (KeyError, TypeError):\n        return None\n\n    start_marker = '<<<DISCERNUS_ANALYSIS_JSON_v6>>>'\n    end_marker = '<<<END_DISCERNUS_ANALYSIS_JSON_v6>>>'\n\n    start_idx = raw_response.find(start_marker)\n    end_idx = raw_response.rfind(end_marker)\n\n    if start_idx != -1:\n        json_content = raw_response[start_idx + len(start_marker):end_idx if end_idx != -1 else None].strip()\n        if not json_content:\n            return None\n        try:\n            analysis = json.loads(json_content)\n            if 'document_analyses' in analysis and isinstance(analysis.get('document_analyses'), list) and analysis['document_analyses']:\n                doc_analysis = analysis['document_analyses'][0]\n                if 'dimensional_scores' in doc_analysis and isinstance(doc_analysis['dimensional_scores'], dict):\n                    return doc_analysis['dimensional_scores']\n        except (json.JSONDecodeError, KeyError, TypeError, IndexError):\n            return None\n\n    return None\n\ndef calculate_identity_tension(scores: Dict[str, Any]) -> Optional[float]:\n    \"\"\"\n    Calculates Identity Tension, measuring contradiction between tribal dominance and individual dignity appeals.\n\n    Formula: min(tribal_dominance_score, individual_dignity_score) * abs(tribal_dominance_salience - individual_dignity_salience)\n\n    Args:\n        scores: A dictionary of dimensional scores for a single document.\n\n    Returns:\n        The calculated tension score as a float, or None if required scores are missing.\n    \"\"\"\n    try:\n        td_score = scores['tribal_dominance']['raw_score']\n        id_score = scores['individual_dignity']['raw_score']\n        td_salience = scores['tribal_dominance']['salience']\n        id_salience = scores['individual_dignity']['salience']\n        return min(td_score, id_score) * abs(td_salience - id_salience)\n    except (KeyError, TypeError):\n        return None\n\ndef calculate_emotional_tension(scores: Dict[str, Any]) -> Optional[float]:\n    \"\"\"\n    Calculates Emotional Tension, measuring contradiction between fear and hope messaging.\n\n    Formula: min(fear_score, hope_score) * abs(fear_salience - hope_salience)\n\n    Args:\n        scores: A dictionary of dimensional scores for a single document.\n\n    Returns:\n        The calculated tension score as a float, or None if required scores are missing.\n    \"\"\"\n    try:\n        fear_score = scores['fear']['raw_score']\n        hope_score = scores['hope']['raw_score']\n        fear_salience = scores['fear']['salience']\n        hope_salience = scores['hope']['salience']\n        return min(fear_score, hope_score) * abs(fear_salience - hope_salience)\n    except (KeyError, TypeError):\n        return None\n\n\ndef calculate_success_tension(scores: Dict[str, Any]) -> Optional[float]:\n    \"\"\"\n    Calculates Success Tension, measuring contradiction between envy and mudita rhetoric.\n\n    Formula: min(envy_score, mudita_score) * abs(envy_salience - mudita_salience)\n\n    Args:\n        scores: A dictionary of dimensional scores for a single document.\n\n    Returns:\n        The calculated tension score as a float, or None if required scores are missing.\n    \"\"\"\n    try:\n        envy_score = scores['envy']['raw_score']\n        mudita_score = scores['mudita']['raw_score']\n        envy_salience = scores['envy']['salience']\n        mudita_salience = scores['mudita']['salience']\n        return min(envy_score, mudita_score) * abs(envy_salience - mudita_salience)\n    except (KeyError, TypeError):\n        return None\n\n\ndef calculate_relational_tension(scores: Dict[str, Any]) -> Optional[float]:\n    \"\"\"\n    Calculates Relational Tension, measuring contradiction between enmity and amity positioning.\n\n    Formula: min(enmity_score, amity_score) * abs(enmity_salience - amity_salience)\n\n    Args:\n        scores: A dictionary of dimensional scores for a single document.\n\n    Returns:\n        The calculated tension score as a float, or None if required scores are missing.\n    \"\"\"\n    try:\n        enmity_score = scores['enmity']['raw_score']\n        amity_score = scores['amity']['raw_score']\n        enmity_salience = scores['enmity']['salience']\n        amity_salience = scores['amity']['salience']\n        return min(enmity_score, amity_score) * abs(enmity_salience - amity_salience)\n    except (KeyError, TypeError):\n        return None\n\n\ndef calculate_goal_tension(scores: Dict[str, Any]) -> Optional[float]:\n    \"\"\"\n    Calculates Goal Tension, measuring contradiction between fragmentative and cohesive objectives.\n\n    Formula: min(fragmentative_goals_score, cohesive_goals_score) * abs(fragmentative_goals_salience - cohesive_goals_salience)\n\n    Args:\n        scores: A dictionary of dimensional scores for a single document.\n\n    Returns:\n        The calculated tension score as a float, or None if required scores are missing.\n    \"\"\"\n    try:\n        frag_score = scores['fragmentative_goals']['raw_score']\n        cohe_score = scores['cohesive_goals']['raw_score']\n        frag_salience = scores['fragmentative_goals']['salience']\n        cohe_salience = scores['cohesive_goals']['salience']\n        return min(frag_score, cohe_score) * abs(frag_salience - cohe_salience)\n    except (KeyError, TypeError):\n        return None\n\n\ndef calculate_strategic_contradiction_index(scores: Dict[str, Any]) -> Optional[float]:\n    \"\"\"\n    Calculates the Strategic Contradiction Index, the average of all tension indices.\n\n    Formula: (identity_tension + emotional_tension + success_tension + relational_tension + goal_tension) / 5\n\n    Args:\n        scores: A dictionary of dimensional scores for a single document.\n\n    Returns:\n        The calculated index as a float, or None if any component tension is missing.\n    \"\"\"\n    tensions = [\n        calculate_identity_tension(scores),\n        calculate_emotional_tension(scores),\n        calculate_success_tension(scores),\n        calculate_relational_tension(scores),\n        calculate_goal_tension(scores)\n    ]\n\n    if any(t is None for t in tensions):\n        return None\n\n    return sum(t for t in tensions if t is not None) / 5.0\n\n\ndef calculate_descriptive_cohesion_index(scores: Dict[str, Any]) -> Optional[float]:\n    \"\"\"\n    Calculates the Descriptive Cohesion Index, measuring immediate emotional and relational climate.\n\n    Formula: ((hope_s*hs - fear_s*fs) + (mudita_s*ms - envy_s*es) + (amity_s*as - enmity_s*ens)) / (sum of relevant saliences + 0.001)\n\n    Args:\n        scores: A dictionary of dimensional scores for a single document.\n\n    Returns:\n        The calculated index as a float, or None if required scores are missing.\n    \"\"\"\n    try:\n        numerator = (\n            (scores['hope']['raw_score'] * scores['hope']['salience']) -\n            (scores['fear']['raw_score'] * scores['fear']['salience']) +\n            (scores['mudita']['raw_score'] * scores['mudita']['salience']) -\n            (scores['envy']['raw_score'] * scores['envy']['salience']) +\n            (scores['amity']['raw_score'] * scores['amity']['salience']) -\n            (scores['enmity']['raw_score'] * scores['enmity']['salience'])\n        )\n        denominator = (\n            scores['hope']['salience'] + scores['fear']['salience'] +\n            scores['mudita']['salience'] + scores['envy']['salience'] +\n            scores['amity']['salience'] + scores['enmity']['salience'] + 0.001\n        )\n        return numerator / denominator\n    except (KeyError, TypeError):\n        return None\n\n\ndef calculate_motivational_cohesion_index(scores: Dict[str, Any]) -> Optional[float]:\n    \"\"\"\n    Calculates the Motivational Cohesion Index, assessing likely behavioral consequences.\n\n    Formula: (descriptive_numerator + (cohesive_goals_s*cs - fragmentative_goals_s*fs)) / (sum of relevant saliences + 0.001)\n\n    Args:\n        scores: A dictionary of dimensional scores for a single document.\n\n    Returns:\n        The calculated index as a float, or None if required scores are missing.\n    \"\"\"\n    try:\n        numerator = (\n            (scores['hope']['raw_score'] * scores['hope']['salience']) -\n            (scores['fear']['raw_score'] * scores['fear']['salience']) +\n            (scores['mudita']['raw_score'] * scores['mudita']['salience']) -\n            (scores['envy']['raw_score'] * scores['envy']['salience']) +\n            (scores['amity']['raw_score'] * scores['amity']['salience']) -\n            (scores['enmity']['raw_score'] * scores['enmity']['salience']) +\n            (scores['cohesive_goals']['raw_score'] * scores['cohesive_goals']['salience']) -\n            (scores['fragmentative_goals']['raw_score'] * scores['fragmentative_goals']['salience'])\n        )\n        denominator = (\n            scores['hope']['salience'] + scores['fear']['salience'] +\n            scores['mudita']['salience'] + scores['envy']['salience'] +\n            scores['amity']['salience'] + scores['enmity']['salience'] +\n            scores['cohesive_goals']['salience'] + scores['fragmentative_goals']['salience'] + 0.001\n        )\n        return numerator / denominator\n    except (KeyError, TypeError):\n        return None\n\n\ndef calculate_full_cohesion_index(scores: Dict[str, Any]) -> Optional[float]:\n    \"\"\"\n    Calculates the Full Cohesion Index, a comprehensive evaluation of democratic health.\n\n    Formula: (motivational_numerator + (dignity_s*ds - dominance_s*ts)) / (sum of all saliences + 0.001)\n\n    Args:\n        scores: A dictionary of dimensional scores for a single document.\n\n    Returns:\n        The calculated index as a float, or None if required scores are missing.\n    \"\"\"\n    try:\n        numerator = (\n            (scores['individual_dignity']['raw_score'] * scores['individual_dignity']['salience']) -\n            (scores['tribal_dominance']['raw_score'] * scores['tribal_dominance']['salience']) +\n            (scores['hope']['raw_score'] * scores['hope']['salience']) -\n            (scores['fear']['raw_score'] * scores['fear']['salience']) +\n            (scores['mudita']['raw_score'] * scores['mudita']['salience']) -\n            (scores['envy']['raw_score'] * scores['envy']['salience']) +\n            (scores['amity']['raw_score'] * scores['amity']['salience']) -\n            (scores['enmity']['raw_score'] * scores['enmity']['salience']) +\n            (scores['cohesive_goals']['raw_score'] * scores['cohesive_goals']['salience']) -\n            (scores['fragmentative_goals']['raw_score'] * scores['fragmentative_goals']['salience'])\n        )\n        denominator = (\n            scores['individual_dignity']['salience'] + scores['tribal_dominance']['salience'] +\n            scores['hope']['salience'] + scores['fear']['salience'] +\n            scores['mudita']['salience'] + scores['envy']['salience'] +\n            scores['amity']['salience'] + scores['enmity']['salience'] +\n            scores['cohesive_goals']['salience'] + scores['fragmentative_goals']['salience'] + 0.001\n        )\n        return numerator / denominator\n    except (KeyError, TypeError):\n        return None\n\n\ndef calculate_all_derived_metrics(scores: Dict[str, Any]) -> Dict[str, Optional[float]]:\n    \"\"\"\n    Calculates all derived metrics from a dictionary of dimensional scores.\n\n    This function calls each individual metric calculation function by name, as requested,\n    and returns a dictionary of the results.\n\n    Args:\n        scores: A dictionary of dimensional scores for a single document.\n\n    Returns:\n        A dictionary where keys are the metric names and values are the calculated scores.\n    \"\"\"\n    return {\n        \"identity_tension\": calculate_identity_tension(scores),\n        \"emotional_tension\": calculate_emotional_tension(scores),\n        \"success_tension\": calculate_success_tension(scores),\n        \"relational_tension\": calculate_relational_tension(scores),\n        \"goal_tension\": calculate_goal_tension(scores),\n        \"strategic_contradiction_index\": calculate_strategic_contradiction_index(scores),\n        \"descriptive_cohesion_index\": calculate_descriptive_cohesion_index(scores),\n        \"motivational_cohesion_index\": calculate_motivational_cohesion_index(scores),\n        \"full_cohesion_index\": calculate_full_cohesion_index(scores),\n    }\n\n\ndef calculate_derived_metrics(data: pd.DataFrame, **kwargs) -> pd.DataFrame:\n    \"\"\"\n    Calculates all derived metrics and adds them as new columns to the input DataFrame.\n\n    This function iterates over each row of the DataFrame, extracts the dimensional scores,\n    calculates all derived metrics, and appends them as new columns. It handles missing\n    or malformed data gracefully by inserting np.nan for the affected rows.\n\n    Args:\n        data: A pandas DataFrame containing the analysis results, including the\n              'raw_analysis_response' column.\n        **kwargs: Additional keyword arguments (not used).\n\n    Returns:\n        A new pandas DataFrame with the original data plus new columns for each derived metric.\n    \"\"\"\n    df = data.copy()\n\n    metric_names = [\n        \"identity_tension\",\n        \"emotional_tension\",\n        \"success_tension\",\n        \"relational_tension\",\n        \"goal_tension\",\n        \"strategic_contradiction_index\",\n        \"descriptive_cohesion_index\",\n        \"motivational_cohesion_index\",\n        \"full_cohesion_index\",\n    ]\n\n    def get_all_metrics_for_row(row):\n        scores = _extract_scores(row)\n        if scores:\n            return calculate_all_derived_metrics(scores)\n        else:\n            return {name: np.nan for name in metric_names}\n\n    derived_data = df.apply(get_all_metrics_for_row, axis=1, result_type='expand')\n\n    df = df.join(derived_data.astype(float))\n\n    return df\n",
  "cached_with_code": true
}