{
  "status": "success",
  "functions_generated": 14,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 33581,
  "function_code_content": "[\n  {\n    \"thought\": \"I need to generate a series of Python functions for statistical analysis based on the provided experiment specification for the 'Cohesive Flourishing Framework (CFF) v10.1'.\\n\\nHere's my plan, following the user's instructions precisely:\\n\\n**Step 1: Extract Explicit Statistical Requirements.**\\n- The 'Statistical Analysis Plan' section explicitly mentions:\\n    - 'Descriptive Statistics: Mean scores, standard deviations, ranges for all dimensions'\\n    - 'Correlation Analysis: Relationships between dimensions and metadata variables'\\n    - 'Pattern Analysis: Qualitative identification of differences across career phases, event types, and audiences'\\n    - 'Tension Analysis: Strategic contradiction index patterns'\\n    - 'Cohesion Analysis: Democratic health impact assessment'\\n- It also explicitly states: 'Inferential statistics (ANOVA) are not appropriate for N=14. Analysis will focus on descriptive patterns and qualitative comparisons.'\\n- The 'Hypotheses' section implies comparisons across groups (career phases, event types, audiences).\\n- The 'BALANCED STATISTICAL APPROACH' section gives tiers for different tests (Correlation, T-tests, ANOVA, Cronbach's alpha). I need to check the sample size to determine which tier to use.\\n\\n**Step 2: Identify Research Design Requirements.**\\n- **Design**: This is a comparative, non-experimental design. It's an observational study of a corpus of speeches.\\n- **Grouping Variables**:\\n    - `career_phase`: 'early_career', 'mid_career', 'late_career' (and 'founding_era' but the sample data doesn't show it). I need to create this mapping.\\n    - `event_type`: 'CPAC', 'RNC', 'America Fest', 'Student Action Summit' (SAS). I need to create this mapping.\\n    - `audience`: 'conservative activists', 'Republican delegates', 'TPUSA supporters', 'college students'. This is linked to `event_type`.\\n- **Dependent Variables**:\\n    - The 10 raw scores: `tribal_dominance_raw`, `individual_dignity_raw`, etc.\\n    - The 10 salience scores: `tribal_dominance_salience`, etc.\\n    - The 10 confidence scores: `tribal_dominance_confidence`, etc.\\n    - The derived metrics: `identity_tension`, `emotional_tension`, `success_tension`, `relational_tension`, `goal_tension`, `strategic_contradiction_index`, `descriptive_cohesion_index`, `motivational_cohesion_index`, `full_cohesion_index`.\\n- **Specific Comparisons**:\\n    - Compare scores across `career_phase`.\\n    - Compare scores across `event_type`.\\n    - Compare scores across `audience` (which is tied to `event_type`).\\n\\n**Step 3: Perform Tiered Power Analysis.**\\n- The prompt states the corpus has N=14 speeches.\\n- This puts the entire analysis into **TIER 3: Exploratory Analysis (N<15)**.\\n- **Correlation analysis**: Tier 3. I will generate a function for this but with strong caveats.\\n- **T-tests**: The prompt says 'N<8 per group' is Tier 3. I need to check the group sizes.\\n    - `career_phase`: early (2), mid (5), late (4). All groups are < 8. So, T-tests are Tier 3.\\n    - `event_type`: CPAC (3), RNC (1), America Fest (3), SAS (4). All groups are < 5.\\n- **ANOVA**: The prompt says 'N<5 per group' is Tier 3. The `event_type` groups are too small. The `career_phase` groups are also very small. The experiment spec itself says 'Inferential statistics (ANOVA) are not appropriate for N=14'.\\n- **Conclusion**: I will *not* generate ANOVA or t-tests as they are explicitly discouraged and the group sizes are too small even for Tier 3. The focus must be on descriptive statistics, effect sizes (like Cohen's d for group comparisons), and correlation with strong caveats. I will generate functions for descriptive statistics by group, which aligns with the 'Qualitative identification of differences' requirement.\\n\\n**Step 4: Map Statistical Tests to Research Questions.**\\n- **RQ1 (Evolutionary Analysis)**: Compare scores across `career_phase`. This will be handled by a descriptive statistics function that groups by `career_phase`.\\n- **RQ2 (Contextual Variation)**: Compare scores across `event_type`. This will be handled by a descriptive statistics function that groups by `event_type`.\\n- **RQ3 (Audience Adaptation)**: This is the same as RQ2, as `event_type` is a proxy for `audience`.\\n- **RQ4 (Strategic Coherence)**: Analyze `strategic_contradiction_index` and tension indices. This will be part of the main descriptive statistics function.\\n- **General Requirements**:\\n    - **Descriptive Statistics**: A function to calculate mean, std, min, max, median for all numeric columns.\\n    - **Grouped Descriptive Statistics**: A function to calculate descriptive stats grouped by `career_phase` and `event_type`. This directly addresses the core research questions in an exploratory manner.\\n    - **Correlation Analysis**: A function to calculate correlations between all numeric variables. This is explicitly requested. It will be Tier 3.\\n\\n**Step 5: Plan Function Architecture.**\\n1.  **`_flatten_and_prepare_data(data)`**: A helper function to parse the nested JSON from the raw data, flatten it into a pandas DataFrame, and merge it with the derived metrics. The sample data shows a complex structure. The raw data is a list of dicts, where each dict has `analysis_result.result_content.document_analyses` which is a list containing the scores. The derived metrics are in a separate file. I'll need to merge them based on `document_name`. The final DataFrame should have columns like `document_name`, `tribal_dominance_raw`, `tribal_dominance_salience`, `identity_tension`, etc.\\n2.  **`_add_metadata_columns(df)`**: A helper function that takes the flattened DataFrame and adds the `career_phase` and `event_type` columns based on the `document_name`. I will hardcode the mappings as instructed.\\n3.  **`calculate_descriptive_statistics(data, **kwargs)`**: This will take the prepared DataFrame and calculate overall descriptive stats (mean, std, min, max, median) for all key numerical columns (raw scores, salience scores, derived metrics).\\n4.  **`analyze_group_differences(data, **kwargs)`**: This function will perform the grouped analysis. It will group by `career_phase` and `event_type` and calculate descriptive statistics for each group. This directly addresses the research questions about evolution and context. It will not perform inferential tests, as per the power analysis.\\n5.  **`perform_correlation_analysis(data, **kwargs)`**: This will calculate a correlation matrix for all key numerical columns. The docstring will include a strong Tier 3 caveat.\\n6.  **`perform_statistical_analysis(data, **kwargs)`**: The final wrapper function that calls the others and aggregates the results.\\n\\n**Data Structure and Column Names:**\\n- The raw data has `dimensional_scores` with keys like `tribal_dominance`. Inside, there are `raw_score`, `salience`, `confidence`. \\n- The user specified: 'Use column names ending with _raw, _salience, _confidence (NOT _score)'. So I will create columns like `tribal_dominance_raw`, `tribal_dominance_salience`, `tribal_dominance_confidence`.\\n- The derived metrics data has columns like `identity_tension`, `full_cohesion_index`.\\n- The `document_name` is the key for merging and for metadata mapping.\\n\\n**Metadata Mapping (CRITICAL):**\\nI need to create the mapping from `document_name` to `career_phase` and `event_type`. I'll inspect the sample `document_name`s:\\n- `early_career/charlie_kirk_western_conservative_summit_2018.txt` -> `career_phase`: 'early_career', `event_type`: 'other' (or 'western_conservative_summit')\\n- `early_career/charlie_kirk_college_conservatism_2018.txt` -> `career_phase`: 'early_career', `event_type`: 'other' (or 'interview')\\n- `cpac/charlie_kirk_cpac_2020_c-WiaPPxIHc.txt` -> `career_phase`: 'mid_career' (2020), `event_type`: 'cpac'\\n- `student_action_summit/charlie_kirk_sas_2022_vUcwKoYEPd4.txt` -> `career_phase`: 'mid_career' (2022), `event_type`: 'sas'\\n\\nThe experiment spec defines the career phases:\\n- `founding_era`: 2015-2016\\n- `early_career`: 2018-2019\\n- `mid_career`: 2020-2022\\n- `late_career`: 2023-2024\\n\\nI will need to parse the year from the filename or use the directory structure. The directory structure seems more reliable (`early_career`, `cpac`, `student_action_summit`). I'll use the directory structure to define `event_type` and `career_phase`.\\n\\nThis seems like a robust plan. I will implement the helper functions first, then the main analysis functions, and finally the wrapper. I will ensure all docstrings have the required power caveats.\\n\\n**Refining the Plan:**\\n- The user provided sample data for `raw_analysis` and `derived_metrics`. The `derived_metrics` sample shows `NaN` for all metrics. This is a problem. The prompt says 'The analysis data contains the following columns: Failed to extract data structure...'. This suggests the derived metrics might not be pre-calculated. The framework spec *defines* the formulas. I should probably calculate them myself from the raw scores, just in case the provided derived metrics file is incomplete or incorrect. The `derived_metrics` section in the YAML provides the exact formulas. This is the most robust approach. I will re-calculate them.\\n- The raw data is a list of JSON objects. I need to parse this. The structure is `[ { 'analysis_result': { 'result_content': { 'document_analyses': [ { 'document_name': '...', 'dimensional_scores': { ... } } ] } } } ]`. I need to iterate through this structure to extract the data for each document.\\n- The user also said 'The analysis data has a 'document_name' column that corresponds to filenames in the corpus manifest'. The sample data has `filename` and `document_name`. I'll use `document_name` as the primary key.\\n- The prompt says 'DO NOT generate code to parse corpus.md - you already have the information'. And 'The corpus manifest has been provided to you in full above. You don't need to read it from disk.' But then it says 'No corpus manifest found. Speaker analysis functions will need to handle missing metadata gracefully.' This is contradictory. I will follow the most recent instruction: 'No corpus manifest found' and 'handle missing metadata gracefully'. I will derive the metadata from the `document_name` path, which is the only reliable source I have. This is the most conservative and robust approach given the conflicting instructions.\\n\\n**Final Function Plan:**\\n1. `_get_full_dataset(data)`: Helper to parse the raw JSON list, flatten it, and create a base DataFrame.\\n2. `_calculate_derived_metrics(df)`: Helper to calculate all derived metrics from the raw/salience scores based on the framework spec. This makes the analysis independent of potentially faulty pre-calculated files.\\n3. `_add_metadata_columns(df)`: Helper to add `career_phase` and `event_type` from `document_name`.\\n4. `_prepare_analysis_data(data)`: A main preparation function that calls the three helpers above to create the final, clean DataFrame for analysis.\\n5. `calculate_descriptive_statistics(data, **kwargs)`: Calculates overall descriptive stats.\\n6. `analyze_group_differences(data, **kwargs)`: Calculates descriptive stats grouped by `career_phase` and `event_type`.\\n7. `perform_correlation_analysis(data, **kwargs)`: Calculates correlation matrix.\\n8. `perform_statistical_analysis(data, **kwargs)`: The wrapper.\\n\\nThis looks solid. I will now start writing the code. I will be careful with column names, error handling, and docstrings. I will use `re` to extract the year from filenames for the `career_phase` mapping.\\n\\nLet's re-check the data structure. The sample data is a list of dicts. The `data` parameter passed to the functions will be a DataFrame. The prompt says 'The LLM has already understood the corpus structure - trust this understanding'. This implies the data is already loaded into a DataFrame. However, the sample data is raw JSON. The prompt also says 'Generate functions that create the grouping directly based on document names'. This suggests the DataFrame will have a `document_name` column.\\n\\nThe prompt also says 'Raw analysis data: JSON files in the current directory (full dataset)'. This implies the function might need to load the data itself. But the signature is `def function_name(data, **kwargs):`. This is a classic pattern where a loader function (not shown) creates the DataFrame and passes it to the analysis functions. I will assume the `data` parameter is a DataFrame that has been pre-processed from the raw JSON.\\n\\nLet's look at the sample data again. It's a list of dicts. The `raw_analysis_response` is a string containing JSON. The outer structure has `filename`. The inner JSON has `document_name`. They seem to be the same. The scores are inside `dimensional_scores`.\\n\\nOkay, let's assume a pre-processing step has happened and the `data` DataFrame looks something like this:\\nColumns: `document_name`, `tribal_dominance_raw`, `tribal_dominance_salience`, `tribal_dominance_confidence`, `individual_dignity_raw`, ..., `cohesive_goals_confidence`.\\nAnd it might also contain the pre-calculated derived metrics. I will write my functions to be robust. I will first check if the derived metric columns exist. If not, I will calculate them. This is the safest bet.\\n\\nLet's define the metadata mapping logic more concretely.\\n```python\\ndef get_career_phase(doc_name):\\n    match = re.search(r'(\\\\d{4})', doc_name)\\n    if not match:\\n        if 'early_career' in doc_name: return 'early_career'\\n        return 'unknown'\\n    year = int(match.group(1))\\n    if 2018 <= year <= 2019: return 'early_career'\\n    if 2020 <= year <= 2022: return 'mid_career'\\n    if 2023 <= year <= 2024: return 'late_career'\\n    return 'unknown'\\n\\ndef get_event_type(doc_name):\\n    if 'cpac/' in doc_name: return 'cpac'\\n    if 'rnc/' in doc_name: return 'rnc'\\n    if 'america_fest/' in doc_name: return 'america_fest'\\n    if 'student_action_summit/' in doc_name or 'sas/' in doc_name: return 'sas'\\n    if 'western_conservative_summit' in doc_name: return 'western_conservative_summit'\\n    if 'college_conservatism' in doc_name: return 'interview'\\n    return 'other'\\n```\\nThis logic seems sound and handles the examples. I will embed this logic inside a helper function.\\n\\nNow, for the main functions.\\n`calculate_descriptive_statistics`: Will use `.describe()` on the relevant columns.\\n`analyze_group_differences`: Will use `.groupby(['career_phase', 'event_type']).agg(...)`.\\n`perform_correlation_analysis`: Will use `.corr()`.\\n\\nI will make sure the output dictionaries have string keys, as requested. E.g., `f'{col}__{agg_func}'`.\\n\\nFinal check on requirements:\\n- Python module, no markdown.\\n- `def function_name(data, **kwargs):` signature.\\n- Handle missing data.\\n- Docstrings with methodology and power caveats.\\n- Production-ready with error handling.\\n- Create grouping mappings directly.\\n- Dictionary keys must be strings.\\n- Final wrapper `perform_statistical_analysis`.\\n\\nEverything seems to be in order. I will now generate the code.\",\n    \"code\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nfrom typing import Dict, Any, Optional, List, Tuple\\nimport warnings\\nimport re\\nimport json\\n\\n# Helper functions to prepare the data\\n\\ndef _add_metadata_columns(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"Adds career_phase and event_type columns based on document_name.\\\"\\\"\\\"\\n    if 'document_name' not in df.columns:\\n        return df\\n\\n    def get_career_phase(doc_name):\\n        if not isinstance(doc_name, str):\\n            return 'unknown'\\n        # Prefer year in filename for accuracy\\n        match = re.search(r'(\\\\d{4})', doc_name)\\n        if match:\\n            year = int(match.group(1))\\n            if 2018 <= year <= 2019:\\n                return 'early_career'\\n            if 2020 <= year <= 2022:\\n                return 'mid_career'\\n            if 2023 <= year <= 2024:\\n                return 'late_career'\\n        # Fallback to directory name\\n        if 'early_career' in doc_name:\\n            return 'early_career'\\n        if 'mid_career' in doc_name:\\n            return 'mid_career'\\n        if 'late_career' in doc_name:\\n            return 'late_career'\\n        return 'unknown'\\n\\n    def get_event_type(doc_name):\\n        if not isinstance(doc_name, str):\\n            return 'unknown'\\n        if 'cpac/' in doc_name:\\n            return 'cpac'\\n        if 'rnc/' in doc_name:\\n            return 'rnc'\\n        if 'america_fest/' in doc_name:\\n            return 'america_fest'\\n        if 'student_action_summit/' in doc_name or 'sas/' in doc_name:\\n            return 'sas'\\n        if 'western_conservative_summit' in doc_name:\\n            return 'western_conservative_summit'\\n        if 'college_conservatism' in doc_name:\\n            return 'interview'\\n        return 'other'\\n\\n    df['career_phase'] = df['document_name'].apply(get_career_phase)\\n    df['event_type'] = df['document_name'].apply(get_event_type)\\n    return df\\n\\ndef _calculate_derived_metrics(df: pd.DataFrame) -> pd.DataFrame:\\n    \\\"\\\"\\\"Calculates all derived metrics from the raw and salience scores.\\\"\\\"\\\"\\n    # Define a safe getter for scores\\n    def get_score(row, dim, type):\\n        col = f\\\"{dim}_{type}\\\"\\n        return row.get(col, 0.0)\\n\\n    # Tension Indices\\n    df['identity_tension'] = df.apply(lambda row: min(get_score(row, 'tribal_dominance', 'raw'), get_score(row, 'individual_dignity', 'raw')) * abs(get_score(row, 'tribal_dominance', 'salience') - get_score(row, 'individual_dignity', 'salience')), axis=1)\\n    df['emotional_tension'] = df.apply(lambda row: min(get_score(row, 'fear', 'raw'), get_score(row, 'hope', 'raw')) * abs(get_score(row, 'fear', 'salience') - get_score(row, 'hope', 'salience')), axis=1)\\n    df['success_tension'] = df.apply(lambda row: min(get_score(row, 'envy', 'raw'), get_score(row, 'mudita', 'raw')) * abs(get_score(row, 'envy', 'salience') - get_score(row, 'mudita', 'salience')), axis=1)\\n    df['relational_tension'] = df.apply(lambda row: min(get_score(row, 'enmity', 'raw'), get_score(row, 'amity', 'raw')) * abs(get_score(row, 'enmity', 'salience') - get_score(row, 'amity', 'salience')), axis=1)\\n    df['goal_tension'] = df.apply(lambda row: min(get_score(row, 'fragmentative_goals', 'raw'), get_score(row, 'cohesive_goals', 'raw')) * abs(get_score(row, 'fragmentative_goals', 'salience') - get_score(row, 'cohesive_goals', 'salience')), axis=1)\\n\\n    # Strategic Contradiction Index\\n    tension_cols = ['identity_tension', 'emotional_tension', 'success_tension', 'relational_tension', 'goal_tension']\\n    df['strategic_contradiction_index'] = df[tension_cols].mean(axis=1)\\n\\n    # Cohesion Components\\n    df['identity_cohesion_component'] = (df.apply(lambda r: get_score(r, 'individual_dignity', 'raw') * get_score(r, 'individual_dignity', 'salience'), axis=1) - df.apply(lambda r: get_score(r, 'tribal_dominance', 'raw') * get_score(r, 'tribal_dominance', 'salience'), axis=1))\\n    df['emotional_cohesion_component'] = (df.apply(lambda r: get_score(r, 'hope', 'raw') * get_score(r, 'hope', 'salience'), axis=1) - df.apply(lambda r: get_score(r, 'fear', 'raw') * get_score(r, 'fear', 'salience'), axis=1))\\n    df['success_cohesion_component'] = (df.apply(lambda r: get_score(r, 'mudita', 'raw') * get_score(r, 'mudita', 'salience'), axis=1) - df.apply(lambda r: get_score(r, 'envy', 'raw') * get_score(r, 'envy', 'salience'), axis=1))\\n    df['relational_cohesion_component'] = (df.apply(lambda r: get_score(r, 'amity', 'raw') * get_score(r, 'amity', 'salience'), axis=1) - df.apply(lambda r: get_score(r, 'enmity', 'raw') * get_score(r, 'enmity', 'salience'), axis=1))\\n    df['goal_cohesion_component'] = (df.apply(lambda r: get_score(r, 'cohesive_goals', 'raw') * get_score(r, 'cohesive_goals', 'salience'), axis=1) - df.apply(lambda r: get_score(r, 'fragmentative_goals', 'raw') * get_score(r, 'fragmentative_goals', 'salience'), axis=1))\\n\\n    # Salience Totals\\n    descriptive_salience_dims = ['hope', 'fear', 'mudita', 'envy', 'amity', 'enmity']\\n    motivational_salience_dims = descriptive_salience_dims + ['cohesive_goals', 'fragmentative_goals']\\n    full_salience_dims = motivational_salience_dims + ['individual_dignity', 'tribal_dominance']\\n    df['descriptive_salience_total'] = df[[f\\\"{dim}_salience\\\" for dim in descriptive_salience_dims]].sum(axis=1)\\n    df['motivational_salience_total'] = df[[f\\\"{dim}_salience\\\" for dim in motivational_salience_dims]].sum(axis=1)\\n    df['full_salience_total'] = df[[f\\\"{dim}_salience\\\" for dim in full_salience_dims]].sum(axis=1)\\n\\n    # Final Cohesion Indices\\n    epsilon = 0.001\\n    df['descriptive_cohesion_index'] = (df['emotional_cohesion_component'] + df['success_cohesion_component'] + df['relational_cohesion_component']) / (df['descriptive_salience_total'] + epsilon)\\n    df['motivational_cohesion_index'] = (df['emotional_cohesion_component'] + df['success_cohesion_component'] + df['relational_cohesion_component'] + df['goal_cohesion_component']) / (df['motivational_salience_total'] + epsilon)\\n    df['full_cohesion_index'] = (df['identity_cohesion_component'] + df['emotional_cohesion_component'] + df['success_cohesion_component'] + df['relational_cohesion_component'] + df['goal_cohesion_component']) / (df['full_salience_total'] + epsilon)\\n    \\n    return df\\n\\ndef _prepare_analysis_data(data: pd.DataFrame) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"Prepares the raw DataFrame for statistical analysis.\\\"\\\"\\\"\\n    if data is None or data.empty:\\n        return None\\n    \\n    df = data.copy()\\n    \\n    # Ensure derived metrics are calculated, as they might be missing or NaN\\n    df = _calculate_derived_metrics(df)\\n    \\n    # Add metadata for grouping\\n    df = _add_metadata_columns(df)\\n    \\n    return df\\n\\n\\ndef calculate_descriptive_statistics(data: pd.DataFrame, **kwargs) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates overall descriptive statistics for all key CFF dimensions and indices.\\n\\n    Methodology:\\n    This function computes the mean, standard deviation, median, min, and max for all\\n    raw scores, salience scores, and derived metrics from the CFF framework.\\n    This provides a baseline understanding of the central tendency and dispersion of rhetorical\\n    patterns across the entire corpus.\\n\\n    Power Assessment (Tier 3: Exploratory):\\n    As an exploratory analysis on a small sample (N < 15), these descriptive statistics are\\n    robust for summarizing the observed data. However, they should not be used to make\\n    generalizations beyond this specific corpus.\\n\\n    Args:\\n        data (pd.DataFrame): DataFrame containing the analysis data.\\n        **kwargs: Additional parameters (not used).\\n\\n    Returns:\\n        dict: A dictionary of descriptive statistics, or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    df = _prepare_analysis_data(data)\\n    if df is None or df.empty:\\n        return None\\n\\n    try:\\n        dims = [\\n            'tribal_dominance', 'individual_dignity', 'fear', 'hope', 'envy', 'mudita',\\n            'enmity', 'amity', 'fragmentative_goals', 'cohesive_goals'\\n        ]\\n        score_types = ['raw', 'salience', 'confidence']\\n        score_cols = [f\\\"{d}_{st}\\\" for d in dims for st in score_types]\\n        \\n        derived_cols = [\\n            'identity_tension', 'emotional_tension', 'success_tension', 'relational_tension',\\n            'goal_tension', 'strategic_contradiction_index', 'descriptive_cohesion_index',\\n            'motivational_cohesion_index', 'full_cohesion_index'\\n        ]\\n        \\n        analysis_cols = [col for col in score_cols + derived_cols if col in df.columns]\\n        \\n        if not analysis_cols:\\n            return {\\\"error\\\": \\\"No valid analysis columns found.\\\"}\\n\\n        desc_stats = df[analysis_cols].describe().to_dict()\\n\\n        # Flatten the dictionary and format keys as requested\\n        results = {}\\n        for col, stats_dict in desc_stats.items():\\n            for stat_name, value in stats_dict.items():\\n                # Use 'std' for standard deviation as is common in pandas\\n                stat_key = 'std' if stat_name == 'std' else stat_name\\n                results[f\\\"{col}__{stat_key}\\\"] = value if pd.notna(value) else None\\n        \\n        results['sample_size'] = int(df.shape[0])\\n        return results\\n\\n    except Exception as e:\\n        return {\\\"error\\\": f\\\"Failed to calculate descriptive statistics: {str(e)}\\\"}\\n\\ndef analyze_group_differences(data: pd.DataFrame, **kwargs) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs exploratory analysis of group differences based on career phase and event type.\\n\\n    Methodology:\\n    This function calculates descriptive statistics (mean, std, count) for all key CFF metrics,\\n    grouped by 'career_phase' and 'event_type'. This directly addresses research questions\\n    about the evolution of rhetoric and adaptation to context.\\n    No inferential tests (e.g., ANOVA, t-tests) are performed due to the small sample size.\\n\\n    Power Assessment (Tier 3: Exploratory):\\n    With a total sample size of N < 15 and even smaller group sizes, this analysis is strictly\\n    exploratory. The results are suggestive of patterns but are not statistically conclusive.\\n    Differences in group means should be interpreted as hypotheses for future research with\\n    larger datasets.\\n\\n    Args:\\n        data (pd.DataFrame): DataFrame containing the analysis data.\\n        **kwargs: Additional parameters (not used).\\n\\n    Returns:\\n        dict: A nested dictionary with descriptive statistics for each group, or None.\\n    \\\"\\\"\\\"\\n    df = _prepare_analysis_data(data)\\n    if df is None or df.empty or 'career_phase' not in df.columns or 'event_type' not in df.columns:\\n        return None\\n\\n    try:\\n        dims = [\\n            'tribal_dominance', 'individual_dignity', 'fear', 'hope', 'envy', 'mudita',\\n            'enmity', 'amity', 'fragmentative_goals', 'cohesive_goals'\\n        ]\\n        score_cols = [f\\\"{d}_raw\\\" for d in dims] + [f\\\"{d}_salience\\\" for d in dims]\\n        derived_cols = [\\n            'strategic_contradiction_index', 'full_cohesion_index'\\n        ]\\n        analysis_cols = [col for col in score_cols + derived_cols if col in df.columns]\\n        \\n        if not analysis_cols:\\n            return {\\\"error\\\": \\\"No valid analysis columns found for grouping.\\\"}\\n\\n        results = {}\\n        grouping_vars = ['career_phase', 'event_type']\\n\\n        for group_var in grouping_vars:\\n            if df[group_var].nunique() < 2:\\n                continue\\n            \\n            grouped_stats = df.groupby(group_var)[analysis_cols].agg(['mean', 'std', 'count']).to_dict()\\n            \\n            # Flatten and format the dictionary\\n            group_results = {}\\n            for col, stats_dict in grouped_stats.items():\\n                metric_name, stat_name = col\\n                for group_name, value in stats_dict.items():\\n                    key = f\\\"{group_name}__{metric_name}__{stat_name}\\\"\\n                    group_results[key] = value if pd.notna(value) else None\\n            results[f\\\"by_{group_var}\\\"] = group_results\\n\\n        return results if results else {\\\"message\\\": \\\"Not enough data for group analysis.\\\"}\\n\\n    except Exception as e:\\n        return {\\\"error\\\": f\\\"Failed to analyze group differences: {str(e)}\\\"}\\n\\ndef perform_correlation_analysis(data: pd.DataFrame, **kwargs) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates the Pearson correlation matrix for key CFF dimensions and indices.\\n\\n    Methodology:\\n    This function computes a correlation matrix to explore linear relationships between\\n    different rhetorical dimensions (e.g., 'fear_raw' and 'enmity_raw') and between\\n    dimensions and composite indices (e.g., 'tribal_dominance_raw' and 'full_cohesion_index').\\n\\n    Power Assessment (Tier 3: Exploratory):\\n    With a sample size of N < 15, this correlation analysis is highly exploratory. The\\n    resulting correlation coefficients are unstable and have wide confidence intervals.\\n    They should be interpreted with extreme caution and are primarily useful for hypothesis\\n    generation, not for confirming relationships.\\n\\n    Args:\\n        data (pd.DataFrame): DataFrame containing the analysis data.\\n        **kwargs: Additional parameters (not used).\\n\\n    Returns:\\n        dict: A dictionary representing the flattened correlation matrix, or None.\\n    \\\"\\\"\\\"\\n    df = _prepare_analysis_data(data)\\n    if df is None or df.empty:\\n        return None\\n\\n    try:\\n        dims = [\\n            'tribal_dominance', 'individual_dignity', 'fear', 'hope', 'envy', 'mudita',\\n            'enmity', 'amity', 'fragmentative_goals', 'cohesive_goals'\\n        ]\\n        score_cols = [f\\\"{d}_raw\\\" for d in dims] + [f\\\"{d}_salience\\\" for d in dims]\\n        derived_cols = [\\n            'strategic_contradiction_index', 'full_cohesion_index'\\n        ]\\n        analysis_cols = [col for col in score_cols + derived_cols if col in df.columns]\\n\\n        if len(analysis_cols) < 2:\\n            return {\\\"error\\\": \\\"Not enough columns for correlation analysis.\\\"}\\n\\n        corr_matrix = df[analysis_cols].corr(method='pearson')\\n        \\n        # Flatten the matrix into a dictionary, avoiding redundant pairs\\n        corr_dict = {}\\n        for i in range(len(corr_matrix.columns)):\\n            for j in range(i + 1, len(corr_matrix.columns)):\\n                col1 = corr_matrix.columns[i]\\n                col2 = corr_matrix.columns[j]\\n                value = corr_matrix.iloc[i, j]\\n                key = f\\\"{col1}__vs__{col2}\\\"\\n                corr_dict[key] = value if pd.notna(value) else None\\n        \\n        corr_dict['sample_size'] = int(df.shape[0])\\n        return corr_dict\\n\\n    except Exception as e:\\n        return {\\\"error\\\": f\\\"Failed to perform correlation analysis: {str(e)}\\\"}\\n\\ndef perform_statistical_analysis(data: pd.DataFrame, **kwargs) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function that executes all statistical analyses and returns combined results.\\n    \\n    This wrapper function orchestrates the entire statistical protocol for the experiment.\\n    It calls specialized functions for:\\n    1. Descriptive Statistics: Overall summary of the data.\\n    2. Group Difference Analysis: Exploratory comparison across career phases and event types.\\n    3. Correlation Analysis: Exploration of relationships between variables.\\n\\n    All analyses are conducted with an awareness of the small sample size (N < 15), and\\n    results are framed as exploratory and hypothesis-generating rather than conclusive.\\n\\n    Args:\\n        data (pd.DataFrame): pandas DataFrame containing analysis data.\\n        **kwargs: Additional parameters passed to individual functions.\\n        \\n    Returns:\\n        dict: Combined results from all statistical analyses.\\n    \\\"\\\"\\\"\\n    results = {}\\n    \\n    if data is None or not isinstance(data, pd.DataFrame) or data.empty:\\n        return {\\\"error\\\": \\\"Input data is missing, not a DataFrame, or empty.\\\"}\\n\\n    # The data passed in is assumed to be the raw, nested structure.\\n    # A more robust implementation would parse it here.\\n    # For this task, we assume a pre-processing step has flattened it.\\n    # However, to be safe, let's try to handle the expected raw format.\\n    \\n    prepared_data = None\\n    try:\\n        # This block attempts to flatten the raw data structure if provided.\\n        records = []\\n        for item in data.to_dict('records'):\\n            try:\\n                doc_analysis = item['analysis_result']['result_content']['document_analyses'][0]\\n                record = {'document_name': doc_analysis['document_name']}\\n                for dim, scores in doc_analysis['dimensional_scores'].items():\\n                    record[f\\\"{dim}_raw\\\"] = scores.get('raw_score')\\n                    record[f\\\"{dim}_salience\\\"] = scores.get('salience')\\n                    record[f\\\"{dim}_confidence\\\"] = scores.get('confidence')\\n                records.append(record)\\n            except (KeyError, IndexError, TypeError):\\n                # If it's already flat, just use it\\n                prepared_data = data\\n                break\\n        if not prepared_data:\\n            prepared_data = pd.DataFrame(records)\\n    except Exception:\\n        # If parsing fails, assume it's already a flat DataFrame\\n        prepared_data = data\\n\\n    if prepared_data is None or prepared_data.empty:\\n         return {\\\"error\\\": \\\"Failed to prepare data for analysis.\\\"}\\n\\n    # Call all individual statistical functions and aggregate results\\n    try:\\n        results['descriptive_statistics'] = calculate_descriptive_statistics(prepared_data, **kwargs)\\n    except Exception as e:\\n        results['descriptive_statistics'] = {'error': str(e)}\\n        \\n    try:\\n        results['group_difference_analysis'] = analyze_group_differences(prepared_data, **kwargs)\\n    except Exception as e:\\n        results['group_difference_analysis'] = {'error': str(e)}\\n        \\n    try:\\n        results['correlation_analysis'] = perform_correlation_analysis(prepared_data, **kwargs)\\n    except Exception as e:\\n        results['correlation_analysis'] = {'error': str(e)}\\n    \\n    # The original spec mentioned other tests, but they are not appropriate for N=14.\\n    # We explicitly do not include them to maintain statistical conservatism.\\n    # For example, ANOVA and reliability analysis are omitted due to low power.\\n\\n    return results\"\n  }\n]",
  "cached_with_code": true
}