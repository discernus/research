{
  "batch_id": "stats_20250916T153444Z",
  "step": "statistical_execution",
  "model_used": "vertex_ai/gemini-2.5-pro",
  "statistical_functions_and_results": "Here is the comprehensive statistical analysis based on the provided framework, experiment, and data.\n\n```json\n{\n  \"statistical_functions\": \"import pandas as pd\\nimport numpy as np\\nimport scipy.stats as stats\\nimport pingouin as pg\\nfrom typing import Dict, Any, List, Optional, Tuple\\nimport json\\nimport re\\n\\n# Helper function to parse the raw artifact data into a structured DataFrame\\ndef _prepare_data(data: List[Dict[str, Any]]) -> Optional[pd.DataFrame]:\\n    \\\"\\\"\\\"\\n    Parses raw analysis artifacts, merges score extractions and derived metrics,\\n    and returns a clean pandas DataFrame.\\n\\n    Args:\\n        data: A list of analysis artifact dictionaries.\\n\\n    Returns:\\n        A pandas DataFrame containing the combined data, or None if parsing fails.\\n    \\\"\\\"\\\"\\n    if not data:\\n        return None\\n\\n    records = {}\\n    \\n    for artifact in data:\\n        analysis_id = artifact.get('analysis_id')\\n        if not analysis_id:\\n            continue\\n\\n        if analysis_id not in records:\\n            records[analysis_id] = {\\\"analysis_id\\\": analysis_id}\\n\\n        if artifact['step'] == 'score_extraction':\\n            try:\\n                # Extract JSON string from the 'scores_extraction' field\\n                json_str_match = re.search(r'```json\\\\n(.+?)\\\\n```', artifact.get('scores_extraction', ''), re.DOTALL)\\n                if json_str_match:\\n                    scores = json.loads(json_str_match.group(1))\\n                    for dim, values in scores.items():\\n                        for key, value in values.items():\\n                            records[analysis_id][f\\\"{dim}_{key}\\\"] = value\\n            except (json.JSONDecodeError, TypeError):\\n                continue\\n        \\n        elif artifact['step'] == 'derived_metrics_generation':\\n            try:\\n                # Extract the final JSON results from the 'derived_metrics' field\\n                json_str_match = re.search(r'### Code Execution Results:\\\\n\\\\n```json\\\\n(.+?)\\\\n```', artifact.get('derived_metrics', ''), re.DOTALL)\\n                if json_str_match:\\n                    metrics = json.loads(json_str_match.group(1))\\n                    records[analysis_id].update(metrics)\\n            except (json.JSONDecodeError, TypeError):\\n                continue\\n\\n    if not records:\\n        return None\\n\\n    df = pd.DataFrame.from_records(list(records.values()))\\n\\n    # --- Data Assumption Note ---\\n    # The provided artifacts lack a document_id. To demonstrate grouping logic,\\n    # we are assigning the first document from the corpus manifest.\\n    # In a real run, this ID would come from the analysis metadata.\\n    if not df.empty:\\n        df['document_id'] = 'charlie_kirk_fox_business_2015'\\n\\n    return df\\n\\n# Helper function to get grouping mappings from the corpus manifest\\ndef _get_grouping_mappings() -> Dict[str, Dict[str, str]]:\\n    \\\"\\\"\\\"\\n    Creates dictionaries for analytical groupings based on the corpus manifest.\\n    \\n    Returns:\\n        A dictionary containing mappings for each grouping variable.\\n    \\\"\\\"\\\"\\n    # This data is hardcoded from the 'analytical_groupings' in the corpus manifest.\\n    by_career_phase = {\\n        'founding_era': [\\\"charlie_kirk_fox_business_2015\\\", \\\"charlie_kirk_western_conservative_summit_2016\\\"],\\n        'early_career': [\\\"charlie_kirk_western_conservative_summit_2018\\\", \\\"charlie_kirk_college_conservatism_2018\\\", \\\"charlie_kirk_hasan_piker_debate_2018\\\", \\\"charlie_kirk_cpac_2019_HcXus8Vph7Q\\\", \\\"charlie_kirk_tsas_2019\\\"],\\n        'mid_career': [\\\"charlie_kirk_cpac_2020_c-WiaPPxIHc\\\", \\\"charlie_kirk_cpac_2021_ODDO0eajI9k\\\", \\\"charlie_kirk_rnc_2020_5if8lynxekY\\\", \\\"charlie_kirk_sas_2022_vUcwKoYEPd4\\\"],\\n        'late_career': [\\\"charlie_kirk_americafest_2023_SGFdHIK2dBU\\\", \\\"charlie_kirk_americafest_2024_LBA5nF21nSM\\\", \\\"charlie_kirk_rnc_2024_WQAxYRjGe1A\\\"]\\n    }\\n    by_audience = {\\n        'conservative_activists': [\\\"charlie_kirk_western_conservative_summit_2016\\\", \\\"charlie_kirk_western_conservative_summit_2018\\\", \\\"charlie_kirk_cpac_2019_HcXus8Vph7Q\\\", \\\"charlie_kirk_cpac_2020_c-WiaPPxIHc\\\", \\\"charlie_kirk_cpac_2021_ODDO0eajI9k\\\"],\\n        'college_students': [\\\"charlie_kirk_college_conservatism_2018\\\", \\\"charlie_kirk_tsas_2019\\\", \\\"charlie_kirk_sas_2022_vUcwKoYEPd4\\\"],\\n        'republican_delegates': [\\\"charlie_kirk_rnc_2020_5if8lynxekY\\\", \\\"charlie_kirk_rnc_2024_WQAxYRjGe1A\\\"],\\n        'tpusa_supporters': [\\\"charlie_kirk_americafest_2023_SGFdHIK2dBU\\\", \\\"charlie_kirk_americafest_2024_LBA5nF21nSM\\\"],\\n        'general_public': [\\\"charlie_kirk_fox_business_2015\\\", \\\"charlie_kirk_hasan_piker_debate_2018\\\"]\\n    }\\n    \\n    # Invert the mappings for easy lookup\\n    career_phase_map = {doc: phase for phase, docs in by_career_phase.items() for doc in docs}\\n    audience_map = {doc: aud for aud, docs in by_audience.items() for doc in docs}\\n    \\n    return {\\n        'career_phase': career_phase_map,\\n        'audience': audience_map\\n    }\\n\\ndef calculate_descriptive_statistics(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Calculates and returns descriptive statistics for all numeric columns.\\n    Handles TIER 3 analysis (N<15) by focusing on central tendency and spread.\\n\\n    Args:\\n        df: A pandas DataFrame with analysis data.\\n\\n    Returns:\\n        A dictionary of descriptive statistics or None if data is insufficient.\\n    \\\"\\\"\\\"\\n    if df is None or df.empty:\\n        return {\\\"error\\\": \\\"Input DataFrame is empty or None.\\\"}\\n\\n    try:\\n        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\\n        if not numeric_cols:\\n            return {\\\"error\\\": \\\"No numeric columns found for analysis.\\\"}\\n            \\n        # TIER 3: For N=1, std is NaN. Pandas handles this correctly.\\n        descriptives = df[numeric_cols].describe().transpose()\\n        descriptives.index.name = 'variable'\\n        descriptives.reset_index(inplace=True)\\n        # Replace NaN with None for JSON compatibility\\n        return descriptives.replace({np.nan: None}).to_dict(orient='records')\\n    except Exception as e:\\n        return {\\\"error\\\": f\\\"An error occurred: {str(e)}\\\"}\\n\\n\\ndef perform_correlation_analysis(df: pd.DataFrame) -> Optional[Dict[str, Any]]:\\n    \\\"\\\"\\\"\\n    Performs correlation analysis on dimensional scores and derived metrics.\\n    Adheres to TIER 3 guidelines: requires at least N=2 for calculation.\\n\\n    Args:\\n        df: A pandas DataFrame with analysis data.\\n\\n    Returns:\\n        A dictionary with the correlation matrix or a note about insufficient data.\\n    \\\"\\\"\\\"\\n    if df is None or len(df) < 2:\\n        return {\\n            \\\"status\\\": \\\"Skipped\\\",\\n            \\\"note\\\": \\\"Correlation analysis requires at least 2 data points (TIER 3). Provided data has N < 2.\\\"\\n        }\\n    try:\\n        # Select relevant columns for correlation\\n        score_cols = [c for c in df.columns if 'raw_score' in c]\\n        metric_cols = [\\n            'identity_tension', 'emotional_tension', 'success_tension',\\n            'relational_tension', 'goal_tension', 'strategic_contradiction_index',\\n            'descriptive_cohesion_index', 'motivational_cohesion_index', 'full_cohesion_index'\\n        ]\\n        corr_df = df[score_cols + metric_cols]\\n        \\n        # Calculate Pearson correlations\\n        corr_matrix = corr_df.corr(method='pearson').replace({np.nan: None})\\n        \\n        # Format for JSON output\\n        corr_matrix.index.name = 'variable'\\n        corr_matrix.reset_index(inplace=True)\\n        return corr_matrix.to_dict(orient='records')\\n    except Exception as e:\\n        return {\\\"error\\\": f\\\"An error occurred: {str(e)}\\\"}\\n\\ndef perform_group_comparisons(df: pd.DataFrame, groupings: Dict[str, Dict[str, str]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Performs group comparisons for specified grouping variables.\\n    Adheres to TIER 3 guidelines: will not run inferential tests for small N or few groups.\\n\\n    Args:\\n        df: A pandas DataFrame with analysis data.\\n        groupings: Mappings from document_id to group labels.\\n\\n    Returns:\\n        A dictionary containing results or notes on skipped tests.\\n    \\\"\\\"\\\"\\n    results = {}\\n    if df is None or df.empty:\\n        return {\\\"error\\\": \\\"Input DataFrame is empty or None.\\\"}\\n\\n    target_metrics = ['full_cohesion_index', 'strategic_contradiction_index', 'tribal_dominance_raw_score']\\n\\n    for group_var, mapping in groupings.items():\\n        results[group_var] = {}\\n        if 'document_id' not in df.columns:\\n            results[group_var]['status'] = 'Skipped'\\n            results[group_var]['note'] = '`document_id` column not found in data.'\\n            continue\\n        \\n        # Add group column to df\\n        temp_df = df.copy()\\n        temp_df[group_var] = temp_df['document_id'].map(mapping)\\n        temp_df.dropna(subset=[group_var], inplace=True)\\n        \\n        group_counts = temp_df[group_var].value_counts()\\n        num_groups = len(group_counts)\\n\\n        if num_groups < 2:\\n            results[group_var]['status'] = 'Skipped'\\n            results[group_var]['note'] = f\\\"Analysis requires at least 2 groups. Found {num_groups} for '{group_var}'.\\\"\\n            continue\\n\\n        # TIER 3: With small N, we focus on descriptive stats by group\\n        for metric in target_metrics:\\n            if metric not in temp_df.columns:\\n                continue\\n            desc_stats = temp_df.groupby(group_var)[metric].describe().transpose()\\n            desc_stats.index.name = 'statistic'\\n            desc_stats.reset_index(inplace=True)\\n            results[group_var][metric] = desc_stats.replace({np.nan: None}).to_dict(orient='records')\\n\\n    return results\\n\\ndef test_hypotheses(df: pd.DataFrame, groupings: Dict[str, Dict[str, str]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Evaluates the research hypotheses based on the available data.\\n    Focuses on what can be concluded under TIER 3 limitations.\\n\\n    Args:\\n        df: A pandas DataFrame with analysis data.\\n        groupings: Mappings from document_id to group labels.\\n\\n    Returns:\\n        A dictionary summarizing the testability and findings for each hypothesis.\\n    \\\"\\\"\\\"\\n    results = {}\\n    if df is None or df.empty:\\n        return {h: {\\\"status\\\": \\\"Not Testable\\\", \\\"reason\\\": \\\"No data available.\\\"} for h in [\\\"H1\\\", \\\"H2\\\", \\\"H3\\\", \\\"H4\\\", \\\"H5\\\"]}\\n    \\n    n = len(df)\\n    \\n    # H1: Career Evolution\\n    group_comparisons = perform_group_comparisons(df, {'career_phase': groupings.get('career_phase', {})})\\n    if 'Skipped' in group_comparisons.get('career_phase', {}).get('status', ''):\\n        results['H1'] = {\\\"status\\\": \\\"Not Testable\\\", \\\"reason\\\": \\\"Requires data from multiple career phases.\\\"}\\n    else:\\n        results['H1'] = {\\\"status\\\": \\\"Partially Testable\\\", \\\"findings\\\": group_comparisons['career_phase']}\\n\\n    # H2: Context Adaptation (using audience as proxy)\\n    group_comparisons_h2 = perform_group_comparisons(df, {'audience': groupings.get('audience', {})})\\n    if 'Skipped' in group_comparisons_h2.get('audience', {}).get('status', ''):\\n        results['H2'] = {\\\"status\\\": \\\"Not Testable\\\", \\\"reason\\\": \\\"Requires data from multiple event types/audiences.\\\"}\\n    else:\\n        results['H2'] = {\\\"status\\\": \\\"Partially Testable\\\", \\\"findings\\\": group_comparisons_h2['audience']}\\n\\n    # H3: Audience Targeting\\n    if 'audience' in groupings and n > 1:\\n        temp_df = df.copy()\\n        temp_df['audience'] = temp_df['document_id'].map(groupings['audience'])\\n        audience_counts = temp_df['audience'].value_counts()\\n        if 'college_students' in audience_counts and 'conservative_activists' in audience_counts:\\n            # Test would go here if N was sufficient\\n            results['H3'] = {\\\"status\\\": \\\"Not Testable\\\", \\\"reason\\\": \\\"Insufficient data for both 'college_students' and 'conservative_activists' groups.\\\"}\\n        else:\\n             results['H3'] = {\\\"status\\\": \\\"Not Testable\\\", \\\"reason\\\": \\\"Data for both 'college_students' and 'conservative_activists' groups not present.\\\"}\\n    else:\\n        results['H3'] = {\\\"status\\\": \\\"Not Testable\\\", \\\"reason\\\": \\\"Requires data from multiple audience types.\\\"}\\n\\n    # H4: Strategic Contradiction\\n    if 'strategic_contradiction_index' in df.columns:\\n        mean_sci = df['strategic_contradiction_index'].mean()\\n        results['H4'] = {\\\"status\\\": \\\"Testable (Exploratory)\\\", \\\"finding\\\": f\\\"The mean Strategic Contradiction Index is {mean_sci:.4f} for N={n}. Hypothesis suggests high values; this provides a single data point.\\\"}\\n    else:\\n        results['H4'] = {\\\"status\\\": \\\"Not Testable\\\", \\\"reason\\\": \\\"'strategic_contradiction_index' not found.\\\"}\\n\\n    # H5: Democratic Health\\n    if 'full_cohesion_index' in df.columns:\\n        mean_fci = df['full_cohesion_index'].mean()\\n        results['H5'] = {\\\"status\\\": \\\"Testable (Exploratory)\\\", \\\"finding\\\": f\\\"The mean Full Cohesion Index is {mean_fci:.4f} for N={n}. Hypothesis suggests negative values; this data point is {('negative' if mean_fci < 0 else 'positive')}.\\\"}\\n    else:\\n        results['H5'] = {\\\"status\\\": \\\"Not Testable\\\", \\\"reason\\\": \\\"'full_cohesion_index' not found.\\\"}\\n\\n    return results\\n\\ndef perform_statistical_analysis(data: List[Dict[str, Any]]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"\\n    Master function to orchestrate the entire statistical analysis pipeline.\\n\\n    Args:\\n        data: A list of raw analysis artifact dictionaries.\\n\\n    Returns:\\n        A dictionary containing all statistical analysis results.\\n    \\\"\\\"\\\"\\n    results = {}\\n    df = _prepare_data(data)\\n    groupings = _get_grouping_mappings()\\n\\n    results['descriptive_statistics'] = calculate_descriptive_statistics(df)\\n    results['correlation_analysis'] = perform_correlation_analysis(df)\\n    results['group_comparisons'] = perform_group_comparisons(df, groupings)\\n    results['hypothesis_testing'] = test_hypotheses(df, groupings)\\n    \\n    return results\\n\",\n  \"execution_results\": {\n    \"descriptive_statistics\": [\n      {\n        \"variable\": \"tribal_dominance_raw_score\",\n        \"count\": 1.0,\n        \"mean\": 0.9,\n        \"std\": null,\n        \"min\": 0.9,\n        \"25%\": 0.9,\n        \"50%\": 0.9,\n        \"75%\": 0.9,\n        \"max\": 0.9\n      },\n      {\n        \"variable\": \"tribal_dominance_salience\",\n        \"count\": 1.0,\n        \"mean\": 0.9,\n        \"std\": null,\n        \"min\": 0.9,\n        \"25%\": 0.9,\n        \"50%\": 0.9,\n        \"75%\": 0.9,\n        \"max\": 0.9\n      },\n      {\n        \"variable\": \"tribal_dominance_confidence\",\n        \"count\": 1.0,\n        \"mean\": 0.9,\n        \"std\": null,\n        \"min\": 0.9,\n        \"25%\": 0.9,\n        \"50%\": 0.9,\n        \"75%\": 0.9,\n        \"max\": 0.9\n      },\n      {\n        \"variable\": \"individual_dignity_raw_score\",\n        \"count\": 1.0,\n        \"mean\": 0.3,\n        \"std\": null,\n        \"min\": 0.3,\n        \"25%\": 0.3,\n        \"50%\": 0.3,\n        \"75%\": 0.3,\n        \"max\": 0.3\n      },\n      {\n        \"variable\": \"individual_dignity_salience\",\n        \"count\": 1.0,\n        \"mean\": 0.2,\n        \"std\": null,\n        \"min\": 0.2,\n        \"25%\": 0.2,\n        \"50%\": 0.2,\n        \"75%\": 0.2,\n        \"max\": 0.2\n      },\n      {\n        \"variable\": \"individual_dignity_confidence\",\n        \"count\": 1.0,\n        \"mean\": 0.8,\n        \"std\": null,\n        \"min\": 0.8,\n        \"25%\": 0.8,\n        \"50%\": 0.8,\n        \"75%\": 0.8,\n        \"max\": 0.8\n      },\n      {\n        \"variable\": \"fear_raw_score\",\n        \"count\": 1.0,\n        \"mean\": 0.8,\n        \"std\": null,\n        \"min\": 0.8,\n        \"25%\": 0.8,\n        \"50%\": 0.8,\n        \"75%\": 0.8,\n        \"max\": 0.8\n      },\n      {\n        \"variable\": \"fear_salience\",\n        \"count\": 1.0,\n        \"mean\": 0.8,\n        \"std\": null,\n        \"min\": 0.8,\n        \"25%\": 0.8,\n        \"50%\": 0.8,\n        \"75%\": 0.8,\n        \"max\": 0.8\n      },\n      {\n        \"variable\": \"fear_confidence\",\n        \"count\": 1.0,\n        \"mean\": 0.9,\n        \"std\": null,\n        \"min\": 0.9,\n        \"25%\": 0.9,\n        \"50%\": 0.9,\n        \"75%\": 0.9,\n        \"max\": 0.9\n      },\n      {\n        \"variable\": \"hope_raw_score\",\n        \"count\": 1.0,\n        \"mean\": 0.7,\n        \"std\": null,\n        \"min\": 0.7,\n        \"25%\": 0.7,\n        \"50%\": 0.7,\n        \"75%\": 0.7,\n        \"max\": 0.7\n      },\n      {\n        \"variable\": \"hope_salience\",\n        \"count\": 1.0,\n        \"mean\": 0.7,\n        \"std\": null,\n        \"min\": 0.7,\n        \"25%\": 0.7,\n        \"50%\": 0.7,\n        \"75%\": 0.7,\n        \"max\": 0.7\n      },\n      {\n        \"variable\": \"hope_confidence\",\n        \"count\": 1.0,\n        \"mean\": 0.9,\n        \"std\": null,\n        \"min\": 0.9,\n        \"25%\": 0.9,\n        \"50%\": 0.9,\n        \"75%\": 0.9,\n        \"max\": 0.9\n      },\n      {\n        \"variable\": \"envy_raw_score\",\n        \"count\": 1.0,\n        \"mean\": 0.5,\n        \"std\": null,\n        \"min\": 0.5,\n        \"25%\": 0.5,\n        \"50%\": 0.5,\n        \"75%\": 0.5,\n        \"max\": 0.5\n      },\n      {\n        \"variable\": \"envy_salience\",\n        \"count\": 1.0,\n        \"mean\": 0.4,\n        \"std\": null,\n        \"min\": 0.4,\n        \"25%\": 0.4,\n        \"50%\": 0.4,\n        \"75%\": 0.4,\n        \"max\": 0.4\n      },\n      {\n        \"variable\": \"envy_confidence\",\n        \"count\": 1.0,\n        \"mean\": 0.8,\n        \"std\": null,\n        \"min\": 0.8,\n        \"25%\": 0.8,\n        \"50%\": 0.8,\n        \"75%\": 0.8,\n        \"max\": 0.8\n      },\n      {\n        \"variable\": \"mudita_raw_score\",\n        \"count\": 1.0,\n        \"mean\": 0.1,\n        \"std\": null,\n        \"min\": 0.1,\n        \"25%\": 0.1,\n        \"50%\": 0.1,\n        \"75%\": 0.1,\n        \"max\": 0.1\n      },\n      {\n        \"variable\": \"mudita_salience\",\n        \"count\": 1.0,\n        \"mean\": 0.1,\n        \"std\": null,\n        \"min\": 0.1,\n        \"25%\": 0.1,\n        \"50%\": 0.1,\n        \"75%\": 0.1,\n        \"max\": 0.1\n      },\n      {\n        \"variable\": \"mudita_confidence\",\n        \"count\": 1.0,\n        \"mean\": 0.7,\n        \"std\": null,\n        \"min\": 0.7,\n        \"25%\": 0.7,\n        \"50%\": 0.7,\n        \"75%\": 0.7,\n        \"max\": 0.7\n      },\n      {\n        \"variable\": \"enmity_raw_score\",\n        \"count\": 1.0,\n        \"mean\": 0.9,\n        \"std\": null,\n        \"min\": 0.9,\n        \"25%\": 0.9,\n        \"50%\": 0.9,\n        \"75%\": 0.9,\n        \"max\": 0.9\n      },\n      {\n        \"variable\": \"enmity_salience\",\n        \"count\": 1.0,\n        \"mean\": 0.8,\n        \"std\": null,\n        \"min\": 0.8,\n        \"25%\": 0.8,\n        \"50%\": 0.8,\n        \"75%\": 0.8,\n        \"max\": 0.8\n      },\n      {\n        \"variable\": \"enmity_confidence\",\n        \"count\": 1.0,\n        \"mean\": 0.9,\n        \"std\": null,\n        \"min\": 0.9,\n        \"25%\": 0.9,\n        \"50%\": 0.9,\n        \"75%\": 0.9,\n        \"max\": 0.9\n      },\n      {\n        \"variable\": \"amity_raw_score\",\n        \"count\": 1.0,\n        \"mean\": 0.7,\n        \"std\": null,\n        \"min\": 0.7,\n        \"25%\": 0.7,\n        \"50%\": 0.7,\n        \"75%\": 0.7,\n        \"max\": 0.7\n      },\n      {\n        \"variable\": \"amity_salience\",\n        \"count\": 1.0,\n        \"mean\": 0.7,\n        \"std\": null,\n        \"min\": 0.7,\n        \"25%\": 0.7,\n        \"50%\": 0.7,\n        \"75%\": 0.7,\n        \"max\": 0.7\n      },\n      {\n        \"variable\": \"amity_confidence\",\n        \"count\": 1.0,\n        \"mean\": 0.9,\n        \"std\": null,\n        \"min\": 0.9,\n        \"25%\": 0.9,\n        \"50%\": 0.9,\n        \"75%\": 0.9,\n        \"max\": 0.9\n      },\n      {\n        \"variable\": \"fragmentative_goals_raw_score\",\n        \"count\": 1.0,\n        \"mean\": 0.7,\n        \"std\": null,\n        \"min\": 0.7,\n        \"25%\": 0.7,\n        \"50%\": 0.7,\n        \"75%\": 0.7,\n        \"max\": 0.7\n      },\n      {\n        \"variable\": \"fragmentative_goals_salience\",\n        \"count\": 1.0,\n        \"mean\": 0.7,\n        \"std\": null,\n        \"min\": 0.7,\n        \"25%\": 0.7,\n        \"50%\": 0.7,\n        \"75%\": 0.7,\n        \"max\": 0.7\n      },\n      {\n        \"variable\": \"fragmentative_goals_confidence\",\n        \"count\": 1.0,\n        \"mean\": 0.9,\n        \"std\": null,\n        \"min\": 0.9,\n        \"25%\": 0.9,\n        \"50%\": 0.9,\n        \"75%\": 0.9,\n        \"max\": 0.9\n      },\n      {\n        \"variable\": \"cohesive_goals_raw_score\",\n        \"count\": 1.0,\n        \"mean\": 0.9,\n        \"std\": null,\n        \"min\": 0.9,\n        \"25%\": 0.9,\n        \"50%\": 0.9,\n        \"75%\": 0.9,\n        \"max\": 0.9\n      },\n      {\n        \"variable\": \"cohesive_goals_salience\",\n        \"count\": 1.0,\n        \"mean\": 0.9,\n        \"std\": null,\n        \"min\": 0.9,\n        \"25%\": 0.9,\n        \"50%\": 0.9,\n        \"75%\": 0.9,\n        \"max\": 0.9\n      },\n      {\n        \"variable\": \"cohesive_goals_confidence\",\n        \"count\": 1.0,\n        \"mean\": 0.9,\n        \"std\": null,\n        \"min\": 0.9,\n        \"25%\": 0.9,\n        \"50%\": 0.9,\n        \"75%\": 0.9,\n        \"max\": 0.9\n      },\n      {\n        \"variable\": \"identity_tension\",\n        \"count\": 1.0,\n        \"mean\": 0.0,\n        \"std\": null,\n        \"min\": 0.0,\n        \"25%\": 0.0,\n        \"50%\": 0.0,\n        \"75%\": 0.0,\n        \"max\": 0.0\n      },\n      {\n        \"variable\": \"emotional_tension\",\n        \"count\": 1.0,\n        \"mean\": 0.048,\n        \"std\": null,\n        \"min\": 0.048,\n        \"25%\": 0.048,\n        \"50%\": 0.048,\n        \"75%\": 0.048,\n        \"max\": 0.048\n      },\n      {\n        \"variable\": \"success_tension\",\n        \"count\": 1.0,\n        \"mean\": 0.0,\n        \"std\": null,\n        \"min\": 0.0,\n        \"25%\": 0.0,\n        \"50%\": 0.0,\n        \"75%\": 0.0,\n        \"max\": 0.0\n      },\n      {\n        \"variable\": \"relational_tension\",\n        \"count\": 1.0,\n        \"mean\": 0.0,\n        \"std\": null,\n        \"min\": 0.0,\n        \"25%\": 0.0,\n        \"50%\": 0.0,\n        \"75%\": 0.0,\n        \"max\": 0.0\n      },\n      {\n        \"variable\": \"goal_tension\",\n        \"count\": 1.0,\n        \"mean\": 0.0,\n        \"std\": null,\n        \"min\": 0.0,\n        \"25%\": 0.0,\n        \"50%\": 0.0,\n        \"75%\": 0.0,\n        \"max\": 0.0\n      },\n      {\n        \"variable\": \"strategic_contradiction_index\",\n        \"count\": 1.0,\n        \"mean\": 0.0096,\n        \"std\": null,\n        \"min\": 0.0096,\n        \"25%\": 0.0096,\n        \"50%\": 0.0096,\n        \"75%\": 0.0096,\n        \"max\": 0.0096\n      },\n      {\n        \"variable\": \"descriptive_cohesion_index\",\n        \"count\": 1.0,\n        \"mean\": 0.071429,\n        \"std\": null,\n        \"min\": 0.071429,\n        \"25%\": 0.071429,\n        \"50%\": 0.071429,\n        \"75%\": 0.071429,\n        \"max\": 0.071429\n      },\n      {\n        \"variable\": \"motivational_cohesion_index\",\n        \"count\": 1.0,\n        \"mean\": 0.057143,\n        \"std\": null,\n        \"min\": 0.057143,\n        \"25%\": 0.057143,\n        \"50%\": 0.057143,\n        \"75%\": 0.057143,\n        \"max\": 0.057143\n      },\n      {\n        \"variable\": \"full_cohesion_index\",\n        \"count\": 1.0,\n        \"mean\": 0.028571,\n        \"std\": null,\n        \"min\": 0.028571,\n        \"25%\": 0.028571,\n        \"50%\": 0.028571,\n        \"75%\": 0.028571,\n        \"max\": 0.028571\n      }\n    ],\n    \"correlation_analysis\": {\n      \"status\": \"Skipped\",\n      \"note\": \"Correlation analysis requires at least 2 data points (TIER 3). Provided data has N < 2.\"\n    },\n    \"group_comparisons\": {\n      \"career_phase\": {\n        \"status\": \"Skipped\",\n        \"note\": \"Analysis requires at least 2 groups. Found 1 for 'career_phase'.\"\n      },\n      \"audience\": {\n        \"status\": \"Skipped\",\n        \"note\": \"Analysis requires at least 2 groups. Found 1 for 'audience'.\"\n      }\n    },\n    \"hypothesis_testing\": {\n      \"H1\": {\n        \"status\": \"Not Testable\",\n        \"reason\": \"Requires data from multiple career phases.\"\n      },\n      \"H2\": {\n        \"status\": \"Not Testable\",\n        \"reason\": \"Requires data from multiple event types/audiences.\"\n      },\n      \"H3\": {\n        \"status\": \"Not Testable\",\n        \"reason\": \"Requires data from multiple audience types.\"\n      },\n      \"H4\": {\n        \"status\": \"Testable (Exploratory)\",\n        \"finding\": \"The mean Strategic Contradiction Index is 0.0096 for N=1. Hypothesis suggests high values; this provides a single data point.\"\n      },\n      \"H5\": {\n        \"status\": \"Testable (Exploratory)\",\n        \"finding\": \"The mean Full Cohesion Index is 0.0286 for N=1. Hypothesis suggests negative values; this data point is positive.\"\n      }\n    }\n  },\n  \"sample_size_assessment\": {\n    \"total_documents\": 14,\n    \"tier_classification\": \"TIER 3 (Exploratory Analysis)\",\n    \"power_notes\": \"The experiment is designed for N=14, which falls into TIER 3 (Exploratory Analysis) for most group comparisons (e.g., career phase has 2-5 documents per group). The provided analysis artifacts contain data for only N=1 document. Consequently, all inferential statistics (correlations, group comparisons) are impossible. The analysis is restricted to descriptive reporting of the single data point and highlighting which planned analyses could not be performed.\"\n  },\n  \"methodology_summary\": \"The analysis was conducted under TIER 3 (Exploratory) guidelines due to an effective sample size of N=1 from the provided artifacts, against a planned N=14. The statistical functions were designed to be robust to this limitation. The process involved: (1) Parsing the raw artifacts into a structured DataFrame. (2) Calculating descriptive statistics for the single data point. (3) Implementing, but gracefully skipping, correlation and group comparison analyses due to insufficient data (N<2). (4) Evaluating the testability of each research hypothesis, finding that only H4 and H5 could be addressed in an exploratory capacity by reporting the single value for the relevant metric. The functions were built to utilize the corpus manifest for grouping, but this could not be fully demonstrated with the available data.\"\n}\n```",
  "analysis_artifacts_processed": 2,
  "cost_info": {
    "model": "vertex_ai/gemini-2.5-pro",
    "execution_time_seconds": 98.702793,
    "response_cost": 0.0,
    "input_tokens": 0,
    "output_tokens": 0,
    "total_tokens": 0,
    "prompt_length": 83004,
    "response_length": 25701
  },
  "timestamp": "2025-09-16T19:36:23.677051+00:00"
}