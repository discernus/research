import pandas as pd
import numpy as np
from scipy import stats
import json

def main(scores_df, evidence_df):
    """
    Performs statistical analysis based on the Character Assessment Framework v6.0.

    Args:
        scores_df (pd.DataFrame): DataFrame with dimensional scores.
        evidence_df (pd.DataFrame): DataFrame with quote evidence.

    Returns:
        dict: A dictionary containing the structured analysis results.
    """
    # Initialize the final results dictionary with required keys
    result_data = {
        'descriptive_stats': {},
        'hypothesis_tests': {},
        'correlations': {},
        'reliability_metrics': {}
    }

    try:
        # --- 1. Framework Configuration & Data Validation ---
        # Define dimensions based on the framework specification
        VIRTUES = ["dignity", "truth", "justice", "hope", "pragmatism"]
        VICES = ["tribalism", "manipulation", "resentment", "fear", "fantasy"]
        TENSION_PAIRS = {
            "dignity": "tribalism",
            "truth": "manipulation",
            "justice": "resentment",
            "hope": "fear",
            "pragmatism": "fantasy"
        }
        
        # Check if input DataFrames are empty
        if scores_df.empty:
            raise ValueError("Input 'scores_df' is empty. Cannot perform analysis.")
            
        # Define column groups for easier access
        score_cols = [f"{dim}_score" for dim in VIRTUES + VICES]
        salience_cols = [f"{dim}_salience" for dim in VIRTUES + VICES]
        confidence_cols = [f"{dim}_confidence" for dim in VIRTUES + VICES]
        
        # Ensure all required score columns are present
        for col in score_cols + salience_cols:
            if col not in scores_df.columns:
                raise ValueError(f"Missing required column in scores_df: {col}")

        # --- 2. Framework-Specific Calculations ---
        # Calculate Character Tension and MC-SCI for each artifact (row)
        tension_cols = []
        for virtue, vice in TENSION_PAIRS.items():
            tension_col_name = f"{virtue}_{vice}_tension"
            tension_cols.append(tension_col_name)
            
            # Formula: min(Virtue_score, Vice_score) * |Virtue_salience - Vice_salience|
            min_score = scores_df[[f"{virtue}_score", f"{vice}_score"]].min(axis=1)
            salience_diff = abs(scores_df[f"{virtue}_salience"] - scores_df[f"{vice}_salience"])
            scores_df[tension_col_name] = min_score * salience_diff

        # Calculate Moral Character Strategic Contradiction Index (MC-SCI)
        # Formula: (Sum of all Character Tension Scores) / Number of Opposing Pairs
        scores_df['mc_sci'] = scores_df[tension_cols].sum(axis=1) / len(TENSION_PAIRS)

        # --- 3. Descriptive Statistics ---
        stats_data = {}
        
        # Descriptive stats for primary scores, tensions, and MC-SCI
        analysis_cols = score_cols + tension_cols + ['mc_sci']
        desc_stats = scores_df[analysis_cols].describe().to_dict()
        stats_data['dimensional_scores'] = desc_stats
        
        # Descriptive stats for salience and confidence
        stats_data['salience_scores'] = scores_df[salience_cols].describe().to_dict()
        stats_data['confidence_scores'] = scores_df[confidence_cols].describe().to_dict()

        # Evidence-based statistics (using safe DataFrame methods)
        if not evidence_df.empty:
            evidence_stats = {
                'total_quotes': int(evidence_df.shape[0]),
                'quotes_per_dimension': evidence_df['dimension'].value_counts().to_dict(),
                'avg_confidence_per_dimension': evidence_df.groupby('dimension')['confidence_score'].mean().to_dict()
            }
            stats_data['evidence_summary'] = evidence_stats
        
        result_data['descriptive_stats'] = stats_data

        # --- 4. Reliability Metrics (Cronbach's Alpha) ---
        def cronbach_alpha(df_items):
            """Calculates Cronbach's alpha for a set of items."""
            if df_items.shape[1] < 2:
                return np.nan # Alpha is not defined for a single item
            k = df_items.shape[1]
            item_vars = df_items.var(axis=0, ddof=1).sum()
            total_var = df_items.sum(axis=1).var(ddof=1)
            if total_var == 0:
                return 1.0 # Or np.nan, perfect correlation or no variance
            return (k / (k - 1)) * (1 - item_vars / total_var)

        reliability_results = {}
        virtue_scores = scores_df[[f"{v}_score" for v in VIRTUES]]
        vice_scores = scores_df[[f"{v}_score" for v in VICES]]

        reliability_results['virtues_scale_alpha'] = cronbach_alpha(virtue_scores)
        reliability_results['vices_scale_alpha'] = cronbach_alpha(vice_scores)
        
        result_data['reliability_metrics'] = reliability_results

        # --- 5. Correlation Analysis ---
        # Calculate Pearson correlation matrix for key metrics
        correlation_matrix = scores_df[analysis_cols].corr(method='pearson')
        
        # Highlight key relationships as defined by the framework
        key_correlations = {}
        for virtue, vice in TENSION_PAIRS.items():
            corr_key = f"{virtue}_vs_{vice}"
            key_correlations[corr_key] = correlation_matrix.loc[f"{virtue}_score", f"{vice}_score"]
            
        key_correlations['mc_sci_vs_avg_virtue'] = correlation_matrix.loc['mc_sci', [f"{v}_score" for v in VIRTUES]].mean()
        key_correlations['mc_sci_vs_avg_vice'] = correlation_matrix.loc['mc_sci', [f"{v}_score" for v in VICES]].mean()

        result_data['correlations'] = {
            'full_matrix': correlation_matrix.to_dict(),
            'key_relationships': key_correlations
        }

        # --- 6. Hypothesis Testing ---
        hypothesis_results = {}

        # Hypothesis 1: Are mean virtue scores significantly different from mean vice scores?
        scores_df['mean_virtue_score'] = virtue_scores.mean(axis=1)
        scores_df['mean_vice_score'] = vice_scores.mean(axis=1)
        
        # Paired t-test is appropriate as scores are from the same artifacts
        ttest_virtue_vs_vice = stats.ttest_rel(scores_df['mean_virtue_score'], scores_df['mean_vice_score'], nan_policy='omit')
        
        # Effect size (Cohen's d for paired samples)
        diff = scores_df['mean_virtue_score'] - scores_df['mean_vice_score']
        cohens_d = diff.mean() / diff.std(ddof=1) if diff.std(ddof=1) != 0 else 0.0

        hypothesis_results['virtue_vs_vice_means'] = {
            'test_type': 'Paired T-test',
            'hypothesis': 'Mean virtue score is different from mean vice score.',
            'statistic': ttest_virtue_vs_vice.statistic,
            'p_value': ttest_virtue_vs_vice.pvalue,
            'effect_size_cohens_d': cohens_d
        }
        
        # Hypothesis 2: Is the Moral Contradiction Index (MC-SCI) significantly greater than zero?
        # One-sample t-test against a population mean of 0
        mc_sci_scores = scores_df['mc_sci'].dropna()
        if len(mc_sci_scores) > 1:
            ttest_mc_sci = stats.ttest_1samp(mc_sci_scores, popmean=0)
            hypothesis_results['mc_sci_significance'] = {
                'test_type': 'One-sample T-test',
                'hypothesis': 'The MC-SCI is significantly greater than 0, indicating moral contradiction.',
                'statistic': ttest_mc_sci.statistic,
                'p_value': ttest_mc_sci.pvalue / 2, # One-tailed test
                'mean_mc_sci': mc_sci_scores.mean()
            }

        result_data['hypothesis_tests'] = hypothesis_results

    except Exception as e:
        # Catch any exceptions during the analysis and report them
        result_data['error'] = f"An error occurred during analysis: {str(e)}"

    return result_data

# In the execution environment, scores_df and evidence_df are pre-loaded.
# The main function is called with these DataFrames to produce the result.
# This check ensures the code can also run if the script is executed directly
# with placeholder data, but it's not needed for the pipeline.
if 'scores_df' not in locals() or 'evidence_df' not in locals():
    # Create placeholder DataFrames for standalone testing if they don't exist
    scores_sample_data = [{'aid': 'artifact_1', 'dignity_score': 0.8, 'dignity_salience': 0.75, 'dignity_confidence': 0.9, 'truth_score': 0.6, 'truth_salience': 0.6, 'truth_confidence': 0.7, 'justice_score': 0.9, 'justice_salience': 0.9, 'justice_confidence': 0.95, 'hope_score': 0.7, 'hope_salience': 0.7, 'hope_confidence': 0.8, 'pragmatism_score': 0.5, 'pragmatism_salience': 0.5, 'pragmatism_confidence': 0.6, 'tribalism_score': 0.7, 'tribalism_salience': 0.7, 'tribalism_confidence': 0.8, 'manipulation_score': 0.7, 'manipulation_salience': 0.7, 'manipulation_confidence': 0.8, 'resentment_score': 0.6, 'resentment_salience': 0.6, 'resentment_confidence': 0.7, 'fear_score': 0.4, 'fear_salience': 0.3, 'fear_confidence': 0.5, 'fantasy_score': 0.3, 'fantasy_salience': 0.3, 'fantasy_confidence': 0.5}, {'aid': 'artifact_2', 'dignity_score': 0.8, 'dignity_salience': 0.7, 'dignity_confidence': 0.85, 'truth_score': 0.7, 'truth_salience': 0.6, 'truth_confidence': 0.8, 'justice_score': 0.9, 'justice_salience': 0.9, 'justice_confidence': 0.9, 'hope_score': 0.75, 'hope_salience': 0.7, 'hope_confidence': 0.85, 'pragmatism_score': 0.6, 'pragmatism_salience': 0.5, 'pragmatism_confidence': 0.75, 'tribalism_score': 0.4, 'tribalism_salience': 0.3, 'tribalism_confidence': 0.7, 'manipulation_score': 0.7, 'manipulation_salience': 0.6, 'manipulation_confidence': 0.8, 'resentment_score': 0.75, 'resentment_salience': 0.7, 'resentment_confidence': 0.85, 'fear_score': 0.3, 'fear_salience': 0.2, 'fear_confidence': 0.65, 'fantasy_score': 0.4, 'fantasy_salience': 0.3, 'fantasy_confidence': 0.7}, {'aid': 'artifact_3', 'dignity_score': 0.7, 'dignity_salience': 0.8, 'dignity_confidence': 0.85, 'truth_score': 0.6, 'truth_salience': 0.7, 'truth_confidence': 0.75, 'justice_score': 0.9, 'justice_salience': 0.95, 'justice_confidence': 0.95, 'hope_score': 0.8, 'hope_salience': 0.9, 'hope_confidence': 0.9, 'pragmatism_score': 0.7, 'pragmatism_salience': 0.8, 'pragmatism_confidence': 0.85, 'tribalism_score': 0.3, 'tribalism_salience': 0.4, 'tribalism_confidence': 0.6, 'manipulation_score': 0.2, 'manipulation_salience': 0.3, 'manipulation_confidence': 0.5, 'resentment_score': 0.1, 'resentment_salience': 0.2, 'resentment_confidence': 0.5, 'fear_score': 0.1, 'fear_salience': 0.2, 'fear_confidence': 0.5, 'fantasy_score': 0.0, 'fantasy_salience': 0.0, 'fantasy_confidence': 0.5}]
    evidence_sample_data = [{'aid': 'artifact_1', 'dimension': 'dignity', 'quote_id': 'q0', 'quote_text': 'text1', 'confidence_score': 0.95, 'context_type': 'direct'}, {'aid': 'artifact_2', 'dimension': 'justice', 'quote_id': 'q1', 'quote_text': 'text2', 'confidence_score': 0.9, 'context_type': 'action'}, {'aid': 'artifact_3', 'dimension': 'tribalism', 'quote_id': 'q2', 'quote_text': 'text3', 'confidence_score': 0.8, 'context_type': 'tactic'}]
    scores_df = pd.DataFrame(scores_sample_data)
    evidence_df = pd.DataFrame(evidence_sample_data)

# CRITICAL: The pipeline depends on this exact variable 'result_data' to capture the analysis.
result_data = main(scores_df, evidence_df)

# Optional: To view the results if running standalone, uncomment the line below.
# print(json.dumps(result_data, indent=2))