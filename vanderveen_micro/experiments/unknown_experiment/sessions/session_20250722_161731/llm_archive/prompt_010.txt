=== SYSTEM PROMPT ===
You are a framework analysis specialist. CRITICAL: Include all provenance stamps in your response.

=== USER PROMPT ===
Here are detailed instructions for analysis agents to implement the Van der Veen 2019 replication experiment using the PDAF v1.3 Tension Enhanced framework:

1. Data Extraction Agent Instructions:

- Extract full text content from each of the 7 specified corpus documents, preserving speech/document metadata.
- Clean and normalize the extracted text, removing any formatting artifacts or non-relevant content.
- Prepare a structured dataset with document ID, speaker/source, date, and full text content for each asset.
- Ensure the dataset is properly formatted for ingestion by the Analysis Agent.

2. Analysis Agent Instructions:

- For each document in the prepared dataset:
  1. Conduct a full PDAF v1.3 Tension Enhanced analysis as specified in the framework.
  2. Score all 9 populist anchors for both intensity (0.0-2.0) and salience (0.0-1.0).
  3. Provide at least 2 direct quotations supporting each populist anchor assessment.
  4. Calculate the 3 populist strategic tensions:
     - Democratic-Authoritarian Tension
     - Internal-External Focus Tension
     - Crisis-Elite Attribution Tension
  5. Compute the Populist Strategic Contradiction Index (PSCI).
  6. Classify the populist strategy pattern based on the PSCI score.
  7. Assess the Populist Salience Concentration.
  8. Determine the Economic Direction Indicator.
  9. Identify the primary Populist Strategic Focus based on salience rankings.

- Pay special attention to:
  - Differences between candidate speeches and party platforms
  - Variations in populist patterns between Republican and Democratic sources
  - Strategic tensions unique to each speaker/document

- Ensure all output adheres to the specified JSON schema in the framework's output contract.

3. Calculation Agent Instructions:

- Aggregate results from the Analysis Agent outputs:
  1. Calculate average Populist Intensity Index scores for:
     - All Trump speeches
     - All Sanders speeches
     - Cruz speech
     - Republican Party Platform
     - Democratic Party Platform
  2. Compute mean PSCI scores for the same groupings.
  3. Identify the most common Populist Strategic Focus for each group.
  4. Calculate the and salience for each populist anchor across all documents.
  5. Determine the overall distribution of populist strategy classifications.

- Prepare summary statistics and data visualizations to highlight:. Synthesis Agent Instructions:

- Compare PDAF v1.3 results to Van der Veen 2019 findings:
  1. Assess alignment on basic populist/non-populist classifications.
  2. Identify areas where PDAF v1.3 provides additional insights.
  3. Evaluate how tension analysis extends the original study's conclusions.

- Address the research questions specified in the experiment:
  - Replicate Van der Veen's populist classifications
  - Describe emerging strategic tension patterns
  - Compare Republican vs Democratic populist coherence
  - Contrast candidate speeches with party platforms
  - platform populist patterns
  - Highlight insights uniquely enabled by tension analysis through tension analysis

- Evaluate experiment success based on specified metrics:
  - Basic replication of Van der Veen classifications
  - Methodological extension through tension analysis
  - Research contribution via strategic communication insights

- Synthesize key findings into a cohesive narrative, focusing on:
  - Replication success and validation of PDAF v1.3
  - Novel insights enabled by tension-enhanced analysis
  - Theoreticalist discourse theory and future research directions

- Prepare a summary suitable for academic publication, including:
  - Methodology comparison (Van der Veen vs PDAF v1.3)
  - Key findings and their significance
  - Limitations of the current study
  - Recommendations for further populist discourse analysis using PDAF v1.3

PROVENANCE VERIFICATION:
You are analyzing content from file: scripts/README.md
Content hash: b765b9b16e84
Content preview: "# Discernus Development Scripts

This directory contains utility scripts for development, testing, a..."

CRITICAL: You must include this provenance stamp in your response: [PROVENANCE:b765b9b16e84@scripts/README.md]

TEXT TO ANALYZE:
# Discernus Development Scripts

This directory contains utility scripts for development, testing, and maintenance of the Discernus platform.

## Scripts Overview

| Script | Purpose | Usage |
|--------|---------|-------|
| `prompt_engineering_harness.py` | Test models/prompts directly | Development & debugging |
| `update_model_registry.py` | Update model registry from APIs | Maintenance |
| `check_environment.py` | Validate development environment | Environment setup |

## üéØ Prompt Engineering Harness

**File**: `prompt_engineering_harness.py`  
**Purpose**: Direct model and prompt testing for development workflows

### Quick Usage (Recommended)
```bash
# Use Make commands (handles environment automatically):
make harness-list                                              # List models
make harness-simple MODEL="vertex_ai/gemini-2.5-flash" PROMPT="Test prompt"
make harness-file MODEL="anthropic/claude-3-5-sonnet-20240620" FILE="prompt.txt"
```

### Direct Usage
```bash
# Always use proper environment activation:
source venv/bin/activate && python3 scripts/prompt_engineering_harness.py --help

# List available models grouped by provider
python3 scripts/prompt_engineering_harness.py --list-models

# Test with direct prompt
python3 scripts/prompt_engineering_harness.py \
  --model "openrouter/perplexity/r1-1776" \
  --prompt "What is quantum computing in 10 words?"

# Test with prompt from file
python3 scripts/prompt_engineering_harness.py \
  --model "vertex_ai/gemini-2.5-pro" \
  --prompt-file "test_prompt.txt"

# Test with experiment corpus
python3 scripts/prompt_engineering_harness.py \
  --model "anthropic/claude-3-5-sonnet-20240620" \
  --experiment "projects/simple_experiment" \
  --corpus "speech1.txt"
```

### Key Features
- **No Fallback**: Fails clearly when models don't work (perfect for debugging)
- **Direct Testing**: Tests exactly the specified model
- **Multiple Inputs**: Direct text, files, or experiment assets
- **Environment Integration**: Loads API keys from .env automatically
- **Clear Output**: Detailed success/failure information with token usage

### When to Use
- **Model Validation**: Test if specific models are accessible
- **Prompt Development**: Iterate on prompts with immediate feedback
- **API Debugging**: Verify authentication and model availability
- **Quick Testing**: Test models without full experiment setup

## üîß Environment Checker

**File**: `check_environment.py`  
**Purpose**: Validate development environment setup

### Usage
```bash
# Quick environment validation
python3 scripts/check_environment.py

# Or use Make command
make check
```

### What It Checks
- Project root location
- Python version and executable path
- Virtual environment status
- Core package availability (PyYAML, litellm, requests, python-dotenv)
- Environment file presence

### Output Example
```
üîç Environment Check
==================================================
‚úÖ Project root: /Volumes/code/discernus
üêç Python executable: /Volumes/code/discernus/venv/bin/python3
üêç Python version: 3.13.5
‚úÖ Virtual environment: ACTIVE
‚úÖ Python executable: Correct venv path
‚úÖ Core packages: Available
‚úÖ Environment file: Found

üéâ Environment check: ALL GOOD!
üí° Ready to run Discernus commands
```

## üìä Automated Model Registry Updater

**File**: `update_model_registry.py`  
**Purpose**: Automatically maintain the model registry (`discernus/gateway/models.yaml`) with up-to-date information from various LLM providers

### Problem Solved

The `models.yaml` file is the single source of truth for model availability, pricing, and rate limits. Manual maintenance leads to:
- ‚ùå Outdated model information (like the Claude 3.5 Sonnet issue)
- ‚ùå Missing newly released models
- ‚ùå Incorrect pricing information
- ‚ùå Stale rate limit data

### Solution

The automated updater:
- ‚úÖ **Queries providers directly** for available models
- ‚úÖ **Updates pricing** from LiteLLM's cost map
- ‚úÖ **Discovers new models** automatically
- ‚úÖ **Removes deprecated models** that are no longer available
- ‚úÖ **Runs on schedule** (weekly) via GitHub Actions
- ‚úÖ **Creates backups** before making changes

### Usage

```bash
# Full update (live changes)
python3 scripts/update_model_registry.py

# Dry run (show what would change)
python3 scripts/update_model_registry.py --dry-run

# Check if updates are needed
python3 scripts/update_model_registry.py --check
```

### GitHub Actions Integration

**Automatic Schedule:**
- Runs every Monday at 2 AM UTC
- Creates a pull request with changes for review

**Manual Trigger:**
1. Go to Actions tab in GitHub
2. Select "Update Model Registry"
3. Click "Run workflow"
4. Choose dry-run or live update

## Provider Support

### Current Providers

| Provider | Method | Models Discovered |
|----------|--------|------------------|
| **Vertex AI** | Known model list + regions | Claude 3.5/3.7/4, Gemini 2.5 |
| **OpenRouter** | API query | All available models |
| **Anthropic** | Known model list | Claude 3.5 Sonnet, Claude 3 Haiku |
| **OpenAI** | Known model list | GPT-4o, GPT-4o Mini |

### Adding New Providers

To add a new provider, extend the `ModelRegistryUpdater` class:

```python
def get_newprovider_models(self) -> List[Dict[str, Any]]:
    """Query NewProvider for available models"""
    try:
        # Query provider API
        response = requests.get("https://api.newprovider.com/models")
        # Parse and return model information
        return models
    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to query NewProvider: {e}")
        return []

# Add to run_update() method:
discovered_models = {
    'vertex_ai': self.get_vertex_ai_models(),
    'openrouter': self.get_openrouter_models(),
    'anthropic': self.get_anthropic_models(),
    'openai': self.get_openai_models(),
    'newprovider': self.get_newprovider_models(),  # Add this
}
```

## Configuration

### Model Registry Format

Each model entry in `models.yaml` follows this format:

```yaml
provider/model-id:
  provider: "provider_name"
  performance_tier: "top-tier" | "cost-effective" | "general-purpose"
  context_window: 200000
  costs:
    input_per_million_tokens: 3.00
    output_per_million_tokens: 15.00
  utility_tier: 1  # 1 = highest priority
  task_suitability: [synthesis, coordination, planning]
  optimal_batch_size: 8
  last_updated: "2025-01-15"
  review_by: "2025-07-15"
  notes: "Human-readable notes about the model"
  
  # Vertex AI specific
  regions:
    us-east5:
      tpm: 350000
      rpm: 80
```

### Auto-Discovery Fields

Models discovered automatically get these additional fields:

```yaml
auto_discovered: true
notes: "Auto-discovered model from provider"
```

## Integration with Experiment System

The updated `models.yaml` ensures that:

1. **Model Selection** works correctly (fixes Claude 3.5 Sonnet issue)
2. **Rate Limits** are accurate for planning
3. **Cost Estimation** uses current pricing
4. **New Models** are available for experiments

## Monitoring

### GitHub Actions Dashboard

Monitor the automated updates:
- Actions tab ‚Üí "Update Model Registry" workflow
- Check for failed runs or PRs requiring review

### Manual Checks

```bash
# Check if registry needs updates
python3 scripts/update_model_registry.py --check
echo $?  # 0 = up to date, 1 = needs updates

# Validate current registry
python3 -c "import yaml; yaml.safe_load(open('discernus/gateway/models.yaml'))"
```

## Troubleshooting

### Common Issues

**Script fails with import errors:**
```bash
pip install -r requirements.txt
pip install PyYAML requests
```

**No models discovered:**
- Check network connectivity
- Verify API credentials (if required)
- Check provider API status

**GitHub Action fails:**
- Check workflow permissions
- Verify Python dependencies
- Check for YAML syntax errors

### Recovery

If the automated update breaks something:

1. **Restore from backup:**
   ```bash
   cp discernus/gateway/models.yaml.backup discernus/gateway/models.yaml
   ```

2. **Reset to known good state:**
   ```bash
   git checkout HEAD~1 -- discernus/gateway/models.yaml
   ```

3. **Fix manually then run update:**
   ```bash
   # Fix the issue
   python3 scripts/update_model_registry.py --dry-run  # Test
   python3 scripts/update_model_registry.py  # Apply
   ```

## Development

### Testing Changes

Always test changes to the updater script:

```bash
# Test without making changes
python3 scripts/update_model_registry.py --dry-run

# Test with a copy of the config
cp discernus/gateway/models.yaml /tmp/models.yaml.test
python3 scripts/update_model_registry.py --config /tmp/models.yaml.test
```

### Extending Functionality

Common extensions:

1. **Add health checks** for model availability
2. **Implement caching** for provider API calls
3. **Add Slack notifications** for changes
4. **Create model usage analytics**
5. **Add cost optimization recommendations**

## Related Files

- `discernus/gateway/models.yaml` - The model registry itself
- `discernus/gateway/llm_gateway.py` - Uses the model registry
- `discernus/core/agent_registry.yaml` - Agent definitions
- `experiments/*/experiment.md` - Request specific models

## Support

For issues with the automated updater:
1. Check this documentation
2. Review GitHub Action logs
3. Test with `--dry-run` flag
4. Create an issue with error details 

Remember to include the provenance stamp [PROVENANCE:b765b9b16e84@scripts/README.md] in your response to verify content integrity.