=== SYSTEM PROMPT ===
You are a framework analysis specialist. CRITICAL: Include all provenance stamps in your response.

=== USER PROMPT ===
Here are detailed instructions for analysis agents to implement the Van der Veen 2019 replication experiment using the PDAF v1.3 Tension Enhanced framework:

1. Data Extraction Agent Instructions:

- Extract full text content from each of the 7 specified corpus documents, preserving speech/document metadata.
- Clean and normalize the extracted text, removing any formatting artifacts or non-relevant content.
- Prepare a structured dataset with document ID, speaker/source, date, and full text content for each asset.
- Ensure the dataset is properly formatted for ingestion by the Analysis Agent.

2. Analysis Agent Instructions:

- For each document in the prepared dataset:
  1. Conduct a full PDAF v1.3 Tension Enhanced analysis as specified in the framework.
  2. Score all 9 populist anchors for both intensity (0.0-2.0) and salience (0.0-1.0).
  3. Provide at least 2 direct quotations supporting each populist anchor assessment.
  4. Calculate the 3 populist strategic tensions:
     - Democratic-Authoritarian Tension
     - Internal-External Focus Tension
     - Crisis-Elite Attribution Tension
  5. Compute the Populist Strategic Contradiction Index (PSCI).
  6. Classify the populist strategy pattern based on the PSCI score.
  7. Assess the Populist Salience Concentration.
  8. Determine the Economic Direction Indicator.
  9. Identify the primary Populist Strategic Focus based on salience rankings.

- Pay special attention to:
  - Differences between candidate speeches and party platforms
  - Variations in populist patterns between Republican and Democratic sources
  - Strategic tensions unique to each speaker/document

- Ensure all output adheres to the specified JSON schema in the framework's output contract.

3. Calculation Agent Instructions:

- Aggregate results from the Analysis Agent outputs:
  1. Calculate average Populist Intensity Index scores for:
     - All Trump speeches
     - All Sanders speeches
     - Cruz speech
     - Republican Party Platform
     - Democratic Party Platform
  2. Compute mean PSCI scores for the same groupings.
  3. Identify the most common Populist Strategic Focus for each group.
  4. Calculate the and salience for each populist anchor across all documents.
  5. Determine the overall distribution of populist strategy classifications.

- Prepare summary statistics and data visualizations to highlight:. Synthesis Agent Instructions:

- Compare PDAF v1.3 results to Van der Veen 2019 findings:
  1. Assess alignment on basic populist/non-populist classifications.
  2. Identify areas where PDAF v1.3 provides additional insights.
  3. Evaluate how tension analysis extends the original study's conclusions.

- Address the research questions specified in the experiment:
  - Replicate Van der Veen's populist classifications
  - Describe emerging strategic tension patterns
  - Compare Republican vs Democratic populist coherence
  - Contrast candidate speeches with party platforms
  - platform populist patterns
  - Highlight insights uniquely enabled by tension analysis through tension analysis

- Evaluate experiment success based on specified metrics:
  - Basic replication of Van der Veen classifications
  - Methodological extension through tension analysis
  - Research contribution via strategic communication insights

- Synthesize key findings into a cohesive narrative, focusing on:
  - Replication success and validation of PDAF v1.3
  - Novel insights enabled by tension-enhanced analysis
  - Theoreticalist discourse theory and future research directions

- Prepare a summary suitable for academic publication, including:
  - Methodology comparison (Van der Veen vs PDAF v1.3)
  - Key findings and their significance
  - Limitations of the current study
  - Recommendations for further populist discourse analysis using PDAF v1.3

PROVENANCE VERIFICATION:
You are analyzing content from file: CLAUDE.md
Content hash: 118c32f26116
Content preview: "# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in t..."

CRITICAL: You must include this provenance stamp in your response: [PROVENANCE:118c32f26116@CLAUDE.md]

TEXT TO ANALYZE:
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Commands

### Development Commands
```bash
# Install dependencies
pip install -r requirements.txt

# Install with development dependencies
pip install -e ".[dev]"

# Run tests
python -m pytest discernus/tests/
pytest discernus/tests/

# Code quality checks
black --line-length 120 discernus/
isort --profile black --line-length 120 discernus/
flake8 discernus/

# CLI commands
python3 discernus_cli.py validate <framework_file> <experiment_file> <corpus_dir>
python3 discernus_cli.py execute <framework_file> <experiment_file> <corpus_dir>
python3 discernus_cli.py resume <project_path>
python3 discernus_cli.py list-frameworks
python3 discernus_cli.py test
```

### Quick Test Commands
```bash
# Run quick validation tests
python discernus/tests/quick_test.py

# Run comprehensive test suite
python discernus/tests/comprehensive_test_suite.py

# Run single test file
pytest discernus/tests/test_analysis_agent.py -v
```

### DiscernusLibrarian Commands
```bash
# Test DiscernusLibrarian research agent
python3 -m discernus.core.discernuslibrarian

# Results stored in discernus/librarian/
# - reports/ - Human-readable markdown reports
# - research_data/ - JSON data for programmatic access  
# - archives/ - Long-term storage by date/topic
```

## Architecture Overview

### THIN Philosophy
This codebase follows the "Thick LLM + Thin Software = Epistemic Trust" philosophy:
- **LLM Intelligence**: Analysis, reasoning, and content generation handled in prompts
- **Software Infrastructure**: Simple routing, storage, and execution
- **Natural Language Flow**: LLM-to-LLM communication without complex parsing
- **Centralized Prompts**: Prompts are part of agents that consume them, not hardcoded

### Core Components

**CLI Entry Point**: `discernus_cli.py` - Main command interface with validate/execute/resume commands

**Core Infrastructure** (`discernus/core/`):
- `spec_loader.py` - Loads V4 frameworks, V2 experiments, and corpus specifications
- `discernuslibrarian.py` - Citation-guided research agent using Vertex AI (located in `discernus/librarian/`)
- `project_chronolog.py` - Git-based provenance and session tracking
- `simple_llm_client.py` / `ultra_thin_llm_client.py` - LLM gateway abstractions

**Agent System** (`discernus/agents/`):
- `analysis_agent.py` - Primary text analysis using framework prompts
- `synthesis_agent.py` - Generates academic-quality reports
- `ensemble_configuration_agent.py` - Multi-model experiment orchestration
- Additional specialized agents for specific analytical tasks

**Orchestration** (`discernus/orchestration/`):
- `workflow_orchestrator.py` - Executes multi-step research workflows
- `ensemble_orchestrator.py` - Manages multi-model ensemble experiments

**Gateway** (`discernus/gateway/`):
- `llm_gateway.py` - Multi-provider LLM access via LiteLLM
- `model_registry.py` - Model configuration and cost management
- `models.yaml` - Model definitions and pricing

### Key Patterns

**Project Structure**: 
- Experiments live in `projects/` with framework, experiment, corpus, and results subdirectories
- Each project has a `PROJECT_CHRONOLOG.jsonl` for session tracking
- Results stored with full provenance in timestamped session directories

**Specification System**:
- V4 Framework specifications define analytical approaches in Markdown with YAML config
- V2 Experiment specifications define models, runs, and statistical plans
- V2 Corpus specifications define text collections with metadata

**Agent Communication**:
- Agents receive natural language prompts and return structured data (not code)
- State flows between agents as enriched dictionaries
- No complex JSON parsing - agents prompted to return clean, parseable responses

### Testing Strategy

**Test Categories**:
- `quick_test.py` - Fast validation of core functionality
- `comprehensive_test_suite.py` - Full system integration tests
- `workflow_integration_tests.py` - End-to-end workflow validation
- Mock LLM responses in `tests/fixtures/realistic_responses/`

**Test Execution**:
- Tests run independently without external LLM calls via mock responses
- Realistic test data generation for agent validation
- Agent isolation testing framework for individual component validation

### Development Guidelines

**THIN Compliance**:
- Keep software logic minimal - delegate intelligence to LLMs
- Avoid hardcoded prompts in orchestrator code
- Use natural language for agent-to-agent communication
- Prefer structured data return over code generation

**Cost Management**:
- Primary models use cost-effective Vertex AI Gemini 2.5 Flash ($0.13/$0.38 per 1M tokens)
- Premium models (Claude) reserved for critique and synthesis
- Transparent cost estimation in experiment planning

**Debugging**:
- Session logs capture full execution traces in `results/session_*/`
- Conversation logs in JSONL format for LLM interaction replay
- State snapshots saved after each workflow step for resumption

Remember to include the provenance stamp [PROVENANCE:118c32f26116@CLAUDE.md] in your response to verify content integrity.