#!/usr/bin/env python3
"""
Test Framework Integration
=========================

Quick test to validate that the framework-aware orchestrator enhancement
is working with CFF v2.0 framework specification.

This test manually adds framework_name to the ResearchConfig to trigger
the enhanced prompts and overwatch monitoring.
"""

import asyncio
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from discernus.orchestration.orchestrator import ThinOrchestrator, ResearchConfig

async def test_cff_framework_integration():
    """Test CFF v2.0 framework integration with enhanced orchestrator"""
    
    print("ğŸ¯ Testing CFF v2.0 Framework Integration")
    print("=" * 50)
    
    # Sample Trump speech text (short excerpt for testing)
    trump_speech = """
    Our country is in serious trouble. We don't have victories anymore. 
    When Mexico sends its people, they're not sending their best. They're 
    bringing drugs. They're bringing crime. They're rapists. And some, I assume, 
    are good people. I will be the greatest jobs president that God ever created.
    """
    
    # Create research config
    config = ResearchConfig(
        research_question="""
        Analyze this speech using the Cohesive Flourishing Framework (CFF) v2.0, 
        focusing on the Identity axis (Individual Dignity vs Tribal Dominance).
        Provide numerical scores with evidence citations and confidence levels.
        """,
        source_texts=trump_speech,
        enable_code_execution=False,  # Keep simple for this test
        dev_mode=False  # Manual test, not automated
    )
    
    # CRITICAL: Add framework name to trigger enhanced prompts
    config.framework_name = "cohesive_flourishing_framework_v2"
    
    print(f"âœ… Research Question: {config.research_question.strip()}")
    print(f"âœ… Framework: {config.framework_name}")
    print(f"âœ… Source text length: {len(config.source_texts)} characters")
    print()
    
    # Initialize orchestrator
    orchestrator = ThinOrchestrator(str(project_root))
    
    try:
        # Start session
        print("ğŸš€ Starting research session...")
        session_id = await orchestrator.start_research_session(config)
        print(f"âœ… Session started: {session_id}")
        
        # Run design consultation 
        print("\nğŸ¨ Running design consultation...")
        design_response = await orchestrator.run_design_consultation(session_id)
        
        # Better error handling for None response
        if design_response is None:
            print("âŒ Design response is None - checking for LLM client issues")
            if orchestrator.llm_client:
                print("âœ… LLM client exists")
                print(f"âœ… LiteLLM available: {orchestrator.llm_client.available}")
                if hasattr(orchestrator.llm_client, 'vertex_config'):
                    print(f"âœ… Vertex AI config: {bool(orchestrator.llm_client.vertex_config)}")
            else:
                print("âŒ No LLM client found")
            
            # Try to use mock response
            print("ğŸ”§ Using mock design response for testing...")
            design_response = """
            I propose a comprehensive CFF v2.0 analysis approach:
            
            1. **Text Analysis Expert**: Analyze the speech for Individual Dignity vs Tribal Dominance themes
            2. **CFF_Analyst_LLM**: Apply CFF v2.0 framework with numerical scoring
            3. **Evidence Validation Expert**: Verify citations and confidence levels
            
            The moderator should orchestrate these experts to produce structured JSON output
            with numerical scores, evidence citations, and confidence levels as required
            by the Cohesive Flourishing Framework v2.0.
            """
        
        print(f"âœ… Design response length: {len(design_response)} characters")
        print(f"ğŸ“„ Design preview: {design_response[:200]}...")
        
        # Approve design
        print("\nâœ… Approving design for execution...")
        orchestrator.approve_design(session_id, True, "Approved for framework testing")
        
        # Execute analysis
        print("\nğŸ”¬ Executing analysis with framework enhancement...")
        results = await orchestrator.execute_approved_analysis(session_id)
        
        print(f"âœ… Analysis completed!")
        print(f"ğŸ“Š Results: {results}")
        
        # Get session info for log location
        session_info = orchestrator.get_session_status(session_id)
        session_path = session_info.get('session_path')
        
        print(f"\nğŸ“ Session logs saved to: {session_path}")
        
        # Check if conversation log exists
        if session_path and Path(session_path).exists():
            log_files = list(Path(session_path).glob("*.jsonl"))
            if log_files:
                print(f"ğŸ“‹ Conversation log: {log_files[0]}")
                
                # Quick check of the log content
                with open(log_files[0], 'r') as f:
                    lines = f.readlines()
                    print(f"ğŸ’¬ Conversation has {len(lines)} messages")
                    
                    # Look for framework context in messages
                    framework_mentions = 0
                    json_mentions = 0
                    score_mentions = 0
                    
                    for line in lines:
                        if "framework" in line.lower():
                            framework_mentions += 1
                        if "json" in line.lower():
                            json_mentions += 1
                        if "score" in line.lower():
                            score_mentions += 1
                    
                    print(f"ğŸ” Framework mentions: {framework_mentions}")
                    print(f"ğŸ” JSON mentions: {json_mentions}")
                    print(f"ğŸ” Score mentions: {score_mentions}")
                    
                    if framework_mentions > 0:
                        print("âœ… Framework context appears to be injected!")
                    else:
                        print("âŒ No framework context detected")
                        
                    if json_mentions > 0 or score_mentions > 0:
                        print("âœ… Structured output requirements appear to be present!")
                    else:
                        print("âŒ No structured output requirements detected")
            else:
                print("âŒ No conversation log found")
        else:
            print("âŒ Session path not found")
        
        print(f"\nğŸ‰ Test completed!")
        print(f"Check the conversation log to see if agents now produce structured outputs with numerical scores.")
        
        return True
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸ”§ CFF v2.0 Framework Integration Test")
    print("Testing THIN Route 1 solution in the actual orchestrator")
    print()
    
    # Run the test
    success = asyncio.run(test_cff_framework_integration())
    
    if success:
        print("\nâœ… Integration test completed - check conversation logs for structured outputs!")
        sys.exit(0)
    else:
        print("\nâŒ Integration test failed")
        sys.exit(1) 