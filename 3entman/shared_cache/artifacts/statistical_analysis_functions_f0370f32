{
  "status": "success",
  "functions_generated": 5,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 18989,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: entman_framing_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-08-29T19:05:17.709232+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates the five derived metrics specified in the Entman Framing Functions Framework v10.0.\n\n    This function computes:\n    1. Message Completeness Index: Arithmetic mean of the four raw framing scores.\n    2. Framing Coherence Index: Geometric mean of the four raw framing scores.\n    3. Salience-Weighted Message Completeness: Raw scores weighted by their salience.\n    4. Strategic Framing Profile: The framing function with the highest salience.\n    5. Framing Independence Score: Standard deviation of the four raw framing scores.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data with columns for raw scores and salience for each of the four dimensions.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pd.DataFrame: The input DataFrame with five additional columns for the derived metrics, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = data.copy()\n\n        raw_scores = [\n            'problem_definition_raw',\n            'causal_attribution_raw',\n            'moral_evaluation_raw',\n            'treatment_recommendation_raw'\n        ]\n        salience_scores = [\n            'problem_definition_salience',\n            'causal_attribution_salience',\n            'moral_evaluation_salience',\n            'treatment_recommendation_salience'\n        ]\n\n        # Ensure all required columns are present\n        required_cols = raw_scores + salience_scores\n        if not all(col in df.columns for col in required_cols):\n            # Find missing columns for a more informative error\n            missing = [col for col in required_cols if col not in df.columns]\n            # This would ideally log an error, but per spec we return None\n            # print(f\"Error: Missing required columns: {missing}\")\n            return None\n\n        # 1. Message Completeness Index\n        df['message_completeness_index'] = df[raw_scores].mean(axis=1)\n\n        # 2. Framing Coherence Index\n        # Geometric mean: (x1 * x2 * ... * xn)^(1/n)\n        # If any score is 0, the geometric mean is 0.\n        df['framing_coherence_index'] = np.power(df[raw_scores].prod(axis=1), 1.0/len(raw_scores))\n\n        # 3. Salience-Weighted Message Completeness\n        numerator = (df['problem_definition_raw'] * df['problem_definition_salience'] +\n                     df['causal_attribution_raw'] * df['causal_attribution_salience'] +\n                     df['moral_evaluation_raw'] * df['moral_evaluation_salience'] +\n                     df['treatment_recommendation_raw'] * df['treatment_recommendation_salience'])\n        denominator = df[salience_scores].sum(axis=1) + 0.001  # Add small epsilon for stability\n        df['salience_weighted_message_completeness'] = numerator / denominator\n\n        # 4. Strategic Framing Profile\n        # 0=Problem, 1=Cause, 2=Values, 3=Solution\n        df['strategic_framing_profile'] = df[salience_scores].idxmax(axis=1).map({\n            'problem_definition_salience': 0,\n            'causal_attribution_salience': 1,\n            'moral_evaluation_salience': 2,\n            'treatment_recommendation_salience': 3\n        })\n\n        # 5. Framing Independence Score\n        df['framing_independence_score'] = df[raw_scores].std(axis=1)\n\n        return df\n\n    except Exception:\n        return None\n\ndef descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Generates descriptive statistics for raw scores, salience, and derived metrics.\n\n    Methodology:\n    This function first calculates the derived metrics using the `calculate_derived_metrics` helper function.\n    It then computes standard descriptive statistics (mean, std, min, 25%, 50%, 75%, max) for all\n    primary and derived numerical columns using pandas' .describe() method.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each relevant column, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        # Calculate derived metrics first\n        data_with_metrics = calculate_derived_metrics(data)\n        if data_with_metrics is None:\n            return None\n\n        analysis_cols = [\n            'problem_definition_raw', 'causal_attribution_raw', 'moral_evaluation_raw', 'treatment_recommendation_raw',\n            'problem_definition_salience', 'causal_attribution_salience', 'moral_evaluation_salience', 'treatment_recommendation_salience',\n            'message_completeness_index', 'framing_coherence_index', 'salience_weighted_message_completeness',\n            'framing_independence_score'\n        ]\n        \n        # Ensure columns exist before describing\n        existing_cols = [col for col in analysis_cols if col in data_with_metrics.columns]\n        if not existing_cols:\n            return None\n\n        stats = data_with_metrics[existing_cols].describe().to_dict()\n        \n        # Add frequency counts for the categorical strategic profile\n        if 'strategic_framing_profile' in data_with_metrics.columns:\n            profile_map = {0: 'Problem-focused', 1: 'Cause-focused', 2: 'Values-focused', 3: 'Solution-focused'}\n            profile_counts = data_with_metrics['strategic_framing_profile'].map(profile_map).value_counts().to_dict()\n            stats['strategic_framing_profile_counts'] = profile_counts\n\n        return stats\n\n    except Exception:\n        return None\n\ndef test_framing_function_independence(data, **kwargs):\n    \"\"\"\n    Tests the Framing Function Independence hypothesis (H1) via correlation analysis.\n\n    Methodology:\n    This function calculates a Pearson correlation matrix for the four raw framing function scores:\n    - problem_definition_raw\n    - causal_attribution_raw\n    - moral_evaluation_raw\n    - treatment_recommendation_raw\n    Low correlation coefficients support Entman's independence hypothesis, suggesting that the\n    framing functions vary independently of each other. High correlations would challenge it.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing the correlation matrix and its interpretation, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        raw_scores_cols = [\n            'problem_definition_raw',\n            'causal_attribution_raw',\n            'moral_evaluation_raw',\n            'treatment_recommendation_raw'\n        ]\n        \n        if not all(col in data.columns for col in raw_scores_cols):\n            return None\n        \n        if len(data) < 2:\n            return {\"error\": \"Insufficient data for correlation analysis (requires at least 2 rows).\"}\n\n        correlation_matrix = data[raw_scores_cols].corr(method='pearson')\n\n        results = {\n            \"correlation_matrix\": correlation_matrix.to_dict(),\n            \"interpretation\": \"Pearson correlation coefficients between the four framing functions. Values near 0 support the independence hypothesis (H1). Values near 1 or -1 suggest a strong linear relationship, challenging the hypothesis.\"\n        }\n        return results\n\n    except Exception:\n        return None\n\ndef analyze_strategic_deployment_by_type(data, **kwargs):\n    \"\"\"\n    Tests the Strategic Function Deployment hypothesis (H2) using ANOVA.\n\n    Methodology:\n    1.  Assigns a 'document_type' to each row based on its 'document_name'. The mapping is derived from the corpus design.\n    2.  For each of the four framing functions, it performs a one-way ANOVA to test if there are statistically\n        significant differences in the mean scores across the different document types.\n    3.  A low p-value (e.g., < 0.05) suggests that the deployment of that framing function differs significantly\n        across communication types, supporting H2.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary with ANOVA results (F-statistic, p-value) for each framing function, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import f_oneway\n\n    def _get_document_type(filename):\n        if 'campaign' in filename or 'attack_ad' in filename:\n            return 'Campaign'\n        elif 'policy_advocacy' in filename:\n            return 'Policy Advocacy'\n        elif 'crisis_management' in filename or 'apology' in filename:\n            return 'Crisis Management'\n        elif 'corporate' in filename:\n            return 'Corporate'\n        else:\n            return 'Unknown'\n\n    try:\n        if data is None or data.empty or 'document_name' not in data.columns:\n            return None\n\n        df = data.copy()\n        df['document_type'] = df['document_name'].apply(_get_document_type)\n        \n        # Filter out 'Unknown' type and ensure there are at least two groups to compare\n        df = df[df['document_type'] != 'Unknown']\n        if df['document_type'].nunique() < 2:\n            return {\"error\": \"Insufficient document types for comparison (requires at least 2).\"}\n\n        results = {}\n        framing_dims = [\n            'problem_definition_raw',\n            'causal_attribution_raw',\n            'moral_evaluation_raw',\n            'treatment_recommendation_raw'\n        ]\n\n        for dim in framing_dims:\n            if dim not in df.columns:\n                results[dim] = {\"error\": f\"Column '{dim}' not found.\"}\n                continue\n\n            groups = [df[dim][df['document_type'] == dtype].dropna() for dtype in df['document_type'].unique()]\n            \n            # ANOVA requires at least one value in each group\n            if any(len(g) == 0 for g in groups):\n                 results[dim] = {\"error\": \"One or more document types have no data for this dimension.\"}\n                 continue\n\n            f_stat, p_value = f_oneway(*groups)\n            results[dim] = {\n                'f_statistic': f_stat,\n                'p_value': p_value,\n                'interpretation': f\"ANOVA test for differences in '{dim}' across document types. A p-value < 0.05 suggests significant differences.\"\n            }\n\n        return results\n\n    except Exception:\n        return None\n\ndef analyze_strategic_profile_distribution(data, **kwargs):\n    \"\"\"\n    Analyzes the distribution of Strategic Framing Profiles across document types using a Chi-square test.\n\n    Methodology:\n    This function addresses H2 by examining if the choice of primary framing strategy (the 'Strategic Framing Profile')\n    is dependent on the type of communication.\n    1.  Calculates derived metrics, including 'strategic_framing_profile'.\n    2.  Assigns a 'document_type' based on 'document_name'.\n    3.  Creates a contingency table (crosstab) of profile vs. type.\n    4.  Performs a Chi-square test of independence. A low p-value (< 0.05) suggests a significant association\n        between document type and the chosen framing profile, supporting H2.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary with the Chi-square test results, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import chi2_contingency\n\n    def _get_document_type(filename):\n        if 'campaign' in filename or 'attack_ad' in filename:\n            return 'Campaign'\n        elif 'policy_advocacy' in filename:\n            return 'Policy Advocacy'\n        elif 'crisis_management' in filename or 'apology' in filename:\n            return 'Crisis Management'\n        elif 'corporate' in filename:\n            return 'Corporate'\n        else:\n            return 'Unknown'\n\n    try:\n        # Calculate derived metrics to get 'strategic_framing_profile'\n        df = calculate_derived_metrics(data)\n        if df is None or df.empty or 'document_name' not in df.columns or 'strategic_framing_profile' not in df.columns:\n            return None\n\n        df['document_type'] = df['document_name'].apply(_get_document_type)\n        \n        # Filter out 'Unknown' type\n        df = df[df['document_type'] != 'Unknown']\n        \n        if df['document_type'].nunique() < 2 or df['strategic_framing_profile'].nunique() < 2:\n            return {\"error\": \"Insufficient variation in document types or profiles for Chi-square test.\"}\n\n        contingency_table = pd.crosstab(df['document_type'], df['strategic_framing_profile'])\n        \n        if contingency_table.empty:\n            return {\"error\": \"Contingency table is empty.\"}\n\n        chi2, p, dof, expected = chi2_contingency(contingency_table)\n\n        profile_map = {0: 'Problem', 1: 'Cause', 2: 'Values', 3: 'Solution'}\n        contingency_table.columns = contingency_table.columns.map(profile_map)\n\n        results = {\n            'chi2_statistic': chi2,\n            'p_value': p,\n            'degrees_of_freedom': dof,\n            'contingency_table': contingency_table.to_dict(),\n            'interpretation': \"Chi-square test of independence between document type and strategic framing profile. A p-value < 0.05 suggests the choice of framing profile is associated with the type of communication.\"\n        }\n        return results\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}