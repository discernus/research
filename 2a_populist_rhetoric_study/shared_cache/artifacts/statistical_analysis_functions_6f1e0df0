{
  "status": "success",
  "functions_generated": 6,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 23205,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: populist_discourse_factorial_analysis\nDescription: Statistical analysis experiment\nGenerated: 2025-08-29T03:23:28.723862+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_all_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived metrics as specified in the PDAF v10.0 framework.\n\n    This function computes three strategic tension indices, the overall Populist\n    Strategic Contradiction Index (PSCI), and four salience-weighted indices.\n    These metrics are essential for subsequent statistical analyses. The formulas\n    are taken directly from the 'derived_metrics' section of the framework specification.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw and salience scores\n                             for the nine core dimensions. Must use the exact\n                             column names from the experiment specification.\n        **kwargs: Not used.\n\n    Returns:\n        pd.DataFrame: The original DataFrame with added columns for each\n                      derived metric, or None if essential columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        df = data.copy()\n\n        # Define column names for clarity and to match the spec\n        # Raw Scores\n        pop_sov_raw = 'popular_sovereignty_claims_raw'\n        anti_plur_raw = 'anti_pluralist_exclusion_raw'\n        homo_ppl_raw = 'homogeneous_people_construction_raw'\n        nat_excl_raw = 'nationalist_exclusion_raw'\n        crisis_raw = 'crisis_restoration_narrative_raw'\n        elite_consp_raw = 'elite_conspiracy_systemic_corruption_raw'\n        manichaean_raw = 'manichaean_people_elite_framing_raw'\n        auth_raw = 'authenticity_vs_political_class_raw'\n        econ_pop_raw = 'economic_populist_appeals_raw'\n\n        # Salience Scores\n        pop_sov_sal = 'popular_sovereignty_claims_salience'\n        anti_plur_sal = 'anti_pluralist_exclusion_salience'\n        homo_ppl_sal = 'homogeneous_people_construction_salience'\n        nat_excl_sal = 'nationalist_exclusion_salience'\n        crisis_sal = 'crisis_restoration_narrative_salience'\n        elite_consp_sal = 'elite_conspiracy_systemic_corruption_salience'\n        manichaean_sal = 'manichaean_people_elite_framing_salience'\n        auth_sal = 'authenticity_vs_political_class_salience'\n        econ_pop_sal = 'economic_populist_appeals_salience'\n\n        required_cols = [\n            pop_sov_raw, anti_plur_raw, homo_ppl_raw, nat_excl_raw, crisis_raw,\n            elite_consp_raw, manichaean_raw, auth_raw, econ_pop_raw,\n            pop_sov_sal, anti_plur_sal, homo_ppl_sal, nat_excl_sal, crisis_sal,\n            elite_consp_sal, manichaean_sal, auth_sal, econ_pop_sal\n        ]\n        if not all(col in df.columns for col in required_cols):\n            # Missing one or more required columns for calculation\n            return None\n\n        # 1. Tension Indices\n        df['democratic_authoritarian_tension'] = np.minimum(df[pop_sov_raw], df[anti_plur_raw]) * \\\n                                                  np.abs(df[pop_sov_sal] - df[anti_plur_sal])\n\n        df['internal_external_focus_tension'] = np.minimum(df[homo_ppl_raw], df[nat_excl_raw]) * \\\n                                                 np.abs(df[homo_ppl_sal] - df[nat_excl_sal])\n\n        df['crisis_elite_attribution_tension'] = np.minimum(df[crisis_raw], df[elite_consp_raw]) * \\\n                                                  np.abs(df[crisis_sal] - df[elite_consp_sal])\n\n        # 2. Populist Strategic Contradiction Index (PSCI)\n        df['populist_strategic_contradiction_index'] = (df['democratic_authoritarian_tension'] +\n                                                         df['internal_external_focus_tension'] +\n                                                         df['crisis_elite_attribution_tension']) / 3\n\n        # 3. Salience-Weighted Indices\n        # Core Populism\n        core_numerator = (df[manichaean_raw] * df[manichaean_sal] +\n                          df[crisis_raw] * df[crisis_sal] +\n                          df[pop_sov_raw] * df[pop_sov_sal] +\n                          df[anti_plur_raw] * df[anti_plur_sal])\n        core_denominator = (df[manichaean_sal] + df[crisis_sal] +\n                            df[pop_sov_sal] + df[anti_plur_sal] + 0.001)\n        df['salience_weighted_core_populism_index'] = core_numerator / core_denominator\n\n        # Populism Mechanisms\n        mech_numerator = (df[elite_consp_raw] * df[elite_consp_sal] +\n                          df[auth_raw] * df[auth_sal] +\n                          df[homo_ppl_raw] * df[homo_ppl_sal])\n        mech_denominator = (df[elite_consp_sal] + df[auth_sal] +\n                            df[homo_ppl_sal] + 0.001)\n        df['salience_weighted_populism_mechanisms_index'] = mech_numerator / mech_denominator\n\n        # Boundary Distinctions\n        bound_numerator = (df[nat_excl_raw] * df[nat_excl_sal] +\n                           df[econ_pop_raw] * df[econ_pop_sal])\n        bound_denominator = (df[nat_excl_sal] + df[econ_pop_sal] + 0.001)\n        df['salience_weighted_boundary_distinctions_index'] = bound_numerator / bound_denominator\n\n        # Overall Populism\n        all_saliences = df[[col for col in df.columns if col.endswith('_salience')]]\n        overall_numerator = (core_numerator + mech_numerator + bound_numerator)\n        overall_denominator = all_saliences.sum(axis=1) + 0.001\n        df['salience_weighted_overall_populism_index'] = overall_numerator / overall_denominator\n\n        return df\n\n    except Exception:\n        return None\n\ndef analyze_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for all core dimensions and derived metrics.\n\n    This function first calculates the derived metrics specified in the PDAF v10.0\n    framework. It then computes the mean score for each document across multiple\n    evaluations. Finally, it generates descriptive statistics (mean, std, min,\n    25%, 50%, 75%, max) for all raw scores and derived metrics across the corpus.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data with\n                             multiple evaluations per document.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing a table of descriptive statistics,\n              or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # Calculate derived metrics first\n        df_derived = calculate_all_derived_metrics(data)\n        if df_derived is None:\n            return None\n\n        # Define columns for analysis\n        raw_score_cols = [col for col in df_derived.columns if col.endswith('_raw')]\n        derived_cols = [\n            'democratic_authoritarian_tension', 'internal_external_focus_tension',\n            'crisis_elite_attribution_tension', 'populist_strategic_contradiction_index',\n            'salience_weighted_core_populism_index', 'salience_weighted_populism_mechanisms_index',\n            'salience_weighted_boundary_distinctions_index', 'salience_weighted_overall_populism_index'\n        ]\n        analysis_cols = raw_score_cols + derived_cols\n\n        if 'document_name' not in df_derived.columns or len(df_derived['document_name'].unique()) < 1:\n            return None\n\n        # Average scores across evaluations for each document\n        df_agg = df_derived.groupby('document_name')[analysis_cols].mean().reset_index()\n\n        if df_agg.empty:\n            return None\n\n        # Calculate descriptive statistics\n        descriptives = df_agg[analysis_cols].describe().to_dict()\n\n        return {\"descriptive_statistics\": descriptives}\n\n    except Exception:\n        return None\n\ndef analyze_evaluation_reliability(data, **kwargs):\n    \"\"\"\n    Calculates inter-rater reliability using Cronbach's Alpha for each dimension.\n\n    This analysis addresses the \"Multi-evaluation reliability\" requirement. It\n    assesses the consistency of scores across the multiple independent evaluations\n    for each document. The function pivots the data to create a (documents x evaluations)\n    matrix for each dimension and calculates Cronbach's Alpha. A high alpha\n    (typically > 0.7) indicates good reliability.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data with\n                             multiple evaluations per document.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary with Cronbach's Alpha scores for each raw dimension,\n              or None if data is not suitable for reliability analysis.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    try:\n        # Pingouin is a standard, user-friendly library for stats.\n        import pingouin as pg\n    except ImportError:\n        # Handle case where pingouin is not installed in the environment\n        return {\"error\": \"The 'pingouin' library is required for this analysis.\"}\n\n    try:\n        if 'document_name' not in data.columns:\n            return None\n\n        # Ensure there are multiple evaluations per document\n        eval_counts = data['document_name'].value_counts()\n        if eval_counts.min() < 2:\n            return {\"error\": \"Insufficient evaluations per document for reliability analysis.\"}\n\n        raw_score_cols = [col for col in data.columns if col.endswith('_raw')]\n        reliability_results = {}\n\n        for dim_col in raw_score_cols:\n            # Create a 'rater' ID for each evaluation within a document group\n            df_dim = data[['document_name', dim_col]].copy()\n            df_dim['rater'] = df_dim.groupby('document_name').cumcount() + 1\n\n            # Pivot data to wide format: rows=documents, cols=raters, values=scores\n            df_wide = df_dim.pivot(index='document_name', columns='rater', values=dim_col)\n            \n            # Drop documents with missing evaluations for this specific dimension\n            df_wide.dropna(inplace=True)\n\n            if df_wide.shape[0] < 2 or df_wide.shape[1] < 2:\n                reliability_results[dim_col] = {'cronbach_alpha': None, 'notes': 'Insufficient data after pivot.'}\n                continue\n\n            # Calculate Cronbach's Alpha\n            alpha_stats = pg.cronbach_alpha(data=df_wide)\n            reliability_results[dim_col] = {\n                'cronbach_alpha': alpha_stats[0],\n                'confidence_interval_95': list(alpha_stats[1])\n            }\n\n        return {\"evaluation_reliability\": reliability_results}\n\n    except Exception:\n        return None\n\ndef analyze_populist_dimension_correlations(data, **kwargs):\n    \"\"\"\n    Analyzes relationships between populist anchor dimensions and strategic tension.\n\n    This function addresses RQ1 and RQ4 by computing a Pearson correlation matrix.\n    It examines the relationships between all nine populist raw scores and the\n    key derived metrics, particularly the Populist Strategic Contradiction Index (PSCI).\n    This helps identify \"Anchor relationship patterns\" and how strategic tension\n    relates to populist intensity.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing the correlation matrix, or None if\n              data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # Calculate derived metrics\n        df_derived = calculate_all_derived_metrics(data)\n        if df_derived is None:\n            return None\n\n        # Define columns for correlation analysis\n        raw_score_cols = [col for col in df_derived.columns if col.endswith('_raw')]\n        key_derived_cols = ['populist_strategic_contradiction_index', 'salience_weighted_overall_populism_index']\n        analysis_cols = raw_score_cols + key_derived_cols\n\n        if 'document_name' not in df_derived.columns or len(df_derived['document_name'].unique()) < 2:\n            return None\n\n        # Average scores across evaluations for each document\n        df_agg = df_derived.groupby('document_name')[analysis_cols].mean().reset_index()\n\n        if df_agg.shape[0] < 2:\n            return None\n\n        # Compute the correlation matrix\n        correlation_matrix = df_agg[analysis_cols].corr(method='pearson')\n\n        return {\"correlation_matrix\": correlation_matrix.to_dict()}\n\n    except Exception:\n        return None\n\ndef analyze_temporal_patterns(data, **kwargs):\n    \"\"\"\n    Performs temporal pattern analysis using One-Way ANOVA.\n\n    This function addresses RQ2 by testing for significant differences in populist\n    discourse dimensions across three time periods: Early (2017), Mid (2018-2020),\n    and Recent (2025). It maps each document to a time period based on its name,\n    averages scores per document, and then runs an ANOVA for each raw score and\n    derived metric.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary of ANOVA results for each dimension, or None if\n              data is insufficient for the analysis.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    try:\n        import pingouin as pg\n    except ImportError:\n        return {\"error\": \"The 'pingouin' library is required for this analysis.\"}\n\n    try:\n        # Step 1: Add metadata grouping variables\n        df = data.copy()\n        if 'document_name' not in df.columns:\n            return None\n        \n        df['year'] = df['document_name'].str.extract(r'(\\d{4})').astype(float)\n\n        def map_time_period(year):\n            if year == 2017:\n                return 'Early (2017)'\n            elif year in [2018, 2019, 2020]:\n                return 'Mid (2018-2020)'\n            elif year == 2025:\n                return 'Recent (2025)'\n            return None\n        df['time_period'] = df['year'].apply(map_time_period)\n        df.dropna(subset=['time_period'], inplace=True)\n\n        if df['time_period'].nunique() < 2:\n            return {\"error\": \"Insufficient time periods for comparison.\"}\n\n        # Step 2: Calculate derived metrics\n        df_derived = calculate_all_derived_metrics(df)\n        if df_derived is None:\n            return None\n\n        # Step 3: Aggregate data by document\n        analysis_cols = [col for col in df_derived.columns if col.endswith('_raw')] + [\n            'populist_strategic_contradiction_index', 'salience_weighted_overall_populism_index'\n        ]\n        grouping_cols = ['document_name', 'time_period']\n        df_agg = df_derived.groupby(grouping_cols)[analysis_cols].mean().reset_index()\n\n        # Step 4: Run ANOVA for each dimension\n        anova_results = {}\n        for col in analysis_cols:\n            # Check if there are enough data points per group\n            if df_agg.groupby('time_period')[col].count().min() < 2:\n                anova_results[col] = \"Skipped: a group has fewer than 2 data points.\"\n                continue\n            \n            aov = pg.anova(data=df_agg, dv=col, between='time_period', detailed=True)\n            anova_results[col] = aov.to_dict('records')\n\n        return {\"temporal_anova_results\": anova_results}\n\n    except Exception:\n        return None\n\ndef analyze_speech_context_patterns(data, **kwargs):\n    \"\"\"\n    Performs speech context analysis using One-Way ANOVA.\n\n    This function addresses RQ3 by testing for significant differences in populist\n    discourse dimensions across different speech contexts (e.g., Inaugural Address,\n    State of the Union). It maps each document to a context based on its name,\n    averages scores per document, and then runs an ANOVA for each raw score and\n    derived metric.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary of ANOVA results for each dimension by speech context,\n              or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    try:\n        import pingouin as pg\n    except ImportError:\n        return {\"error\": \"The 'pingouin' library is required for this analysis.\"}\n\n    try:\n        # Step 1: Add metadata grouping variables\n        df = data.copy()\n        if 'document_name' not in df.columns:\n            return None\n\n        def map_speech_context(name):\n            name_lower = name.lower()\n            if 'inaugural' in name_lower:\n                return 'Inaugural Address'\n            # Per experiment spec, 2017 SOTU is the Joint Session\n            if 'sotu_2017' in name_lower or 'joint_session' in name_lower:\n                return 'Joint Session Address'\n            if 'sotu' in name_lower:\n                return 'State of the Union'\n            return None\n        df['speech_context'] = df['document_name'].apply(map_speech_context)\n        df.dropna(subset=['speech_context'], inplace=True)\n\n        if df['speech_context'].nunique() < 2:\n            return {\"error\": \"Insufficient speech contexts for comparison.\"}\n\n        # Step 2: Calculate derived metrics\n        df_derived = calculate_all_derived_metrics(df)\n        if df_derived is None:\n            return None\n\n        # Step 3: Aggregate data by document\n        analysis_cols = [col for col in df_derived.columns if col.endswith('_raw')] + [\n            'populist_strategic_contradiction_index', 'salience_weighted_overall_populism_index'\n        ]\n        grouping_cols = ['document_name', 'speech_context']\n        df_agg = df_derived.groupby(grouping_cols)[analysis_cols].mean().reset_index()\n\n        # Step 4: Run ANOVA for each dimension\n        anova_results = {}\n        for col in analysis_cols:\n            # Check if there are enough data points per group\n            if df_agg.groupby('speech_context')[col].count().min() < 2:\n                anova_results[col] = \"Skipped: a group has fewer than 2 data points.\"\n                continue\n            \n            aov = pg.anova(data=df_agg, dv=col, between='speech_context', detailed=True)\n            anova_results[col] = aov.to_dict('records')\n\n        return {\"speech_context_anova_results\": anova_results}\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}