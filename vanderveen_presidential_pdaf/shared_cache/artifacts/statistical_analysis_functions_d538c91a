{
  "status": "success",
  "functions_generated": 10,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 38320,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: vanderveen_presidential_pdaf_replication\nDescription: Statistical analysis experiment\nGenerated: 2025-09-03T14:36:33.630332+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef _add_metadata_and_derived_metrics(data):\n    \"\"\"\n    Internal helper function to preprocess the DataFrame. It adds metadata columns \n    (candidate, party, etc.) by parsing document names and calculates all derived \n    metrics as specified in the PDAF v10.0.2 framework.\n\n    This function is designed to be called by other analysis functions to ensure\n    a consistent and enriched dataset.\n\n    Args:\n        data (pd.DataFrame): The raw analysis data.\n\n    Returns:\n        pd.DataFrame: The DataFrame with added metadata and derived metrics, or None if input is invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if not isinstance(data, pd.DataFrame) or 'document_name' not in data.columns:\n        return None\n\n    df = data.copy()\n\n    # --- Metadata Mapping ---\n    # Avoid re-processing if already done\n    if 'candidate_short' in df.columns:\n        return df\n\n    # 1. Extract candidate name from 'document_name'\n    df['candidate_short'] = df['document_name'].apply(lambda x: x.split('_')[0] if isinstance(x, str) else None)\n\n    # 2. Define and apply metadata mappings based on the experiment specification\n    candidate_map = {\n        'trump': {'party': 'Republican', 'candidate_type': 'outsider', 'electoral_success': 3},\n        'clinton': {'party': 'Democratic', 'candidate_type': 'establishment', 'electoral_success': 3},\n        'sanders': {'party': 'Democratic', 'candidate_type': 'outsider', 'electoral_success': 2},\n        'cruz': {'party': 'Republican', 'candidate_type': 'challenger', 'electoral_success': 2},\n        'rubio': {'party': 'Republican', 'candidate_type': 'establishment', 'electoral_success': 1},\n        'kasich': {'party': 'Republican', 'candidate_type': 'establishment', 'electoral_success': 1}\n    }\n    df['party'] = df['candidate_short'].map(lambda x: candidate_map.get(x, {}).get('party'))\n    df['candidate_type'] = df['candidate_short'].map(lambda x: candidate_map.get(x, {}).get('candidate_type'))\n    df['electoral_success'] = df['candidate_short'].map(lambda x: candidate_map.get(x, {}).get('electoral_success'))\n\n    # Extract campaign phase from 'filename' if it exists\n    if 'filename' in df.columns:\n        # Path can be like 'candidate/phase/file.txt'\n        df['campaign_phase'] = df['filename'].str.split('/', expand=True)[1]\n    else:\n        df['campaign_phase'] = 'unknown'\n\n    # --- Derived Metrics Calculation ---\n    # For numerical stability\n    epsilon = 0.001\n\n    # Define dimension prefixes for easier access\n    dims = {\n        'manichaean': 'manichaean_people_elite_framing',\n        'crisis': 'crisis_restoration_narrative',\n        'sovereignty': 'popular_sovereignty_claims',\n        'anti_pluralist': 'anti_pluralist_exclusion',\n        'conspiracy': 'elite_conspiracy_systemic_corruption',\n        'authenticity': 'authenticity_vs_political_class',\n        'homogeneous': 'homogeneous_people_construction',\n        'nationalist': 'nationalist_exclusion',\n        'economic': 'economic_populist_appeals'\n    }\n\n    # Helper to get raw score and salience\n    def get_rs(dim_key): return df[f\"{dims[dim_key]}_raw\"]\n    def get_s(dim_key): return df[f\"{dims[dim_key]}_salience\"]\n\n    # Tension Indices\n    df['democratic_authoritarian_tension'] = np.minimum(get_rs('sovereignty'), get_rs('anti_pluralist')) * abs(get_s('sovereignty') - get_s('anti_pluralist'))\n    df['internal_external_focus_tension'] = np.minimum(get_rs('homogeneous'), get_rs('nationalist')) * abs(get_s('homogeneous') - get_s('nationalist'))\n    df['crisis_elite_attribution_tension'] = np.minimum(get_rs('crisis'), get_rs('conspiracy')) * abs(get_s('crisis') - get_s('conspiracy'))\n\n    # Populist Strategic Contradiction Index (PSCI)\n    df['populist_strategic_contradiction_index'] = (df['democratic_authoritarian_tension'] + df['internal_external_focus_tension'] + df['crisis_elite_attribution_tension']) / 3\n\n    # Salience-Weighted Indices\n    core_num = (get_rs('manichaean') * get_s('manichaean') + get_rs('crisis') * get_s('crisis') + get_rs('sovereignty') * get_s('sovereignty') + get_rs('anti_pluralist') * get_s('anti_pluralist'))\n    core_den = (get_s('manichaean') + get_s('crisis') + get_s('sovereignty') + get_s('anti_pluralist') + epsilon)\n    df['salience_weighted_core_populism_index'] = core_num / core_den\n\n    mech_num = (get_rs('conspiracy') * get_s('conspiracy') + get_rs('authenticity') * get_s('authenticity') + get_rs('homogeneous') * get_s('homogeneous'))\n    mech_den = (get_s('conspiracy') + get_s('authenticity') + get_s('homogeneous') + epsilon)\n    df['salience_weighted_populism_mechanisms_index'] = mech_num / mech_den\n\n    bound_num = (get_rs('nationalist') * get_s('nationalist') + get_rs('economic') * get_s('economic'))\n    bound_den = (get_s('nationalist') + get_s('economic') + epsilon)\n    df['salience_weighted_boundary_distinctions_index'] = bound_num / bound_den\n\n    all_salience_scores = [get_s(key) for key in dims]\n    all_weighted_scores = [get_rs(key) * get_s(key) for key in dims]\n    \n    overall_num = sum(all_weighted_scores)\n    overall_den = sum(all_salience_scores) + epsilon\n    df['salience_weighted_overall_populism_index'] = overall_num / overall_den\n    \n    # Clean up column names from raw data which have _raw, _salience suffixes\n    # The framework spec uses names like 'manichaean_people_elite_framing', but the data has suffixes.\n    # Let's create clean column names for easier use in other functions.\n    for dim_name in dims.values():\n        df[f\"{dim_name}_raw_score\"] = df[f\"{dim_name}_raw\"]\n        df[f\"{dim_name}_salience_score\"] = df[f\"{dim_name}_salience\"]\n\n    return df\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates comprehensive descriptive statistics for all PDAF dimensions and derived indices,\n    grouped by presidential candidate. This addresses RQ1, RQ2, and RQ5.\n\n    Methodology:\n    - The function first enriches the data with metadata and derived metrics.\n    - It then calculates the mean, standard deviation, median, and interquartile range (IQR)\n      for each numerical score.\n    - Results are grouped by the 'candidate_short' field.\n    - This provides a foundational overview of populist discourse patterns across the corpus.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary of DataFrames, with each key representing a grouping\n              (e.g., 'by_candidate') and the value being the descriptive statistics table.\n              Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        df = _add_metadata_and_derived_metrics(data)\n        if df is None or df.empty:\n            return None\n\n        grouping_var = 'candidate_short'\n        \n        # Identify all numerical columns to be analyzed\n        score_cols = [col for col in df.columns if '_index' in col] + \\\n                     [col for col in df.columns if col.endswith(('_raw', '_salience'))]\n        \n        if not score_cols:\n            return None\n\n        # Calculate descriptive statistics\n        descriptives = df.groupby(grouping_var)[score_cols].agg(['mean', 'std', 'median', lambda x: x.quantile(0.75) - x.quantile(0.25)])\n        descriptives.columns = ['_'.join(col).strip() for col in descriptives.columns.values]\n        descriptives.rename(columns=lambda x: x.replace('<lambda_0>', 'iqr'), inplace=True)\n\n        return {'by_candidate': descriptives.to_dict()}\n\n    except Exception as e:\n        # In a production environment, one might log the error e\n        return None\n\ndef test_overall_candidate_differences(data, **kwargs):\n    \"\"\"\n    Tests Hypothesis H1: At least one candidate differs significantly in Salience-Weighted \n    Overall Populism Index scores.\n\n    Methodology:\n    - Tier 3 Analysis (Exploratory): Due to small sample sizes for some candidates (n<5),\n      results are suggestive rather than conclusive.\n    - A non-parametric Kruskal-Wallis H-test is used to compare the distributions of the\n      'salience_weighted_overall_populism_index' across all 6 candidates. This is robust\n      to non-normality and unequal variances.\n    - If the overall test is significant (p < 0.05), Dunn's post-hoc test is performed for\n      pairwise comparisons, with Bonferroni correction for multiple tests.\n    - Effect size (epsilon-squared) is calculated for the Kruskal-Wallis test.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: Contains the Kruskal-Wallis test statistic, p-value, effect size, and\n              post-hoc test results if applicable. Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import kruskal\n    import scikit_posthocs as sp\n\n    try:\n        df = _add_metadata_and_derived_metrics(data)\n        if df is None or df.empty:\n            return None\n\n        target_variable = 'salience_weighted_overall_populism_index'\n        grouping_variable = 'candidate_short'\n        \n        # Check for sufficient data\n        if df[grouping_variable].nunique() < 2:\n            return None\n\n        groups = [group[target_variable].dropna().values for name, group in df.groupby(grouping_variable)]\n        \n        # Ensure there are at least two groups with data\n        groups = [g for g in groups if len(g) > 0]\n        if len(groups) < 2:\n            return None\n\n        # Tier 3 Power Caveat\n        sample_sizes = {name: len(group) for name, group in df.groupby(grouping_variable)}\n        is_exploratory = any(n < 5 for n in sample_sizes.values())\n        \n        # Perform Kruskal-Wallis test\n        stat, p_value = kruskal(*groups)\n        \n        # Calculate effect size (epsilon-squared)\n        n = len(df)\n        k = len(groups)\n        epsilon_squared = (stat - k + 1) / (n - k)\n\n        results = {\n            'test_name': 'Kruskal-Wallis H-test',\n            'hypothesis': 'H1: Difference in overall populism scores across candidates.',\n            'dependent_variable': target_variable,\n            'groups': list(sample_sizes.keys()),\n            'sample_sizes': sample_sizes,\n            'power_assessment': 'Tier 3 (Exploratory) due to small sample sizes for some groups. Results are suggestive.',\n            'statistic': stat,\n            'p_value': p_value,\n            'effect_size_epsilon_squared': epsilon_squared,\n            'post_hoc_results': None\n        }\n\n        # Perform post-hoc test if overall result is significant\n        if p_value < 0.05:\n            posthoc_df = sp.posthoc_dunn(df, val_col=target_variable, group_col=grouping_variable, p_adjust='bonferroni')\n            results['post_hoc_results'] = posthoc_df.to_dict()\n\n        return results\n\n    except Exception as e:\n        return None\n\ndef test_pairwise_candidate_hypotheses(data, **kwargs):\n    \"\"\"\n    Tests secondary pairwise hypotheses H2, H3, and H4 using non-parametric tests.\n\n    Methodology:\n    - The Mann-Whitney U test is used for all comparisons, as it does not assume normality.\n    - Cohen's d is calculated as an effect size measure.\n    - Each hypothesis is tested independently.\n    - Power assessment is provided for each test based on group sizes.\n\n    Hypotheses Tested:\n    - H2: Trump > Clinton on Salience-Weighted Overall Populism Index (Tier 1)\n    - H3: Sanders > Clinton on Salience-Weighted Overall Populism Index (Tier 3)\n    - H4: Trump > Rubio & Kasich on Salience-Weighted Overall Populism Index (Tier 3)\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary where each key is a hypothesis (e.g., 'H2') and the value\n              contains the test results for that hypothesis. Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import mannwhitneyu\n    import pingouin as pg\n\n    try:\n        df = _add_metadata_and_derived_metrics(data)\n        if df is None or df.empty:\n            return None\n\n        target_var = 'salience_weighted_overall_populism_index'\n        results = {}\n\n        def run_comparison(df, cand1, cand2, hypothesis, tier):\n            group1 = df[df['candidate_short'] == cand1][target_var].dropna()\n            group2 = df[df['candidate_short'] == cand2][target_var].dropna()\n\n            if group1.empty or group2.empty:\n                return {\n                    'status': 'Skipped',\n                    'reason': f'Insufficient data for {cand1} or {cand2}',\n                    'power_assessment': tier\n                }\n\n            stat, p_value = mannwhitneyu(group1, group2, alternative='greater')\n            effect_size = pg.compute_effsize(group1, group2, eftype='cohen')\n\n            return {\n                'hypothesis': hypothesis,\n                'comparison': f'{cand1} > {cand2}',\n                'power_assessment': tier,\n                'sample_sizes': {cand1: len(group1), cand2: len(group2)},\n                'statistic_U': stat,\n                'p_value_one_tailed': p_value,\n                'effect_size_cohens_d': effect_size,\n                'group1_median': group1.median(),\n                'group2_median': group2.median()\n            }\n\n        # H2: Trump > Clinton\n        results['H2'] = run_comparison(df, 'trump', 'clinton', 'H2: Trump > Clinton', 'Tier 1 (Well-Powered)')\n\n        # H3: Sanders > Clinton\n        results['H3'] = run_comparison(df, 'sanders', 'clinton', 'H3: Sanders > Clinton', 'Tier 3 (Exploratory)')\n\n        # H4: Trump > Establishment Republicans (Rubio, Kasich)\n        estab_reps = df[df['candidate_short'].isin(['rubio', 'kasich'])]\n        group_trump = df[df['candidate_short'] == 'trump'][target_var].dropna()\n        group_estab = estab_reps[target_var].dropna()\n        \n        if not group_trump.empty and not group_estab.empty:\n            stat_h4, p_h4 = mannwhitneyu(group_trump, group_estab, alternative='greater')\n            effect_size_h4 = pg.compute_effsize(group_trump, group_estab, eftype='cohen')\n            results['H4'] = {\n                'hypothesis': 'H4: Trump > Establishment Republicans',\n                'comparison': 'Trump > (Rubio + Kasich)',\n                'power_assessment': 'Tier 3 (Exploratory)',\n                'sample_sizes': {'Trump': len(group_trump), 'Establishment Reps': len(group_estab)},\n                'statistic_U': stat_h4,\n                'p_value_one_tailed': p_h4,\n                'effect_size_cohens_d': effect_size_h4,\n                'group1_median': group_trump.median(),\n                'group2_median': group_estab.median()\n            }\n        else:\n            results['H4'] = {'status': 'Skipped', 'reason': 'Insufficient data for comparison groups.'}\n\n        return results\n\n    except Exception as e:\n        return None\n\ndef test_party_differences(data, **kwargs):\n    \"\"\"\n    Tests party-based hypotheses H5 and H6 using non-parametric tests.\n\n    Methodology:\n    - Tier 1 Analysis (Well-Powered): The sample sizes for both parties (Rep: 30, Dem: 26)\n      are sufficient for robust inferential testing.\n    - The Mann-Whitney U test is used to compare the distributions between Republican and\n      Democratic candidates.\n    - Cohen's d is calculated as an effect size measure.\n\n    Hypotheses Tested:\n    - H5: Republicans > Democrats on 'nationalist_exclusion_raw_score'\n    - H6: Democrats > Republicans on 'economic_populist_appeals_raw_score'\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: A dictionary containing results for H5 and H6. Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import mannwhitneyu\n    import pingouin as pg\n\n    try:\n        df = _add_metadata_and_derived_metrics(data)\n        if df is None or df.empty or 'party' not in df.columns:\n            return None\n\n        results = {}\n        \n        republicans = df[df['party'] == 'Republican']\n        democrats = df[df['party'] == 'Democratic']\n\n        if republicans.empty or democrats.empty:\n            return None\n\n        def run_party_comparison(group1_df, group2_df, variable, hypothesis, alternative):\n            group1 = group1_df[variable].dropna()\n            group2 = group2_df[variable].dropna()\n            \n            if group1.empty or group2.empty:\n                return {'status': 'Skipped', 'reason': 'Insufficient data for one or both parties.'}\n\n            stat, p_value = mannwhitneyu(group1, group2, alternative=alternative)\n            effect_size = pg.compute_effsize(group1, group2, eftype='cohen')\n\n            return {\n                'hypothesis': hypothesis,\n                'dependent_variable': variable,\n                'power_assessment': 'Tier 1 (Well-Powered)',\n                'sample_sizes': {'Republican': len(republicans), 'Democratic': len(democrats)},\n                'statistic_U': stat,\n                'p_value_one_tailed': p_value,\n                'effect_size_cohens_d': effect_size,\n                'republican_median': republicans[variable].median(),\n                'democratic_median': democrats[variable].median()\n            }\n\n        # H5: Republicans > Democrats on Nationalist Exclusion\n        results['H5'] = run_party_comparison(republicans, democrats, 'nationalist_exclusion_raw_score', 'H5: Rep > Dem on Nationalist Exclusion', 'greater')\n\n        # H6: Democrats > Republicans on Economic Populist Appeals\n        results['H6'] = run_party_comparison(democrats, republicans, 'economic_populist_appeals_raw_score', 'H6: Dem > Rep on Economic Populist Appeals', 'greater')\n\n        return results\n\n    except Exception as e:\n        return None\n\ndef test_temporal_effects(data, **kwargs):\n    \"\"\"\n    Tests Hypothesis H7: Rhetoric on the 'Manichaean People-Elite Framing' dimension\n    intensifies from the primary to the general election.\n\n    Methodology:\n    - The analysis compares scores from the 'primary_season' phase with the 'general_election' phase.\n    - A non-parametric Mann-Whitney U test is used to compare the two independent groups,\n      robust to potential non-normality of the score distributions.\n    - Power assessment is dynamic based on the number of speeches in each phase.\n    - Cohen's d is reported as a measure of effect size.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: Contains the Mann-Whitney U test results, effect size, and descriptive\n              statistics for the two campaign phases. Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import mannwhitneyu\n    import pingouin as pg\n\n    try:\n        df = _add_metadata_and_derived_metrics(data)\n        if df is None or df.empty or 'campaign_phase' not in df.columns:\n            return None\n\n        target_var = 'manichaean_people_elite_framing_raw_score'\n        \n        # The experiment spec mentions 'early_primary' but file paths show 'primary_season'.\n        # We will use 'primary_season' and 'general_election'.\n        primary_data = df[df['campaign_phase'] == 'primary_season'][target_var].dropna()\n        general_data = df[df['campaign_phase'] == 'general_election'][target_var].dropna()\n\n        if primary_data.empty or general_data.empty:\n            return {'status': 'Skipped', 'reason': 'Insufficient data for one or both campaign phases.'}\n\n        # Tiered Power Assessment\n        n1, n2 = len(primary_data), len(general_data)\n        if n1 >= 15 and n2 >= 15:\n            tier = 'Tier 1 (Well-Powered)'\n        elif (n1 >= 8 and n2 >= 8):\n            tier = 'Tier 2 (Moderately-Powered)'\n        else:\n            tier = 'Tier 3 (Exploratory)'\n\n        # H7: general_election > primary_season\n        stat, p_value = mannwhitneyu(general_data, primary_data, alternative='greater')\n        effect_size = pg.compute_effsize(general_data, primary_data, eftype='cohen')\n\n        results = {\n            'hypothesis': 'H7: Manichaean framing intensifies from primary to general election.',\n            'dependent_variable': target_var,\n            'power_assessment': tier,\n            'sample_sizes': {'primary_season': n1, 'general_election': n2},\n            'statistic_U': stat,\n            'p_value_one_tailed': p_value,\n            'effect_size_cohens_d': effect_size,\n            'primary_median': primary_data.median(),\n            'general_election_median': general_data.median()\n        }\n        return results\n\n    except Exception as e:\n        return None\n\ndef analyze_strategic_contradiction(data, **kwargs):\n    \"\"\"\n    Tests Hypothesis H8: The Populist Strategic Contradiction Index (PSCI) correlates\n    negatively with electoral success.\n\n    Methodology:\n    - Tier 1 Analysis (Well-Powered): The total sample size (N=56) is sufficient for\n      correlation analysis.\n    - Spearman's rank correlation (rho) is used as the primary test, as it is suitable\n      for assessing monotonic relationships between two ordinal variables (PSCI and\n      electoral_success).\n    - Kendall's Tau is also calculated as a robust alternative measure of rank correlation.\n    - The analysis tests for a negative correlation (\u03c1 < -0.3, p < 0.05).\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: Contains the correlation coefficients (Spearman's rho, Kendall's tau) and\n              their corresponding p-values. Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    from scipy.stats import spearmanr, kendalltau\n\n    try:\n        df = _add_metadata_and_derived_metrics(data)\n        if df is None or df.empty or 'populist_strategic_contradiction_index' not in df.columns or 'electoral_success' not in df.columns:\n            return None\n\n        analysis_df = df[['populist_strategic_contradiction_index', 'electoral_success']].dropna()\n\n        if len(analysis_df) < 15:\n            power = 'Tier 3 (Exploratory)'\n        elif len(analysis_df) < 30:\n            power = 'Tier 2 (Moderately-Powered)'\n        else:\n            power = 'Tier 1 (Well-Powered)'\n\n        if len(analysis_df) < 5:\n            return {'status': 'Skipped', 'reason': 'Insufficient data for correlation analysis.'}\n\n        # Spearman's Rank Correlation\n        spearman_rho, spearman_p = spearmanr(analysis_df['populist_strategic_contradiction_index'], analysis_df['electoral_success'])\n\n        # Kendall's Tau\n        kendall_tau, kendall_p = kendalltau(analysis_df['populist_strategic_contradiction_index'], analysis_df['electoral_success'])\n\n        results = {\n            'hypothesis': 'H8: PSCI correlates negatively with electoral success.',\n            'variables': ['populist_strategic_contradiction_index', 'electoral_success'],\n            'power_assessment': power,\n            'sample_size': len(analysis_df),\n            'spearman_correlation': {\n                'rho': spearman_rho,\n                'p_value': spearman_p\n            },\n            'kendall_correlation': {\n                'tau': kendall_tau,\n                'p_value': kendall_p\n            },\n            'conclusion': f\"Hypothesis (\u03c1 < -0.3) is {'supported' if spearman_rho < -0.3 and spearman_p < 0.05 else 'not supported'}.\"\n        }\n        return results\n\n    except Exception as e:\n        return None\n\ndef analyze_score_variance(data, **kwargs):\n    \"\"\"\n    Performs a descriptive analysis of Hypothesis H9: Trump shows significantly higher\n    variance in Salience-Weighted Overall Populism Index than other candidates.\n\n    Methodology:\n    - This is an exploratory analysis focusing on descriptive statistics of variance.\n    - It calculates the variance of the 'salience_weighted_overall_populism_index' for each candidate.\n    - Levene's test is used to formally test for homogeneity of variances across all candidate groups.\n      This is a robust test for equality of variances that is less sensitive to non-normality\n      than Bartlett's test.\n    - A significant p-value (p < 0.05) in Levene's test suggests that at least one candidate's\n      score variance is different from the others.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: Contains the calculated variances for each candidate and the results of\n              Levene's test for homogeneity of variances. Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    from scipy.stats import levene\n\n    try:\n        df = _add_metadata_and_derived_metrics(data)\n        if df is None or df.empty:\n            return None\n\n        target_var = 'salience_weighted_overall_populism_index'\n        grouping_var = 'candidate_short'\n\n        # Calculate variance for each candidate\n        variances = df.groupby(grouping_var)[target_var].var().to_dict()\n\n        # Prepare data for Levene's test\n        groups = [group[target_var].dropna().values for name, group in df.groupby(grouping_var)]\n        groups = [g for g in groups if len(g) > 1] # Levene's test needs groups with >1 member\n\n        if len(groups) < 2:\n            return {'status': 'Skipped', 'reason': 'Levene\\'s test requires at least two groups with more than one data point.'}\n\n        # Perform Levene's test\n        stat, p_value = levene(*groups)\n\n        results = {\n            'hypothesis': 'H9 (Descriptive): Trump shows higher variance in populism scores.',\n            'dependent_variable': target_var,\n            'analysis_type': 'Homogeneity of Variance Test',\n            'power_assessment': 'Tier 3 (Exploratory) due to small sample sizes in some groups.',\n            'variances_by_candidate': variances,\n            'levene_test': {\n                'statistic': stat,\n                'p_value': p_value,\n                'interpretation': 'p < 0.05 suggests variances are not equal across groups.'\n            }\n        }\n        return results\n\n    except Exception as e:\n        return None\n\ndef test_candidate_type_differences(data, **kwargs):\n    \"\"\"\n    Tests Hypothesis H10: At least 4 of the 9 PDAF dimensions show significant differences\n    between 'outsider' and 'establishment' candidate types.\n\n    Methodology:\n    - Tier 1 Analysis (Well-Powered): The group sizes ('outsider': 27, 'establishment': 27)\n      are well-balanced and sufficient for robust testing.\n    - For each of the 9 core PDAF dimensions, a non-parametric Mann-Whitney U test is\n      performed to compare the 'outsider' and 'establishment' groups.\n    - Bonferroni correction is applied to the significance threshold (alpha = 0.05 / 9)\n      to account for multiple comparisons.\n    - Cohen's d is calculated for each dimension to measure effect size.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: Contains a summary of the number of significant differences found and a\n              detailed breakdown of test results for each of the 9 dimensions.\n              Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    from scipy.stats import mannwhitneyu\n    import pingouin as pg\n\n    try:\n        df = _add_metadata_and_derived_metrics(data)\n        if df is None or df.empty or 'candidate_type' not in df.columns:\n            return None\n\n        # Filter for the two main groups\n        analysis_df = df[df['candidate_type'].isin(['outsider', 'establishment'])]\n        \n        outsiders = analysis_df[analysis_df['candidate_type'] == 'outsider']\n        establishment = analysis_df[analysis_df['candidate_type'] == 'establishment']\n\n        if outsiders.empty or establishment.empty:\n            return None\n\n        dimensions = [\n            'manichaean_people_elite_framing_raw_score', 'crisis_restoration_narrative_raw_score',\n            'popular_sovereignty_claims_raw_score', 'anti_pluralist_exclusion_raw_score',\n            'elite_conspiracy_systemic_corruption_raw_score', 'authenticity_vs_political_class_raw_score',\n            'homogeneous_people_construction_raw_score', 'nationalist_exclusion_raw_score',\n            'economic_populist_appeals_raw_score'\n        ]\n        \n        alpha = 0.05\n        corrected_alpha = alpha / len(dimensions)\n        \n        detailed_results = {}\n        significant_diffs = 0\n\n        for dim in dimensions:\n            group1 = outsiders[dim].dropna()\n            group2 = establishment[dim].dropna()\n\n            if group1.empty or group2.empty:\n                continue\n\n            stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n            effect_size = pg.compute_effsize(group1, group2, eftype='cohen')\n            \n            is_significant = p_value < corrected_alpha\n\n            if is_significant:\n                significant_diffs += 1\n\n            detailed_results[dim] = {\n                'statistic_U': stat,\n                'p_value': p_value,\n                'is_significant_after_bonferroni': is_significant,\n                'effect_size_cohens_d': effect_size,\n                'outsider_median': group1.median(),\n                'establishment_median': group2.median()\n            }\n\n        summary = {\n            'hypothesis': \"H10: At least 4/9 dimensions differ between 'outsider' and 'establishment' types.\",\n            'power_assessment': 'Tier 1 (Well-Powered)',\n            'sample_sizes': {'outsider': len(outsiders), 'establishment': len(establishment)},\n            'multiple_comparison_correction': f'Bonferroni (alpha = {corrected_alpha:.4f})',\n            'significant_dimensions_found': significant_diffs,\n            'hypothesis_supported': significant_diffs >= 4\n        }\n\n        return {'summary': summary, 'detailed_results': detailed_results}\n\n    except Exception as e:\n        return None\n\ndef calculate_internal_consistency(data, **kwargs):\n    \"\"\"\n    Calculates the internal consistency (reliability) of the 9 core PDAF dimensions\n    using Cronbach's alpha.\n\n    Methodology:\n    - Tier 1 Analysis (Well-Powered): The total sample size (N=56) is sufficient for\n      a reliable estimate of Cronbach's alpha.\n    - Cronbach's alpha measures how closely related a set of items are as a group. It is\n      considered a measure of scale reliability.\n    - The analysis is performed on the raw scores of the 9 core dimensions.\n    - The function also calculates the alpha if each item (dimension) were to be dropped,\n      which is useful for identifying dimensions that may be lowering the scale's reliability.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Not used.\n\n    Returns:\n        dict: Contains the overall Cronbach's alpha for the scale and the alpha values\n              if each item were dropped. Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import pingouin as pg\n\n    try:\n        df = _add_metadata_and_derived_metrics(data)\n        if df is None or df.empty:\n            return None\n\n        dimensions = [\n            'manichaean_people_elite_framing_raw_score', 'crisis_restoration_narrative_raw_score',\n            'popular_sovereignty_claims_raw_score', 'anti_pluralist_exclusion_raw_score',\n            'elite_conspiracy_systemic_corruption_raw_score', 'authenticity_vs_political_class_raw_score',\n            'homogeneous_people_construction_raw_score', 'nationalist_exclusion_raw_score',\n            'economic_populist_appeals_raw_score'\n        ]\n        \n        # Select only the dimension columns and drop rows with any missing values for this analysis\n        reliability_df = df[dimensions].dropna()\n\n        if len(reliability_df) < 15:\n            power = 'Tier 3 (Exploratory)'\n        elif len(reliability_df) < 30:\n            power = 'Tier 2 (Moderately-Powered)'\n        else:\n            power = 'Tier 1 (Well-Powered)'\n\n        if len(reliability_df) < 10:\n            return {'status': 'Skipped', 'reason': 'Insufficient data for reliability analysis.'}\n\n        # Calculate Cronbach's alpha\n        alpha_results = pg.cronbach_alpha(data=reliability_df)\n        \n        # The result is a tuple: (alpha, confidence_interval)\n        overall_alpha = alpha_results[0]\n        ci = alpha_results[1]\n\n        # To get alpha-if-item-dropped, we need to iterate\n        alpha_if_dropped = {}\n        for dim in dimensions:\n            sub_df = reliability_df.drop(columns=[dim])\n            alpha_if_dropped[dim] = pg.cronbach_alpha(data=sub_df)[0]\n\n        return {\n            'analysis_name': 'Internal Consistency Analysis (Cronbach\\'s Alpha)',\n            'power_assessment': power,\n            'sample_size': len(reliability_df),\n            'overall_cronbach_alpha': overall_alpha,\n            'confidence_interval_95': list(ci),\n            'alpha_if_item_dropped': alpha_if_dropped\n        }\n\n    except Exception as e:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}