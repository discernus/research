{
  "status": "success",
  "functions_generated": 5,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 23234,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: simple_test_pdaf\nDescription: Statistical analysis experiment\nGenerated: 2025-08-25T04:11:50.648742+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics (mean, std, min, max, count) for all raw scores,\n    salience scores, and confidence scores in the dataset. This provides a foundational\n    overview of the data distribution for each measured dimension.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data with columns\n                             following the PDAF specification.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        dict: A nested dictionary where keys are column names and values are\n              dictionaries of descriptive statistics. Returns None if the\n              input data is invalid or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        # Select only numeric columns for statistical analysis\n        numeric_cols = data.select_dtypes(include=np.number).columns.tolist()\n        if not numeric_cols:\n            return {}\n\n        stats = data[numeric_cols].describe().transpose()\n        stats_dict = stats[['mean', 'std', 'min', 'max', 'count']].to_dict('index')\n\n        # Ensure all values are standard Python types for JSON serialization\n        for col, values in stats_dict.items():\n            for stat_name, value in values.items():\n                if pd.isna(value):\n                    stats_dict[col][stat_name] = None\n                elif stat_name == 'count':\n                    stats_dict[col][stat_name] = int(value)\n                else:\n                    stats_dict[col][stat_name] = float(value)\n\n        return stats_dict\n\n    except Exception:\n        return None\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived metrics as defined in the PDAF v10.0.0 specification,\n    including tension scores, salience-weighted indices, and the overall strategic\n    contradiction index. This function enriches the dataset for further analysis.\n\n    The formulas are implemented directly from the framework specification:\n    - Tension: min(Score_A, Score_B) * |Salience_A - Salience_B|\n    - Salience-Weighted Index: Sum(Score * Salience) / (Sum(Salience) + 0.001)\n    - PSCI: Average of the three tension scores.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the raw and salience scores for\n                             the nine core dimensions.\n        **kwargs: Additional parameters (not used in this function).\n\n    Returns:\n        pd.DataFrame: The original DataFrame with new columns for each of the\n                      eight derived metrics. Returns None if essential columns\n                      are missing or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = data.copy()\n\n        # Define required columns to ensure data integrity\n        required_cols = [\n            'popular_sovereignty_claims_raw', 'popular_sovereignty_claims_salience',\n            'anti_pluralist_exclusion_raw', 'anti_pluralist_exclusion_salience',\n            'homogeneous_people_construction_raw', 'homogeneous_people_construction_salience',\n            'nationalist_exclusion_raw', 'nationalist_exclusion_salience',\n            'crisis_restoration_narrative_raw', 'crisis_restoration_narrative_salience',\n            'elite_conspiracy_systemic_corruption_raw', 'elite_conspiracy_systemic_corruption_salience',\n            'manichaean_people_elite_framing_raw', 'manichaean_people_elite_framing_salience',\n            'authenticity_vs_political_class_raw', 'authenticity_vs_political_class_salience',\n            'economic_populist_appeals_raw', 'economic_populist_appeals_salience'\n        ]\n        if not all(col in df.columns for col in required_cols):\n            # Missing one or more essential columns for calculation\n            return None\n\n        # 1. Democratic-Authoritarian Tension\n        df['democratic_authoritarian_tension'] = np.minimum(\n            df['popular_sovereignty_claims_raw'], df['anti_pluralist_exclusion_raw']\n        ) * (df['popular_sovereignty_claims_salience'] - df['anti_pluralist_exclusion_salience']).abs()\n\n        # 2. Internal-External Focus Tension\n        df['internal_external_focus_tension'] = np.minimum(\n            df['homogeneous_people_construction_raw'], df['nationalist_exclusion_raw']\n        ) * (df['homogeneous_people_construction_salience'] - df['nationalist_exclusion_salience']).abs()\n\n        # 3. Crisis-Elite Attribution Tension\n        df['crisis_elite_attribution_tension'] = np.minimum(\n            df['crisis_restoration_narrative_raw'], df['elite_conspiracy_systemic_corruption_raw']\n        ) * (df['crisis_restoration_narrative_salience'] - df['elite_conspiracy_systemic_corruption_salience']).abs()\n\n        # 4. Populist Strategic Contradiction Index (PSCI)\n        df['populist_strategic_contradiction_index'] = (\n            df['democratic_authoritarian_tension'] +\n            df['internal_external_focus_tension'] +\n            df['crisis_elite_attribution_tension']\n        ) / 3\n\n        # 5. Salience-Weighted Core Populism Index\n        core_numerator = (df['manichaean_people_elite_framing_raw'] * df['manichaean_people_elite_framing_salience'] +\n                          df['crisis_restoration_narrative_raw'] * df['crisis_restoration_narrative_salience'] +\n                          df['popular_sovereignty_claims_raw'] * df['popular_sovereignty_claims_salience'] +\n                          df['anti_pluralist_exclusion_raw'] * df['anti_pluralist_exclusion_salience'])\n        core_denominator = (df['manichaean_people_elite_framing_salience'] +\n                            df['crisis_restoration_narrative_salience'] +\n                            df['popular_sovereignty_claims_salience'] +\n                            df['anti_pluralist_exclusion_salience'] + 0.001)\n        df['salience_weighted_core_populism_index'] = core_numerator / core_denominator\n\n        # 6. Salience-Weighted Populism Mechanisms Index\n        mech_numerator = (df['elite_conspiracy_systemic_corruption_raw'] * df['elite_conspiracy_systemic_corruption_salience'] +\n                          df['authenticity_vs_political_class_raw'] * df['authenticity_vs_political_class_salience'] +\n                          df['homogeneous_people_construction_raw'] * df['homogeneous_people_construction_salience'])\n        mech_denominator = (df['elite_conspiracy_systemic_corruption_salience'] +\n                            df['authenticity_vs_political_class_salience'] +\n                            df['homogeneous_people_construction_salience'] + 0.001)\n        df['salience_weighted_populism_mechanisms_index'] = mech_numerator / mech_denominator\n\n        # 7. Salience-Weighted Boundary Distinctions Index\n        bound_numerator = (df['nationalist_exclusion_raw'] * df['nationalist_exclusion_salience'] +\n                           df['economic_populist_appeals_raw'] * df['economic_populist_appeals_salience'])\n        bound_denominator = (df['nationalist_exclusion_salience'] +\n                             df['economic_populist_appeals_salience'] + 0.001)\n        df['salience_weighted_boundary_distinctions_index'] = bound_numerator / bound_denominator\n\n        # 8. Salience-Weighted Overall Populism Index\n        all_dims_raw = [\n            'manichaean_people_elite_framing_raw', 'crisis_restoration_narrative_raw',\n            'popular_sovereignty_claims_raw', 'anti_pluralist_exclusion_raw',\n            'elite_conspiracy_systemic_corruption_raw', 'authenticity_vs_political_class_raw',\n            'homogeneous_people_construction_raw', 'nationalist_exclusion_raw',\n            'economic_populist_appeals_raw'\n        ]\n        all_dims_salience = [col.replace('_raw', '_salience') for col in all_dims_raw]\n        \n        overall_numerator = sum(df[raw] * df[sal] for raw, sal in zip(all_dims_raw, all_dims_salience))\n        overall_denominator = sum(df[sal] for sal in all_dims_salience) + 0.001\n        df['salience_weighted_overall_populism_index'] = overall_numerator / overall_denominator\n\n        return df\n\n    except Exception:\n        return None\n\ndef calculate_correlation_matrix(data, **kwargs):\n    \"\"\"\n    Calculates the Pearson correlation matrix for populist discourse dimensions.\n    This analysis helps identify relationships between different rhetorical strategies.\n    By default, it correlates the raw scores and the derived metrics.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs:\n            use_derived_metrics (bool): If True, calculates and includes derived\n                                        metrics in the correlation. Defaults to True.\n\n    Returns:\n        dict: A nested dictionary representing the correlation matrix, suitable for\n              JSON serialization. Returns None if data is insufficient or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data is None or data.empty:\n            return None\n        \n        use_derived_metrics = kwargs.get('use_derived_metrics', True)\n        \n        df = data.copy()\n\n        if use_derived_metrics:\n            # First, calculate derived metrics to include them\n            df = calculate_derived_metrics(df)\n            if df is None:\n                # Calculation failed, cannot proceed\n                return None\n\n        # Select columns for correlation: all raw scores and all derived metrics\n        raw_score_cols = [col for col in df.columns if col.endswith('_raw')]\n        derived_metric_cols = [\n            'democratic_authoritarian_tension', 'internal_external_focus_tension',\n            'crisis_elite_attribution_tension', 'populist_strategic_contradiction_index',\n            'salience_weighted_core_populism_index', 'salience_weighted_populism_mechanisms_index',\n            'salience_weighted_boundary_distinctions_index', 'salience_weighted_overall_populism_index'\n        ]\n        \n        # Filter out derived metric columns that might not have been created\n        valid_derived_cols = [col for col in derived_metric_cols if col in df.columns]\n        \n        cols_to_correlate = raw_score_cols + valid_derived_cols\n        \n        if len(cols_to_correlate) < 2:\n            return None # Not enough columns to correlate\n\n        correlation_matrix = df[cols_to_correlate].corr(method='pearson')\n        \n        # Convert to dictionary and handle NaN values for JSON compatibility\n        correlation_dict = correlation_matrix.where(pd.notnull(correlation_matrix), None).to_dict()\n\n        return correlation_dict\n\n    except Exception:\n        return None\n\ndef compare_groups_on_dimensions(data, **kwargs):\n    \"\"\"\n    Performs a comparative analysis of populist dimensions across different groups.\n    This function calculates descriptive statistics (mean, std) for specified\n    dimensions for each group defined by a grouping column. If scipy is available,\n    it also performs statistical tests (t-test for 2 groups, ANOVA for >2 groups)\n    to assess the significance of differences.\n\n    This is useful for answering questions like \"How do populist conservative and\n    progressive styles differ?\". The user must first add a column to the data\n    (e.g., 'ideology') to define the groups.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs:\n            grouping_column (str): The name of the column to group the data by.\n                                   This column MUST exist in the DataFrame.\n            dimensions_to_compare (list): A list of column names (dimensions or\n                                          derived metrics) to compare across groups.\n                                          If not provided, all '_raw' columns are used.\n\n    Returns:\n        dict: A nested dictionary with results for each dimension, including\n              group statistics and p-value from the statistical test.\n              Returns None if inputs are invalid or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        grouping_column = kwargs.get('grouping_column')\n        if grouping_column is None or grouping_column not in data.columns:\n            # Grouping column is essential for this analysis\n            return {\"error\": \"Mandatory 'grouping_column' not provided or not found in data.\"}\n\n        if data[grouping_column].nunique() < 2:\n            return {\"error\": \"Grouping column must have at least two unique groups.\"}\n\n        dimensions_to_compare = kwargs.get('dimensions_to_compare')\n        if dimensions_to_compare is None:\n            dimensions_to_compare = [col for col in data.columns if col.endswith('_raw')]\n\n        # Ensure all specified dimensions exist in the data\n        dimensions_to_compare = [d for d in dimensions_to_compare if d in data.columns]\n        if not dimensions_to_compare:\n            return {\"error\": \"None of the specified dimensions_to_compare were found in the data.\"}\n\n        results = {}\n        groups = data[grouping_column].unique()\n        \n        # Check for scipy for statistical testing\n        try:\n            from scipy.stats import ttest_ind, f_oneway\n            scipy_available = True\n        except ImportError:\n            scipy_available = False\n\n        for dim in dimensions_to_compare:\n            results[dim] = {'group_stats': {}}\n            \n            # Calculate descriptive stats for each group\n            for group in groups:\n                group_data = data[data[grouping_column] == group][dim].dropna()\n                results[dim]['group_stats'][group] = {\n                    'mean': float(group_data.mean()),\n                    'std': float(group_data.std()),\n                    'count': int(group_data.count())\n                }\n\n            # Perform statistical test if possible\n            if scipy_available:\n                group_samples = [data[data[grouping_column] == group][dim].dropna() for group in groups]\n                # Filter out empty groups\n                group_samples = [s for s in group_samples if not s.empty]\n\n                if len(group_samples) == 2:\n                    stat, p_value = ttest_ind(group_samples[0], group_samples[1], equal_var=False) # Welch's t-test\n                    results[dim]['test_type'] = 't-test'\n                    results[dim]['statistic'] = float(stat)\n                    results[dim]['p_value'] = float(p_value)\n                elif len(group_samples) > 2:\n                    stat, p_value = f_oneway(*group_samples)\n                    results[dim]['test_type'] = 'ANOVA'\n                    results[dim]['statistic'] = float(stat)\n                    results[dim]['p_value'] = float(p_value)\n                else:\n                    results[dim]['test_type'] = 'not_enough_groups_with_data'\n            else:\n                results[dim]['test_type'] = 'scipy_not_available'\n\n        return results\n\n    except Exception:\n        return None\n\ndef identify_top_documents_by_metric(data, **kwargs):\n    \"\"\"\n    Identifies and returns the top N documents based on a specified metric score.\n    This function is useful for qualitative validation and for finding archetypal\n    examples of specific populist rhetorical strategies.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the analysis data.\n        **kwargs:\n            metric_name (str): The column name of the metric to sort by. This can be\n                               a raw score, salience, or a derived metric.\n            n (int): The number of top documents to return. Defaults to 5.\n            use_derived_metrics (bool): If True, calculates derived metrics first,\n                                        allowing sorting by them. Defaults to True.\n\n    Returns:\n        list: A list of dictionaries, where each dictionary contains the document\n              name and its score for the specified metric. Returns None if the\n              metric is not found or an error occurs.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        metric_name = kwargs.get('metric_name')\n        if not metric_name:\n            return {\"error\": \"Mandatory 'metric_name' kwarg not provided.\"}\n\n        n = kwargs.get('n', 5)\n        use_derived_metrics = kwargs.get('use_derived_metrics', True)\n        \n        df = data.copy()\n\n        # Calculate derived metrics if requested and the metric is a derived one\n        derived_metric_cols = [\n            'democratic_authoritarian_tension', 'internal_external_focus_tension',\n            'crisis_elite_attribution_tension', 'populist_strategic_contradiction_index',\n            'salience_weighted_core_populism_index', 'salience_weighted_populism_mechanisms_index',\n            'salience_weighted_boundary_distinctions_index', 'salience_weighted_overall_populism_index'\n        ]\n        if use_derived_metrics and metric_name in derived_metric_cols:\n            df = calculate_derived_metrics(df)\n            if df is None:\n                return {\"error\": \"Failed to calculate derived metrics.\"}\n\n        if metric_name not in df.columns:\n            return {\"error\": f\"Metric '{metric_name}' not found in data columns.\"}\n        \n        if 'document_name' not in df.columns:\n            return {\"error\": \"'document_name' column not found in data.\"}\n\n        # Sort by the metric and get the top N\n        top_docs = df.sort_values(by=metric_name, ascending=False).head(n)\n        \n        result = top_docs[['document_name', metric_name]].to_dict('records')\n\n        return result\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}