"""
Automated Statistical Analysis Functions
========================================

Generated by AutomatedStatisticalAnalysisAgent for experiment: simple_test
Description: No description
Generated: 2025-08-15T04:04:24.458032+00:00

This module contains automatically generated statistical analysis functions
for comprehensive data analysis including ANOVA, correlations, reliability,
and hypothesis testing as appropriate for the research questions.
"""

import pandas as pd
import numpy as np
import scipy.stats as stats
from typing import Dict, Any, Optional, List, Tuple
import warnings

# Suppress common statistical warnings for cleaner output
warnings.filterwarnings('ignore', category=RuntimeWarning)


def calculate_descriptive_statistics(data, **kwargs):
    """
    Calculate descriptive statistics (mean, standard deviation, count) for all
    dimensional scores, salience, and confidence metrics.

    Statistical Methodology:
    Computes standard descriptive statistics (mean, standard deviation, and count)
    for numerical columns. It specifically targets columns ending with '_score',
    '_salience', and '_confidence' to provide insights into the central tendency,
    dispersion, and data availability for each dimension.

    Args:
        data (pd.DataFrame): A pandas DataFrame expected to contain columns
                             for dimensional scores, salience, and confidence
                             (e.g., 'tribal_dominance_score',
                             'tribal_dominance_salience', etc.).
        **kwargs: Additional parameters (not used in this function).

    Returns:
        dict: A dictionary where keys are the column names and values are
              dictionaries containing 'mean', 'std', and 'count' for that column.
              Returns None if the input DataFrame is empty or an error occurs.
    """
    import pandas as pd
    import numpy as np

    try:
        if not isinstance(data, pd.DataFrame) or data.empty:
            return None

        results = {}
        # Identify relevant columns: scores, salience, confidence
        relevant_columns = [col for col in data.columns if
                            col.endswith('_score') or
                            col.endswith('_salience') or
                            col.endswith('_confidence')]

        for col in relevant_columns:
            if pd.api.types.is_numeric_dtype(data[col]):
                series = data[col].dropna() # Drop NaNs for accurate stats
                if not series.empty:
                    results[col] = {
                        'mean': float(series.mean()),
                        'std': float(series.std()),
                        'count': int(series.count())
                    }
                else:
                    results[col] = {
                        'mean': np.nan,
                        'std': np.nan,
                        'count': 0
                    }
        return results

    except Exception as e:
        print(f"Error in calculate_descriptive_statistics: {e}")
        return None

def calculate_derived_metrics(data, **kwargs):
    """
    Calculate the five derived metrics as specified in the framework:
    Identity Balance, Emotional Balance, Success Climate, Relational Climate,
    and Goal Orientation. These metrics represent differences between
    paired dimensions.

    Statistical Methodology:
    Each derived metric is calculated as the direct difference between two
    dimensional scores (e.g., `hope_score - fear_score`). The result is a
    continuous value ranging from -1.0 to +1.0, indicating the dominance
    of one dimension over its counterpart. Missing values in input scores
    will result in NaN for the derived metric.

    Args:
        data (pd.DataFrame): A pandas DataFrame containing the raw dimensional
                             scores (e.g., 'tribal_dominance_score',
                             'individual_dignity_score', etc.).
        **kwargs: Additional parameters (not used in this function).

    Returns:
        pd.DataFrame: A new DataFrame with the original data and five
                      new columns for the derived metrics. Returns None
                      if the input DataFrame is invalid or required columns
                      are missing.
    """
    import pandas as pd
    import numpy as np

    try:
        if not isinstance(data, pd.DataFrame) or data.empty:
            return None

        df_copy = data.copy()

        # Define required columns for each derived metric
        metric_definitions = {
            'identity_balance': ('individual_dignity_score', 'tribal_dominance_score'),
            'emotional_balance': ('hope_score', 'fear_score'),
            'success_climate': ('compersion_score', 'envy_score'),
            'relational_climate': ('amity_score', 'enmity_score'),
            'goal_orientation': ('cohesive_goals_score', 'fragmentative_goals_score')
        }

        for metric_name, (positive_dim, negative_dim) in metric_definitions.items():
            if positive_dim in df_copy.columns and negative_dim in df_copy.columns:
                df_copy[metric_name] = df_copy[positive_dim] - df_copy[negative_dim]
            else:
                print(f"Warning: Missing one or both columns for {metric_name}. "
                      f"Expected '{positive_dim}' and '{negative_dim}'.")
                df_copy[metric_name] = np.nan # Assign NaN if columns are missing

        return df_copy

    except Exception as e:
        print(f"Error in calculate_derived_metrics: {e}")
        return None

def calculate_overall_cohesion_index(data, **kwargs):
    """
    Calculate the Overall Cohesion Index as defined in the framework.
    This is a comprehensive measure of discourse cohesiveness.

    Statistical Methodology:
    The index is calculated as the average of five 'cohesive' dimension scores
    minus the average of five 'fragmentative' dimension scores.
    `Overall Cohesion Index = (Avg(Dignity, Hope, Compersion, Amity, Cohesive Goals)) - (Avg(Tribal Dominance, Fear, Envy, Enmity, Fragmentative Goals))`
    The result ranges from -1.0 (maximally fragmentative) to +1.0 (maximally cohesive).
    Rows with missing scores for any of the required dimensions will result in NaN for the index.

    Args:
        data (pd.DataFrame): A pandas DataFrame containing the raw dimensional
                             scores.
        **kwargs: Additional parameters (not used in this function).

    Returns:
        pd.DataFrame: A new DataFrame with the original data and an additional
                      column 'overall_cohesion_index'. Returns None if the
                      input DataFrame is invalid or required columns are missing.
    """
    import pandas as pd
    import numpy as np

    try:
        if not isinstance(data, pd.DataFrame) or data.empty:
            return None

        df_copy = data.copy()

        cohesive_dims = [
            'individual_dignity_score',
            'hope_score',
            'compersion_score',
            'amity_score',
            'cohesive_goals_score'
        ]
        fragmentative_dims = [
            'tribal_dominance_score',
            'fear_score',
            'envy_score',
            'enmity_score',
            'fragmentative_goals_score'
        ]

        # Check if all required columns exist
        required_cols = cohesive_dims + fragmentative_dims
        if not all(col in df_copy.columns for col in required_cols):
            missing_cols = [col for col in required_cols if col not in df_copy.columns]
            print(f"Error: Missing required columns for Overall Cohesion Index: {missing_cols}")
            df_copy['overall_cohesion_index'] = np.nan
            return df_copy # Return with NaN column if missing

        # Calculate average for cohesive and fragmentative groups
        # Using .mean(axis=1) will ignore NaNs in the row for that average,
        # but if all are NaN, it will be NaN.
        df_copy['avg_cohesive_score'] = df_copy[cohesive_dims].mean(axis=1)
        df_copy['avg_fragmentative_score'] = df_copy[fragmentative_dims].mean(axis=1)

        df_copy['overall_cohesion_index'] = df_copy['avg_cohesive_score'] - df_copy['avg_fragmentative_score']

        # Drop intermediate columns if not needed for final output
        df_copy = df_copy.drop(columns=['avg_cohesive_score', 'avg_fragmentative_score'])

        return df_copy

    except Exception as e:
        print(f"Error in calculate_overall_cohesion_index: {e}")
        return None

def calculate_tension_mathematics(data, **kwargs):
    """
    Calculate the five 'Tension Mathematics' scores as defined in the framework.
    These scores quantify the strategic contradiction between paired dimensions,
    considering both their raw scores and salience.

    Statistical Methodology:
    The Tension Score Formula is `min(Dimension_A_score, Dimension_B_score) Ã— |Salience_A - Salience_B|`.
    This formula captures tension where both opposing dimensions are present (min score > 0)
    and where their salience differs significantly. A higher score indicates greater tension.
    Missing values in scores or salience will result in NaN for the tension score.

    Args:
        data (pd.DataFrame): A pandas DataFrame containing the raw dimensional
                             scores and salience metrics (e.g.,
                             'tribal_dominance_score', 'tribal_dominance_salience', etc.).
        **kwargs: Additional parameters (not used in this function).

    Returns:
        pd.DataFrame: A new DataFrame with the original data and five
                      new columns for the tension mathematics scores. Returns None
                      if the input DataFrame is invalid or required columns are missing.
    """
    import pandas as pd
    import numpy as np

    try:
        if not isinstance(data, pd.DataFrame) or data.empty:
            return None

        df_copy = data.copy()

        # Define required columns for each tension metric
        tension_definitions = {
            'identity_tension_score': {
                'dim1_score': 'tribal_dominance_score', 'dim1_salience': 'tribal_dominance_salience',
                'dim2_score': 'individual_dignity_score', 'dim2_salience': 'individual_dignity_salience'
            },
            'emotional_tension_score': {
                'dim1_score': 'fear_score', 'dim1_salience': 'fear_salience',
                'dim2_score': 'hope_score', 'dim2_salience': 'hope_salience'
            },
            'success_tension_score': {
                'dim1_score': 'envy_score', 'dim1_salience': 'envy_salience',
                'dim2_score': 'compersion_score', 'dim2_salience': 'compersion_salience'
            },
            'relational_tension_score': {
                'dim1_score': 'enmity_score', 'dim1_salience': 'enmity_salience',
                'dim2_score': 'amity_score', 'dim2_salience': 'amity_salience'
            },
            'goal_tension_score': {
                'dim1_score': 'fragmentative_goals_score', 'dim1_salience': 'fragmentative_goals_salience',
                'dim2_score': 'cohesive_goals_score', 'dim2_salience': 'cohesive_goals_salience'
            }
        }

        for tension_name, dims in tension_definitions.items():
            score1_col, salience1_col = dims['dim1_score'], dims['dim1_salience']
            score2_col, salience2_col = dims['dim2_score'], dims['dim2_salience']

            if all(col in df_copy.columns for col in [score1_col, salience1_col, score2_col, salience2_col]):
                min_score = df_copy[[score1_col, score2_col]].min(axis=1)
                salience_diff_abs = (df_copy[salience1_col] - df_copy[salience2_col]).abs()
                df_copy[tension_name] = min_score * salience_diff_abs
            else:
                missing_cols = [col for col in [score1_col, salience1_col, score2_col, salience2_col] if col not in df_copy.columns]
                print(f"Warning: Missing columns for {tension_name}: {missing_cols}. Assigning NaN.")
                df_copy[tension_name] = np.nan

        return df_copy

    except Exception as e:
        print(f"Error in calculate_tension_mathematics: {e}")
        return None

def calculate_strategic_indices(data, **kwargs):
    """
    Calculate the three 'Strategic Indices' as defined in the framework:
    Strategic Contradiction Index, Salience-Weighted Cohesive Index, and
    Salience-Weighted Fragmentative Index.

    Statistical Methodology:
    1. Strategic Contradiction Index: The average of the five individual
       tension mathematics scores. It quantifies the overall level of
       internal contradiction in the discourse.
    2. Salience-Weighted Cohesive Index: A weighted average of the cohesive
       dimension scores, where each score is weighted by its corresponding
       salience. This highlights the strength of cohesive messaging considering
       its prominence.
    3. Salience-Weighted Fragmentative Index: Similar to the cohesive index,
       but for fragmentative dimensions.

    Missing values in required columns will result in NaN for the respective index.

    Args:
        data (pd.DataFrame): A pandas DataFrame containing the raw dimensional
                             scores, salience metrics, and the previously
                             calculated tension mathematics scores.
        **kwargs: Additional parameters (not used in this function).

    Returns:
        pd.DataFrame: A new DataFrame with the original data and three
                      new columns for the strategic indices. Returns None
                      if the input DataFrame is invalid or required columns are missing.
    """
    import pandas as pd
    import numpy as np

    try:
        if not isinstance(data, pd.DataFrame) or data.empty:
            return None

        df_copy = data.copy()

        # 1. Strategic Contradiction Index
        tension_cols = [
            'identity_tension_score',
            'emotional_tension_score',
            'success_tension_score',
            'relational_tension_score',
            'goal_tension_score'
        ]
        if all(col in df_copy.columns for col in tension_cols):
            df_copy['strategic_contradiction_index'] = df_copy[tension_cols].mean(axis=1)
        else:
            missing_tension_cols = [col for col in tension_cols if col not in df_copy.columns]
            print(f"Warning: Missing tension columns for Strategic Contradiction Index: {missing_tension_cols}. Assigning NaN.")
            df_copy['strategic_contradiction_index'] = np.nan

        # 2. Salience-Weighted Cohesive Index
        cohesive_weighted_pairs = [
            ('individual_dignity_score', 'individual_dignity_salience'),
            ('hope_score', 'hope_salience'),
            ('compersion_score', 'compersion_salience'),
            ('amity_score', 'amity_salience'),
            ('cohesive_goals_score', 'cohesive_goals_salience')
        ]
        weighted_sum_cohesive = pd.Series(0.0, index=df_copy.index)
        sum_salience_cohesive = pd.Series(0.0, index=df_copy.index)
        
        all_cohesive_cols_exist = True
        for score_col, salience_col in cohesive_weighted_pairs:
            if score_col in df_copy.columns and salience_col in df_copy.columns:
                weighted_sum_cohesive += df_copy[score_col] * df_copy[salience_col]
                sum_salience_cohesive += df_copy[salience_col]
            else:
                all_cohesive_cols_exist = False
                print(f"Warning: Missing column(s) for Salience-Weighted Cohesive Index: {score_col} or {salience_col}. This index might be incomplete or NaN.")
                break # Break if any pair is missing, as the sum will be incorrect

        if all_cohesive_cols_exist:
            # Handle division by zero if sum_salience_cohesive is 0
            df_copy['salience_weighted_cohesive_index'] = weighted_sum_cohesive / sum_salience_cohesive
            df_copy['salience_weighted_cohesive_index'] = df_copy['salience_weighted_cohesive_index'].replace([np.inf, -np.inf], np.nan)
        else:
            df_copy['salience_weighted_cohesive_index'] = np.nan


        # 3. Salience-Weighted Fragmentative Index
        fragmentative_weighted_pairs = [
            ('tribal_dominance_score', 'tribal_dominance_salience'),
            ('fear_score', 'fear_salience'),
            ('envy_score', 'envy_salience'),
            ('enmity_score', 'enmity_salience'),
            ('fragmentative_goals_score', 'fragmentative_goals_salience')
        ]
        weighted_sum_fragmentative = pd.Series(0.0, index=df_copy.index)
        sum_salience_fragmentative = pd.Series(0.0, index=df_copy.index)

        all_fragmentative_cols_exist = True
        for score_col, salience_col in fragmentative_weighted_pairs:
            if score_col in df_copy.columns and salience_col in df_copy.columns:
                weighted_sum_fragmentative += df_copy[score_col] * df_copy[salience_col]
                sum_salience_fragmentative += df_copy[salience_col]
            else:
                all_fragmentative_cols_exist = False
                print(f"Warning: Missing column(s) for Salience-Weighted Fragmentative Index: {score_col} or {salience_col}. This index might be incomplete or NaN.")
                break # Break if any pair is missing

        if all_fragmentative_cols_exist:
            # Handle division by zero if sum_salience_fragmentative is 0
            df_copy['salience_weighted_fragmentative_index'] = weighted_sum_fragmentative / sum_salience_fragmentative
            df_copy['salience_weighted_fragmentative_index'] = df_copy['salience_weighted_fragmentative_index'].replace([np.inf, -np.inf], np.nan)
        else:
            df_copy['salience_weighted_fragmentative_index'] = np.nan

        return df_copy

    except Exception as e:
        print(f"Error in calculate_strategic_indices: {e}")
        return None

def run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:
    """
    Run complete statistical analysis suite on the dataset.
    
    Args:
        data: pandas DataFrame with dimension scores
        alpha: Significance level for hypothesis tests (default: 0.05)
        
    Returns:
        Dictionary with all statistical analysis results
    """
    results = {
        'analysis_metadata': {
            'timestamp': pd.Timestamp.now().isoformat(),
            'sample_size': len(data),
            'alpha_level': alpha,
            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)
        }
    }
    
    # Get all analysis functions from this module
    import inspect
    current_module = inspect.getmodule(inspect.currentframe())
    
    for name, obj in inspect.getmembers(current_module):
        if (inspect.isfunction(obj) and 
            name.startswith(('calculate_', 'perform_', 'test_')) and 
            name != 'run_complete_statistical_analysis'):
            try:
                # Pass alpha parameter to functions that might need it
                if 'alpha' in inspect.signature(obj).parameters:
                    results[name] = obj(data, alpha=alpha)
                else:
                    results[name] = obj(data)
            except Exception as e:
                results[name] = {'error': f'Analysis failed: {str(e)}'}
                
    return results


def generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:
    """
    Generate a human-readable summary report from statistical analysis results.
    
    Args:
        analysis_results: Results from run_complete_statistical_analysis()
        
    Returns:
        String containing formatted statistical report
    """
    report_lines = []
    report_lines.append("STATISTICAL ANALYSIS SUMMARY REPORT")
    report_lines.append("=" * 50)
    
    metadata = analysis_results.get('analysis_metadata', {})
    report_lines.append(f"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}")
    report_lines.append(f"Sample Size: {metadata.get('sample_size', 'Unknown')}")
    report_lines.append(f"Alpha Level: {metadata.get('alpha_level', 'Unknown')}")
    report_lines.append(f"Variables: {len(metadata.get('variables_analyzed', []))}")
    report_lines.append("")
    
    # Summarize key findings
    for analysis_name, result in analysis_results.items():
        if analysis_name != 'analysis_metadata' and isinstance(result, dict):
            if 'error' not in result:
                report_lines.append(f"{analysis_name.replace('_', ' ').title()}:")
                
                # Extract key statistics based on analysis type
                if 'p_value' in result:
                    p_val = result['p_value']
                    significance = "significant" if p_val < metadata.get('alpha_level', 0.05) else "not significant"
                    report_lines.append(f"  - p-value: {p_val:.4f} ({significance})")
                
                if 'effect_size' in result:
                    report_lines.append(f"  - Effect size: {result['effect_size']:.4f}")
                
                if 'correlation_matrix' in result:
                    report_lines.append(f"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables")
                
                if 'cronbach_alpha' in result:
                    alpha_val = result['cronbach_alpha']
                    reliability = "excellent" if alpha_val > 0.9 else "good" if alpha_val > 0.8 else "acceptable" if alpha_val > 0.7 else "questionable"
                    report_lines.append(f"  - Cronbach's Î±: {alpha_val:.3f} ({reliability})")
                
                report_lines.append("")
            else:
                report_lines.append(f"{analysis_name}: ERROR - {result['error']}")
                report_lines.append("")
    
    return "\n".join(report_lines)
