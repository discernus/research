test_name,test_type,statistic_name,statistic_value,p_value,effect_size,degrees_of_freedom,sample_size,dependent_variable,grouping_variable,significance_level,interpretation,notes
task_01_calculate_derived_metrics,derived_metrics_calculation,result_value,"{'type': 'derived_metrics_calculation', 'success': True, 'calculated_metrics': {'hope_fear_tension': [0.45, 0.875], 'dignity_tribalism_tension': [0.19999999999999998, 0.875], 'truth_manipulation_tension': [0.4, 0.8500000000000001], 'justice_resentment_tension': [0.25, 0.875], 'pragmatism_fantasy_tension': [0.4, 0.875], 'virtue_index': [0.42000000000000004, 0.8400000000000001], 'pathology_index': [0.74, 0.1], 'avg_virtue_salience': [0.5, 0.8200000000000001], 'avg_pathology_salience': [0.76, 0.07], 'civic_character_index': [0.34, 0.8699999999999999], 'dimension_variance': [0.04360000000000001, 0.13810000000000003], 'salience_weighted_civic_character_index': [0.352, 0.8704268292682927]}, 'successful_calculations': ['hope_fear_tension', 'dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'pragmatism_fantasy_tension', 'virtue_index', 'pathology_index', 'avg_virtue_salience', 'avg_pathology_salience', 'civic_character_index', 'dimension_variance', 'salience_weighted_civic_character_index'], 'failed_calculations': [], 'formulas_used': ['dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'hope_fear_tension', 'pragmatism_fantasy_tension', 'civic_character_index', 'salience_weighted_civic_character_index', 'virtue_index', 'pathology_index', 'dimension_variance', 'avg_virtue_salience', 'avg_pathology_salience'], 'input_columns': ['dignity_score', 'tribalism_score', 'truth_score', 'manipulation_score', 'justice_score', 'resentment_score', 'hope_score', 'fear_score', 'pragmatism_score', 'fantasy_score', 'dignity_salience', 'tribalism_salience', 'truth_salience', 'manipulation_salience', 'justice_salience', 'resentment_salience', 'hope_salience', 'fear_salience', 'pragmatism_salience', 'fantasy_salience'], 'total_metrics': 12, 'success_rate': 1.0}",,,,,,,,Generic derived_metrics_calculation result,
task_02_descriptive_statistics,descriptive_stats,result_value,"{'type': 'descriptive_stats', 'columns_analyzed': ['dignity_score', 'tribalism_score', 'truth_score', 'manipulation_score', 'justice_score', 'resentment_score', 'hope_score', 'fear_score', 'pragmatism_score', 'fantasy_score', 'civic_character_index', 'salience_weighted_civic_character_index', 'virtue_index', 'pathology_index', 'dimension_variance'], 'results': {'dignity_score': {'count': 2, 'mean': 0.575, 'std': 0.38890872965260115, 'min': 0.3, 'max': 0.85, 'median': 0.575, 'q25': 0.4375, 'q75': 0.7124999999999999, 'skewness': nan, 'kurtosis': nan}, 'tribalism_score': {'count': 2, 'mean': 0.5, 'std': 0.5656854249492381, 'min': 0.1, 'max': 0.9, 'median': 0.5, 'q25': 0.30000000000000004, 'q75': 0.7, 'skewness': nan, 'kurtosis': nan}, 'truth_score': {'count': 2, 'mean': 0.65, 'std': 0.21213203435596428, 'min': 0.5, 'max': 0.8, 'median': 0.65, 'q25': 0.575, 'q75': 0.7250000000000001, 'skewness': nan, 'kurtosis': nan}, 'manipulation_score': {'count': 2, 'mean': 0.39999999999999997, 'std': 0.42426406871192845, 'min': 0.1, 'max': 0.7, 'median': 0.39999999999999997, 'q25': 0.25, 'q75': 0.5499999999999999, 'skewness': nan, 'kurtosis': nan}, 'justice_score': {'count': 2, 'mean': 0.6000000000000001, 'std': 0.28284271247461906, 'min': 0.4, 'max': 0.8, 'median': 0.6000000000000001, 'q25': 0.5, 'q75': 0.7000000000000001, 'skewness': nan, 'kurtosis': nan}, 'resentment_score': {'count': 2, 'mean': 0.47500000000000003, 'std': 0.6010407640085654, 'min': 0.05, 'max': 0.9, 'median': 0.47500000000000003, 'q25': 0.2625, 'q75': 0.6875, 'skewness': nan, 'kurtosis': nan}, 'hope_score': {'count': 2, 'mean': 0.75, 'std': 0.21213203435596428, 'min': 0.6, 'max': 0.9, 'median': 0.75, 'q25': 0.675, 'q75': 0.825, 'skewness': nan, 'kurtosis': nan}, 'fear_score': {'count': 2, 'mean': 0.425, 'std': 0.38890872965260115, 'min': 0.15, 'max': 0.7, 'median': 0.425, 'q25': 0.2875, 'q75': 0.5625, 'skewness': nan, 'kurtosis': nan}, 'pragmatism_score': {'count': 2, 'mean': 0.575, 'std': 0.38890872965260115, 'min': 0.3, 'max': 0.85, 'median': 0.575, 'q25': 0.4375, 'q75': 0.7124999999999999, 'skewness': nan, 'kurtosis': nan}, 'fantasy_score': {'count': 2, 'mean': 0.3, 'std': 0.282842712474619, 'min': 0.1, 'max': 0.5, 'median': 0.3, 'q25': 0.2, 'q75': 0.4, 'skewness': nan, 'kurtosis': nan}, 'civic_character_index': {'count': 2, 'mean': 0.605, 'std': 0.3747665940288701, 'min': 0.34, 'max': 0.8699999999999999, 'median': 0.605, 'q25': 0.4725, 'q75': 0.7374999999999999, 'skewness': nan, 'kurtosis': nan}, 'salience_weighted_civic_character_index': {'count': 2, 'mean': 0.6112134146341464, 'std': 0.3665831265246503, 'min': 0.352, 'max': 0.8704268292682927, 'median': 0.6112134146341464, 'q25': 0.48160670731707317, 'q75': 0.7408201219512196, 'skewness': nan, 'kurtosis': nan}, 'virtue_index': {'count': 2, 'mean': 0.6300000000000001, 'std': 0.29698484809834996, 'min': 0.42000000000000004, 'max': 0.8400000000000001, 'median': 0.6300000000000001, 'q25': 0.525, 'q75': 0.7350000000000001, 'skewness': nan, 'kurtosis': nan}, 'pathology_index': {'count': 2, 'mean': 0.42, 'std': 0.4525483399593904, 'min': 0.1, 'max': 0.74, 'median': 0.42, 'q25': 0.26, 'q75': 0.58, 'skewness': nan, 'kurtosis': nan}, 'dimension_variance': {'count': 2, 'mean': 0.09085000000000001, 'std': 0.06682159082212875, 'min': 0.04360000000000001, 'max': 0.13810000000000003, 'median': 0.09085000000000001, 'q25': 0.06722500000000001, 'q75': 0.11447500000000002, 'skewness': nan, 'kurtosis': nan}}}",,,,,,,,Generic descriptive_stats result,
task_04_validate_architecture_and_metrics,metric_validation,result_value,"{'type': 'metric_validation', 'validation_rules': [""{'rule': 'missing_data_check', 'columns': ['civic_character_index', 'salience_weighted_civic_character_index', 'virtue_index', 'pathology_index'], 'tolerance': 0.0}"", ""{'rule': 'range_check', 'columns': ['civic_character_index', 'salience_weighted_civic_character_index', 'virtue_index', 'pathology_index'], 'min': 0.0, 'max': 1.0}""], 'results': {'unknown_rule': {'status': 'not_found', 'message': ""Metric 'unknown_rule' not found in dataframe"", 'available_columns': ['aid', 'document_type', 'political_party', 'year', 'temporal_sequence', 'speaker', 'ideology', 'character_profile', 'event', 'word_count', 'source', 'preparation_notes', 'dignity_score', 'tribalism_score', 'dignity_salience', 'tribalism_salience', 'dignity_confidence', 'tribalism_confidence', 'truth_score', 'manipulation_score', 'truth_salience', 'manipulation_salience', 'truth_confidence', 'manipulation_confidence', 'justice_score', 'resentment_score', 'justice_salience', 'resentment_salience', 'justice_confidence', 'resentment_confidence', 'hope_score', 'fear_score', 'hope_salience', 'fear_salience', 'hope_confidence', 'fear_confidence', 'pragmatism_score', 'fantasy_score', 'pragmatism_salience', 'fantasy_salience', 'pragmatism_confidence', 'fantasy_confidence', 'gasket_version', 'extraction_time_seconds', 'hope_fear_tension', 'dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'pragmatism_fantasy_tension', 'virtue_index', 'pathology_index', 'avg_virtue_salience', 'avg_pathology_salience', 'civic_character_index', 'dimension_variance', 'salience_weighted_civic_character_index'], 'note': 'This is a framework-agnostic validation - LLMs determine validation logic'}}, 'quality_thresholds': {'min_valid_ratio': 0.8, 'max_outlier_ratio': 0.1, 'min_variance': 0.01, 'correlation_threshold': 0.95}}",,,,,,,,Generic metric_validation result,
