"""
Automated Statistical Analysis Functions
========================================

Generated by AutomatedStatisticalAnalysisAgent for experiment: simple_test
Description: No description
Generated: 2025-08-15T04:09:37.705558+00:00

This module contains automatically generated statistical analysis functions
for comprehensive data analysis including ANOVA, correlations, reliability,
and hypothesis testing as appropriate for the research questions.
"""

import pandas as pd
import numpy as np
import scipy.stats as stats
from typing import Dict, Any, Optional, List, Tuple
import warnings

# Suppress common statistical warnings for cleaner output
warnings.filterwarnings('ignore', category=RuntimeWarning)


def calculate_descriptive_statistics(data, **kwargs):
    """
    Calculate descriptive statistics for all primary dimension raw scores and salience values.

    This function iterates through the expected dimension names and their associated
    '_raw_score' and '_salience' columns to compute mean, standard deviation,
    minimum, maximum, and count. It provides an overview of the distribution
    of scores and salience across all analyzed documents.

    Args:
        data (pandas.DataFrame): A DataFrame containing columns for each dimension's
                                 raw score (e.g., 'tribal_dominance_score_raw_score')
                                 and salience (e.g., 'tribal_dominance_score_salience').
        **kwargs: Additional parameters (not used in this function).

    Returns:
        dict: A dictionary where keys are the column names (e.g., 'tribal_dominance_score_raw_score')
              and values are dictionaries containing 'mean', 'std', 'min', 'max', and 'count'.
              Returns None if the input data is empty or an error occurs.
    """
    import pandas as pd
    import numpy as np

    try:
        if not isinstance(data, pd.DataFrame) or data.empty:
            print("Input data is not a valid pandas DataFrame or is empty.")
            return None

        results = {}
        
        # Define the base dimension names
        dimensions = [
            "tribal_dominance", "individual_dignity", "fear", "hope", "envy",
            "compersion", "enmity", "amity", "fragmentative_goals", "cohesive_goals"
        ]

        for dim in dimensions:
            raw_score_col = f"{dim}_score_raw_score"
            salience_col = f"{dim}_score_salience"

            # Process raw scores
            if raw_score_col in data.columns:
                series = data[raw_score_col].dropna()
                if not series.empty:
                    results[raw_score_col] = {
                        'mean': float(series.mean()),
                        'std': float(series.std()),
                        'min': float(series.min()),
                        'max': float(series.max()),
                        'count': int(series.count())
                    }
            
            # Process salience scores
            if salience_col in data.columns:
                series = data[salience_col].dropna()
                if not series.empty:
                    results[salience_col] = {
                        'mean': float(series.mean()),
                        'std': float(series.std()),
                        'min': float(series.min()),
                        'max': float(series.max()),
                        'count': int(series.count())
                    }
        
        if not results:
            print("No relevant dimension score columns found in the DataFrame.")
            return None

        return results

    except Exception as e:
        print(f"An error occurred during descriptive statistics calculation: {e}")
        return None

def calculate_derived_metrics(data, **kwargs):
    """
    Calculates the five derived metrics as defined in the Cohesive Flourishing Framework v8.0.

    These metrics quantify specific aspects of the discourse by comparing opposing dimensions.
    The calculations are performed row-wise (per document) and added as new columns to the DataFrame.

    Metrics calculated:
    - Identity Tension (derived): tribal_dominance_score - individual_dignity_score
    - Emotional Balance: hope_score - fear_score
    - Success Climate: compersion_score - envy_score
    - Relational Climate: amity_score - enmity_score
    - Goal Orientation: cohesive_goals_score - fragmentative_goals_score

    Args:
        data (pandas.DataFrame): A DataFrame containing columns for each dimension's
                                 raw score (e.g., 'tribal_dominance_score_raw_score').
        **kwargs: Additional parameters (not used in this function).

    Returns:
        pandas.DataFrame: The input DataFrame with five new columns representing the
                          calculated derived metrics. Returns None if input data is invalid
                          or required columns are missing.
    """
    import pandas as pd
    import numpy as np

    try:
        if not isinstance(data, pd.DataFrame) or data.empty:
            print("Input data is not a valid pandas DataFrame or is empty.")
            return None

        df = data.copy()

        # Define pairs for derived metrics and their corresponding column names
        metric_definitions = {
            'identity_tension_derived': ('tribal_dominance_score_raw_score', 'individual_dignity_score_raw_score', lambda t, i: t - i),
            'emotional_balance': ('hope_score_raw_score', 'fear_score_raw_score', lambda h, f: h - f),
            'success_climate': ('compersion_score_raw_score', 'envy_score_raw_score', lambda c, e: c - e),
            'relational_climate': ('amity_score_raw_score', 'enmity_score_raw_score', lambda a, en: a - en),
            'goal_orientation': ('cohesive_goals_score_raw_score', 'fragmentative_goals_score_raw_score', lambda cg, fg: cg - fg)
        }

        for metric_name, (pos_col, neg_col, func) in metric_definitions.items():
            if pos_col in df.columns and neg_col in df.columns:
                df[metric_name] = df.apply(
                    lambda row: func(row[pos_col], row[neg_col]) if pd.notna(row[pos_col]) and pd.notna(row[neg_col]) else np.nan,
                    axis=1
                )
            else:
                print(f"Warning: Missing one or both columns for '{metric_name}' ({pos_col}, {neg_col}). This metric will not be calculated.")
                df[metric_name] = np.nan # Add column with NaNs if dependencies are missing

        return df

    except Exception as e:
        print(f"An error occurred during derived metrics calculation: {e}")
        return None

def calculate_overall_cohesion_index(data, **kwargs):
    """
    Calculates the Overall Cohesion Index for each document.

    This composite index provides a comprehensive measure of discourse cohesiveness
    by contrasting the average of cohesive dimensions with the average of fragmentative
    dimensions. The calculation is performed row-wise (per document) and added as a
    new column to the DataFrame.

    Formula:
    (individual_dignity + hope + compersion + amity + cohesive_goals) / 5 -
    (tribal_dominance + fear + envy + enmity + fragmentative_goals) / 5

    Args:
        data (pandas.DataFrame): A DataFrame containing columns for each dimension's
                                 raw score (e.g., 'individual_dignity_score_raw_score').
        **kwargs: Additional parameters (not used in this function).

    Returns:
        pandas.DataFrame: The input DataFrame with a new column 'overall_cohesion_index'.
                          Returns None if input data is invalid or required columns are missing.
    """
    import pandas as pd
    import numpy as np

    try:
        if not isinstance(data, pd.DataFrame) or data.empty:
            print("Input data is not a valid pandas DataFrame or is empty.")
            return None

        df = data.copy()

        cohesive_dims = [
            "individual_dignity_score_raw_score",
            "hope_score_raw_score",
            "compersion_score_raw_score",
            "amity_score_raw_score",
            "cohesive_goals_score_raw_score"
        ]

        fragmentative_dims = [
            "tribal_dominance_score_raw_score",
            "fear_score_raw_score",
            "envy_score_raw_score",
            "enmity_score_raw_score",
            "fragmentative_goals_score_raw_score"
        ]

        # Check if all required columns exist
        required_cols = cohesive_dims + fragmentative_dims
        if not all(col in df.columns for col in required_cols):
            missing_cols = [col for col in required_cols if col not in df.columns]
            print(f"Error: Missing required columns for Overall Cohesion Index calculation: {missing_cols}")
            return None

        # Calculate the average of cohesive dimensions
        df['avg_cohesive_scores'] = df[cohesive_dims].mean(axis=1)

        # Calculate the average of fragmentative dimensions
        df['avg_fragmentative_scores'] = df[fragmentative_dims].mean(axis=1)

        # Calculate the Overall Cohesion Index
        df['overall_cohesion_index'] = df['avg_cohesive_scores'] - df['avg_fragmentative_scores']

        # Drop intermediate columns if desired, or keep for transparency
        df = df.drop(columns=['avg_cohesive_scores', 'avg_fragmentative_scores'])

        return df

    except Exception as e:
        print(f"An error occurred during Overall Cohesion Index calculation: {e}")
        return None

def calculate_tension_mathematics(data, **kwargs):
    """
    Calculates the five tension scores based on the 'Tension Mathematics' formula
    defined in the Cohesive Flourishing Framework v8.0.

    The formula is: min(Dimension_A_score, Dimension_B_score) × |Salience_A - Salience_B|.
    These scores quantify the internal tension or contradiction within specific
    dimensional pairs, considering both their raw scores and salience.
    Calculations are performed row-wise (per document) and added as new columns.

    Tension scores calculated:
    - identity_tension_math
    - emotional_tension_math
    - success_tension_math
    - relational_tension_math
    - goal_tension_math

    Args:
        data (pandas.DataFrame): A DataFrame containing columns for each dimension's
                                 raw score (e.g., 'tribal_dominance_score_raw_score')
                                 and salience (e.g., 'tribal_dominance_score_salience').
        **kwargs: Additional parameters (not used in this function).

    Returns:
        pandas.DataFrame: The input DataFrame with five new columns representing the
                          calculated tension mathematics scores. Returns None if input
                          data is invalid or required columns are missing.
    """
    import pandas as pd
    import numpy as np

    try:
        if not isinstance(data, pd.DataFrame) or data.empty:
            print("Input data is not a valid pandas DataFrame or is empty.")
            return None

        df = data.copy()

        # Define pairs for tension mathematics and their corresponding column names
        tension_definitions = {
            'identity_tension_math': ('tribal_dominance_score', 'individual_dignity_score'),
            'emotional_tension_math': ('fear_score', 'hope_score'),
            'success_tension_math': ('envy_score', 'compersion_score'),
            'relational_tension_math': ('enmity_score', 'amity_score'),
            'goal_tension_math': ('fragmentative_goals_score', 'cohesive_goals_score')
        }

        for tension_name, (dim1_base, dim2_base) in tension_definitions.items():
            score1_col = f"{dim1_base}_raw_score"
            salience1_col = f"{dim1_base}_salience"
            score2_col = f"{dim2_base}_raw_score"
            salience2_col = f"{dim2_base}_salience"

            required_cols = [score1_col, salience1_col, score2_col, salience2_col]
            if all(col in df.columns for col in required_cols):
                df[tension_name] = df.apply(
                    lambda row: (min(row[score1_col], row[score2_col]) * abs(row[salience1_col] - row[salience2_col]))
                                if pd.notna(row[score1_col]) and pd.notna(row[score2_col]) and
                                   pd.notna(row[salience1_col]) and pd.notna(row[salience2_col])
                                else np.nan,
                    axis=1
                )
            else:
                missing_cols = [col for col in required_cols if col not in df.columns]
                print(f"Warning: Missing columns for '{tension_name}' calculation: {missing_cols}. This metric will not be calculated.")
                df[tension_name] = np.nan # Add column with NaNs if dependencies are missing

        return df

    except Exception as e:
        print(f"An error occurred during tension mathematics calculation: {e}")
        return None

def run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:
    """
    Run complete statistical analysis suite on the dataset.
    
    Args:
        data: pandas DataFrame with dimension scores
        alpha: Significance level for hypothesis tests (default: 0.05)
        
    Returns:
        Dictionary with all statistical analysis results
    """
    results = {
        'analysis_metadata': {
            'timestamp': pd.Timestamp.now().isoformat(),
            'sample_size': len(data),
            'alpha_level': alpha,
            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)
        }
    }
    
    # Get all analysis functions from this module
    import inspect
    current_module = inspect.getmodule(inspect.currentframe())
    
    for name, obj in inspect.getmembers(current_module):
        if (inspect.isfunction(obj) and 
            name.startswith(('calculate_', 'perform_', 'test_')) and 
            name != 'run_complete_statistical_analysis'):
            try:
                # Pass alpha parameter to functions that might need it
                if 'alpha' in inspect.signature(obj).parameters:
                    results[name] = obj(data, alpha=alpha)
                else:
                    results[name] = obj(data)
            except Exception as e:
                results[name] = {'error': f'Analysis failed: {str(e)}'}
                
    return results


def generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:
    """
    Generate a human-readable summary report from statistical analysis results.
    
    Args:
        analysis_results: Results from run_complete_statistical_analysis()
        
    Returns:
        String containing formatted statistical report
    """
    report_lines = []
    report_lines.append("STATISTICAL ANALYSIS SUMMARY REPORT")
    report_lines.append("=" * 50)
    
    metadata = analysis_results.get('analysis_metadata', {})
    report_lines.append(f"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}")
    report_lines.append(f"Sample Size: {metadata.get('sample_size', 'Unknown')}")
    report_lines.append(f"Alpha Level: {metadata.get('alpha_level', 'Unknown')}")
    report_lines.append(f"Variables: {len(metadata.get('variables_analyzed', []))}")
    report_lines.append("")
    
    # Summarize key findings
    for analysis_name, result in analysis_results.items():
        if analysis_name != 'analysis_metadata' and isinstance(result, dict):
            if 'error' not in result:
                report_lines.append(f"{analysis_name.replace('_', ' ').title()}:")
                
                # Extract key statistics based on analysis type
                if 'p_value' in result:
                    p_val = result['p_value']
                    significance = "significant" if p_val < metadata.get('alpha_level', 0.05) else "not significant"
                    report_lines.append(f"  - p-value: {p_val:.4f} ({significance})")
                
                if 'effect_size' in result:
                    report_lines.append(f"  - Effect size: {result['effect_size']:.4f}")
                
                if 'correlation_matrix' in result:
                    report_lines.append(f"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables")
                
                if 'cronbach_alpha' in result:
                    alpha_val = result['cronbach_alpha']
                    reliability = "excellent" if alpha_val > 0.9 else "good" if alpha_val > 0.8 else "acceptable" if alpha_val > 0.7 else "questionable"
                    report_lines.append(f"  - Cronbach's α: {alpha_val:.3f} ({reliability})")
                
                report_lines.append("")
            else:
                report_lines.append(f"{analysis_name}: ERROR - {result['error']}")
                report_lines.append("")
    
    return "\n".join(report_lines)
