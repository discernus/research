"""
Automated Statistical Analysis Functions
========================================

Generated by AutomatedStatisticalAnalysisAgent for experiment: simple_test
Description: No description
Generated: 2025-08-15T11:48:35.992487+00:00

This module contains automatically generated statistical analysis functions
for comprehensive data analysis including ANOVA, correlations, reliability,
and hypothesis testing as appropriate for the research questions.
"""

import pandas as pd
import numpy as np
import scipy.stats as stats
from typing import Dict, Any, Optional, List, Tuple
import warnings

# Suppress common statistical warnings for cleaner output
warnings.filterwarnings('ignore', category=RuntimeWarning)


def calculate_descriptive_statistics(data, columns_to_analyze=None, **kwargs):
    """
    Calculate descriptive statistics (mean, standard deviation, count, min, max, median)
    for specified numeric columns in the DataFrame.

    Statistical Methodology:
    This function computes standard descriptive statistics for each column provided.
    It handles missing values by excluding them from calculations.

    Args:
        data (pd.DataFrame): The input pandas DataFrame containing the analysis data.
        columns_to_analyze (list, optional): A list of column names for which to calculate
                                             descriptive statistics. If None, it will
                                             analyze all numeric columns ending with
                                             '_score', '_salience', or '_confidence'.
        **kwargs: Additional parameters (not used in this function but for API consistency).

    Returns:
        dict: A dictionary where keys are column names and values are dictionaries
              containing 'mean', 'std', 'count', 'min', 'max', 'median' for that column.
              Returns None if the input data is empty or no numeric columns are found.
    """
    import pandas as pd
    import numpy as np

    try:
        if not isinstance(data, pd.DataFrame) or data.empty:
            return None

        results = {}
        
        if columns_to_analyze is None:
            # Identify relevant numeric columns if not specified
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            target_cols = [col for col in numeric_cols if col.endswith(('_score', '_salience', '_confidence'))]
        else:
            # Filter for numeric columns from the specified list
            target_cols = [col for col in columns_to_analyze if col in data.columns and pd.api.types.is_numeric_dtype(data[col])]

        if not target_cols:
            return None # No relevant numeric columns to analyze

        for col in target_cols:
            series = data[col].dropna() # Drop NaNs for calculations
            if not series.empty:
                results[col] = {
                    'mean': float(series.mean()),
                    'std': float(series.std()),
                    'count': int(series.count()),
                    'min': float(series.min()),
                    'max': float(series.max()),
                    'median': float(series.median())
                }
        
        return results if results else None

    except Exception as e:
        print(f"Error in calculate_descriptive_statistics: {e}")
        return None

def add_derived_metrics_to_dataframe(data, **kwargs):
    """
    Calculates and adds derived metrics as new columns to the input DataFrame.

    Derived Metrics:
    - Identity Tension: tribal_dominance_score - individual_dignity_score
    - Emotional Balance: hope_score - fear_score
    - Success Climate: compersion_score - envy_score
    - Relational Climate: amity_score - enmity_score
    - Goal Orientation: cohesive_goals_score - fragmentative_goals_score

    Statistical Methodology:
    These metrics are calculated as simple differences between pairs of
    dimensional scores, providing a continuous measure of balance or tension
    between opposing psychological constructs. The range for these metrics is -1.0 to +1.0.

    Args:
        data (pd.DataFrame): The input pandas DataFrame containing the base dimensional scores.
                             Expected columns: 'tribal_dominance_score', 'individual_dignity_score',
                             'hope_score', 'fear_score', 'compersion_score', 'envy_score',
                             'amity_score', 'enmity_score', 'cohesive_goals_score',
                             'fragmentative_goals_score'.
        **kwargs: Additional parameters (not used in this function but for API consistency).

    Returns:
        pd.DataFrame: The DataFrame with new columns for derived metrics.
                      Returns None if essential columns are missing or data is empty.
    """
    import pandas as pd
    import numpy as np

    try:
        if not isinstance(data, pd.DataFrame) or data.empty:
            return None

        df = data.copy()
        required_scores = [
            'tribal_dominance_score', 'individual_dignity_score',
            'hope_score', 'fear_score',
            'compersion_score', 'envy_score',
            'amity_score', 'enmity_score',
            'cohesive_goals_score', 'fragmentative_goals_score'
        ]

        # Check if all required columns exist
        if not all(col in df.columns for col in required_scores):
            missing_cols = [col for col in required_scores if col not in df.columns]
            print(f"Error: Missing required score columns for derived metrics: {missing_cols}")
            return None

        # Ensure columns are numeric, coercing errors to NaN
        for col in required_scores:
            df[col] = pd.to_numeric(df[col], errors='coerce')

        df['identity_tension_derived'] = df['tribal_dominance_score'] - df['individual_dignity_score']
        df['emotional_balance_derived'] = df['hope_score'] - df['fear_score']
        df['success_climate_derived'] = df['compersion_score'] - df['envy_score']
        df['relational_climate_derived'] = df['amity_score'] - df['enmity_score']
        df['goal_orientation_derived'] = df['cohesive_goals_score'] - df['fragmentative_goals_score']

        return df

    except Exception as e:
        print(f"Error in add_derived_metrics_to_dataframe: {e}")
        return None

def add_composite_indices_to_dataframe(data, **kwargs):
    """
    Calculates and adds the Overall Cohesion Index as a new column to the input DataFrame.

    Composite Index:
    - Overall Cohesion Index: (individual_dignity + hope + compersion + amity + cohesive_goals) / 5
                              - (tribal_dominance + fear + envy + enmity + fragmentative_goals) / 5

    Statistical Methodology:
    This index provides a comprehensive measure of discourse cohesiveness by
    averaging the scores of cohesive dimensions and subtracting the average of
    fragmentative dimensions. The result is normalized to a range of -1.0 to +1.0,
    where +1.0 indicates maximal cohesiveness and -1.0 indicates maximal fragmentation.

    Args:
        data (pd.DataFrame): The input pandas DataFrame containing the base dimensional scores.
                             Expected columns: 'individual_dignity_score', 'hope_score',
                             'compersion_score', 'amity_score', 'cohesive_goals_score',
                             'tribal_dominance_score', 'fear_score', 'envy_score',
                             'enmity_score', 'fragmentative_goals_score'.
        **kwargs: Additional parameters (not used in this function but for API consistency).

    Returns:
        pd.DataFrame: The DataFrame with the new 'overall_cohesion_index' column.
                      Returns None if essential columns are missing or data is empty.
    """
    import pandas as pd
    import numpy as np

    try:
        if not isinstance(data, pd.DataFrame) or data.empty:
            return None

        df = data.copy()
        cohesive_dims = [
            'individual_dignity_score', 'hope_score', 'compersion_score',
            'amity_score', 'cohesive_goals_score'
        ]
        fragmentative_dims = [
            'tribal_dominance_score', 'fear_score', 'envy_score',
            'enmity_score', 'fragmentative_goals_score'
        ]
        required_scores = cohesive_dims + fragmentative_dims

        # Check if all required columns exist
        if not all(col in df.columns for col in required_scores):
            missing_cols = [col for col in required_scores if col not in df.columns]
            print(f"Error: Missing required score columns for composite index: {missing_cols}")
            return None

        # Ensure columns are numeric, coercing errors to NaN
        for col in required_scores:
            df[col] = pd.to_numeric(df[col], errors='coerce')

        # Calculate the average of cohesive and fragmentative scores, handling NaNs
        # Use .mean(axis=1) to calculate row-wise mean, skipping NaNs
        df['cohesive_avg'] = df[cohesive_dims].mean(axis=1)
        df['fragmentative_avg'] = df[fragmentative_dims].mean(axis=1)

        df['overall_cohesion_index'] = df['cohesive_avg'] - df['fragmentative_avg']

        # Drop intermediate columns
        df = df.drop(columns=['cohesive_avg', 'fragmentative_avg'])

        return df

    except Exception as e:
        print(f"Error in add_composite_indices_to_dataframe: {e}")
        return None

def run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:
    """
    Run complete statistical analysis suite on the dataset.
    
    Args:
        data: pandas DataFrame with dimension scores
        alpha: Significance level for hypothesis tests (default: 0.05)
        
    Returns:
        Dictionary with all statistical analysis results
    """
    results = {
        'analysis_metadata': {
            'timestamp': pd.Timestamp.now().isoformat(),
            'sample_size': len(data),
            'alpha_level': alpha,
            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)
        }
    }
    
    # Get all analysis functions from this module
    import inspect
    current_module = inspect.getmodule(inspect.currentframe())
    
    for name, obj in inspect.getmembers(current_module):
        if (inspect.isfunction(obj) and 
            name.startswith(('calculate_', 'perform_', 'test_')) and 
            name != 'run_complete_statistical_analysis'):
            try:
                # Pass alpha parameter to functions that might need it
                if 'alpha' in inspect.signature(obj).parameters:
                    results[name] = obj(data, alpha=alpha)
                else:
                    results[name] = obj(data)
            except Exception as e:
                results[name] = {'error': f'Analysis failed: {str(e)}'}
                
    return results


def generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:
    """
    Generate a human-readable summary report from statistical analysis results.
    
    Args:
        analysis_results: Results from run_complete_statistical_analysis()
        
    Returns:
        String containing formatted statistical report
    """
    report_lines = []
    report_lines.append("STATISTICAL ANALYSIS SUMMARY REPORT")
    report_lines.append("=" * 50)
    
    metadata = analysis_results.get('analysis_metadata', {})
    report_lines.append(f"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}")
    report_lines.append(f"Sample Size: {metadata.get('sample_size', 'Unknown')}")
    report_lines.append(f"Alpha Level: {metadata.get('alpha_level', 'Unknown')}")
    report_lines.append(f"Variables: {len(metadata.get('variables_analyzed', []))}")
    report_lines.append("")
    
    # Summarize key findings
    for analysis_name, result in analysis_results.items():
        if analysis_name != 'analysis_metadata' and isinstance(result, dict):
            if 'error' not in result:
                report_lines.append(f"{analysis_name.replace('_', ' ').title()}:")
                
                # Extract key statistics based on analysis type
                if 'p_value' in result:
                    p_val = result['p_value']
                    significance = "significant" if p_val < metadata.get('alpha_level', 0.05) else "not significant"
                    report_lines.append(f"  - p-value: {p_val:.4f} ({significance})")
                
                if 'effect_size' in result:
                    report_lines.append(f"  - Effect size: {result['effect_size']:.4f}")
                
                if 'correlation_matrix' in result:
                    report_lines.append(f"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables")
                
                if 'cronbach_alpha' in result:
                    alpha_val = result['cronbach_alpha']
                    reliability = "excellent" if alpha_val > 0.9 else "good" if alpha_val > 0.8 else "acceptable" if alpha_val > 0.7 else "questionable"
                    report_lines.append(f"  - Cronbach's α: {alpha_val:.3f} ({reliability})")
                
                report_lines.append("")
            else:
                report_lines.append(f"{analysis_name}: ERROR - {result['error']}")
                report_lines.append("")
    
    return "\n".join(report_lines)
