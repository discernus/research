test_name,test_type,statistic_name,statistic_value,p_value,effect_size,degrees_of_freedom,sample_size,dependent_variable,grouping_variable,significance_level,interpretation,notes
calculate_all_derived_metrics,derived_metrics_calculation,result_value,"{'type': 'derived_metrics_calculation', 'success': True, 'calculated_metrics': {'hope_fear_tension': [0.6499999999999999, 0.8], 'dignity_tribalism_tension': [0.55, 0.8], 'truth_manipulation_tension': [0.55, 0.75], 'justice_resentment_tension': [0.55, 0.65], 'pragmatism_fantasy_tension': [0.7, 0.7], 'virtue_index': [0.6599999999999999, 0.6], 'pathology_index': [0.45999999999999996, 0.12], 'civic_character_index': [0.6, 0.74], 'salience_weighted_civic_character_index': [0.5972972972972973, 0.7551724137931034]}, 'successful_calculations': ['hope_fear_tension', 'dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'pragmatism_fantasy_tension', 'virtue_index', 'pathology_index', 'civic_character_index', 'salience_weighted_civic_character_index'], 'failed_calculations': [], 'formulas_used': ['dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'hope_fear_tension', 'pragmatism_fantasy_tension', 'civic_character_index', 'salience_weighted_civic_character_index', 'virtue_index', 'pathology_index'], 'input_columns': ['dignity_score', 'tribalism_score', 'truth_score', 'manipulation_score', 'justice_score', 'resentment_score', 'hope_score', 'fear_score', 'pragmatism_score', 'fantasy_score', 'dignity_salience', 'truth_salience', 'justice_salience', 'hope_salience', 'pragmatism_salience'], 'total_metrics': 9, 'success_rate': 1.0}",,,,,,,,Generic derived_metrics_calculation result,
descriptive_stats_raw_scores,descriptive_stats,result_value,"{'type': 'descriptive_stats', 'columns_analyzed': ['dignity_score', 'tribalism_score', 'truth_score', 'manipulation_score', 'justice_score', 'resentment_score', 'hope_score', 'fear_score', 'pragmatism_score', 'fantasy_score', 'dignity_salience', 'tribalism_salience', 'dignity_confidence', 'tribalism_confidence', 'truth_salience', 'manipulation_salience', 'truth_confidence', 'manipulation_confidence', 'justice_salience', 'resentment_salience', 'justice_confidence', 'resentment_confidence', 'hope_salience', 'fear_salience', 'hope_confidence', 'fear_confidence', 'pragmatism_salience', 'fantasy_salience', 'pragmatism_confidence', 'fantasy_confidence'], 'results': {'dignity_score': {'count': 2, 'mean': 0.75, 'std': 0.07071067811865482, 'min': 0.7, 'max': 0.8, 'median': 0.75, 'q25': 0.725, 'q75': 0.775, 'skewness': nan, 'kurtosis': nan}, 'tribalism_score': {'count': 2, 'mean': 0.4, 'std': 0.282842712474619, 'min': 0.2, 'max': 0.6, 'median': 0.4, 'q25': 0.3, 'q75': 0.5, 'skewness': nan, 'kurtosis': nan}, 'truth_score': {'count': 2, 'mean': 0.55, 'std': 0.07071067811865474, 'min': 0.5, 'max': 0.6, 'median': 0.55, 'q25': 0.525, 'q75': 0.575, 'skewness': nan, 'kurtosis': nan}, 'manipulation_score': {'count': 2, 'mean': 0.25, 'std': 0.21213203435596428, 'min': 0.1, 'max': 0.4, 'median': 0.25, 'q25': 0.17500000000000002, 'q75': 0.325, 'skewness': nan, 'kurtosis': nan}, 'justice_score': {'count': 2, 'mean': 0.6000000000000001, 'std': 0.282842712474619, 'min': 0.4, 'max': 0.8, 'median': 0.6000000000000001, 'q25': 0.5, 'q75': 0.7000000000000001, 'skewness': nan, 'kurtosis': nan}, 'resentment_score': {'count': 2, 'mean': 0.39999999999999997, 'std': 0.42426406871192845, 'min': 0.1, 'max': 0.7, 'median': 0.39999999999999997, 'q25': 0.25, 'q75': 0.5499999999999999, 'skewness': nan, 'kurtosis': nan}, 'hope_score': {'count': 2, 'mean': 0.6499999999999999, 'std': 0.07071067811865474, 'min': 0.6, 'max': 0.7, 'median': 0.6499999999999999, 'q25': 0.625, 'q75': 0.6749999999999999, 'skewness': nan, 'kurtosis': nan}, 'fear_score': {'count': 2, 'mean': 0.2, 'std': 0.1414213562373095, 'min': 0.1, 'max': 0.3, 'median': 0.2, 'q25': 0.15, 'q75': 0.25, 'skewness': nan, 'kurtosis': nan}, 'pragmatism_score': {'count': 2, 'mean': 0.6, 'std': 0.14142135623730948, 'min': 0.5, 'max': 0.7, 'median': 0.6, 'q25': 0.55, 'q75': 0.6499999999999999, 'skewness': nan, 'kurtosis': nan}, 'fantasy_score': {'count': 2, 'mean': 0.2, 'std': 0.1414213562373095, 'min': 0.1, 'max': 0.3, 'median': 0.2, 'q25': 0.15, 'q75': 0.25, 'skewness': nan, 'kurtosis': nan}, 'dignity_salience': {'count': 2, 'mean': 0.75, 'std': 0.07071067811865482, 'min': 0.7, 'max': 0.8, 'median': 0.75, 'q25': 0.725, 'q75': 0.775, 'skewness': nan, 'kurtosis': nan}, 'tribalism_salience': {'count': 2, 'mean': 0.5, 'std': 0.282842712474619, 'min': 0.3, 'max': 0.7, 'median': 0.5, 'q25': 0.39999999999999997, 'q75': 0.6, 'skewness': nan, 'kurtosis': nan}, 'dignity_confidence': {'count': 2, 'mean': 0.9, 'std': 0.0, 'min': 0.9, 'max': 0.9, 'median': 0.9, 'q25': 0.9, 'q75': 0.9, 'skewness': nan, 'kurtosis': nan}, 'tribalism_confidence': {'count': 2, 'mean': 0.75, 'std': 0.07071067811865482, 'min': 0.7, 'max': 0.8, 'median': 0.75, 'q25': 0.725, 'q75': 0.775, 'skewness': nan, 'kurtosis': nan}, 'truth_salience': {'count': 2, 'mean': 0.55, 'std': 0.07071067811865474, 'min': 0.5, 'max': 0.6, 'median': 0.55, 'q25': 0.525, 'q75': 0.575, 'skewness': nan, 'kurtosis': nan}, 'manipulation_salience': {'count': 2, 'mean': 0.35, 'std': 0.21213203435596426, 'min': 0.2, 'max': 0.5, 'median': 0.35, 'q25': 0.275, 'q75': 0.425, 'skewness': nan, 'kurtosis': nan}, 'truth_confidence': {'count': 2, 'mean': 0.75, 'std': 0.07071067811865482, 'min': 0.7, 'max': 0.8, 'median': 0.75, 'q25': 0.725, 'q75': 0.775, 'skewness': nan, 'kurtosis': nan}, 'manipulation_confidence': {'count': 2, 'mean': 0.6499999999999999, 'std': 0.07071067811865474, 'min': 0.6, 'max': 0.7, 'median': 0.6499999999999999, 'q25': 0.625, 'q75': 0.6749999999999999, 'skewness': nan, 'kurtosis': nan}, 'justice_salience': {'count': 2, 'mean': 0.6, 'std': 0.42426406871192857, 'min': 0.3, 'max': 0.9, 'median': 0.6, 'q25': 0.45, 'q75': 0.75, 'skewness': nan, 'kurtosis': nan}, 'resentment_salience': {'count': 2, 'mean': 0.5, 'std': 0.42426406871192857, 'min': 0.2, 'max': 0.8, 'median': 0.5, 'q25': 0.35000000000000003, 'q75': 0.65, 'skewness': nan, 'kurtosis': nan}, 'justice_confidence': {'count': 2, 'mean': 0.7749999999999999, 'std': 0.24748737341529162, 'min': 0.6, 'max': 0.95, 'median': 0.7749999999999999, 'q25': 0.6875, 'q75': 0.8624999999999999, 'skewness': nan, 'kurtosis': nan}, 'resentment_confidence': {'count': 2, 'mean': 0.725, 'std': 0.1767766952966369, 'min': 0.6, 'max': 0.85, 'median': 0.725, 'q25': 0.6625, 'q75': 0.7875, 'skewness': nan, 'kurtosis': nan}, 'hope_salience': {'count': 2, 'mean': 0.75, 'std': 0.07071067811865482, 'min': 0.7, 'max': 0.8, 'median': 0.75, 'q25': 0.725, 'q75': 0.775, 'skewness': nan, 'kurtosis': nan}, 'fear_salience': {'count': 2, 'mean': 0.25, 'std': 0.21213203435596428, 'min': 0.1, 'max': 0.4, 'median': 0.25, 'q25': 0.17500000000000002, 'q75': 0.325, 'skewness': nan, 'kurtosis': nan}, 'hope_confidence': {'count': 2, 'mean': 0.8500000000000001, 'std': 0.07071067811865474, 'min': 0.8, 'max': 0.9, 'median': 0.8500000000000001, 'q25': 0.8250000000000001, 'q75': 0.875, 'skewness': nan, 'kurtosis': nan}, 'fear_confidence': {'count': 2, 'mean': 0.55, 'std': 0.07071067811865474, 'min': 0.5, 'max': 0.6, 'median': 0.55, 'q25': 0.525, 'q75': 0.575, 'skewness': nan, 'kurtosis': nan}, 'pragmatism_salience': {'count': 2, 'mean': 0.6499999999999999, 'std': 0.07071067811865474, 'min': 0.6, 'max': 0.7, 'median': 0.6499999999999999, 'q25': 0.625, 'q75': 0.6749999999999999, 'skewness': nan, 'kurtosis': nan}, 'fantasy_salience': {'count': 2, 'mean': 0.25, 'std': 0.21213203435596428, 'min': 0.1, 'max': 0.4, 'median': 0.25, 'q25': 0.17500000000000002, 'q75': 0.325, 'skewness': nan, 'kurtosis': nan}, 'pragmatism_confidence': {'count': 2, 'mean': 0.8, 'std': 0.0, 'min': 0.8, 'max': 0.8, 'median': 0.8, 'q25': 0.8, 'q75': 0.8, 'skewness': nan, 'kurtosis': nan}, 'fantasy_confidence': {'count': 2, 'mean': 0.55, 'std': 0.07071067811865474, 'min': 0.5, 'max': 0.6, 'median': 0.55, 'q25': 0.525, 'q75': 0.575, 'skewness': nan, 'kurtosis': nan}}}",,,,,,,,Generic descriptive_stats result,
descriptive_stats_derived_metrics,descriptive_stats,result_value,"{'type': 'descriptive_stats', 'columns_analyzed': ['dignity_tribalism_tension', 'truth_manipulation_tension', 'justice_resentment_tension', 'hope_fear_tension', 'pragmatism_fantasy_tension', 'civic_character_index', 'salience_weighted_civic_character_index', 'virtue_index', 'pathology_index'], 'results': {'dignity_tribalism_tension': {'count': 2, 'mean': 0.675, 'std': 0.1767766952966369, 'min': 0.55, 'max': 0.8, 'median': 0.675, 'q25': 0.6125, 'q75': 0.7375, 'skewness': nan, 'kurtosis': nan}, 'truth_manipulation_tension': {'count': 2, 'mean': 0.65, 'std': 0.14142135623730948, 'min': 0.55, 'max': 0.75, 'median': 0.65, 'q25': 0.6000000000000001, 'q75': 0.7, 'skewness': nan, 'kurtosis': nan}, 'justice_resentment_tension': {'count': 2, 'mean': 0.6000000000000001, 'std': 0.07071067811865474, 'min': 0.55, 'max': 0.65, 'median': 0.6000000000000001, 'q25': 0.5750000000000001, 'q75': 0.625, 'skewness': nan, 'kurtosis': nan}, 'hope_fear_tension': {'count': 2, 'mean': 0.725, 'std': 0.10606601717798222, 'min': 0.6499999999999999, 'max': 0.8, 'median': 0.725, 'q25': 0.6875, 'q75': 0.7625, 'skewness': nan, 'kurtosis': nan}, 'pragmatism_fantasy_tension': {'count': 2, 'mean': 0.7, 'std': 0.0, 'min': 0.7, 'max': 0.7, 'median': 0.7, 'q25': 0.7, 'q75': 0.7, 'skewness': nan, 'kurtosis': nan}, 'civic_character_index': {'count': 2, 'mean': 0.6699999999999999, 'std': 0.09899494936611666, 'min': 0.6, 'max': 0.74, 'median': 0.6699999999999999, 'q25': 0.635, 'q75': 0.705, 'skewness': nan, 'kurtosis': nan}, 'salience_weighted_civic_character_index': {'count': 2, 'mean': 0.6762348555452004, 'std': 0.11163456545480066, 'min': 0.5972972972972973, 'max': 0.7551724137931034, 'median': 0.6762348555452004, 'q25': 0.6367660764212488, 'q75': 0.7157036346691519, 'skewness': nan, 'kurtosis': nan}, 'virtue_index': {'count': 2, 'mean': 0.6299999999999999, 'std': 0.04242640687119281, 'min': 0.6, 'max': 0.6599999999999999, 'median': 0.6299999999999999, 'q25': 0.615, 'q75': 0.6449999999999999, 'skewness': nan, 'kurtosis': nan}, 'pathology_index': {'count': 2, 'mean': 0.29, 'std': 0.24041630560342614, 'min': 0.12, 'max': 0.45999999999999996, 'median': 0.29, 'q25': 0.205, 'q75': 0.375, 'skewness': nan, 'kurtosis': nan}}}",,,,,,,,Generic descriptive_stats result,
validate_calculated_metrics_quality,metric_validation,result_value,"{'type': 'metric_validation', 'validation_rules': ['missing_data_check', 'range_check', 'consistency_check'], 'results': {'missing_data_check': {'status': 'completed', 'missing_data_by_column': {'aid': 0, 'document_type': 0, 'political_party': 0, 'year': 0, 'temporal_sequence': 0, 'speaker': 0, 'ideology': 0, 'character_profile': 0, 'event': 0, 'word_count': 0, 'source': 0, 'preparation_notes': 0, 'dignity_score': 0, 'tribalism_score': 0, 'dignity_salience': 0, 'tribalism_salience': 0, 'dignity_confidence': 0, 'tribalism_confidence': 0, 'truth_score': 0, 'manipulation_score': 0, 'truth_salience': 0, 'manipulation_salience': 0, 'truth_confidence': 0, 'manipulation_confidence': 0, 'justice_score': 0, 'resentment_score': 0, 'justice_salience': 0, 'resentment_salience': 0, 'justice_confidence': 0, 'resentment_confidence': 0, 'hope_score': 0, 'fear_score': 0, 'hope_salience': 0, 'fear_salience': 0, 'hope_confidence': 0, 'fear_confidence': 0, 'pragmatism_score': 0, 'fantasy_score': 0, 'pragmatism_salience': 0, 'fantasy_salience': 0, 'pragmatism_confidence': 0, 'fantasy_confidence': 0, 'gasket_version': 0, 'extraction_time_seconds': 0, 'hope_fear_tension': 0, 'dignity_tribalism_tension': 0, 'truth_manipulation_tension': 0, 'justice_resentment_tension': 0, 'pragmatism_fantasy_tension': 0, 'virtue_index': 0, 'pathology_index': 0, 'civic_character_index': 0, 'salience_weighted_civic_character_index': 0}, 'total_missing': 0}, 'range_check': {'status': 'completed', 'ranges': {'year': {'min': 2008.0, 'max': 2025.0, 'mean': 2016.5}, 'temporal_sequence': {'min': 1.0, 'max': 2.0, 'mean': 1.5}, 'word_count': {'min': 892.0, 'max': 1247.0, 'mean': 1069.5}, 'dignity_score': {'min': 0.7, 'max': 0.8, 'mean': 0.75}, 'tribalism_score': {'min': 0.2, 'max': 0.6, 'mean': 0.4}, 'dignity_salience': {'min': 0.7, 'max': 0.8, 'mean': 0.75}, 'tribalism_salience': {'min': 0.3, 'max': 0.7, 'mean': 0.5}, 'dignity_confidence': {'min': 0.9, 'max': 0.9, 'mean': 0.9}, 'tribalism_confidence': {'min': 0.7, 'max': 0.8, 'mean': 0.75}, 'truth_score': {'min': 0.5, 'max': 0.6, 'mean': 0.55}, 'manipulation_score': {'min': 0.1, 'max': 0.4, 'mean': 0.25}, 'truth_salience': {'min': 0.5, 'max': 0.6, 'mean': 0.55}, 'manipulation_salience': {'min': 0.2, 'max': 0.5, 'mean': 0.35}, 'truth_confidence': {'min': 0.7, 'max': 0.8, 'mean': 0.75}, 'manipulation_confidence': {'min': 0.6, 'max': 0.7, 'mean': 0.6499999999999999}, 'justice_score': {'min': 0.4, 'max': 0.8, 'mean': 0.6000000000000001}, 'resentment_score': {'min': 0.1, 'max': 0.7, 'mean': 0.39999999999999997}, 'justice_salience': {'min': 0.3, 'max': 0.9, 'mean': 0.6}, 'resentment_salience': {'min': 0.2, 'max': 0.8, 'mean': 0.5}, 'justice_confidence': {'min': 0.6, 'max': 0.95, 'mean': 0.7749999999999999}, 'resentment_confidence': {'min': 0.6, 'max': 0.85, 'mean': 0.725}, 'hope_score': {'min': 0.6, 'max': 0.7, 'mean': 0.6499999999999999}, 'fear_score': {'min': 0.1, 'max': 0.3, 'mean': 0.2}, 'hope_salience': {'min': 0.7, 'max': 0.8, 'mean': 0.75}, 'fear_salience': {'min': 0.1, 'max': 0.4, 'mean': 0.25}, 'hope_confidence': {'min': 0.8, 'max': 0.9, 'mean': 0.8500000000000001}, 'fear_confidence': {'min': 0.5, 'max': 0.6, 'mean': 0.55}, 'pragmatism_score': {'min': 0.5, 'max': 0.7, 'mean': 0.6}, 'fantasy_score': {'min': 0.1, 'max': 0.3, 'mean': 0.2}, 'pragmatism_salience': {'min': 0.6, 'max': 0.7, 'mean': 0.6499999999999999}, 'fantasy_salience': {'min': 0.1, 'max': 0.4, 'mean': 0.25}, 'pragmatism_confidence': {'min': 0.8, 'max': 0.8, 'mean': 0.8}, 'fantasy_confidence': {'min': 0.5, 'max': 0.6, 'mean': 0.55}, 'extraction_time_seconds': {'min': 1.2810049057006836, 'max': 1.3471081256866455, 'mean': 1.3140565156936646}, 'hope_fear_tension': {'min': 0.6499999999999999, 'max': 0.8, 'mean': 0.725}, 'dignity_tribalism_tension': {'min': 0.55, 'max': 0.8, 'mean': 0.675}, 'truth_manipulation_tension': {'min': 0.55, 'max': 0.75, 'mean': 0.65}, 'justice_resentment_tension': {'min': 0.55, 'max': 0.65, 'mean': 0.6000000000000001}, 'pragmatism_fantasy_tension': {'min': 0.7, 'max': 0.7, 'mean': 0.7}, 'virtue_index': {'min': 0.6, 'max': 0.6599999999999999, 'mean': 0.6299999999999999}, 'pathology_index': {'min': 0.12, 'max': 0.45999999999999996, 'mean': 0.29}, 'civic_character_index': {'min': 0.6, 'max': 0.74, 'mean': 0.6699999999999999}, 'salience_weighted_civic_character_index': {'min': 0.5972972972972973, 'max': 0.7551724137931034, 'mean': 0.6762348555452004}}}, 'consistency_check': {'status': 'completed', 'notes': 'Basic consistency check completed'}}, 'quality_thresholds': {'missing_data_tolerance': 0.0, 'range_min': 0.0, 'range_max': 1.0}}",,,,,,,,Generic metric_validation result,
