"""
Automated Statistical Analysis Functions
========================================

Generated by AutomatedStatisticalAnalysisAgent for experiment: simple_test
Description: No description
Generated: 2025-08-15T04:06:34.832681+00:00

This module contains automatically generated statistical analysis functions
for comprehensive data analysis including ANOVA, correlations, reliability,
and hypothesis testing as appropriate for the research questions.
"""

import pandas as pd
import numpy as np
import scipy.stats as stats
from typing import Dict, Any, Optional, List, Tuple
import warnings

# Suppress common statistical warnings for cleaner output
warnings.filterwarnings('ignore', category=RuntimeWarning)


def calculate_descriptive_statistics(data, **kwargs):
    """
    Calculate descriptive statistics (mean, standard deviation, count) for all
    dimensional scores, salience, and confidence values present in the DataFrame.

    Args:
        data (pd.DataFrame): A pandas DataFrame expected to contain columns
                             like 'dimension_score', 'dimension_salience',
                             'dimension_confidence'.
        **kwargs: Additional parameters (not used in this function).

    Returns:
        dict: A dictionary where keys are the column names and values are
              dictionaries containing 'mean', 'std', and 'count' for that column.
              Returns None if the input DataFrame is empty or an error occurs.
    """
    import pandas as pd
    import numpy as np

    try:
        if not isinstance(data, pd.DataFrame) or data.empty:
            return None

        results = {}
        # Identify columns related to scores, salience, and confidence
        relevant_columns = [
            col for col in data.columns
            if col.endswith('_score') or col.endswith('_salience') or col.endswith('_confidence')
        ]

        if not relevant_columns:
            return None # No relevant columns found

        for col in relevant_columns:
            if col in data.columns and pd.api.types.is_numeric_dtype(data[col]):
                series = data[col].dropna() # Drop NaNs for calculation
                if not series.empty:
                    results[col] = {
                        'mean': float(series.mean()),
                        'std': float(series.std()),
                        'count': int(series.count())
                    }
        return results

    except Exception as e:
        print(f"Error in calculate_descriptive_statistics: {e}")
        return None

def add_derived_metrics(data, **kwargs):
    """
    Calculates and adds derived metrics to the input DataFrame.
    These metrics quantify the balance between opposing dimensions.

    Derived Metrics:
    - Identity Tension: tribal_dominance_score - individual_dignity_score
    - Emotional Balance: hope_score - fear_score
    - Success Climate: compersion_score - envy_score
    - Relational Climate: amity_score - enmity_score
    - Goal Orientation: cohesive_goals_score - fragmentative_goals_score

    Args:
        data (pd.DataFrame): A pandas DataFrame containing the raw dimensional scores.
        **kwargs: Additional parameters (not used in this function).

    Returns:
        pd.DataFrame: The DataFrame with new columns for derived metrics.
                      Returns None if the input DataFrame is invalid or an error occurs.
    """
    import pandas as pd
    import numpy as np

    try:
        if not isinstance(data, pd.DataFrame) or data.empty:
            return None

        df_copy = data.copy()

        # Define required columns for each derived metric
        metric_definitions = {
            'identity_tension_metric': ('tribal_dominance_score', 'individual_dignity_score'),
            'emotional_balance_metric': ('hope_score', 'fear_score'),
            'success_climate_metric': ('compersion_score', 'envy_score'),
            'relational_climate_metric': ('amity_score', 'enmity_score'),
            'goal_orientation_metric': ('cohesive_goals_score', 'fragmentative_goals_score')
        }

        for metric_name, (pos_dim, neg_dim) in metric_definitions.items():
            if pos_dim in df_copy.columns and neg_dim in df_copy.columns:
                # Ensure columns are numeric, coerce errors to NaN
                df_copy[pos_dim] = pd.to_numeric(df_copy[pos_dim], errors='coerce')
                df_copy[neg_dim] = pd.to_numeric(df_copy[neg_dim], errors='coerce')
                df_copy[metric_name] = df_copy[pos_dim] - df_copy[neg_dim]
            else:
                # If required columns are missing, fill with NaN
                df_copy[metric_name] = np.nan
                print(f"Warning: Missing columns for {metric_name} ({pos_dim}, {neg_dim}). Column filled with NaN.")

        return df_copy

    except Exception as e:
        print(f"Error in add_derived_metrics: {e}")
        return None

def add_composite_indices(data, **kwargs):
    """
    Calculates and adds the Overall Cohesion Index to the input DataFrame.
    This index provides a comprehensive measure of discourse cohesiveness.

    Formula:
    (individual_dignity + hope + compersion + amity + cohesive_goals) / 5 -
    (tribal_dominance + fear + envy + enmity + fragmentative_goals) / 5

    Args:
        data (pd.DataFrame): A pandas DataFrame containing the raw dimensional scores.
        **kwargs: Additional parameters (not used in this function).

    Returns:
        pd.DataFrame: The DataFrame with a new column for 'overall_cohesion_index'.
                      Returns None if the input DataFrame is invalid or an error occurs.
    """
    import pandas as pd
    import numpy as np

    try:
        if not isinstance(data, pd.DataFrame) or data.empty:
            return None

        df_copy = data.copy()

        cohesive_dims = [
            'individual_dignity_score', 'hope_score', 'compersion_score',
            'amity_score', 'cohesive_goals_score'
        ]
        fragmentative_dims = [
            'tribal_dominance_score', 'fear_score', 'envy_score',
            'enmity_score', 'fragmentative_goals_score'
        ]

        # Ensure all required columns exist and are numeric
        all_dims = cohesive_dims + fragmentative_dims
        for dim in all_dims:
            if dim not in df_copy.columns:
                df_copy[dim] = np.nan # Add missing columns as NaN
                print(f"Warning: Missing dimension '{dim}'. Will be treated as NaN in calculation.")
            else:
                df_copy[dim] = pd.to_numeric(df_copy[dim], errors='coerce')

        # Calculate cohesive and fragmentative sums, handling NaNs
        # .sum(axis=1, skipna=True) will sum non-NaN values. If all are NaN, sum is 0.
        # To ensure division by 5 is correct, we need to count non-NaN values.
        cohesive_sum = df_copy[cohesive_dims].sum(axis=1, skipna=True)
        cohesive_count = df_copy[cohesive_dims].count(axis=1)
        cohesive_avg = cohesive_sum / cohesive_count.replace(0, np.nan) # Avoid division by zero if all are NaN

        fragmentative_sum = df_copy[fragmentative_dims].sum(axis=1, skipna=True)
        fragmentative_count = df_copy[fragmentative_dims].count(axis=1)
        fragmentative_avg = fragmentative_sum / fragmentative_count.replace(0, np.nan) # Avoid division by zero

        df_copy['overall_cohesion_index'] = cohesive_avg - fragmentative_avg

        return df_copy

    except Exception as e:
        print(f"Error in add_composite_indices: {e}")
        return None

def run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:
    """
    Run complete statistical analysis suite on the dataset.
    
    Args:
        data: pandas DataFrame with dimension scores
        alpha: Significance level for hypothesis tests (default: 0.05)
        
    Returns:
        Dictionary with all statistical analysis results
    """
    results = {
        'analysis_metadata': {
            'timestamp': pd.Timestamp.now().isoformat(),
            'sample_size': len(data),
            'alpha_level': alpha,
            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)
        }
    }
    
    # Get all analysis functions from this module
    import inspect
    current_module = inspect.getmodule(inspect.currentframe())
    
    for name, obj in inspect.getmembers(current_module):
        if (inspect.isfunction(obj) and 
            name.startswith(('calculate_', 'perform_', 'test_')) and 
            name != 'run_complete_statistical_analysis'):
            try:
                # Pass alpha parameter to functions that might need it
                if 'alpha' in inspect.signature(obj).parameters:
                    results[name] = obj(data, alpha=alpha)
                else:
                    results[name] = obj(data)
            except Exception as e:
                results[name] = {'error': f'Analysis failed: {str(e)}'}
                
    return results


def generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:
    """
    Generate a human-readable summary report from statistical analysis results.
    
    Args:
        analysis_results: Results from run_complete_statistical_analysis()
        
    Returns:
        String containing formatted statistical report
    """
    report_lines = []
    report_lines.append("STATISTICAL ANALYSIS SUMMARY REPORT")
    report_lines.append("=" * 50)
    
    metadata = analysis_results.get('analysis_metadata', {})
    report_lines.append(f"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}")
    report_lines.append(f"Sample Size: {metadata.get('sample_size', 'Unknown')}")
    report_lines.append(f"Alpha Level: {metadata.get('alpha_level', 'Unknown')}")
    report_lines.append(f"Variables: {len(metadata.get('variables_analyzed', []))}")
    report_lines.append("")
    
    # Summarize key findings
    for analysis_name, result in analysis_results.items():
        if analysis_name != 'analysis_metadata' and isinstance(result, dict):
            if 'error' not in result:
                report_lines.append(f"{analysis_name.replace('_', ' ').title()}:")
                
                # Extract key statistics based on analysis type
                if 'p_value' in result:
                    p_val = result['p_value']
                    significance = "significant" if p_val < metadata.get('alpha_level', 0.05) else "not significant"
                    report_lines.append(f"  - p-value: {p_val:.4f} ({significance})")
                
                if 'effect_size' in result:
                    report_lines.append(f"  - Effect size: {result['effect_size']:.4f}")
                
                if 'correlation_matrix' in result:
                    report_lines.append(f"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables")
                
                if 'cronbach_alpha' in result:
                    alpha_val = result['cronbach_alpha']
                    reliability = "excellent" if alpha_val > 0.9 else "good" if alpha_val > 0.8 else "acceptable" if alpha_val > 0.7 else "questionable"
                    report_lines.append(f"  - Cronbach's Î±: {alpha_val:.3f} ({reliability})")
                
                report_lines.append("")
            else:
                report_lines.append(f"{analysis_name}: ERROR - {result['error']}")
                report_lines.append("")
    
    return "\n".join(report_lines)
