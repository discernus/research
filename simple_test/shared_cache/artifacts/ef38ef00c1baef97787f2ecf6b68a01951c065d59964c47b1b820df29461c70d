import pandas as pd
import numpy as np
import json
from scipy import stats

# This script assumes that 'scores_df' and 'evidence_df' are pre-loaded pandas DataFrames.
# It is designed to be framework-agnostic, operating on the provided data structure
# while applying analytical concepts inspired by the framework specification.

def calculate_cronbach_alpha(df_items):
    """
    Calculates Cronbach's alpha for a given set of items (columns in a DataFrame).
    Handles cases with fewer than two items or zero variance.
    """
    try:
        # Drop rows with any missing values for this calculation
        df_items = df_items.dropna()
        
        # Check if there are enough items and data to calculate alpha
        if df_items.shape[1] < 2 or df_items.shape[0] < 2:
            return np.nan

        # Number of items
        k = df_items.shape[1]
        
        # Sum of item variances
        sum_item_variances = df_items.var(axis=0, ddof=1).sum()
        
        # Variance of the total score (sum of items for each row)
        total_score_variance = df_items.sum(axis=1).var(ddof=1)
        
        # If total variance is zero, it means all items are constant.
        # In this case, reliability is perfect, but the formula would divide by zero.
        if total_score_variance == 0:
            # If item variances are also zero, perfect agreement.
            return 1.0 if sum_item_variances == 0 else 0.0

        # Cronbach's alpha formula
        alpha = (k / (k - 1)) * (1 - (sum_item_variances / total_score_variance))
        return alpha
    except Exception:
        return np.nan

def analyze_data(scores_df, evidence_df):
    """
    Main analysis function to perform statistical analysis based on the framework's principles.
    
    Args:
        scores_df (pd.DataFrame): DataFrame with scores data.
        evidence_df (pd.DataFrame): DataFrame with evidence data.
        
    Returns:
        str: A JSON string containing the structured analysis results.
    """
    results = {
        "sample_characteristics": {},
        "descriptive_stats": {},
        "reliability_metrics": {},
        "correlations": {},
        "hypothesis_tests": {},
        "effect_sizes": {}
    }

    # --- 1. Sample Characteristics ---
    # Provides metadata about the dataset being analyzed.
    try:
        results["sample_characteristics"] = {
            "num_analyses": int(scores_df.shape[0]),
            "num_evidence_snippets": int(evidence_df.shape[0]),
            "scores_columns": list(scores_df.columns),
            "evidence_columns": list(evidence_df.columns),
            "scores_missing_values": int(scores_df.isnull().sum().sum()),
            "evidence_missing_values": int(evidence_df.isnull().sum().sum())
        }
    except Exception as e:
        results["sample_characteristics"]["error"] = f"Failed to get sample characteristics: {str(e)}"

    # --- 2. Descriptive Statistics ---
    # Summarizes the central tendency, dispersion, and shape of the dataset's distribution.
    try:
        # Identify numeric columns for analysis, excluding the identifier 'aid'
        score_cols = [col for col in scores_df.columns if scores_df[col].dtype in ['int64', 'float64']]
        
        # Summary for score dimensions
        scores_summary = scores_df[score_cols].describe().to_dict()
        
        # Summary for evidence confidence
        evidence_confidence_summary = evidence_df['confidence'].describe().to_dict()
        
        # Statistical analysis of evidence text length, avoiding direct string manipulation
        evidence_quote_length_summary = evidence_df['quote'].str.len().describe().to_dict()
        
        results["descriptive_stats"] = {
            "scores_summary": scores_summary,
            "evidence_confidence_summary": evidence_confidence_summary,
            "evidence_quote_length_summary": evidence_quote_length_summary
        }
    except Exception as e:
        results["descriptive_stats"]["error"] = f"Failed to calculate descriptive statistics: {str(e)}"

    # --- 3. Reliability Metrics (Cronbach's Alpha) ---
    # Assesses the internal consistency of composite scores.
    try:
        # Define item groups based on the provided data structure's composite scores
        item_groups = {
            "content_quality_items": ['accuracy_verification', 'curriculum_alignment', 'depth_coverage', 'relevance_context'],
            "pedagogical_effectiveness_items": ['instructional_clarity', 'engagement_strategies', 'differentiation_support', 'assessment_integration'],
            "learning_accessibility_items": ['universal_design', 'language_clarity', 'multimedia_integration', 'barrier_reduction']
        }
        
        alpha_results = {}
        for group_name, items in item_groups.items():
            # Ensure all items for the group exist in the DataFrame
            if all(item in scores_df.columns for item in items):
                alpha_results[group_name] = calculate_cronbach_alpha(scores_df[items])
            else:
                alpha_results[group_name] = "Not all items present in data for calculation."
        
        results["reliability_metrics"] = {"cronbach_alpha": alpha_results}
    except Exception as e:
        results["reliability_metrics"]["error"] = f"Failed to calculate reliability metrics: {str(e)}"

    # --- 4. Correlation Analysis ---
    # Explores relationships between different dimensions.
    try:
        score_cols = [col for col in scores_df.columns if scores_df[col].dtype in ['int64', 'float64']]
        correlation_matrix = scores_df[score_cols].corr()
        
        # Highlight key correlations with the main outcome measure
        key_correlations = correlation_matrix['overall_educational_value'].drop('overall_educational_value').to_dict()

        results["correlations"] = {
            "correlation_matrix": correlation_matrix.to_dict(),
            "key_correlations_with_overall_value": key_correlations
        }
    except Exception as e:
        results["correlations"]["error"] = f"Failed to calculate correlations: {str(e)}"

    # --- 5. Hypothesis Testing & 6. Effect Sizes ---
    # Tests specific hypotheses derived from the data structure.
    try:
        hypothesis_results = {}
        effect_size_results = {}

        # Hypothesis 1: Is the mean 'overall_educational_value' significantly different from a neutral 0.5?
        if 'overall_educational_value' in scores_df.columns and len(scores_df['overall_educational_value'].dropna()) > 1:
            overall_scores = scores_df['overall_educational_value'].dropna()
            t_stat, p_val = stats.ttest_1samp(overall_scores, 0.5)
            hypothesis_results['one_sample_t_test_overall_value_vs_0.5'] = {"statistic": t_stat, "p_value": p_val}
            
            # Effect Size: Cohen's d for one sample
            mean_diff = overall_scores.mean() - 0.5
            std_dev = overall_scores.std(ddof=1)
            cohens_d = mean_diff / std_dev if std_dev > 0 else 0
            effect_size_results['cohens_d_overall_value_vs_0.5'] = cohens_d

        # Hypothesis 2: Is there a significant difference between 'content_quality_score' and 'pedagogical_effectiveness_score'?
        cols_exist = 'content_quality_score' in scores_df.columns and 'pedagogical_effectiveness_score' in scores_df.columns
        if cols_exist:
            paired_df = scores_df[['content_quality_score', 'pedagogical_effectiveness_score']].dropna()
            if len(paired_df) > 1:
                score1 = paired_df['content_quality_score']
                score2 = paired_df['pedagogical_effectiveness_score']
                t_stat_paired, p_val_paired = stats.ttest_rel(score1, score2)
                hypothesis_results['paired_t_test_quality_vs_pedagogy'] = {"statistic": t_stat_paired, "p_value": p_val_paired}

                # Effect Size: Cohen's d for paired samples
                diff = score1 - score2
                mean_diff_paired = diff.mean()
                std_diff_paired = diff.std(ddof=1)
                cohens_d_paired = mean_diff_paired / std_diff_paired if std_diff_paired > 0 else 0
                effect_size_results['cohens_d_quality_vs_pedagogy'] = cohens_d_paired

        results["hypothesis_tests"] = hypothesis_results
        results["effect_sizes"] = effect_size_results
    except Exception as e:
        results["hypothesis_tests"]["error"] = f"Failed during hypothesis testing: {str(e)}"
        results["effect_sizes"]["error"] = f"Failed during effect size calculation: {str(e)}"

    # Convert numpy types to native Python types for JSON serialization
    def convert_numpy(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, pd.Timestamp):
            return obj.isoformat()
        return obj

    # Return the final results as a JSON string
    return json.dumps(results, indent=2, default=convert_numpy)

# --- Main execution block ---
# This block will be executed by the system.
# It calls the main analysis function and prints the output.
try:
    # The 'scores_df' and 'evidence_df' are assumed to be in the global scope.
    # This check ensures the script doesn't fail if they are not defined.
    if 'scores_df' in locals() and 'evidence_df' in locals():
        final_json_output = analyze_data(scores_df, evidence_df)
        print(final_json_output)
    else:
        # Provide a structured error message if DataFrames are not found
        error_output = {
            "error": "Input DataFrames 'scores_df' and 'evidence_df' not found.",
            "message": "The script requires pre-loaded pandas DataFrames to run."
        }
        print(json.dumps(error_output, indent=2))

except Exception as e:
    # Catch any unexpected errors during execution and report them in a structured way.
    error_output = {
        "error": "An unexpected error occurred during script execution.",
        "error_type": type(e).__name__,
        "error_message": str(e)
    }
    print(json.dumps(error_output, indent=2))