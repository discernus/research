import pandas as pd
import numpy as np
import scipy.stats
import json

def run_analysis(scores_df: pd.DataFrame, evidence_df: pd.DataFrame):
    """
    Performs a complete statistical analysis based on the Constitutional Health Framework (CHF) v5.0.

    Args:
        scores_df (pd.DataFrame): DataFrame containing scores for each dimension and calculated health metrics.
        evidence_df (pd.DataFrame): DataFrame containing evidence snippets, dimensions, and confidence scores.

    Returns:
        str: A JSON string containing the structured results of the analysis.
    """
    # --- Configuration based on CHF v5.0 Framework ---
    BASE_DIMENSIONS = [
        'procedural_legitimacy', 'procedural_rejection',
        'institutional_respect', 'institutional_subversion',
        'systemic_continuity', 'systemic_replacement'
    ]
    HEALTH_SCORES = [
        'procedural_health_score', 'institutional_health_score', 'systemic_health_score'
    ]
    OVERALL_INDEX = 'constitutional_direction_index'
    ALL_METRICS = BASE_DIMENSIONS + HEALTH_SCORES + [OVERALL_INDEX]

    # --- Initialize Results Dictionary ---
    results = {
        "framework_name": "chf_v5_0",
        "sample_characteristics": {},
        "descriptive_stats": {},
        "reliability_metrics": {},
        "correlations": {},
        "hypothesis_tests": {},
        "effect_sizes": {}
    }

    # --- Helper function for Cronbach's Alpha ---
    def calculate_cronbach_alpha(df: pd.DataFrame):
        """Calculates Cronbach's alpha for a given set of items."""
        try:
            if df.shape[1] < 2:
                return np.nan, "Need at least 2 items for alpha"
            if df.shape[0] < 2:
                return np.nan, "Need at least 2 observations for alpha"
            
            # Drop rows with any NaN values for this calculation
            df_clean = df.dropna()
            if df_clean.shape[0] < 2:
                return np.nan, "Not enough valid observations after dropping NaN"

            k = df_clean.shape[1]
            item_vars = df_clean.var(axis=0, ddof=1).sum()
            total_var = df_clean.sum(axis=1).var(ddof=1)

            if total_var == 0:
                return 1.0, "Total variance is zero; items are perfectly correlated" if item_vars == 0 else np.nan, "Total variance is zero, cannot compute alpha"

            alpha = (k / (k - 1)) * (1 - item_vars / total_var)
            return alpha, "OK"
        except Exception as e:
            return np.nan, str(e)

    def get_alpha_interpretation(alpha):
        """Interprets Cronbach's alpha based on the framework's rubric."""
        if pd.isna(alpha):
            return "Not applicable"
        if alpha >= 0.80: return "excellent"
        if alpha >= 0.70: return "good"
        if alpha >= 0.60: return "acceptable"
        return "poor"

    try:
        # --- Data Integrity Check ---
        if not all(col in scores_df.columns for col in ALL_METRICS):
            missing_cols = [col for col in ALL_METRICS if col not in scores_df.columns]
            raise ValueError(f"Scores DataFrame is missing required columns: {missing_cols}")
        if not all(col in evidence_df.columns for col in ['aid', 'dimension', 'confidence']):
            missing_cols = [col for col in ['aid', 'dimension', 'confidence'] if col not in evidence_df.columns]
            raise ValueError(f"Evidence DataFrame is missing required columns: {missing_cols}")

        # --- 1. Sample Characteristics ---
        try:
            num_analyses = len(scores_df)
            num_evidence = len(evidence_df)
            avg_evidence_per_analysis = num_evidence / num_analyses if num_analyses > 0 else 0
            results['sample_characteristics'] = {
                'num_analyses': num_analyses,
                'num_evidence_snippets': num_evidence,
                'avg_evidence_per_analysis': round(avg_evidence_per_analysis, 2)
            }
        except Exception as e:
            results['sample_characteristics'] = {'error': str(e)}

        # --- 2. Descriptive Statistics ---
        try:
            # For scores
            scores_stats = scores_df[ALL_METRICS].describe().round(4).to_dict()
            # For evidence
            evidence_stats = {
                'confidence_scores': evidence_df['confidence'].describe().round(4).to_dict(),
                'evidence_per_dimension': evidence_df['dimension'].value_counts().to_dict()
            }
            results['descriptive_stats'] = {
                'scores': scores_stats,
                'evidence': evidence_stats
            }
        except Exception as e:
            results['descriptive_stats'] = {'error': str(e)}

        # --- 3. Reliability Analysis (Cronbach's Alpha) ---
        try:
            reliability_data = {}
            # Reverse code the negative items for consistency calculation
            reversed_df = scores_df.copy()
            reversed_df['procedural_rejection_rev'] = 1 - reversed_df['procedural_rejection']
            reversed_df['institutional_subversion_rev'] = 1 - reversed_df['institutional_subversion']
            reversed_df['systemic_replacement_rev'] = 1 - reversed_df['systemic_replacement']

            # Alpha for Procedural Axis
            proc_items = reversed_df[['procedural_legitimacy', 'procedural_rejection_rev']]
            proc_alpha, proc_msg = calculate_cronbach_alpha(proc_items)
            reliability_data['procedural_axis'] = {
                'cronbach_alpha': round(proc_alpha, 4) if pd.notna(proc_alpha) else None,
                'interpretation': get_alpha_interpretation(proc_alpha),
                'notes': proc_msg
            }

            # Alpha for Institutional Axis
            inst_items = reversed_df[['institutional_respect', 'institutional_subversion_rev']]
            inst_alpha, inst_msg = calculate_cronbach_alpha(inst_items)
            reliability_data['institutional_axis'] = {
                'cronbach_alpha': round(inst_alpha, 4) if pd.notna(inst_alpha) else None,
                'interpretation': get_alpha_interpretation(inst_alpha),
                'notes': inst_msg
            }

            # Alpha for Systemic Axis
            sys_items = reversed_df[['systemic_continuity', 'systemic_replacement_rev']]
            sys_alpha, sys_msg = calculate_cronbach_alpha(sys_items)
            reliability_data['systemic_axis'] = {
                'cronbach_alpha': round(sys_alpha, 4) if pd.notna(sys_alpha) else None,
                'interpretation': get_alpha_interpretation(sys_alpha),
                'notes': sys_msg
            }
            
            # Alpha for Overall Framework (all 6 items)
            all_items = reversed_df[[
                'procedural_legitimacy', 'procedural_rejection_rev',
                'institutional_respect', 'institutional_subversion_rev',
                'systemic_continuity', 'systemic_replacement_rev'
            ]]
            overall_alpha, overall_msg = calculate_cronbach_alpha(all_items)
            reliability_data['overall_framework'] = {
                'cronbach_alpha': round(overall_alpha, 4) if pd.notna(overall_alpha) else None,
                'interpretation': get_alpha_interpretation(overall_alpha),
                'notes': overall_msg
            }

            results['reliability_metrics'] = reliability_data
        except Exception as e:
            results['reliability_metrics'] = {'error': str(e)}

        # --- 4. Correlation Analysis ---
        try:
            # Replace infinite values with NaN before correlation
            corr_df = scores_df[ALL_METRICS].replace([np.inf, -np.inf], np.nan)
            correlation_matrix = corr_df.corr().round(4)
            results['correlations'] = {
                'full_matrix': correlation_matrix.fillna(0).to_dict(),
                'notes': "Correlation matrix of all framework dimensions and scores."
            }
        except Exception as e:
            results['correlations'] = {'error': str(e)}

        # --- 5. Hypothesis Testing (Paired T-Tests) ---
        try:
            tests = {}
            # Test 1: Procedural Legitimacy vs. Rejection
            t_stat, p_val = scipy.stats.ttest_rel(scores_df['procedural_legitimacy'], scores_df['procedural_rejection'], nan_policy='omit')
            tests['procedural_axis_tension'] = {'t_statistic': round(t_stat, 4), 'p_value': round(p_val, 6), 'alpha': 0.05, 'significant': bool(p_val < 0.05)}

            # Test 2: Institutional Respect vs. Subversion
            t_stat, p_val = scipy.stats.ttest_rel(scores_df['institutional_respect'], scores_df['institutional_subversion'], nan_policy='omit')
            tests['institutional_axis_tension'] = {'t_statistic': round(t_stat, 4), 'p_value': round(p_val, 6), 'alpha': 0.05, 'significant': bool(p_val < 0.05)}

            # Test 3: Systemic Continuity vs. Replacement
            t_stat, p_val = scipy.stats.ttest_rel(scores_df['systemic_continuity'], scores_df['systemic_replacement'], nan_policy='omit')
            tests['systemic_axis_tension'] = {'t_statistic': round(t_stat, 4), 'p_value': round(p_val, 6), 'alpha': 0.05, 'significant': bool(p_val < 0.05)}
            
            # Test 4: One-sample T-test for CDI against 0
            cdi_scores = scores_df[OVERALL_INDEX].dropna()
            if len(cdi_scores) > 1:
                t_stat, p_val = scipy.stats.ttest_1samp(cdi_scores, 0)
                tests['cdi_deviation_from_zero'] = {'t_statistic': round(t_stat, 4), 'p_value': round(p_val, 6), 'alpha': 0.05, 'significant': bool(p_val < 0.05)}
            else:
                tests['cdi_deviation_from_zero'] = {'error': 'Not enough data for 1-sample t-test'}

            results['hypothesis_tests'] = tests
        except Exception as e:
            results['hypothesis_tests'] = {'error': str(e)}

        # --- 6. Effect Sizes (Cohen's d) ---
        try:
            sizes = {}
            # Function for Cohen's d for paired samples
            def cohen_d_paired(x, y):
                diff = (x - y).dropna()
                return diff.mean() / diff.std(ddof=1) if diff.std(ddof=1) != 0 else 0

            # Effect size for Procedural tension
            sizes['procedural_axis_tension'] = round(cohen_d_paired(scores_df['procedural_legitimacy'], scores_df['procedural_rejection']), 4)
            
            # Effect size for Institutional tension
            sizes['institutional_axis_tension'] = round(cohen_d_paired(scores_df['institutional_respect'], scores_df['institutional_subversion']), 4)

            # Effect size for Systemic tension
            sizes['systemic_axis_tension'] = round(cohen_d_paired(scores_df['systemic_continuity'], scores_df['systemic_replacement']), 4)
            
            # Effect size for CDI deviation from zero
            cdi_scores = scores_df[OVERALL_INDEX].dropna()
            if cdi_scores.std(ddof=1) > 0:
                sizes['cdi_deviation_from_zero'] = round(cdi_scores.mean() / cdi_scores.std(ddof=1), 4)
            else:
                sizes['cdi_deviation_from_zero'] = 0

            results['effect_sizes'] = sizes
        except Exception as e:
            results['effect_sizes'] = {'error': str(e)}

    except Exception as e:
        # Catch-all for major errors like missing data
        results['error'] = f"A critical error occurred during analysis: {str(e)}"

    # --- Final Output ---
    # Convert numpy types to native Python types for JSON serialization
    def convert(o):
        if isinstance(o, np.generic): return o.item()
        raise TypeError

    return json.dumps(results, indent=2, default=convert)

# This block demonstrates how the function would be called with pre-loaded DataFrames.
# In the actual execution environment, `scores_df` and `evidence_df` are assumed to exist.
if __name__ == '__main__':
    # Create sample dataframes for local testing, mirroring the provided structure.
    scores_data = [{'aid': 'e1760be72296a9453c802595519953de776fb48becbfabef66f64317ce1dc799', 'procedural_legitimacy': 0.1, 'procedural_rejection': 0.9, 'institutional_respect': 0.1, 'institutional_subversion': 0.9, 'systemic_continuity': 0.1, 'systemic_replacement': 0.9, 'procedural_health_score': -0.8, 'institutional_health_score': -0.8, 'systemic_health_score': -0.8, 'constitutional_direction_index': -0.8}, {'aid': 'af7878b2ed5740ed1115fbc509a4d5de4e3b61572dcbd81cef8e9f8a067151cf', 'procedural_legitimacy': 0.8, 'procedural_rejection': 0.2, 'institutional_respect': 0.7, 'institutional_subversion': 0.3, 'systemic_continuity': 0.6, 'systemic_replacement': 0.4, 'procedural_health_score': 0.6, 'institutional_health_score': 0.4, 'systemic_health_score': 0.2, 'constitutional_direction_index': 0.4666666666666667}, {'aid': 'b1c2d3e4f5a6b7c8d9e0f1a2b3c4d5e6f7a8b9c0d1e2f3a4b5c6d7e8f9a0b1c2', 'procedural_legitimacy': 0.6, 'procedural_rejection': 0.3, 'institutional_respect': 0.5, 'institutional_subversion': 0.4, 'systemic_continuity': 0.7, 'systemic_replacement': 0.2, 'procedural_health_score': 0.3, 'institutional_health_score': 0.1, 'systemic_health_score': 0.5, 'constitutional_direction_index': 0.3}]
    evidence_data = [{'aid': 'e1760be72296a9453c802595519953de776fb48becbfabef66f64317ce1dc799', 'dimension': 'procedural_rejection', 'quote': 'The old ways are corrupt.', 'confidence': 0.9}, {'aid': 'e1760be72296a9453c802595519953de776fb48becbfabef66f64317ce1dc799', 'dimension': 'institutional_subversion', 'quote': 'We must dismantle the courts.', 'confidence': 0.95}, {'aid': 'af7878b2ed5740ed1115fbc509a4d5de4e3b61572dcbd81cef8e9f8a067151cf', 'dimension': 'procedural_legitimacy', 'quote': 'We must follow the process.', 'confidence': 0.85}, {'aid': 'af7878b2ed5740ed1115fbc509a4d5de4e3b61572dcbd81cef8e9f8a067151cf', 'dimension': 'institutional_respect', 'quote': 'The institutions are sound.', 'confidence': 0.8}, {'aid': 'b1c2d3e4f5a6b7c8d9e0f1a2b3c4d5e6f7a8b9c0d1e2f3a4b5c6d7e8f9a0b1c2', 'dimension': 'systemic_continuity', 'quote': 'Incremental change is best.', 'confidence': 0.9}]
    
    scores_df = pd.DataFrame(scores_data)
    evidence_df = pd.DataFrame(evidence_data)

    # Execute the analysis and print the JSON output
    json_output = run_analysis(scores_df, evidence_df)
    print(json_output)