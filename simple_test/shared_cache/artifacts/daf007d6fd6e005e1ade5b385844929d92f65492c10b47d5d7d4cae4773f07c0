import pandas as pd
import numpy as np
import scipy.stats
import json

# --- Helper Functions ---

def cronbach_alpha(df):
    """
    Calculates Cronbach's alpha for a given DataFrame of items.
    Items are expected to be in columns, with observations in rows.
    """
    try:
        # Drop rows with any missing values for this calculation
        df_complete = df.dropna()
        
        # Check for sufficient data
        if df_complete.shape[0] < 2 or df_complete.shape[1] < 2:
            return np.nan

        k = df_complete.shape[1]
        item_variances = df_complete.var(ddof=1).sum()
        total_scores = df_complete.sum(axis=1)
        total_variance = total_scores.var(ddof=1)

        # Avoid division by zero if all items are constant
        if total_variance == 0:
            # If item variances are also 0, items are perfectly correlated.
            # If not, it's an undefined state, but we return 0 for safety.
            return 1.0 if item_variances == 0 else 0.0
            
        alpha = (k / (k - 1)) * (1 - (item_variances / total_variance))
        return alpha
    except Exception:
        return np.nan

def cohens_d_paired(x, y):
    """
    Calculates Cohen's d for two paired series.
    """
    try:
        # Ensure data is numeric and has no NaNs for this calculation
        x = pd.to_numeric(x, errors='coerce').dropna()
        y = pd.to_numeric(y, errors='coerce').dropna()
        
        # Align indices to ensure proper pairing
        x, y = x.align(y, join='inner')

        if len(x) < 2:
            return np.nan
            
        diff = x - y
        std_dev_diff = diff.std(ddof=1)

        # Avoid division by zero
        if std_dev_diff == 0:
            return np.inf if diff.mean() != 0 else 0.0
            
        d = diff.mean() / std_dev_diff
        return d
    except Exception:
        return np.nan

def run_analysis(scores_df, evidence_df):
    """
    Main function to perform the PDAF v5.0 statistical analysis.
    """
    results = {
        "sample_characteristics": {},
        "descriptive_stats": {},
        "reliability_metrics": {},
        "correlations": {},
        "hypothesis_tests": {},
        "effect_sizes": {}
    }

    # --- 1. Data Cleaning and Preparation ---
    
    # Store original counts
    results["sample_characteristics"]["initial_scores_rows"] = len(scores_df)
    results["sample_characteristics"]["initial_evidence_rows"] = len(evidence_df)

    # Define dimension groups from the framework
    dimension_groups = {
        "core_anchors": ["ideological_grounding", "strategic_positioning", "value_system_appeals", "authority_claims"],
        "mechanism_anchors": ["evidence_deployment", "emotional_appeals", "logical_structure", "narrative_framing"],
        "boundary_anchors": ["audience_targeting", "opposition_framing", "issue_scope", "temporal_orientation"]
    }
    all_primary_dimensions = [dim for group in dimension_groups.values() for dim in group]
    all_composite_scores = [
        'core_anchors_score', 'mechanism_anchors_score', 'boundary_anchors_score', 'overall_discourse_effectiveness'
    ]

    # Clean scores_df: ensure numeric types and drop rows with missing primary scores
    for col in all_primary_dimensions + all_composite_scores:
        if col in scores_df.columns:
            scores_df[col] = pd.to_numeric(scores_df[col], errors='coerce')
    
    cleaned_scores_df = scores_df.dropna(subset=all_primary_dimensions).copy()
    
    # Clean evidence_df: drop rows with missing essential data
    cleaned_evidence_df = evidence_df.dropna(subset=['aid', 'dimension', 'quote', 'confidence']).copy()
    cleaned_evidence_df['confidence'] = pd.to_numeric(cleaned_evidence_df['confidence'], errors='coerce')
    cleaned_evidence_df = cleaned_evidence_df.dropna(subset=['confidence'])

    # Update sample characteristics with cleaned counts
    results["sample_characteristics"]["analyzed_scores_rows"] = len(cleaned_scores_df)
    results["sample_characteristics"]["analyzed_evidence_rows"] = len(cleaned_evidence_df)
    results["sample_characteristics"]["num_artifacts_with_scores"] = cleaned_scores_df['aid'].nunique()
    results["sample_characteristics"]["num_artifacts_with_evidence"] = cleaned_evidence_df['aid'].nunique()

    # Handle case with insufficient data for analysis
    if len(cleaned_scores_df) < 2:
        results["error"] = "Insufficient data for analysis after cleaning (less than 2 valid rows in scores data)."
        return results

    # --- 2. Descriptive Statistics ---
    try:
        # For scores
        desc_scores = cleaned_scores_df[all_primary_dimensions + all_composite_scores].describe().to_dict()
        results["descriptive_stats"]["scores_summary"] = desc_scores
        
        # For evidence confidence
        desc_evidence_conf = cleaned_evidence_df['confidence'].describe().to_dict()
        results["descriptive_stats"]["evidence_confidence_summary"] = desc_evidence_conf
        
        # Evidence counts per dimension (statistical analysis of text data)
        evidence_counts = cleaned_evidence_df['dimension'].value_counts().to_dict()
        results["sample_characteristics"]["evidence_per_dimension"] = evidence_counts
        
        # Analysis of quote length without accessing string content
        quote_len_stats = cleaned_evidence_df['quote'].str.len().describe().to_dict()
        results["descriptive_stats"]["evidence_quote_length_summary"] = quote_len_stats

    except Exception as e:
        results["descriptive_stats"]["error"] = f"Failed to compute descriptive statistics: {str(e)}"

    # --- 3. Reliability Assessment (Cronbach's Alpha) ---
    try:
        reliability = {}
        for group_name, dims in dimension_groups.items():
            # Ensure all dimensions for the group exist in the DataFrame
            if all(d in cleaned_scores_df.columns for d in dims):
                group_df = cleaned_scores_df[dims]
                alpha = cronbach_alpha(group_df)
                reliability[f"{group_name}_alpha"] = alpha
            else:
                reliability[f"{group_name}_alpha"] = "Not all dimensions present in data"
        results["reliability_metrics"] = reliability
    except Exception as e:
        results["reliability_metrics"]["error"] = f"Failed to compute reliability: {str(e)}"

    # --- 4. Correlation Analysis ---
    try:
        # Correlation matrix for all primary dimensions
        primary_corr_matrix = cleaned_scores_df[all_primary_dimensions].corr().to_dict()
        results["correlations"]["primary_dimensions_matrix"] = primary_corr_matrix
        
        # Correlation of composite scores with overall effectiveness
        composite_corr = {}
        overall_score_col = 'overall_discourse_effectiveness'
        if overall_score_col in cleaned_scores_df.columns:
            for score in ['core_anchors_score', 'mechanism_anchors_score', 'boundary_anchors_score']:
                if score in cleaned_scores_df.columns:
                    # Using pearsonr to get both correlation and p-value
                    corr_val, p_val = scipy.stats.pearsonr(cleaned_scores_df[score], cleaned_scores_df[overall_score_col])
                    composite_corr[f"{score}_vs_overall"] = {"correlation_coefficient": corr_val, "p_value": p_val}
            results["correlations"]["composites_vs_overall"] = composite_corr
    except Exception as e:
        results["correlations"]["error"] = f"Failed to compute correlations: {str(e)}"

    # --- 5. Hypothesis Testing ---
    try:
        # H1: Test for significant differences between anchor group composite scores
        # Using paired t-tests as scores are from the same artifacts
        tests = {}
        comparisons = [
            ('core_anchors_score', 'mechanism_anchors_score'),
            ('core_anchors_score', 'boundary_anchors_score'),
            ('mechanism_anchors_score', 'boundary_anchors_score')
        ]
        for s1, s2 in comparisons:
            if s1 in cleaned_scores_df.columns and s2 in cleaned_scores_df.columns:
                stat, p_val = scipy.stats.ttest_rel(cleaned_scores_df[s1], cleaned_scores_df[s2], nan_policy='omit')
                tests[f"paired_t-test_{s1}_vs_{s2}"] = {"statistic": stat, "p_value": p_val}
        results["hypothesis_tests"]["group_score_differences"] = tests
    except Exception as e:
        results["hypothesis_tests"]["error"] = f"Failed to perform hypothesis tests: {str(e)}"

    # --- 6. Effect Size Calculations ---
    try:
        # Cohen's d for the paired t-tests
        effects = {}
        for s1, s2 in comparisons:
             if s1 in cleaned_scores_df.columns and s2 in cleaned_scores_df.columns:
                d = cohens_d_paired(cleaned_scores_df[s1], cleaned_scores_df[s2])
                effects[f"cohens_d_{s1}_vs_{s2}"] = d
        results["effect_sizes"]["group_score_differences"] = effects
        
        # Correlation coefficient 'r' is itself an effect size.
        # We can add a note or copy them here for clarity.
        if "composites_vs_overall" in results["correlations"]:
            r_effects = {k: v["correlation_coefficient"] for k, v in results["correlations"]["composites_vs_overall"].items()}
            results["effect_sizes"]["composites_vs_overall_r"] = r_effects
    except Exception as e:
        results["effect_sizes"]["error"] = f"Failed to compute effect sizes: {str(e)}"

    return results

# --- Main Execution Block ---
# In a real environment, scores_df and evidence_df would be pre-loaded.
# For this script to be runnable, we define them with the provided sample data.
scores_data = [{'aid': 'e1760be72296a9453c802595519953de776fb48becbfabef66f64317ce1dc799', 'ideological_grounding': 0.9, 'strategic_positioning': 0.8, 'value_system_appeals': 0.7, 'authority_claims': 0.6, 'evidence_deployment': 0.8, 'emotional_appeals': 0.9, 'logical_structure': 0.7, 'narrative_framing': 0.8, 'audience_targeting': 0.9, 'opposition_framing': 0.8, 'issue_scope': 0.7, 'temporal_orientation': 0.8, 'core_anchors_score': 0.775, 'mechanism_anchors_score': 0.8, 'boundary_anchors_score': 0.8, 'overall_discourse_effectiveness': 0.794}, {'aid': 'af7878b2ed5740ed1115fbc509a4d5de4e3b61572dcbd81cef8e9f8a067151cf', 'ideological_grounding': 0.8, 'strategic_positioning': 0.7, 'value_system_appeals': 0.9, 'authority_claims': 0.6, 'evidence_deployment': 0.7, 'emotional_appeals': 0.8, 'logical_structure': 0.7, 'narrative_framing': 0.8, 'audience_targeting': 0.9, 'opposition_framing': 0.5, 'issue_scope': 0.7, 'temporal_orientation': 0.8, 'core_anchors_score': 0.75, 'mechanism_anchors_score': 0.75, 'boundary_anchors_score': 0.7, 'overall_discourse_effectiveness': 0.75}]
evidence_data = [{'aid': 'e1760be72296a9453c802595519953de776fb48becbfabef66f64317ce1dc799', 'dimension': 'opposition_framing', 'quote': "every institution in this country was built to keep *them* on top and *us* beneath. There is no neutral ground—there are only oppressors and the oppressed. If you're not with us, you're against us.", 'confidence': 0.9}, {'aid': 'af7878b2ed5740ed1115fbc509a4d5de4e3b61572dcbd81cef8e9f8a067151cf', 'dimension': 'value_system_appeals', 'quote': 'We must return to the timeless values of family, faith, and freedom that made this nation great.', 'confidence': 0.95}, {'aid': None, 'dimension': None, 'quote': None, 'confidence': np.nan}, {'aid': 'e1760be72296a9453c802595519953de776fb48becbfabef66f64317ce1dc799', 'dimension': 'emotional_appeals', 'quote': 'They want to take away your security and leave you vulnerable. Be afraid. Be very afraid.', 'confidence': 0.88}]

scores_df = pd.DataFrame(scores_data)
evidence_df = pd.DataFrame(evidence_data)

# Execute the analysis and print the JSON output
try:
    analysis_results = run_analysis(scores_df, evidence_df)
    # Convert numpy types to native Python types for JSON serialization
    def convert_numpy(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif pd.isna(obj):
            return None
        return obj

    # Use a custom JSON encoder to handle potential numpy types
    class NpEncoder(json.JSONEncoder):
        def default(self, obj):
            return convert_numpy(obj)

    # Pretty print the final JSON output
    json_output = json.dumps(analysis_results, indent=2, cls=NpEncoder)
    print(json_output)

except Exception as e:
    # Catch-all for any unexpected errors during execution
    error_output = json.dumps({"critical_error": f"An unexpected error occurred: {str(e)}"}, indent=2)
    print(error_output)