import pandas as pd
import numpy as np
from scipy import stats
import json

# This script is designed to run in an environment where 'scores_df' and 'evidence_df'
# are pre-loaded pandas DataFrames.

def run_analysis(scores_df, evidence_df):
    """
    Performs statistical analysis based on a framework specification, adapted for the
    provided data structures.

    Args:
        scores_df (pd.DataFrame): DataFrame containing scores for various dimensions.
        evidence_df (pd.DataFrame): DataFrame containing evidence quotes and confidence.

    Returns:
        str: A JSON string containing the structured analysis results.
    """
    # Initialize the main results dictionary which will be converted to JSON.
    results = {
        "descriptive_stats": {},
        "reliability_metrics": {},
        "correlations": {},
        "hypothesis_tests": {},
        "effect_sizes": {},
        "sample_characteristics": {}
    }

    try:
        # --- 1. Sample Characteristics & Data Validation ---
        # This section provides metadata about the dataset being analyzed.
        # It's crucial for understanding the context of the results.
        results['sample_characteristics']['num_scores_records'] = len(scores_df)
        results['sample_characteristics']['num_evidence_records'] = len(evidence_df)
        results['sample_characteristics']['scores_columns'] = list(scores_df.columns)
        results['sample_characteristics']['scores_missing_values'] = int(scores_df.isnull().sum().sum())
        results['sample_characteristics']['evidence_missing_values'] = int(evidence_df.isnull().sum().sum())
        results['sample_characteristics']['framework_note'] = (
            "Analysis structure is based on the ECF v5.0 specification but applied to the "
            "available data columns. ECF-specific indices (e.g., affective_climate_index) "
            "were not calculated due to a mismatch between ECF dimensions and data columns."
        )

        # Define the columns to be used in the analysis, excluding non-numeric 'aid'.
        score_cols = [col for col in scores_df.columns if col != 'aid' and pd.api.types.is_numeric_dtype(scores_df[col])]
        
        # Define the base dimensions from the actual data for reliability analysis.
        base_dimension_cols = [
            'accuracy_verification', 'curriculum_alignment', 'depth_coverage',
            'relevance_context', 'instructional_clarity', 'engagement_strategies',
            'differentiation_support', 'assessment_integration', 'universal_design',
            'language_clarity', 'multimedia_integration', 'barrier_reduction'
        ]
        # Ensure we only use columns that actually exist in the dataframe.
        base_dimension_cols = [col for col in base_dimension_cols if col in scores_df.columns]

        # --- 2. Descriptive Statistics ---
        # Provides a statistical summary of the core metrics.
        if not scores_df.empty and score_cols:
            results['descriptive_stats']['scores_summary'] = scores_df[score_cols].describe().to_dict()
        
        # Safely analyze the evidence data without accessing raw text content.
        if not evidence_df.empty and 'dimension' in evidence_df.columns and 'confidence' in evidence_df.columns:
            evidence_summary = evidence_df.groupby('dimension')['confidence'].agg(['mean', 'std', 'count']).to_dict('index')
            results['descriptive_stats']['evidence_confidence_summary'] = evidence_summary

        # --- 3. Reliability Analysis (Cronbach's Alpha) ---
        # Measures the internal consistency of the base dimension items to see if they
        # reliably measure a single underlying construct.
        alpha_results = {}
        if len(base_dimension_cols) > 1 and len(scores_df) > 1:
            items_df = scores_df[base_dimension_cols].copy().dropna() # Use only complete cases
            if len(items_df) > 1:
                k = items_df.shape[1]
                total_variance = items_df.sum(axis=1).var(ddof=1)
                item_variance_sum = items_df.var(ddof=1).sum()

                if total_variance > 0:
                    alpha = (k / (k - 1)) * (1 - (item_variance_sum / total_variance))
                    alpha_results['cronbachs_alpha'] = alpha
                    # Interpretation based on the ECF v5.0 reliability rubric
                    if alpha >= 0.80: interpretation = "Excellent"
                    elif alpha >= 0.70: interpretation = "Good"
                    elif alpha >= 0.60: interpretation = "Acceptable"
                    else: interpretation = "Poor"
                    alpha_results['interpretation'] = interpretation
                else:
                    alpha_results['error'] = "Total variance is zero; cannot compute alpha."
            else:
                alpha_results['error'] = "Not enough valid data rows to calculate reliability."
        else:
            alpha_results['error'] = "Not enough items or data rows to calculate reliability."
        results['reliability_metrics']['base_dimensions_scale'] = alpha_results

        # --- 4. Correlation Analysis ---
        # Examines the relationships between different score dimensions.
        if len(score_cols) > 1:
            # Using .to_dict('list') for a clean, JSON-serializable format.
            corr_matrix = scores_df[score_cols].corr().to_dict()
            results['correlations']['scores_correlation_matrix'] = corr_matrix
        else:
            results['correlations']['message'] = "Not enough numeric columns to compute a correlation matrix."

        # --- 5. Hypothesis Testing ---
        # Example test: Is the 'overall_educational_value' significantly different from a neutral midpoint of 0.5?
        hypothesis_results = {}
        target_col = 'overall_educational_value'
        if target_col in scores_df.columns and pd.api.types.is_numeric_dtype(scores_df[target_col]):
            data_series = scores_df[target_col].dropna()
            if len(data_series) > 1:
                pop_mean = 0.5
                t_stat, p_value = stats.ttest_1samp(data_series, pop_mean, nan_policy='omit')
                hypothesis_results['test_description'] = f"One-Sample T-test on '{target_col}' against a population mean of {pop_mean}."
                hypothesis_results['t_statistic'] = t_stat
                hypothesis_results['p_value'] = p_value
                hypothesis_results['is_significant_at_0_05'] = bool(p_value < 0.05)
            else:
                hypothesis_results['error'] = f"Not enough valid data in '{target_col}' for t-test."
        else:
            hypothesis_results['error'] = f"Target column '{target_col}' not found or not numeric."
        results['hypothesis_tests']['mean_value_test'] = hypothesis_results

        # --- 6. Effect Size Calculation ---
        # Calculates Cohen's d for the t-test to measure the magnitude of the effect.
        effect_size_results = {}
        if 't_statistic' in hypothesis_results:
            data_series = scores_df[target_col].dropna()
            sample_mean = data_series.mean()
            sample_std = data_series.std(ddof=1)
            pop_mean = 0.5
            if sample_std is not None and sample_std > 0:
                cohens_d = (sample_mean - pop_mean) / sample_std
                effect_size_results["cohens_d"] = cohens_d
            else:
                effect_size_results["error"] = "Cannot calculate Cohen's d; standard deviation is zero or undefined."
        else:
            effect_size_results["note"] = "Effect size not calculated as the prerequisite hypothesis test failed."
        results['effect_sizes']['mean_value_effect'] = effect_size_results

    except Exception as e:
        # Comprehensive error handling to ensure a valid JSON output even if analysis fails.
        results = {
            "status": "error",
            "error_message": str(e),
            "error_type": type(e).__name__
        }

    # Custom JSON encoder to handle numpy data types gracefully.
    class NpEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, np.integer):
                return int(obj)
            if isinstance(obj, np.floating):
                return float(obj)
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            if isinstance(obj, np.bool_):
                return bool(obj)
            return super(NpEncoder, self).default(obj)

    return json.dumps(results, indent=2, cls=NpEncoder)

# The script execution starts here. The function is called with the pre-loaded DataFrames.
# The final output will be the printed JSON string.
print(run_analysis(scores_df, evidence_df))