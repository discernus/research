import pandas as pd
import numpy as np
from scipy import stats
import json

# This script assumes that 'scores_df' and 'evidence_df' are pre-loaded pandas DataFrames.

# --- 1. Initialization and Setup ---

# Initialize the main dictionary to store all analysis results.
# This structure is required for the downstream pipeline.
result_data = {
    'descriptive_stats': {},
    'hypothesis_tests': {},
    'correlations': {},
    'reliability_metrics': {}
}

# Define the column groupings based on the composite scores in the data structure.
# This is analogous to the 'dimension_groups' in the framework.
CONSTRUCTS = {
    'content_quality': [
        'accuracy_verification', 'curriculum_alignment', 'depth_coverage', 'relevance_context'
    ],
    'pedagogical_effectiveness': [
        'instructional_clarity', 'engagement_strategies', 'differentiation_support', 'assessment_integration'
    ],
    'learning_accessibility': [
        'universal_design', 'language_clarity', 'multimedia_integration', 'barrier_reduction'
    ]
}

# --- 2. Main Analysis Block ---

try:
    # --- 2.1. Data Validation and Sample Characteristics ---
    if 'scores_df' not in locals() or scores_df.empty:
        raise ValueError("scores_df is not defined or is empty.")
    if 'evidence_df' not in locals() or evidence_df.empty:
        raise ValueError("evidence_df is not defined or is empty.")

    # Get all numeric columns from scores_df for analysis, excluding the identifier.
    score_columns = [col for col in scores_df.columns if col != 'aid' and pd.api.types.is_numeric_dtype(scores_df[col])]
    
    # Store sample characteristics
    result_data['descriptive_stats']['sample_characteristics'] = {
        'num_scores_records': scores_df.shape[0],
        'num_evidence_records': evidence_df.shape[0],
        'num_score_dimensions': len(score_columns)
    }

    # --- 2.2. Descriptive Statistics ---
    # Calculate summary statistics for all numeric score dimensions.
    scores_summary = scores_df[score_columns].describe().to_dict()
    result_data['descriptive_stats']['scores_summary'] = scores_summary

    # Analyze evidence data statistically without accessing raw text.
    # Calculate descriptive stats for confidence scores.
    evidence_confidence_summary = evidence_df['confidence'].describe().to_dict()
    
    # Safely calculate quote length and get its summary statistics.
    evidence_df['quote_length'] = evidence_df['quote'].str.len()
    evidence_quote_length_summary = evidence_df['quote_length'].describe().to_dict()
    
    result_data['descriptive_stats']['evidence_summary'] = {
        'confidence': evidence_confidence_summary,
        'quote_length': evidence_quote_length_summary
    }

    # --- 2.3. Reliability Metrics (Cronbach's Alpha) ---
    # Function to calculate Cronbach's Alpha for internal consistency.
    def calculate_cronbach_alpha(df_items):
        """Calculates Cronbach's Alpha for a given set of items (columns)."""
        try:
            # Ensure we have at least two items to compare.
            k = df_items.shape[1]
            if k < 2:
                return np.nan, "Not enough items for alpha calculation (k < 2)."

            # Drop rows with any missing values for this calculation.
            df_items = df_items.dropna()
            n = df_items.shape[0]
            if n < 2:
                return np.nan, "Not enough data points for alpha calculation (n < 2)."

            # Calculate sum of item variances and variance of total scores.
            item_vars = df_items.var(axis=0, ddof=1).sum()
            total_scores = df_items.sum(axis=1)
            total_var = total_scores.var(ddof=1)

            # Handle case where there is no variance in the total score.
            if total_var == 0:
                return 1.0, "Perfect consistency (all items are constant)."

            alpha = (k / (k - 1)) * (1 - (item_vars / total_var))
            return alpha, "Calculation successful."
        except Exception as e:
            return np.nan, f"An error occurred: {str(e)}"

    # Calculate alpha for each defined construct.
    for construct_name, items in CONSTRUCTS.items():
        # Ensure all items for the construct exist in the DataFrame
        valid_items = [item for item in items if item in scores_df.columns]
        if len(valid_items) < 2:
            alpha_value = np.nan
            message = f"Skipped: Construct '{construct_name}' has fewer than 2 valid columns in the data."
        else:
            alpha_value, message = calculate_cronbach_alpha(scores_df[valid_items])
        
        result_data['reliability_metrics'][f'{construct_name}_cronbach_alpha'] = {
            'alpha': alpha_value if not np.isnan(alpha_value) else None,
            'items_used': valid_items,
            'notes': message
        }

    # --- 2.4. Correlation Analysis ---
    # Calculate the Pearson correlation matrix for all score dimensions.
    correlation_matrix = scores_df[score_columns].corr(method='pearson')
    result_data['correlations']['correlation_matrix'] = correlation_matrix.to_dict()

    # Identify and list highly correlated pairs to find key relationships.
    # Using a threshold of |r| >= 0.7.
    highly_correlated_pairs = []
    corr_matrix_unstacked = correlation_matrix.unstack()
    for idx, value in corr_matrix_unstacked.items():
        dim1, dim2 = idx
        if dim1 != dim2 and dim1 < dim2:  # Avoid duplicates and self-correlation
            if abs(value) >= 0.7:
                highly_correlated_pairs.append({
                    'dimension_1': dim1,
                    'dimension_2': dim2,
                    'correlation': value
                })
    result_data['correlations']['highly_correlated_pairs'] = highly_correlated_pairs

    # --- 2.5. Hypothesis Testing ---
    sample_size = scores_df.shape[0]
    
    # Test 1: One-Sample T-test on 'overall_educational_value'
    # H0: The mean 'overall_educational_value' is 0.5 (neutral).
    # H1: The mean 'overall_educational_value' is not 0.5.
    test_col = 'overall_educational_value'
    if test_col in scores_df.columns and sample_size > 1:
        t_stat, p_value = stats.ttest_1samp(scores_df[test_col].dropna(), popmean=0.5)
        # Effect Size: Cohen's d for one-sample t-test
        mean_val = scores_df[test_col].mean()
        std_dev = scores_df[test_col].std()
        cohens_d = (mean_val - 0.5) / std_dev if std_dev > 0 else 0
        
        result_data['hypothesis_tests']['overall_value_vs_midpoint'] = {
            'test_description': "One-Sample T-test on 'overall_educational_value' against a neutral midpoint of 0.5.",
            't_statistic': t_stat,
            'p_value': p_value,
            'effect_size_cohens_d': cohens_d,
            'outcome': 'Statistically significant' if p_value < 0.05 else 'Not statistically significant',
            'notes': f"Sample size (N={sample_size}) should be considered when interpreting results."
        }
    else:
        result_data['hypothesis_tests']['overall_value_vs_midpoint'] = {
            'outcome': 'Test not performed',
            'notes': f"Column '{test_col}' not found or insufficient sample size (N={sample_size})."
        }

    # Test 2: Compare mean scores of the two main composite scores
    # H0: The mean of content_quality scores is equal to the mean of pedagogical_effectiveness scores.
    # H1: The means are not equal.
    if sample_size > 1 and all(item in scores_df.columns for item in CONSTRUCTS['content_quality']) and all(item in scores_df.columns for item in CONSTRUCTS['pedagogical_effectiveness']):
        group1_scores = scores_df[CONSTRUCTS['content_quality']].mean(axis=1)
        group2_scores = scores_df[CONSTRUCTS['pedagogical_effectiveness']].mean(axis=1)
        
        # Using a paired t-test as the scores come from the same subjects (analyses).
        t_stat_paired, p_value_paired = stats.ttest_rel(group1_scores, group2_scores)
        
        # Effect Size: Cohen's d for paired samples
        diff = group1_scores - group2_scores
        cohens_d_paired = diff.mean() / diff.std() if diff.std() > 0 else 0

        result_data['hypothesis_tests']['content_vs_pedagogy'] = {
            'test_description': "Paired T-test comparing mean 'content_quality' and 'pedagogical_effectiveness' scores.",
            't_statistic': t_stat_paired,
            'p_value': p_value_paired,
            'effect_size_cohens_d': cohens_d_paired,
            'outcome': 'Statistically significant difference' if p_value_paired < 0.05 else 'No statistically significant difference',
            'notes': f"Sample size (N={sample_size}) should be considered when interpreting results."
        }
    else:
        result_data['hypothesis_tests']['content_vs_pedagogy'] = {
            'outcome': 'Test not performed',
            'notes': f"One or more required columns missing or insufficient sample size (N={sample_size})."
        }

except Exception as e:
    # Catch any exceptions during the analysis and report them.
    result_data['error'] = f"An error occurred during analysis: {str(e)}"

# --- 3. Final Output ---
# The 'result_data' dictionary now contains all the analysis results
# and is ready for downstream processing.
# The pipeline will capture this variable.