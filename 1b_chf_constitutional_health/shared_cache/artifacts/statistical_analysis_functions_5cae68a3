{
  "status": "success",
  "functions_generated": 6,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 20030,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: presidential_sotu_constitutional_health_trends\nDescription: Statistical analysis experiment\nGenerated: 2025-08-27T22:24:46.758028+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived metrics specified in the Constitutional Health Framework v10.0.\n\n    This function implements the formulas from the framework's 'derived_metrics' section.\n    It computes intermediate salience totals, axis-level health indices, and the\n    final summary metrics (Constitutional Health Index and Constitutional Pathology Index).\n    A small epsilon (0.001) is added to salience denominators to prevent division by zero.\n\n    Args:\n        data (pd.DataFrame): DataFrame with raw and salience scores for the 6 base dimensions.\n                             Must contain columns like 'procedural_legitimacy_raw', \n                             'procedural_legitimacy_salience', etc.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pd.DataFrame: The input DataFrame with added columns for each derived metric,\n                      or None if essential columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        df = data.copy()\n        \n        # Define required columns\n        required_cols = [\n            'procedural_legitimacy_raw', 'procedural_legitimacy_salience',\n            'procedural_rejection_raw', 'procedural_rejection_salience',\n            'institutional_respect_raw', 'institutional_respect_salience',\n            'institutional_subversion_raw', 'institutional_subversion_salience',\n            'systemic_continuity_raw', 'systemic_continuity_salience',\n            'systemic_replacement_raw', 'systemic_replacement_salience'\n        ]\n        if not all(col in df.columns for col in required_cols):\n            # Missing one or more required columns\n            return None\n\n        # Intermediate calculations for salience weighting\n        df['procedural_health_salience_total'] = df['procedural_legitimacy_salience'] + df['procedural_rejection_salience'] + 0.001\n        df['institutional_health_salience_total'] = df['institutional_respect_salience'] + df['institutional_subversion_salience'] + 0.001\n        df['systemic_health_salience_total'] = df['systemic_continuity_salience'] + df['systemic_replacement_salience'] + 0.001\n        df['total_constitutional_salience'] = df['procedural_health_salience_total'] + df['institutional_health_salience_total'] + df['systemic_health_salience_total']\n\n        # Axis-level health indices\n        df['procedural_health_index'] = ((df['procedural_legitimacy_raw'] * df['procedural_legitimacy_salience']) - (df['procedural_rejection_raw'] * df['procedural_rejection_salience'])) / df['procedural_health_salience_total']\n        df['institutional_health_index'] = ((df['institutional_respect_raw'] * df['institutional_respect_salience']) - (df['institutional_subversion_raw'] * df['institutional_subversion_salience'])) / df['institutional_health_salience_total']\n        df['systemic_health_index'] = ((df['systemic_continuity_raw'] * df['systemic_continuity_salience']) - (df['systemic_replacement_raw'] * df['systemic_replacement_salience'])) / df['systemic_health_salience_total']\n\n        # Summary metrics\n        numerator_chi = (df['procedural_health_index'] * df['procedural_health_salience_total']) + \\\n                        (df['institutional_health_index'] * df['institutional_health_salience_total']) + \\\n                        (df['systemic_health_index'] * df['systemic_health_salience_total'])\n        df['constitutional_health_index'] = numerator_chi / df['total_constitutional_salience']\n\n        numerator_cpi = (df['procedural_rejection_raw'] * df['procedural_rejection_salience']) + \\\n                        (df['institutional_subversion_raw'] * df['institutional_subversion_salience']) + \\\n                        (df['systemic_replacement_raw'] * df['systemic_replacement_salience'])\n        df['constitutional_pathology_index'] = numerator_cpi / df['total_constitutional_salience']\n        \n        return df\n\n    except (KeyError, TypeError, Exception):\n        return None\n\ndef summarize_constitutional_health(data, **kwargs):\n    \"\"\"\n    Provides descriptive statistics for key constitutional health indices.\n\n    This function first calculates the derived metrics from the raw scores, then computes\n    summary statistics (mean, std, min, 25%, 50%, 75%, max) for the three axis-level\n    indices and the two main summary indices (Constitutional Health and Pathology).\n    This addresses Research Question 1 by revealing overall patterns in the dataset.\n\n    Args:\n        data (pd.DataFrame): DataFrame with raw and salience scores.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each key index,\n              or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        if data is None or data.empty:\n            return None\n\n        # First, calculate all derived metrics using the dedicated function\n        data_with_metrics = calculate_derived_metrics(data)\n        if data_with_metrics is None:\n            return None\n\n        index_cols = [\n            'constitutional_health_index',\n            'constitutional_pathology_index',\n            'procedural_health_index',\n            'institutional_health_index',\n            'systemic_health_index'\n        ]\n        \n        if not all(col in data_with_metrics.columns for col in index_cols):\n            return None\n\n        summary = data_with_metrics[index_cols].describe()\n        \n        # Convert to a nested dictionary for a clean JSON output\n        return summary.to_dict()\n\n    except Exception:\n        return None\n\ndef analyze_dimensional_variation(data, **kwargs):\n    \"\"\"\n    Analyzes the variation across each of the six base constitutional dimensions.\n\n    This function calculates the mean, standard deviation, and range (max - min) for\n    both the raw scores and salience scores of each of the six fundamental dimensions.\n    The results are sorted by the standard deviation of the raw scores to highlight\n    which dimensions are most variable in the dataset, directly addressing\n    Research Question 2.\n\n    Args:\n        data (pd.DataFrame): DataFrame with raw and salience scores.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary detailing the variation metrics for each dimension,\n              or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        if data is None or data.empty:\n            return None\n\n        dimensions = [\n            \"procedural_legitimacy\", \"procedural_rejection\",\n            \"institutional_respect\", \"institutional_subversion\",\n            \"systemic_continuity\", \"systemic_replacement\"\n        ]\n        \n        results = {}\n        for dim in dimensions:\n            raw_col = f\"{dim}_raw\"\n            sal_col = f\"{dim}_salience\"\n            \n            if raw_col not in data.columns or sal_col not in data.columns:\n                continue\n\n            results[dim] = {\n                'raw_score_mean': data[raw_col].mean(),\n                'raw_score_std': data[raw_col].std(),\n                'raw_score_range': data[raw_col].max() - data[raw_col].min(),\n                'salience_mean': data[sal_col].mean(),\n                'salience_std': data[sal_col].std(),\n                'salience_range': data[sal_col].max() - data[sal_col].min(),\n            }\n        \n        if not results:\n            return None\n            \n        # Sort results by standard deviation of raw scores to see most variant dimensions\n        sorted_results = dict(sorted(results.items(), key=lambda item: item[1]['raw_score_std'], reverse=True))\n        \n        return sorted_results\n\n    except Exception:\n        return None\n\ndef compare_speech_contexts(data, **kwargs):\n    \"\"\"\n    Compares constitutional health metrics across different speech contexts.\n\n    This function addresses Research Question 3 by analyzing how constitutional health\n    varies by speech context (e.g., 'Inaugural', 'SOTU', 'Joint Session'). Since the\n    corpus manifest is unavailable, it extracts context from filenames as a fallback.\n    It then groups the data by this context and calculates the mean Constitutional\n    Health Index and Constitutional Pathology Index for each.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis scores and a 'document_name' column.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary with mean health and pathology scores for each identified context,\n              or None if context cannot be determined or data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import re\n\n    def _extract_context_from_filename(filename):\n        \"\"\"Helper to extract context from filename as a fallback.\"\"\"\n        filename_lower = filename.lower()\n        if 'inaugural' in filename_lower:\n            return 'Inaugural'\n        if 'sotu' in filename_lower or 'state of the union' in filename_lower:\n            return 'SOTU'\n        if 'joint_session' in filename_lower or 'joint session' in filename_lower:\n            return 'Joint Session'\n        return 'Unknown'\n\n    try:\n        if data is None or data.empty or 'document_name' not in data.columns:\n            return None\n\n        data_with_metrics = calculate_derived_metrics(data)\n        if data_with_metrics is None:\n            return None\n\n        # NOTE: Per instructions, using filename parsing as a fallback due to missing corpus manifest.\n        data_with_metrics['context'] = data_with_metrics['document_name'].apply(_extract_context_from_filename)\n        \n        # Filter out documents where context could not be identified\n        context_data = data_with_metrics[data_with_metrics['context'] != 'Unknown']\n        if context_data.empty:\n            return {\"message\": \"No known speech contexts (Inaugural, SOTU, Joint Session) could be identified from filenames.\"}\n\n        # Group by the extracted context and calculate mean scores\n        context_summary = context_data.groupby('context')[[\n            'constitutional_health_index',\n            'constitutional_pathology_index'\n        ]].mean()\n\n        return context_summary.to_dict('index')\n\n    except Exception:\n        return None\n\ndef identify_rhetorical_correlates(data, **kwargs):\n    \"\"\"\n    Calculates the correlation matrix for salience-weighted constitutional dimensions.\n\n    This function investigates the relationships between different rhetorical dimensions,\n    addressing Research Question 4. It first computes a salience-weighted score for each\n    of the six base dimensions (raw_score * salience). It then calculates a Pearson\n    correlation matrix for these weighted scores to reveal which rhetorical strategies\n    tend to co-occur (e.g., if institutional subversion is correlated with procedural\n    rejection).\n\n    Args:\n        data (pd.DataFrame): DataFrame with raw and salience scores.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary representing the correlation matrix, or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty:\n            return None\n\n        df = data.copy()\n        dimensions = [\n            \"procedural_legitimacy\", \"procedural_rejection\",\n            \"institutional_respect\", \"institutional_subversion\",\n            \"systemic_continuity\", \"systemic_replacement\"\n        ]\n        \n        weighted_scores = pd.DataFrame()\n        for dim in dimensions:\n            raw_col = f\"{dim}_raw\"\n            sal_col = f\"{dim}_salience\"\n            if raw_col not in df.columns or sal_col not in df.columns:\n                return None # Missing required columns\n            \n            weighted_col_name = f\"{dim}_weighted\"\n            weighted_scores[weighted_col_name] = df[raw_col] * df[sal_col]\n        \n        if weighted_scores.empty:\n            return None\n\n        correlation_matrix = weighted_scores.corr(method='pearson')\n        \n        # Replace NaN with None for clean JSON output\n        correlation_matrix = correlation_matrix.where(pd.notnull(correlation_matrix), None)\n\n        return correlation_matrix.to_dict()\n\n    except Exception:\n        return None\n\ndef detect_constitutional_anomalies(data, **kwargs):\n    \"\"\"\n    Identifies documents with anomalous constitutional health or pathology scores.\n\n    This function addresses Research Question 5 by detecting outliers. It calculates\n    the z-scores for the 'constitutional_health_index' and 'constitutional_pathology_index'.\n    Documents with an absolute z-score greater than a specified threshold (default 2.0)\n    are flagged as anomalies. This helps identify speeches that are statistically\n    unusual compared to the rest of the corpus.\n\n    Args:\n        data (pd.DataFrame): DataFrame with analysis scores.\n        **kwargs:\n            threshold (float): The z-score threshold for identifying an anomaly. Default is 2.0.\n\n    Returns:\n        dict: A dictionary containing lists of documents flagged as 'health_anomalies'\n              and 'pathology_anomalies', or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data is None or data.empty or len(data) < 3:\n            return None # Not enough data for meaningful z-score calculation\n\n        threshold = float(kwargs.get('threshold', 2.0))\n\n        data_with_metrics = calculate_derived_metrics(data)\n        if data_with_metrics is None:\n            return None\n\n        # Calculate Z-scores for the two main indices\n        for col in ['constitutional_health_index', 'constitutional_pathology_index']:\n            mean = data_with_metrics[col].mean()\n            std = data_with_metrics[col].std()\n            if std > 0:\n                data_with_metrics[f'{col}_zscore'] = (data_with_metrics[col] - mean) / std\n            else:\n                data_with_metrics[f'{col}_zscore'] = 0 # No variation\n\n        # Identify anomalies\n        health_anomalies = data_with_metrics[data_with_metrics['constitutional_health_index_zscore'].abs() > threshold]\n        pathology_anomalies = data_with_metrics[data_with_metrics['constitutional_pathology_index_zscore'].abs() > threshold]\n\n        results = {\n            'health_anomalies': health_anomalies[['document_name', 'constitutional_health_index', 'constitutional_health_index_zscore']].to_dict('records'),\n            'pathology_anomalies': pathology_anomalies[['document_name', 'constitutional_pathology_index', 'constitutional_pathology_index_zscore']].to_dict('records')\n        }\n        \n        return results\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}