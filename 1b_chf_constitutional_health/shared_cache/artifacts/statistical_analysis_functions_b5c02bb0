{
  "status": "success",
  "functions_generated": 12,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 31631,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: presidential_sotu_constitutional_health_trends\nDescription: Statistical analysis experiment\nGenerated: 2025-08-28T19:33:59.096171+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef _add_metadata_columns(data):\n    \"\"\"\n    Internal helper to add 'administration' and 'speech_type' columns.\n\n    This function maps document names to administrations and speech types based on\n    pre-defined patterns derived from the experiment's corpus manifest. This is a\n    critical step for grouping data for statistical analysis.\n\n    Args:\n        data (pd.DataFrame): The input dataframe with a 'document_name' column.\n\n    Returns:\n        pd.DataFrame: The dataframe with added 'administration' and 'speech_type' columns.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    if 'document_name' not in data.columns:\n        raise ValueError(\"Input DataFrame must contain a 'document_name' column.\")\n\n    conditions_admin = [\n        data['document_name'].str.contains('Bush_HW', case=False),\n        data['document_name'].str.contains('Clinton', case=False),\n        data['document_name'].str.contains('Bush_W|Bush_Inaugural_2005', case=False),\n        data['document_name'].str.contains('Obama', case=False),\n        data['document_name'].str.contains('Trump', case=False),\n        data['document_name'].str.contains('Biden|joseph_r_biden_jr', case=False)\n    ]\n    admins = ['Bush H.W.', 'Clinton', 'Bush W.', 'Obama', 'Trump', 'Biden']\n    data['administration'] = np.select(conditions_admin, admins, default='Unknown')\n\n    conditions_speech = [\n        data['document_name'].str.contains('SOTU', case=False),\n        data['document_name'].str.contains('Inaugural', case=False),\n        data['document_name'].str.contains('Joint_Session|joint_session', case=False)\n    ]\n    speech_types = ['SOTU', 'Inaugural', 'Joint Session']\n    data['speech_type'] = np.select(conditions_speech, speech_types, default='Other')\n\n    return data\n\ndef _calculate_derived_metrics(data):\n    \"\"\"\n    Internal helper to calculate derived metrics from the framework specification.\n\n    This function computes the axis-level and summary health indices based on the\n    formulas provided in the Constitutional Health Framework v10.0. It uses the\n    raw score and salience for each of the six primary dimensions. A small epsilon\n    (0.001) is added to denominators to prevent division by zero.\n\n    Args:\n        data (pd.DataFrame): DataFrame with raw and salience columns.\n\n    Returns:\n        pd.DataFrame: DataFrame with added derived metric columns.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # Intermediate salience totals\n        data['procedural_health_salience_total'] = data['procedural_legitimacy_salience'] + data['procedural_rejection_salience'] + 0.001\n        data['institutional_health_salience_total'] = data['institutional_respect_salience'] + data['institutional_subversion_salience'] + 0.001\n        data['systemic_health_salience_total'] = data['systemic_continuity_salience'] + data['systemic_replacement_salience'] + 0.001\n        data['total_constitutional_salience'] = data['procedural_health_salience_total'] + data['institutional_health_salience_total'] + data['systemic_health_salience_total']\n\n        # Axis-level health indices\n        data['procedural_health_index'] = ((data['procedural_legitimacy_raw'] * data['procedural_legitimacy_salience']) - (data['procedural_rejection_raw'] * data['procedural_rejection_salience'])) / data['procedural_health_salience_total']\n        data['institutional_health_index'] = ((data['institutional_respect_raw'] * data['institutional_respect_salience']) - (data['institutional_subversion_raw'] * data['institutional_subversion_salience'])) / data['institutional_health_salience_total']\n        data['systemic_health_index'] = ((data['systemic_continuity_raw'] * data['systemic_continuity_salience']) - (data['systemic_replacement_raw'] * data['systemic_replacement_salience'])) / data['systemic_health_salience_total']\n\n        # Summary metrics\n        data['constitutional_health_index'] = ((data['procedural_health_index'] * data['procedural_health_salience_total']) + (data['institutional_health_index'] * data['institutional_health_salience_total']) + (data['systemic_health_index'] * data['systemic_health_salience_total'])) / data['total_constitutional_salience']\n        data['constitutional_pathology_index'] = ((data['procedural_rejection_raw'] * data['procedural_rejection_salience']) + (data['institutional_subversion_raw'] * data['institutional_subversion_salience']) + (data['systemic_replacement_raw'] * data['systemic_replacement_salience'])) / data['total_constitutional_salience']\n\n        return data\n    except KeyError as e:\n        raise KeyError(f\"Missing required column for derived metric calculation: {e}\")\n    except Exception as e:\n        raise RuntimeError(f\"An error occurred during derived metric calculation: {e}\")\n\ndef calculate_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for key constitutional health metrics, grouped by administration.\n\n    This function provides a summary of central tendency and dispersion (mean, std, min, max)\n    for the main health indices. It addresses Research Question 1 by revealing overall\n    patterns in constitutional health across presidential rhetoric.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing a table of descriptive statistics or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        if data.empty:\n            return None\n\n        # Ensure metadata and derived metrics are present\n        df = _add_metadata_columns(data.copy())\n        df = _calculate_derived_metrics(df)\n\n        metrics_to_describe = [\n            'constitutional_health_index',\n            'constitutional_pathology_index',\n            'procedural_health_index',\n            'institutional_health_index',\n            'systemic_health_index'\n        ]\n\n        if not all(metric in df.columns for metric in metrics_to_describe):\n            return {\"error\": \"One or more required metric columns are missing.\"}\n\n        # Group by administration and calculate descriptive stats\n        desc_stats = df.groupby('administration')[metrics_to_describe].agg(['mean', 'std', 'min', 'max', 'count']).reset_index()\n        desc_stats.columns = ['_'.join(col).strip() for col in desc_stats.columns.values]\n        desc_stats.rename(columns={'administration_': 'administration'}, inplace=True)\n\n        return {\"descriptive_statistics_by_administration\": desc_stats.to_dict('records')}\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred: {str(e)}\"}\n\ndef perform_anova_on_health_index(data, **kwargs):\n    \"\"\"\n    Performs a one-way ANOVA and Tukey HSD post-hoc test on the Constitutional Health Index.\n\n    This analysis tests the primary hypothesis (H1) that there are significant differences\n    in constitutional health scores across presidential administrations. It excludes the\n    Bush H.W. baseline (n=1). If the ANOVA is significant, a Tukey HSD test is performed\n    to identify which specific administration pairs differ (H2-H5).\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs: p_value_threshold (float, default 0.05).\n\n    Returns:\n        dict: Results of ANOVA and Tukey HSD tests, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import f_oneway\n    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n\n    p_value_threshold = kwargs.get('p_value_threshold', 0.05)\n\n    try:\n        if data.empty:\n            return None\n\n        df = _add_metadata_columns(data.copy())\n        df = _calculate_derived_metrics(df)\n\n        # Filter out administrations with insufficient data for ANOVA (n<2)\n        df_filtered = df[df['administration'] != 'Bush H.W.']\n        admin_counts = df_filtered['administration'].value_counts()\n        valid_admins = admin_counts[admin_counts >= 2].index\n        df_anova = df_filtered[df_filtered['administration'].isin(valid_admins)]\n\n        if df_anova['administration'].nunique() < 2:\n            return {\"error\": \"Insufficient number of administration groups (at least 2 with n>=2) for ANOVA.\"}\n\n        groups = [df_anova['constitutional_health_index'][df_anova['administration'] == admin] for admin in valid_admins]\n        \n        # ANOVA\n        f_stat, p_value = f_oneway(*groups)\n\n        # Effect Size (Eta-squared)\n        ss_between = sum(len(g) * (g.mean() - df_anova['constitutional_health_index'].mean())**2 for g in groups)\n        ss_total = sum((df_anova['constitutional_health_index'] - df_anova['constitutional_health_index'].mean())**2)\n        eta_squared = ss_between / ss_total if ss_total > 0 else 0\n\n        results = {\n            \"test\": \"One-Way ANOVA on Constitutional Health Index\",\n            \"groups_included\": valid_admins.tolist(),\n            \"f_statistic\": f_stat,\n            \"p_value\": p_value,\n            \"eta_squared\": eta_squared,\n            \"is_significant\": bool(p_value < p_value_threshold)\n        }\n\n        # Post-hoc Tukey HSD if ANOVA is significant\n        if results[\"is_significant\"]:\n            tukey_results = pairwise_tukeyhsd(endog=df_anova['constitutional_health_index'], groups=df_anova['administration'], alpha=p_value_threshold)\n            results[\"tukey_hsd_results\"] = str(tukey_results)\n            results[\"tukey_hsd_summary\"] = pd.read_html(tukey_results.summary().as_html(), header=0, index_col=0)[0].reset_index().to_dict('records')\n\n        return results\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred: {str(e)}\"}\n\ndef test_variance_homogeneity(data, **kwargs):\n    \"\"\"\n    Performs Levene's test for homogeneity of variances on the Constitutional Health Index.\n\n    This test addresses hypothesis H6, which posits that the Trump administration shows\n    significantly different variance in constitutional health scores compared to others.\n    A significant p-value suggests that the assumption of equal variances for ANOVA\n    is violated and that variance differs between groups.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs: p_value_threshold (float, default 0.05).\n\n    Returns:\n        dict: Results of Levene's test, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import levene\n\n    p_value_threshold = kwargs.get('p_value_threshold', 0.05)\n\n    try:\n        if data.empty:\n            return None\n\n        df = _add_metadata_columns(data.copy())\n        df = _calculate_derived_metrics(df)\n\n        df_filtered = df[df['administration'] != 'Bush H.W.']\n        admin_counts = df_filtered['administration'].value_counts()\n        valid_admins = admin_counts[admin_counts >= 2].index\n        df_levene = df_filtered[df_filtered['administration'].isin(valid_admins)]\n\n        if df_levene['administration'].nunique() < 2:\n            return {\"error\": \"Insufficient number of administration groups for Levene's test.\"}\n\n        groups = [df_levene['constitutional_health_index'][df_levene['administration'] == admin] for admin in valid_admins]\n        \n        w_stat, p_value = levene(*groups)\n\n        return {\n            \"test\": \"Levene's Test for Homogeneity of Variances\",\n            \"dependent_variable\": \"constitutional_health_index\",\n            \"groups_included\": valid_admins.tolist(),\n            \"w_statistic\": w_stat,\n            \"p_value\": p_value,\n            \"is_significant\": bool(p_value < p_value_threshold),\n            \"interpretation\": \"A significant result (p < 0.05) indicates that the variances are not equal across administrations.\"\n        }\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred: {str(e)}\"}\n\ndef analyze_dimensional_variation(data, **kwargs):\n    \"\"\"\n    Performs one-way ANOVA on each axis-level health index to find which dimensions vary most.\n\n    This analysis addresses Research Question 2 by identifying which constitutional axes\n    (Procedural, Institutional, Systemic) show the most significant variation across\n    administrations. This helps pinpoint the primary drivers of any overall differences\n    in constitutional health.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs: p_value_threshold (float, default 0.05).\n\n    Returns:\n        dict: A dictionary of ANOVA results for each axis-level index, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import f_oneway\n\n    p_value_threshold = kwargs.get('p_value_threshold', 0.05)\n\n    try:\n        if data.empty:\n            return None\n\n        df = _add_metadata_columns(data.copy())\n        df = _calculate_derived_metrics(df)\n\n        df_filtered = df[df['administration'] != 'Bush H.W.']\n        admin_counts = df_filtered['administration'].value_counts()\n        valid_admins = admin_counts[admin_counts >= 2].index\n        df_anova = df_filtered[df_filtered['administration'].isin(valid_admins)]\n\n        if df_anova['administration'].nunique() < 2:\n            return {\"error\": \"Insufficient number of administration groups for ANOVA.\"}\n\n        results = {}\n        indices = ['procedural_health_index', 'institutional_health_index', 'systemic_health_index']\n\n        for index in indices:\n            groups = [df_anova[index][df_anova['administration'] == admin] for admin in valid_admins]\n            f_stat, p_value = f_oneway(*groups)\n            results[index] = {\n                \"f_statistic\": f_stat,\n                \"p_value\": p_value,\n                \"is_significant\": bool(p_value < p_value_threshold)\n            }\n        \n        return {\n            \"test\": \"Dimensional Variation Analysis (ANOVA on Axis Indices)\",\n            \"groups_included\": valid_admins.tolist(),\n            \"results\": results\n        }\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred: {str(e)}\"}\n\ndef analyze_speech_context_effects(data, **kwargs):\n    \"\"\"\n    Analyzes the effect of speech context (e.g., SOTU, Inaugural) on constitutional health.\n\n    This function addresses Research Question 3 by performing a one-way ANOVA to determine\n    if the `constitutional_health_index` varies significantly across different speech types.\n    This helps understand if the context of the address influences its constitutional tenor.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs: p_value_threshold (float, default 0.05).\n\n    Returns:\n        dict: Results of the ANOVA on speech types, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import f_oneway\n\n    p_value_threshold = kwargs.get('p_value_threshold', 0.05)\n\n    try:\n        if data.empty:\n            return None\n\n        df = _add_metadata_columns(data.copy())\n        df = _calculate_derived_metrics(df)\n\n        speech_counts = df['speech_type'].value_counts()\n        valid_types = speech_counts[speech_counts >= 2].index\n        df_anova = df[df['speech_type'].isin(valid_types)]\n\n        if df_anova['speech_type'].nunique() < 2:\n            return {\"error\": \"Insufficient number of speech types (at least 2 with n>=2) for ANOVA.\"}\n\n        groups = [df_anova['constitutional_health_index'][df_anova['speech_type'] == s_type] for s_type in valid_types]\n        \n        f_stat, p_value = f_oneway(*groups)\n\n        return {\n            \"test\": \"Speech Context Effects Analysis (ANOVA on Speech Type)\",\n            \"dependent_variable\": \"constitutional_health_index\",\n            \"groups_included\": valid_types.tolist(),\n            \"f_statistic\": f_stat,\n            \"p_value\": p_value,\n            \"is_significant\": bool(p_value < p_value_threshold)\n        }\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred: {str(e)}\"}\n\ndef calculate_profile_distances(data, **kwargs):\n    \"\"\"\n    Calculates the Euclidean distance between mean administration profiles.\n\n    This analysis addresses hypothesis H7 by quantifying the similarity/dissimilarity\n    between administrations in a 6-dimensional space defined by the raw scores of the\n    framework. A smaller distance indicates a more similar constitutional health profile.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary with a matrix of pairwise Euclidean distances, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.spatial.distance import pdist, squareform\n    from itertools import combinations\n\n    try:\n        if data.empty:\n            return None\n\n        df = _add_metadata_columns(data.copy())\n        \n        dimension_cols = [\n            'procedural_legitimacy_raw', 'procedural_rejection_raw',\n            'institutional_respect_raw', 'institutional_subversion_raw',\n            'systemic_continuity_raw', 'systemic_replacement_raw'\n        ]\n        if not all(col in df.columns for col in dimension_cols):\n            return {\"error\": \"One or more required raw dimension columns are missing.\"}\n\n        # Exclude baseline\n        df_filtered = df[df['administration'] != 'Bush H.W.']\n        \n        # Calculate mean profile for each administration\n        mean_profiles = df_filtered.groupby('administration')[dimension_cols].mean()\n\n        if len(mean_profiles) < 2:\n            return {\"error\": \"Cannot calculate distances with fewer than two administration profiles.\"}\n\n        # Calculate pairwise Euclidean distances\n        dist_matrix = pd.DataFrame(squareform(pdist(mean_profiles, 'euclidean')),\n                                   columns=mean_profiles.index,\n                                   index=mean_profiles.index)\n\n        return {\n            \"test\": \"Euclidean Distance Between Administration Profiles\",\n            \"description\": \"Measures the distance between the mean 6-dimensional raw score profiles of each administration.\",\n            \"distance_matrix\": dist_matrix.to_dict()\n        }\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred: {str(e)}\"}\n\ndef perform_cluster_analysis(data, **kwargs):\n    \"\"\"\n    Performs hierarchical clustering on administration profiles.\n\n    This function visually groups administrations based on the similarity of their\n    6-dimensional constitutional health profiles, addressing hypothesis H7. The output\n    is a linkage matrix suitable for generating a dendrogram to visualize the clusters.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs: method (str, default 'ward'): The linkage algorithm to use.\n\n    Returns:\n        dict: A dictionary containing the linkage matrix and labels, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.cluster.hierarchy import linkage\n\n    method = kwargs.get('method', 'ward')\n\n    try:\n        if data.empty:\n            return None\n\n        df = _add_metadata_columns(data.copy())\n        \n        dimension_cols = [\n            'procedural_legitimacy_raw', 'procedural_rejection_raw',\n            'institutional_respect_raw', 'institutional_subversion_raw',\n            'systemic_continuity_raw', 'systemic_replacement_raw'\n        ]\n        if not all(col in df.columns for col in dimension_cols):\n            return {\"error\": \"One or more required raw dimension columns are missing.\"}\n\n        df_filtered = df[df['administration'] != 'Bush H.W.']\n        mean_profiles = df_filtered.groupby('administration')[dimension_cols].mean()\n\n        if len(mean_profiles) < 2:\n            return {\"error\": \"Cannot perform clustering with fewer than two administration profiles.\"}\n\n        # Perform hierarchical clustering\n        linkage_matrix = linkage(mean_profiles, method=method)\n\n        return {\n            \"test\": \"Hierarchical Cluster Analysis of Administration Profiles\",\n            \"description\": f\"Generates a linkage matrix for creating a dendrogram using the '{method}' method.\",\n            \"labels\": mean_profiles.index.tolist(),\n            \"linkage_matrix\": linkage_matrix.tolist()\n        }\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred: {str(e)}\"}\n\ndef calculate_internal_consistency(data, **kwargs):\n    \"\"\"\n    Calculates Cronbach's alpha to assess the internal consistency of dimension sets.\n\n    This function assesses the reliability of the 'health' and 'pathology' dimensions\n    as separate constructs. A high alpha value suggests that the items within each set\n    are measuring a similar underlying concept.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary with Cronbach's alpha for health and pathology dimensions, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # Lazy import for optional dependency\n        import pingouin as pg\n    except ImportError:\n        return {\"error\": \"The 'pingouin' package is required for this function. Please install it.\"}\n\n    try:\n        if data.empty:\n            return None\n\n        health_dims = [\n            'procedural_legitimacy_raw',\n            'institutional_respect_raw',\n            'systemic_continuity_raw'\n        ]\n        pathology_dims = [\n            'procedural_rejection_raw',\n            'institutional_subversion_raw',\n            'systemic_replacement_raw'\n        ]\n\n        if not all(col in data.columns for col in health_dims + pathology_dims):\n            return {\"error\": \"One or more required raw dimension columns are missing.\"}\n\n        # Drop rows with NaN in the relevant columns for calculation\n        health_data = data[health_dims].dropna()\n        pathology_data = data[pathology_dims].dropna()\n\n        if len(health_data) < 2 or len(pathology_data) < 2:\n            return {\"error\": \"Insufficient data to calculate Cronbach's alpha.\"}\n\n        alpha_health = pg.cronbach_alpha(data=health_data)\n        alpha_pathology = pg.cronbach_alpha(data=pathology_data)\n\n        return {\n            \"test\": \"Internal Consistency (Cronbach's Alpha)\",\n            \"health_dimensions_alpha\": {\n                \"alpha\": alpha_health[0],\n                \"confidence_interval_95\": alpha_health[1].tolist()\n            },\n            \"pathology_dimensions_alpha\": {\n                \"alpha\": alpha_pathology[0],\n                \"confidence_interval_95\": alpha_pathology[1].tolist()\n            }\n        }\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred: {str(e)}\"}\n\ndef generate_correlation_matrix(data, **kwargs):\n    \"\"\"\n    Generates a correlation matrix for the six raw constitutional health dimensions.\n\n    This analysis addresses Research Question 4 by exploring the relationships between\n    the different dimensions. It can reveal patterns, such as whether certain forms of\n    pathology tend to co-occur, or if support for one health dimension correlates with\n    support for others.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs: method (str, default 'pearson'): Correlation method ('pearson', 'kendall', 'spearman').\n\n    Returns:\n        dict: A dictionary containing the correlation matrix, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    method = kwargs.get('method', 'pearson')\n\n    try:\n        if data.empty:\n            return None\n\n        dimension_cols = [\n            'procedural_legitimacy_raw', 'procedural_rejection_raw',\n            'institutional_respect_raw', 'institutional_subversion_raw',\n            'systemic_continuity_raw', 'systemic_replacement_raw'\n        ]\n\n        if not all(col in data.columns for col in dimension_cols):\n            return {\"error\": \"One or more required raw dimension columns are missing.\"}\n\n        corr_matrix = data[dimension_cols].corr(method=method)\n\n        return {\n            \"test\": \"Correlation Matrix of Raw Dimensions\",\n            \"method\": method,\n            \"correlation_matrix\": corr_matrix.to_dict()\n        }\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred: {str(e)}\"}\n\ndef identify_outliers(data, **kwargs):\n    \"\"\"\n    Identifies outliers in the constitutional_health_index for each administration.\n\n    This function addresses Research Question 5 by detecting anomalous speeches that\n    deviate significantly from the central tendency of their respective administration.\n    It uses the Interquartile Range (IQR) method to define outliers.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs: iqr_multiplier (float, default 1.5): Multiplier for the IQR to define outlier bounds.\n\n    Returns:\n        dict: A dictionary listing outliers for each administration, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    iqr_multiplier = kwargs.get('iqr_multiplier', 1.5)\n\n    try:\n        if data.empty:\n            return None\n\n        df = _add_metadata_columns(data.copy())\n        df = _calculate_derived_metrics(df)\n\n        outliers_by_admin = {}\n        \n        for admin_name, group in df.groupby('administration'):\n            if len(group) < 5:  # IQR is less reliable for very small samples\n                continue\n\n            q1 = group['constitutional_health_index'].quantile(0.25)\n            q3 = group['constitutional_health_index'].quantile(0.75)\n            iqr = q3 - q1\n            lower_bound = q1 - (iqr_multiplier * iqr)\n            upper_bound = q3 + (iqr_multiplier * iqr)\n\n            outliers = group[\n                (group['constitutional_health_index'] < lower_bound) |\n                (group['constitutional_health_index'] > upper_bound)\n            ]\n\n            if not outliers.empty:\n                outliers_by_admin[admin_name] = outliers[['document_name', 'constitutional_health_index']].to_dict('records')\n\n        return {\n            \"test\": \"Outlier Identification (IQR Method)\",\n            \"dependent_variable\": \"constitutional_health_index\",\n            \"iqr_multiplier\": iqr_multiplier,\n            \"outliers\": outliers_by_admin\n        }\n\n    except Exception as e:\n        return {\"error\": f\"An error occurred: {str(e)}\"}\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}