{
  "status": "success",
  "functions_generated": 6,
  "output_file": "automatedderivedmetricsagent_functions.py",
  "module_size": 13507,
  "function_code_content": "\"\"\"\nAutomated Derived Metrics Functions\n===================================\n\nGenerated by AutomatedDerivedMetricsAgent for experiment: Test Experiment\nDescription: Test experiment for derived metrics\nGenerated: 2025-08-28T20:08:48.291908+00:00\n\nThis module contains automatically generated calculation functions for derived metrics\nas specified in the framework's natural language descriptions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom typing import Optional, Dict, Any\n\n\ndef calculate_identity_tension(data, **kwargs):\n    \"\"\"\n    Calculate identity_tension: Conflict between tribal dominance and individual dignity dimensions.\n\n    This calculation measures the absolute difference between the 'tribal_dominance'\n    and 'individual_dignity' scores. A higher value indicates greater tension\n    or conflict between these two dimensions of political identity.\n\n    Formula:\n    abs(tribal_dominance - individual_dignity)\n\n    Args:\n        data (pd.Series or pd.DataFrame): A single row of data.\n        **kwargs: Additional parameters (unused).\n\n    Returns:\n        float: The calculated identity tension, or None if the required\n               'tribal_dominance' or 'individual_dignity' columns are\n               missing, contain non-numeric data, or are NaN.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # This function requires 'tribal_dominance' and 'individual_dignity' columns.\n        # It is designed to be robust: if these columns are not present in the\n        # input data, it will gracefully return None.\n        tribal_dominance_col = 'tribal_dominance'\n        individual_dignity_col = 'individual_dignity'\n\n        # .get() safely retrieves a value, returning None if the key does not exist.\n        # This handles cases where the input data schema does not contain the required columns.\n        tribal_score = data.get(tribal_dominance_col)\n        dignity_score = data.get(individual_dignity_col)\n\n        # pd.isna() is a robust way to check for various missing value types\n        # (None, np.nan, etc.) for both required scores.\n        if pd.isna(tribal_score) or pd.isna(dignity_score):\n            return None\n\n        # Convert scores to float to ensure numeric calculation. This will raise\n        # a ValueError for non-numeric strings (e.g., 'high'), which is caught.\n        tribal_score = float(tribal_score)\n        dignity_score = float(dignity_score)\n\n        # The core calculation: absolute difference represents the conflict.\n        tension = abs(tribal_score - dignity_score)\n\n        return tension\n\n    except (KeyError, TypeError, ValueError):\n        # Handles errors from incorrect data types (e.g., non-numeric strings)\n        # or if trying to access a non-existent key on a dict-like object.\n        return None\n    except Exception:\n        # A general catch-all for any other unexpected errors during execution.\n        return None\n\ndef calculate_emotional_balance(data, **kwargs):\n    \"\"\"\n    Calculate emotional_balance: Difference between hope and fear scores\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        **kwargs: Additional parameters\n        \n    Returns:\n        float: Calculated result or None if insufficient data\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # The calculation requires 'hope' and 'fear' scores. These names are\n        # derived from the calculation's description.\n        hope_score = data['hope']\n        fear_score = data['fear']\n\n        # Ensure that both scores are present and not null/NaN before calculation.\n        if pd.isna(hope_score) or pd.isna(fear_score):\n            return None\n            \n        # Perform the calculation, ensuring inputs are treated as floats.\n        emotional_balance = float(hope_score) - float(fear_score)\n\n        # A final check to ensure the result is a finite number.\n        if not np.isfinite(emotional_balance):\n            return None\n            \n        return emotional_balance\n\n    except (KeyError, TypeError, ValueError):\n        # KeyError: If 'hope' or 'fear' columns are missing.\n        # TypeError/ValueError: If scores are not numeric or cannot be cast to float.\n        return None\n\ndef calculate_success_climate(data, **kwargs):\n    \"\"\"\n    Calculate success_climate: Difference between compersion and envy scores.\n\n    Formula: success_climate = compersion - envy\n\n    Args:\n        data (pd.Series): A single row of data from the analysis.\n        **kwargs: Additional keyword arguments (unused).\n\n    Returns:\n        float: The calculated score, or None if input data is missing or invalid.\n    \"\"\"\n    import pandas as pd\n\n    try:\n        # The calculation requires 'compersion' and 'envy' scores.\n        # .get() is used to safely access these values, returning None if a key is missing.\n        compersion_score = data.get('compersion')\n        envy_score = data.get('envy')\n\n        # pd.notna() checks for both None and np.nan to ensure both scores are present.\n        if pd.notna(compersion_score) and pd.notna(envy_score):\n            # Cast to float to ensure numeric operation and handle various data types.\n            return float(compersion_score) - float(envy_score)\n        else:\n            # If either score is missing, the calculation cannot be performed.\n            return None\n\n    except (TypeError, ValueError):\n        # This handles cases where scores are present but not numeric (e.g., strings).\n        return None\n    except Exception:\n        # A general catch-all for any other unexpected errors.\n        return None\n\ndef calculate_relational_climate(data, **kwargs):\n    \"\"\"\n    Calculate relational_climate: Difference between amity and enmity scores\n\n    Formula: relational_climate = amity - enmity\n    \n    Args:\n        data (pd.Series): A single row of data from the DataFrame.\n        **kwargs: Additional parameters (not used in this calculation).\n        \n    Returns:\n        float: The calculated relational climate score, or None if 'amity' or 'enmity' scores are missing or invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # The calculation requires 'amity' and 'enmity' scores.\n        # These columns are not in the provided sample data structure,\n        # but are required by the calculation's description.\n        amity_score = data['amity']\n        enmity_score = data['enmity']\n        \n        # Check for missing values (NaN) in the required scores\n        if pd.isna(amity_score) or pd.isna(enmity_score):\n            return None\n            \n        # Ensure scores are numeric before calculation\n        relational_climate = float(amity_score) - float(enmity_score)\n        \n        return relational_climate\n        \n    except (KeyError, TypeError, ValueError):\n        # KeyError: If 'amity' or 'enmity' columns do not exist.\n        # TypeError/ValueError: If scores are not convertible to float.\n        return None\n    except Exception:\n        # Catch any other unexpected errors during execution.\n        return None\n\ndef calculate_goal_orientation(data, **kwargs):\n    \"\"\"\n    Calculate goal_orientation: Difference between cohesive goals and fragmentative goals\n\n    Formula: cohesive_goals - fragmentative_goals\n    \n    Args:\n        data (pd.Series): A single row of data from a pandas DataFrame.\n        **kwargs: Additional parameters (not used in this calculation).\n        \n    Returns:\n        float: The difference between cohesive and fragmentative goals, or None if\n               the necessary data is missing or invalid.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        # The calculation requires 'cohesive_goals' and 'fragmentative_goals' columns.\n        # These are expected to be present in the 'data' Series passed to the function,\n        # even if not listed in the base table schema.\n        cohesive_goals = data['cohesive_goals']\n        fragmentative_goals = data['fragmentative_goals']\n\n        # Ensure that both values are valid numbers before performing the calculation.\n        if pd.isna(cohesive_goals) or pd.isna(fragmentative_goals):\n            return None\n\n        # Calculate the difference\n        goal_orientation = float(cohesive_goals) - float(fragmentative_goals)\n        \n        return goal_orientation\n\n    except (KeyError, TypeError, ValueError):\n        # KeyError: If 'cohesive_goals' or 'fragmentative_goals' columns are not found.\n        # TypeError: If the data types are not suitable for subtraction.\n        # ValueError: If conversion to float fails.\n        return None\n    except Exception:\n        # A general catch-all for any other unexpected errors.\n        return None\n\ndef calculate_overall_cohesion_index(data, **kwargs):\n    \"\"\"\n    Calculate overall_cohesion_index: Comprehensive measure combining all dimensions.\n\n    This index represents the overall health of constitutional discourse by calculating\n    a weighted average of the three core dimension scores. The weights are based on\n    the rhetorical emphasis given to each dimension in the discourse.\n\n    Formula:\n        OCI = (\u03a3(score * emphasis)) / \u03a3(emphasis)\n    \n    The sum is over the three dimensions:\n    1. Procedural Legitimacy\n    2. Institutional Respect\n    3. Systemic Continuity\n\n    Expected columns in the input data:\n    - procedural_legitimacy_score (float)\n    - institutional_respect_score (float)\n    - systemic_continuity_score (float)\n    - procedural_legitimacy_emphasis (float)\n    - institutional_respect_emphasis (float)\n    - systemic_continuity_emphasis (float)\n\n    Args:\n        data (pd.Series or pd.DataFrame): A single row of analysis data containing\n                                          the required score and emphasis columns.\n        **kwargs: Additional keyword arguments. Not used in this calculation.\n\n    Returns:\n        float: The calculated Overall Cohesion Index, or None if essential\n               data is missing, invalid, or if total emphasis is zero.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # Ensure data is a pandas Series for consistent access\n        if isinstance(data, pd.DataFrame):\n            if data.empty:\n                return None\n            data = data.iloc[0]\n\n        score_cols = [\n            'procedural_legitimacy_score',\n            'institutional_respect_score',\n            'systemic_continuity_score'\n        ]\n        weight_cols = [\n            'procedural_legitimacy_emphasis',\n            'institutional_respect_emphasis',\n            'systemic_continuity_emphasis'\n        ]\n\n        # Extract score and weight values\n        scores = data.get(score_cols)\n        weights = data.get(weight_cols)\n\n        # Check if all required columns were found and contain non-null data\n        if scores is None or weights is None or scores.isnull().any() or weights.isnull().any():\n            return None\n\n        # Calculate the total weight (denominator)\n        total_weight = weights.sum()\n\n        # Avoid division by zero if total emphasis is 0\n        if total_weight == 0:\n            return None\n\n        # Calculate the weighted sum of scores (numerator)\n        weighted_sum = (scores.values * weights.values).sum()\n\n        # Calculate and return the final index\n        overall_cohesion_index = weighted_sum / total_weight\n\n        return float(overall_cohesion_index)\n\n    except Exception:\n        # Catches any exceptions during processing, e.g., non-numeric data,\n        # attribute errors on wrong input types, etc., and returns None.\n        return None\n\ndef calculate_all_derived_metrics(data: pd.DataFrame) -> Dict[str, Optional[float]]:\n    \"\"\"\n    Calculate all derived metrics for the given dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        \n    Returns:\n        Dictionary mapping metric names to calculated values\n    \"\"\"\n    results = {}\n    \n    # Get all calculation functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith('calculate_') and \n            name not in ['calculate_all_derived_metrics', 'calculate_derived_metrics']):\n            try:\n                results[name.replace('calculate_', '')] = obj(data)\n            except Exception as e:\n                results[name.replace('calculate_', '')] = None\n                \n    return results\n\n\ndef calculate_derived_metrics(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Template-compatible wrapper function for derived metrics calculation.\n    \n    This function is called by the universal notebook template and returns\n    the original data with additional derived metric columns.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        \n    Returns:\n        DataFrame with original data plus derived metric columns\n    \"\"\"\n    # Calculate all derived metrics\n    derived_metrics = calculate_all_derived_metrics(data)\n    \n    # Create a copy of the original data\n    result = data.copy()\n    \n    # Add derived metrics as new columns\n    for metric_name, metric_value in derived_metrics.items():\n        if metric_value is not None:\n            # For scalar metrics, broadcast to all rows\n            result[metric_name] = metric_value\n        else:\n            # For failed calculations, use NaN\n            result[metric_name] = np.nan\n    \n    return result\n",
  "cached_with_code": true
}