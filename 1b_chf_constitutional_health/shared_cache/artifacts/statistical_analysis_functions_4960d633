{
  "status": "success",
  "functions_generated": 12,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 30116,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: presidential_sotu_constitutional_health_trends\nDescription: Statistical analysis experiment\nGenerated: 2025-08-28T03:17:28.113648+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef _preprocess_data_for_analysis(data):\n    \"\"\"\n    Internal helper to preprocess data, extract metadata, and calculate derived metrics.\n    This function is not meant to be called directly but is used by all analysis functions.\n    \n    Args:\n        data (pd.DataFrame): The raw analysis data.\n        \n    Returns:\n        pd.DataFrame: Preprocessed DataFrame with metadata and derived metrics, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import re\n\n    if data is None or data.empty:\n        return None\n\n    df = data.copy()\n\n    # --- Speaker/Metadata Identification ---\n    def extract_metadata(filename):\n        # Handle conflicting instructions: \"No corpus manifest found\" requires filename parsing.\n        filename_lower = filename.lower()\n        \n        # Administration\n        admin = None\n        if 'biden' in filename_lower or 'joseph_r_biden' in filename_lower:\n            admin = 'Biden'\n        elif 'trump' in filename_lower:\n            admin = 'Trump'\n        elif 'obama' in filename_lower:\n            admin = 'Obama'\n        elif 'clinton' in filename_lower:\n            admin = 'Clinton'\n        elif 'bush' in filename_lower:\n            year_match = re.search(r'(\\d{4})', filename)\n            if year_match:\n                year = int(year_match.group(1))\n                if year >= 2001:\n                    admin = 'Bush W.'\n                else:\n                    admin = 'Bush H.W.'\n        \n        # Speech Type\n        speech_type = 'Unknown'\n        if 'sotu' in filename_lower or 'state of the union' in filename_lower:\n            speech_type = 'SOTU'\n        elif 'inaugural' in filename_lower:\n            speech_type = 'Inaugural'\n        elif 'joint session' in filename_lower or 'joint_session' in filename_lower:\n            speech_type = 'Joint Session'\n            \n        return pd.Series([admin, speech_type])\n\n    df[['administration', 'speech_type']] = df['document_name'].apply(extract_metadata)\n    df.dropna(subset=['administration'], inplace=True)\n    \n    if df.empty:\n        return None\n\n    # --- Derived Metrics Calculation ---\n    # Use actual column names from data structure\n    dim_map = {\n        'procedural_legitimacy': ('procedural_legitimacy_raw', 'procedural_legitimacy_salience'),\n        'procedural_rejection': ('procedural_rejection_raw', 'procedural_rejection_salience'),\n        'institutional_respect': ('institutional_respect_raw', 'institutional_respect_salience'),\n        'institutional_subversion': ('institutional_subversion_raw', 'institutional_subversion_salience'),\n        'systemic_continuity': ('systemic_continuity_raw', 'systemic_continuity_salience'),\n        'systemic_replacement': ('systemic_replacement_raw', 'systemic_replacement_salience'),\n    }\n\n    # Intermediate salience totals\n    df['procedural_health_salience_total'] = df[dim_map['procedural_legitimacy'][1]] + df[dim_map['procedural_rejection'][1]] + 0.001\n    df['institutional_health_salience_total'] = df[dim_map['institutional_respect'][1]] + df[dim_map['institutional_subversion'][1]] + 0.001\n    df['systemic_health_salience_total'] = df[dim_map['systemic_continuity'][1]] + df[dim_map['systemic_replacement'][1]] + 0.001\n    df['total_constitutional_salience'] = df['procedural_health_salience_total'] + df['institutional_health_salience_total'] + df['systemic_health_salience_total']\n\n    # Axis-level health indices\n    df['procedural_health_index'] = ((df[dim_map['procedural_legitimacy'][0]] * df[dim_map['procedural_legitimacy'][1]]) - (df[dim_map['procedural_rejection'][0]] * df[dim_map['procedural_rejection'][1]])) / df['procedural_health_salience_total']\n    df['institutional_health_index'] = ((df[dim_map['institutional_respect'][0]] * df[dim_map['institutional_respect'][1]]) - (df[dim_map['institutional_subversion'][0]] * df[dim_map['institutional_subversion'][1]])) / df['institutional_health_salience_total']\n    df['systemic_health_index'] = ((df[dim_map['systemic_continuity'][0]] * df[dim_map['systemic_continuity'][1]]) - (df[dim_map['systemic_replacement'][0]] * df[dim_map['systemic_replacement'][1]])) / df['systemic_health_salience_total']\n\n    # Summary metrics\n    df['constitutional_health_index'] = ((df['procedural_health_index'] * df['procedural_health_salience_total']) + (df['institutional_health_index'] * df['institutional_health_salience_total']) + (df['systemic_health_index'] * df['systemic_health_salience_total'])) / df['total_constitutional_salience']\n    df['constitutional_pathology_index'] = ((df[dim_map['procedural_rejection'][0]] * df[dim_map['procedural_rejection'][1]]) + (df[dim_map['institutional_subversion'][0]] * df[dim_map['institutional_subversion'][1]]) + (df[dim_map['systemic_replacement'][0]] * df[dim_map['systemic_replacement'][1]])) / df['total_constitutional_salience']\n    \n    return df\n\ndef calculate_derived_metrics_and_metadata(data, **kwargs):\n    \"\"\"\n    Preprocesses the raw data to extract metadata and calculate all derived metrics from the framework.\n    This function is useful for inspecting the data that will be used in subsequent statistical tests.\n    \n    Args:\n        data: pandas DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n        \n    Returns:\n        pd.DataFrame: The processed DataFrame with metadata and all derived metrics, or None if an error occurs.\n    \"\"\"\n    import pandas as pd\n    \n    try:\n        processed_data = _preprocess_data_for_analysis(data)\n        if processed_data is None:\n            return None\n        return processed_data\n    except Exception:\n        return None\n\ndef calculate_descriptive_stats_by_admin(data, **kwargs):\n    \"\"\"\n    Calculates descriptive statistics for key constitutional health metrics, grouped by administration.\n    This addresses RQ1: \"What patterns emerge in constitutional health across presidential rhetoric?\"\n    \n    Args:\n        data: pandas DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n        \n    Returns:\n        dict: A dictionary of pandas DataFrames (as JSON) for each key metric, or None if insufficient data.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    try:\n        df = _preprocess_data_for_analysis(data)\n        if df is None or 'administration' not in df.columns:\n            return None\n\n        metrics_to_describe = [\n            'constitutional_health_index',\n            'constitutional_pathology_index',\n            'procedural_health_index',\n            'institutional_health_index',\n            'systemic_health_index'\n        ]\n        \n        results = {}\n        for metric in metrics_to_describe:\n            if metric in df.columns:\n                desc_stats = df.groupby('administration')[metric].describe()\n                results[metric] = desc_stats.to_dict('index')\n        \n        return results if results else None\n        \n    except Exception:\n        return None\n\ndef perform_anova_on_health_index(data, **kwargs):\n    \"\"\"\n    Performs a one-way ANOVA to test for significant differences in the constitutional_health_index\n    across presidential administrations. It also calculates eta-squared for effect size and runs\n    a Tukey HSD post-hoc test for pairwise comparisons. This addresses H1, H2, H3, H4, H5.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data.\n        **kwargs: Additional parameters (e.g., alpha for significance level).\n        \n    Returns:\n        dict: A dictionary with ANOVA results, effect size, and Tukey HSD results, or None if the test cannot be run.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from scipy.stats import f_oneway\n    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n\n    try:\n        df = _preprocess_data_for_analysis(data)\n        if df is None or 'administration' not in df.columns or 'constitutional_health_index' not in df.columns:\n            return None\n\n        alpha = kwargs.get('alpha', 0.05)\n        \n        groups = df.groupby('administration')['constitutional_health_index'].apply(list)\n        \n        # Ensure we have at least 2 groups with sufficient data\n        valid_groups = [g for g in groups if len(g) > 1]\n        if len(valid_groups) < 2:\n            return {'error': 'Insufficient data: Need at least two administrations with more than one speech.'}\n\n        # ANOVA test\n        f_stat, p_value = f_oneway(*valid_groups)\n        \n        # Effect size (Eta-squared)\n        ss_between = sum(len(g) * (np.mean(g) - df['constitutional_health_index'].mean())**2 for g in valid_groups)\n        ss_total = sum((x - df['constitutional_health_index'].mean())**2 for x in df['constitutional_health_index'])\n        eta_squared = ss_between / ss_total if ss_total > 0 else 0\n\n        results = {\n            'anova_summary': {\n                'f_statistic': f_stat,\n                'p_value': p_value,\n                'is_significant': p_value < alpha,\n                'alpha': alpha\n            },\n            'effect_size': {\n                'eta_squared': eta_squared\n            },\n            'post_hoc_tukey_hsd': None\n        }\n\n        # Post-hoc Tukey HSD test\n        if p_value < alpha:\n            tukey_results = pairwise_tukeyhsd(endog=df['constitutional_health_index'], groups=df['administration'], alpha=alpha)\n            results['post_hoc_tukey_hsd'] = str(tukey_results)\n            \n        return results\n\n    except Exception as e:\n        return {'error': str(e)}\n\ndef test_variance_homogeneity(data, **kwargs):\n    \"\"\"\n    Performs Levene's test to check for homogeneity of variance in constitutional_health_index\n    scores across different administrations. This directly tests hypothesis H6.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data.\n        **kwargs: Additional parameters (e.g., alpha for significance level).\n        \n    Returns:\n        dict: A dictionary with Levene's test results, or None if the test cannot be run.\n    \"\"\"\n    import pandas as pd\n    from scipy.stats import levene\n\n    try:\n        df = _preprocess_data_for_analysis(data)\n        if df is None or 'administration' not in df.columns or 'constitutional_health_index' not in df.columns:\n            return None\n            \n        alpha = kwargs.get('alpha', 0.05)\n\n        groups = [group['constitutional_health_index'].values for name, group in df.groupby('administration')]\n        \n        # Levene's test requires at least 2 groups\n        if len(groups) < 2:\n            return {'error': 'Insufficient data: Need at least two administrations to compare variance.'}\n\n        # Filter out groups with no variance\n        valid_groups = [g for g in groups if np.var(g) > 0]\n        if len(valid_groups) < 2:\n             return {'error': 'Insufficient data: At least two groups must have variance > 0.'}\n\n        w_stat, p_value = levene(*valid_groups)\n\n        return {\n            'levene_test_summary': {\n                'w_statistic': w_stat,\n                'p_value': p_value,\n                'is_significant': p_value < alpha,\n                'conclusion': 'Variances are not equal (reject H0)' if p_value < alpha else 'Variances are equal (fail to reject H0)',\n                'alpha': alpha\n            }\n        }\n    except Exception as e:\n        return {'error': str(e)}\n\ndef analyze_dimensional_variation(data, **kwargs):\n    \"\"\"\n    Analyzes which constitutional dimensions show the most variation across administrations.\n    It performs a one-way ANOVA for each of the 6 base dimensions and 3 axis indices.\n    This addresses RQ2: \"Which constitutional dimensions show the most variation and why?\"\n    \n    Args:\n        data: pandas DataFrame containing the analysis data.\n        **kwargs: Additional parameters (e.g., alpha for significance level).\n        \n    Returns:\n        dict: A dictionary of ANOVA results for each dimension, sorted by significance.\n    \"\"\"\n    import pandas as pd\n    from scipy.stats import f_oneway\n\n    try:\n        df = _preprocess_data_for_analysis(data)\n        if df is None or 'administration' not in df.columns:\n            return None\n            \n        alpha = kwargs.get('alpha', 0.05)\n        \n        dimensions = [\n            'procedural_legitimacy_raw', 'procedural_rejection_raw',\n            'institutional_respect_raw', 'institutional_subversion_raw',\n            'systemic_continuity_raw', 'systemic_replacement_raw',\n            'procedural_health_index', 'institutional_health_index', 'systemic_health_index'\n        ]\n        \n        results = {}\n        \n        admin_groups = df.groupby('administration')\n        if len(admin_groups) < 2:\n            return {'error': 'Need at least two administrations for comparison.'}\n\n        for dim in dimensions:\n            if dim in df.columns:\n                groups = [group[dim].values for name, group in admin_groups]\n                \n                # Ensure groups are valid for ANOVA\n                valid_groups = [g for g in groups if len(g) > 0]\n                if len(valid_groups) < 2:\n                    continue\n\n                f_stat, p_value = f_oneway(*valid_groups)\n                results[dim] = {\n                    'f_statistic': f_stat,\n                    'p_value': p_value,\n                    'is_significant': p_value < alpha\n                }\n        \n        # Sort results by p-value to show most significant variations first\n        sorted_results = sorted(results.items(), key=lambda item: item[1]['p_value'])\n        return dict(sorted_results) if sorted_results else None\n\n    except Exception as e:\n        return {'error': str(e)}\n\ndef analyze_speech_context_effects(data, **kwargs):\n    \"\"\"\n    Analyzes the effect of speech context (SOTU vs. Inaugural vs. Joint Session) on\n    the constitutional_health_index using a one-way ANOVA.\n    This addresses RQ3: \"How do different speech contexts affect constitutional health?\"\n    \n    Args:\n        data: pandas DataFrame containing the analysis data.\n        **kwargs: Additional parameters (e.g., alpha for significance level).\n        \n    Returns:\n        dict: A dictionary with ANOVA results for speech context, or None if the test cannot be run.\n    \"\"\"\n    import pandas as pd\n    from scipy.stats import f_oneway\n\n    try:\n        df = _preprocess_data_for_analysis(data)\n        if df is None or 'speech_type' not in df.columns or 'constitutional_health_index' not in df.columns:\n            return None\n            \n        alpha = kwargs.get('alpha', 0.05)\n        \n        # Filter out 'Unknown' and groups with too few samples\n        df_filtered = df[df['speech_type'] != 'Unknown']\n        context_counts = df_filtered['speech_type'].value_counts()\n        valid_contexts = context_counts[context_counts > 1].index.tolist()\n        df_filtered = df_filtered[df_filtered['speech_type'].isin(valid_contexts)]\n\n        if len(valid_contexts) < 2:\n            return {'error': 'Insufficient data: Need at least two speech types with more than one speech.'}\n\n        groups = [group['constitutional_health_index'].values for name, group in df_filtered.groupby('speech_type')]\n        \n        f_stat, p_value = f_oneway(*groups)\n\n        return {\n            'anova_summary': {\n                'f_statistic': f_stat,\n                'p_value': p_value,\n                'is_significant': p_value < alpha,\n                'compared_groups': valid_contexts,\n                'alpha': alpha\n            }\n        }\n    except Exception as e:\n        return {'error': str(e)}\n\ndef calculate_correlation_matrix(data, **kwargs):\n    \"\"\"\n    Calculates the Pearson correlation matrix for the 6 raw dimensions and 5 key derived metrics.\n    This helps explore relationships between dimensions (related to RQ4).\n    \n    Args:\n        data: pandas DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n        \n    Returns:\n        dict: A dictionary containing the correlation matrix as JSON, or None on error.\n    \"\"\"\n    import pandas as pd\n\n    try:\n        df = _preprocess_data_for_analysis(data)\n        if df is None:\n            return None\n\n        metrics_to_correlate = [\n            'procedural_legitimacy_raw', 'procedural_rejection_raw',\n            'institutional_respect_raw', 'institutional_subversion_raw',\n            'systemic_continuity_raw', 'systemic_replacement_raw',\n            'procedural_health_index', 'institutional_health_index', 'systemic_health_index',\n            'constitutional_health_index', 'constitutional_pathology_index'\n        ]\n        \n        # Ensure all columns exist\n        valid_metrics = [m for m in metrics_to_correlate if m in df.columns]\n        if len(valid_metrics) < 2:\n            return None\n\n        corr_matrix = df[valid_metrics].corr(method='pearson')\n        \n        return {'correlation_matrix': corr_matrix.to_dict()}\n        \n    except Exception:\n        return None\n\ndef calculate_administration_profile_distance(data, **kwargs):\n    \"\"\"\n    Calculates the Euclidean distance between the mean dimensional profiles of each administration.\n    The profile is defined by the 6 core constitutional dimensions. This addresses part of H7.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n        \n    Returns:\n        dict: A dictionary containing a distance matrix (as JSON), or None on error.\n    \"\"\"\n    import pandas as pd\n    from sklearn.metrics.pairwise import euclidean_distances\n\n    try:\n        df = _preprocess_data_for_analysis(data)\n        if df is None or 'administration' not in df.columns:\n            return None\n\n        dimensions = [\n            'procedural_legitimacy_raw', 'procedural_rejection_raw',\n            'institutional_respect_raw', 'institutional_subversion_raw',\n            'systemic_continuity_raw', 'systemic_replacement_raw'\n        ]\n        \n        # Check if all dimension columns exist\n        if not all(dim in df.columns for dim in dimensions):\n            return {'error': 'One or more required dimension columns are missing.'}\n\n        # Calculate mean profile for each administration\n        admin_profiles = df.groupby('administration')[dimensions].mean()\n        \n        if len(admin_profiles) < 2:\n            return {'error': 'Need at least two administrations to calculate distances.'}\n\n        # Calculate Euclidean distance matrix\n        dist_matrix = euclidean_distances(admin_profiles)\n        dist_df = pd.DataFrame(dist_matrix, index=admin_profiles.index, columns=admin_profiles.index)\n        \n        return {'euclidean_distance_matrix': dist_df.to_dict()}\n\n    except Exception as e:\n        return {'error': str(e)}\n\ndef perform_hierarchical_clustering(data, **kwargs):\n    \"\"\"\n    Performs hierarchical clustering on the mean administration profiles to visually group\n    administrations based on their constitutional rhetoric. This addresses part of H7.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n        \n    Returns:\n        dict: A dictionary containing the linkage matrix for dendrogram plotting, or None on error.\n    \"\"\"\n    import pandas as pd\n    from scipy.cluster.hierarchy import linkage, fcluster\n    from sklearn.preprocessing import StandardScaler\n\n    try:\n        df = _preprocess_data_for_analysis(data)\n        if df is None or 'administration' not in df.columns:\n            return None\n\n        dimensions = [\n            'procedural_legitimacy_raw', 'procedural_rejection_raw',\n            'institutional_respect_raw', 'institutional_subversion_raw',\n            'systemic_continuity_raw', 'systemic_replacement_raw'\n        ]\n        \n        if not all(dim in df.columns for dim in dimensions):\n            return {'error': 'One or more required dimension columns are missing.'}\n\n        admin_profiles = df.groupby('administration')[dimensions].mean()\n        \n        if len(admin_profiles) < 3:\n            return {'error': 'Need at least three administrations for meaningful clustering.'}\n            \n        # Standardize the data before clustering\n        scaler = StandardScaler()\n        scaled_profiles = scaler.fit_transform(admin_profiles)\n\n        # Perform hierarchical clustering using Ward's method\n        linkage_matrix = linkage(scaled_profiles, method='ward')\n        \n        # Create a serializable format for the linkage matrix\n        linkage_list = [list(row) for row in linkage_matrix]\n        \n        return {\n            'linkage_matrix': linkage_list,\n            'labels': list(admin_profiles.index),\n            'method': 'ward',\n            'note': 'This linkage matrix can be used to generate a dendrogram for visualization.'\n        }\n\n    except Exception as e:\n        return {'error': str(e)}\n\ndef calculate_scale_reliability(data, **kwargs):\n    \"\"\"\n    Calculates the internal consistency (Cronbach's alpha) for the 'health' and 'pathology'\n    dimensions, treating them as two separate measurement scales. This addresses the reliability\n    assessment requirement.\n    \n    Args:\n        data: pandas DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n        \n    Returns:\n        dict: A dictionary with Cronbach's alpha for each scale, or None on error.\n    \"\"\"\n    import pandas as pd\n    import pingouin as pg\n\n    try:\n        df = _preprocess_data_for_analysis(data)\n        if df is None:\n            return None\n\n        health_dims = ['procedural_legitimacy_raw', 'institutional_respect_raw', 'systemic_continuity_raw']\n        pathology_dims = ['procedural_rejection_raw', 'institutional_subversion_raw', 'systemic_replacement_raw']\n        \n        if not all(d in df.columns for d in health_dims + pathology_dims):\n            return {'error': 'One or more required dimension columns are missing.'}\n\n        results = {}\n        \n        # Health Scale Reliability\n        if len(df[health_dims].dropna()) > 1:\n            alpha_health = pg.cronbach_alpha(data=df[health_dims])\n            results['health_scale_reliability'] = {\n                'cronbach_alpha': alpha_health[0],\n                'confidence_interval_95': list(alpha_health[1]),\n                'dimensions': health_dims\n            }\n            \n        # Pathology Scale Reliability\n        if len(df[pathology_dims].dropna()) > 1:\n            # Items may need to be reverse-scored if they are not all in the same direction.\n            # Here, all pathology items are in the same direction, so no reversal is needed.\n            alpha_pathology = pg.cronbach_alpha(data=df[pathology_dims])\n            results['pathology_scale_reliability'] = {\n                'cronbach_alpha': alpha_pathology[0],\n                'confidence_interval_95': list(alpha_pathology[1]),\n                'dimensions': pathology_dims\n            }\n            \n        return results if results else None\n\n    except Exception as e:\n        return {'error': str(e)}\n\ndef detect_outliers_by_administration(data, **kwargs):\n    \"\"\"\n    Detects outliers in constitutional_health_index for each administration using the IQR method.\n    An outlier is defined as any point below Q1 - 1.5*IQR or above Q3 + 1.5*IQR.\n    This addresses RQ5: \"Are there unexpected constitutional health patterns, outliers, or anomalies?\"\n    \n    Args:\n        data: pandas DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n        \n    Returns:\n        dict: A dictionary where keys are administrations and values are lists of outlier documents, or None on error.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        df = _preprocess_data_for_analysis(data)\n        if df is None or 'administration' not in df.columns or 'constitutional_health_index' not in df.columns:\n            return None\n\n        outlier_results = {}\n        \n        for admin, group in df.groupby('administration'):\n            if len(group) < 5:  # IQR is not robust for very small samples\n                continue\n                \n            Q1 = group['constitutional_health_index'].quantile(0.25)\n            Q3 = group['constitutional_health_index'].quantile(0.75)\n            IQR = Q3 - Q1\n            \n            lower_bound = Q1 - 1.5 * IQR\n            upper_bound = Q3 + 1.5 * IQR\n            \n            outliers = group[(group['constitutional_health_index'] < lower_bound) | (group['constitutional_health_index'] > upper_bound)]\n            \n            if not outliers.empty:\n                outlier_results[admin] = outliers[['document_name', 'constitutional_health_index']].to_dict('records')\n\n        return outlier_results if outlier_results else {'message': 'No outliers detected.'}\n\n    except Exception as e:\n        return {'error': str(e)}\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}