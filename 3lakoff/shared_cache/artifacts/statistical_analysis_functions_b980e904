{
  "status": "success",
  "functions_generated": 6,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 18000,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: lakoff_framing_experiment\nDescription: Statistical analysis experiment\nGenerated: 2025-08-29T18:53:51.723023+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef _create_ideology_mapping(data):\n    \"\"\"\n    Internal helper function to create an 'ideology' column based on document names.\n    This function implements the 'thin architecture' for metadata by creating a\n    grouping variable directly from filenames, as no formal corpus manifest is available.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with a 'document_name' column.\n\n    Returns:\n        pd.DataFrame: The DataFrame with an added 'ideology' column.\n    \"\"\"\n    import numpy as np\n\n    conditions = [\n        data['document_name'].str.startswith('conservative'),\n        data['document_name'].str.startswith('progressive'),\n        data['document_name'].str.startswith('libertarian'),\n        data['document_name'].str.startswith('centrist')\n    ]\n    choices = ['Conservative', 'Progressive', 'Libertarian', 'Centrist']\n    data['ideology'] = np.select(conditions, choices, default='Unknown')\n    return data\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates the derived metrics as specified in the Lakoff Framing Framework v10.0.\n    This function computes:\n    1.  strict_father_model_score: The average Strict Father orientation.\n    2.  family_model_coherence_index: The average extremity of scores (polarization).\n    3.  family_model_dominance: The overall balance between Strict Father and Nurturant Parent models.\n    4.  family_model_strategic_contradiction_index: The degree of strategic contradiction.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the raw and salience scores for each dimension.\n                                 Must include columns:\n                                 - authority_vs_empathy_raw\n                                 - competition_vs_cooperation_raw\n                                 - self_reliance_vs_interdependence_raw\n                                 - authority_vs_empathy_salience\n                                 - competition_vs_cooperation_salience\n                                 - self_reliance_vs_interdependence_salience\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pandas.DataFrame: The original DataFrame with added columns for each derived metric,\n                          or None if required columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        required_raw = [\n            \"authority_vs_empathy_raw\",\n            \"competition_vs_cooperation_raw\",\n            \"self_reliance_vs_interdependence_raw\"\n        ]\n        required_salience = [\n            \"authority_vs_empathy_salience\",\n            \"competition_vs_cooperation_salience\",\n            \"self_reliance_vs_interdependence_salience\"\n        ]\n        required_cols = required_raw + required_salience\n\n        if not all(col in data.columns for col in required_cols):\n            # Log this error in a real system\n            return None\n\n        df = data.copy()\n        auth_raw = df[\"authority_vs_empathy_raw\"]\n        comp_raw = df[\"competition_vs_cooperation_raw\"]\n        self_raw = df[\"self_reliance_vs_interdependence_raw\"]\n\n        auth_sal = df[\"authority_vs_empathy_salience\"]\n        comp_sal = df[\"competition_vs_cooperation_salience\"]\n        self_sal = df[\"self_reliance_vs_interdependence_salience\"]\n\n        # strict_father_model_score\n        df[\"strict_father_model_score\"] = (auth_raw + comp_raw + self_raw) / 3\n\n        # family_model_coherence_index\n        df[\"family_model_coherence_index\"] = (abs(auth_raw - 0.5) + abs(comp_raw - 0.5) + abs(self_raw - 0.5)) / 3\n\n        # family_model_dominance\n        df[\"family_model_dominance\"] = (auth_raw + comp_raw + self_raw) - 1.5\n\n        # family_model_strategic_contradiction_index\n        term1 = (np.minimum(auth_raw, 1 - auth_raw) * abs(auth_sal - 0.5))\n        term2 = (np.minimum(comp_raw, 1 - comp_raw) * abs(comp_sal - 0.5))\n        term3 = (np.minimum(self_raw, 1 - self_raw) * abs(self_sal - 0.5))\n        df[\"family_model_strategic_contradiction_index\"] = (term1 + term2 + term3) / 3\n\n        return df\n\n    except Exception:\n        # Log exception in a real system\n        return None\n\ndef descriptive_statistics_summary(data, **kwargs):\n    \"\"\"\n    Generates descriptive statistics for all core and derived metrics.\n    This function calculates the mean, standard deviation, min, and max for each\n    raw score, salience score, and calculated derived metric. It first calculates\n    the derived metrics if they are not already present.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing descriptive statistics for each metric,\n              or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n\n    try:\n        # First, ensure derived metrics are calculated\n        metrics_df = calculate_derived_metrics(data)\n        if metrics_df is None:\n            return None\n\n        if metrics_df.empty:\n            return None\n\n        metrics_to_describe = [\n            \"authority_vs_empathy_raw\", \"authority_vs_empathy_salience\",\n            \"competition_vs_cooperation_raw\", \"competition_vs_cooperation_salience\",\n            \"self_reliance_vs_interdependence_raw\", \"self_reliance_vs_interdependence_salience\",\n            \"strict_father_model_score\", \"family_model_coherence_index\",\n            \"family_model_dominance\", \"family_model_strategic_contradiction_index\"\n        ]\n\n        # Filter out any columns that might not exist, just in case\n        existing_metrics = [col for col in metrics_to_describe if col in metrics_df.columns]\n        \n        if not existing_metrics:\n            return None\n\n        # Calculate descriptive statistics\n        desc_stats = metrics_df[existing_metrics].describe().loc[['mean', 'std', 'min', 'max']]\n\n        return desc_stats.to_dict()\n\n    except Exception:\n        return None\n\ndef test_family_model_coherence(data, **kwargs):\n    \"\"\"\n    Tests the Family Model Coherence hypothesis (H1) by calculating a correlation matrix.\n    This function assesses the relationships between the three core dimensional raw scores\n    (Authority vs Empathy, Competition vs Cooperation, Self-Reliance vs Interdependence).\n    Lakoff's theory predicts positive correlations among these dimensions, indicating\n    that they cluster together to form a coherent 'Strict Father' model.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the raw scores for each dimension.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary representing the Pearson correlation matrix, or None if\n              data is insufficient or required columns are missing.\n    \"\"\"\n    import pandas as pd\n\n    try:\n        required_cols = [\n            \"authority_vs_empathy_raw\",\n            \"competition_vs_cooperation_raw\",\n            \"self_reliance_vs_interdependence_raw\"\n        ]\n\n        if not all(col in data.columns for col in required_cols):\n            return None\n\n        if len(data) < 3:\n            return None # Not enough data for meaningful correlation\n\n        correlation_df = data[required_cols]\n        \n        # Rename columns for clarity in the output\n        correlation_df.columns = [\"Authority\", \"Competition\", \"SelfReliance\"]\n        \n        correlation_matrix = correlation_df.corr(method='pearson')\n\n        return correlation_matrix.to_dict()\n\n    except Exception:\n        return None\n\ndef test_salience_intensity_alignment(data, **kwargs):\n    \"\"\"\n    Tests the Salience-Intensity Alignment hypothesis (H3).\n    This function calculates the Pearson correlation between the intensity (raw_score)\n    and salience (salience) for each of the three family model dimensions. The hypothesis\n    predicts a positive correlation, suggesting that as a moral dimension becomes more\n    intense in its framing, it also receives greater rhetorical emphasis.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing raw and salience scores.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing the Pearson correlation coefficient (r) and the\n              p-value for each dimension, or None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    from scipy.stats import pearsonr\n\n    try:\n        dimensions = {\n            \"authority_vs_empathy\": (\"authority_vs_empathy_raw\", \"authority_vs_empathy_salience\"),\n            \"competition_vs_cooperation\": (\"competition_vs_cooperation_raw\", \"competition_vs_cooperation_salience\"),\n            \"self_reliance_vs_interdependence\": (\"self_reliance_vs_interdependence_raw\", \"self_reliance_vs_interdependence_salience\")\n        }\n\n        results = {}\n\n        for dim_name, (raw_col, sal_col) in dimensions.items():\n            if raw_col not in data.columns or sal_col not in data.columns:\n                results[dim_name] = {\"error\": \"Missing required columns\"}\n                continue\n\n            # Drop rows with NaN in either column for this specific correlation\n            clean_data = data[[raw_col, sal_col]].dropna()\n\n            if len(clean_data) < 3:\n                results[dim_name] = {\"error\": \"Insufficient data for correlation\"}\n                continue\n\n            r, p_value = pearsonr(clean_data[raw_col], clean_data[sal_col])\n            results[dim_name] = {\n                \"correlation_coefficient\": r,\n                \"p_value\": p_value,\n                \"n_observations\": len(clean_data)\n            }\n\n        return results if results else None\n\n    except Exception:\n        return None\n\ndef analyze_strategic_deployment_by_ideology(data, **kwargs):\n    \"\"\"\n    Tests the Strategic Deployment hypothesis (H2) by comparing model scores across ideologies.\n    This function uses a one-way ANOVA to determine if there are statistically significant\n    differences in the mean scores of the core dimensions and the overall 'strict_father_model_score'\n    among different political ideologies. Ideology is inferred from document filenames.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary with ANOVA results (F-statistic, p-value) for each tested metric,\n              or None if data is insufficient for analysis.\n    \"\"\"\n    import pandas as pd\n    from scipy.stats import f_oneway\n\n    try:\n        # Step 1: Add ideology and derived metrics to the data\n        data_with_groups = _create_ideology_mapping(data)\n        analysis_df = calculate_derived_metrics(data_with_groups)\n\n        if analysis_df is None:\n            return {\"error\": \"Failed to calculate derived metrics.\"}\n\n        # Remove 'Unknown' ideology and groups with fewer than 2 members\n        analysis_df = analysis_df[analysis_df['ideology'] != 'Unknown']\n        group_counts = analysis_df['ideology'].value_counts()\n        valid_groups = group_counts[group_counts >= 2].index\n        analysis_df = analysis_df[analysis_df['ideology'].isin(valid_groups)]\n\n        if analysis_df['ideology'].nunique() < 2:\n            return {\"error\": \"Insufficient number of ideology groups for comparison (need at least 2).\"}\n\n        # Step 2: Define metrics to test\n        metrics_to_test = [\n            \"authority_vs_empathy_raw\",\n            \"competition_vs_cooperation_raw\",\n            \"self_reliance_vs_interdependence_raw\",\n            \"strict_father_model_score\"\n        ]\n        \n        results = {}\n\n        # Step 3: Perform ANOVA for each metric\n        for metric in metrics_to_test:\n            if metric not in analysis_df.columns:\n                results[metric] = {\"error\": f\"Metric column '{metric}' not found.\"}\n                continue\n\n            # Create a list of series, one for each ideology group\n            groups = [\n                analysis_df[metric][analysis_df['ideology'] == g].dropna()\n                for g in analysis_df['ideology'].unique()\n            ]\n            \n            # Ensure all groups have data\n            groups = [g for g in groups if not g.empty]\n\n            if len(groups) < 2:\n                results[metric] = {\"error\": \"Not enough groups with valid data for ANOVA.\"}\n                continue\n\n            f_stat, p_value = f_oneway(*groups)\n            results[metric] = {\n                \"f_statistic\": f_stat,\n                \"p_value\": p_value,\n                \"groups_compared\": analysis_df['ideology'].nunique()\n            }\n\n        # Step 4: Add group means for context\n        group_means = analysis_df.groupby('ideology')[metrics_to_test].mean().to_dict()\n        results['group_means'] = group_means\n\n        return results\n\n    except Exception:\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}