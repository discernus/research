{
  "run_metadata": {
    "experiment_name": "presidential_populism_emergence_study_enhanced",
    "run_timestamp": "20250805T200525Z",
    "framework_version": "../../frameworks/reference/flagship/pdaf_v7.3.md",
    "model_used": "vertex_ai/gemini-2.5-flash-lite",
    "total_artifacts": 153,
    "organized_artifacts": 80
  },
  "directory_structure": {
    "data/": "CSV files for external statistical analysis",
    "artifacts/analysis_plans/": "What the LLM planned to analyze",
    "artifacts/analysis_results/": "Raw analysis outputs from LLM",
    "artifacts/statistical_results/": "Mathematical computations and metrics",
    "artifacts/evidence/": "Curated quotes and supporting data",
    "artifacts/reports/": "Final synthesis outputs and reports",
    "artifacts/inputs/": "Framework, corpus, and experiment configuration",
    "technical/": "System logs and model interaction records"
  },
  "pipeline_stages": {
    "inputs_and_system": {
      "artifacts": 76,
      "total_size_mb": 1.345968246459961
    },
    "analysis": {
      "artifacts": 73,
      "total_size_mb": 0.46775341033935547
    },
    "evidence_curation": {
      "artifacts": 1,
      "total_size_mb": 0.02044200897216797
    },
    "synthesis": {
      "artifacts": 2,
      "total_size_mb": 0.08437442779541016
    },
    "statistical_computation": {
      "artifacts": 1,
      "total_size_mb": 0.058180809020996094
    }
  },
  "artifact_descriptions": {
    "raw_analysis_response_v6_0397fcc9": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_073cf5da": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_075a48be": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_095c3bc3": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_0b21acdd": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_139351cb": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_13ffbe4a": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_15bd7575": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_19592801": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_1f78415b": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_20d2c2e4": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_222c438e": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_241a85af": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_25135f08": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_28a0e7c1": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_2abbcce6": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_2cd72486": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_327f7031": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_362298f4": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_3b7e7c9f": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_3c430517": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_3d06202a": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_3f9b61f4": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_40ba3596": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_49430830": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_4973759d": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_4d444ff8": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_5103dbb6": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_53310435": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_5a25a4b5": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_623a7563": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_638d3a64": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_672987fe": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_711da52e": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_715ad741": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_81b0e117": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_85494332": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_8873a2a9": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_8f549df1": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_9635c2b5": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_973975eb": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_98c26039": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_9c52ecb1": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_a34ac8ae": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_a821bde0": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_aabe3b56": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_abd06d1e": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_abdf11d5": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_adb727d8": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_adc73d6f": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_aed68907": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_afcf640f": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_afec1356": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_b4e58282": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_b7924e75": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_bac0b37b": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_bce36536": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_bdee4327": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_c161db5c": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_c4906f5f": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_cb29e50a": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_d3f910b0": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_db760d9e": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_dc72a6d9": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_e268bedc": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_e37b636a": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_e7685c19": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_ee6fcc5c": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_f4a01997": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_f7c33584": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_f7ca1bbc": "Raw analysis response from LLM with scores and reasoning",
    "raw_analysis_response_v6_fdb014e5": "Raw analysis response from LLM with scores and reasoning",
    "analysis_plan_0a0d7ad0.md": "Analysis plan generated by LLM for systematic evaluation",
    "analysis_response_0a9e201a.json": "Artifact of type analysis_json_v6",
    "curated_evidence_2b47de52.json": "Highest-confidence evidence supporting findings",
    "pdaf_v7.3_68ac8695.md": "Analytical framework used for evaluation",
    "synthesis_report_2025-08-05_7dd1412b.md": "Synthesis report combining multiple analyses",
    "statistical_results_9b4b06c6.json": "Statistical computations and significance tests",
    "final_report_2025-08-05_9d936b97.md": "Final research report with findings and implications",
    "Trump_SOTU_2020_fbaf4dc2.txt": "Source document from research corpus"
  },
  "navigation_guide": {
    "primary_researcher": [
      "FINAL_REPORT.md",
      "data/scores.csv",
      "data/evidence.csv"
    ],
    "internal_reviewer": [
      "METHODOLOGY_SUMMARY.md",
      "STATISTICAL_SUMMARY.md"
    ],
    "replication_researcher": [
      "artifacts/",
      "technical/manifest.json",
      "README.md"
    ],
    "fraud_auditor": [
      "technical/manifest.json",
      "technical/logs/",
      "provenance.json"
    ],
    "llm_skeptic": [
      "technical/model_interactions/",
      "data/reliability_metrics.csv"
    ]
  }
}