{
  "status": "success",
  "functions_generated": 5,
  "output_file": "automatedstatisticalanalysisagent_functions.py",
  "module_size": 22296,
  "function_code_content": "\"\"\"\nAutomated Statistical Analysis Functions\n========================================\n\nGenerated by AutomatedStatisticalAnalysisAgent for experiment: simple_test_pdaf\nDescription: Statistical analysis experiment\nGenerated: 2025-08-27T13:59:37.605270+00:00\n\nThis module contains automatically generated statistical analysis functions\nfor comprehensive data analysis including ANOVA, correlations, reliability,\nand hypothesis testing as appropriate for the research questions.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom typing import Dict, Any, Optional, List, Tuple\nimport warnings\n\n# Suppress common statistical warnings for cleaner output\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\ndef _add_speaker_column(data):\n    \"\"\"\n    Internal helper function to add a 'speaker' column to the DataFrame.\n\n    This function attempts to identify a speaker from the 'document_name' column.\n    It is designed as a fallback since the prompt states \"No corpus manifest found.\"\n    The logic extracts a plausible name from the start of the filename.\n    For example, 'bernie_sanders_2025_speech.txt' becomes 'bernie_sanders'.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame with a 'document_name' column.\n\n    Returns:\n        pd.DataFrame: The DataFrame with an added 'speaker' column.\n    \"\"\"\n    if 'document_name' not in data.columns:\n        # If no document_name, cannot infer speaker. Add a placeholder.\n        data['speaker'] = 'unknown'\n        return data\n\n    def extract_speaker(filename):\n        try:\n            # Assumes format like 'speaker_name_...' or 'speaker-name_...'\n            # Handles names with one or two parts, e.g., \"trump\" or \"bernie_sanders\"\n            parts = Path(filename).stem.replace('-', '_').split('_')\n            if len(parts) > 1 and not parts[1].isdigit():\n                # Likely a two-part name like \"bernie_sanders\"\n                return f\"{parts[0]}_{parts[1]}\"\n            return parts[0]\n        except Exception:\n            return \"unknown\"\n\n    data['speaker'] = data['document_name'].apply(extract_speaker)\n    return data\n\ndef calculate_derived_metrics(data, **kwargs):\n    \"\"\"\n    Calculates all derived metrics as specified in the PDAF v10.0.0 framework.\n\n    This function computes the three strategic tension scores, the overall\n    Populist Strategic Contradiction Index (PSCI), and the four salience-weighted\n    indices. It adds these new metrics as columns to the input DataFrame.\n    This is a foundational function used by other analysis functions.\n\n    Methodology:\n    - Tension Scores: Calculated using the formula `min(ScoreA, ScoreB) * |SalienceA - SalienceB|`.\n    - PSCI: Calculated as the average of the three tension scores, using the full formula\n      from the framework for numerical stability.\n    - Salience-Weighted Indices: Calculated as the weighted average of raw scores,\n      where weights are the corresponding salience scores. A small epsilon (0.001) is\n      added to the denominator to prevent division by zero.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the raw analysis data with columns\n                                 matching the framework specification.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        pandas.DataFrame: The original DataFrame with added columns for each derived metric,\n                          or None if essential columns are missing.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        df = data.copy()\n\n        # Define column names for easier access and to match the spec\n        # NOTE: Using 'homogenous' as specified in the 'ACTUAL DATA STRUCTURE'\n        dim_cols = {\n            'manichaean': 'manichaean_people_elite_framing',\n            'crisis': 'crisis_restoration_narrative',\n            'sovereignty': 'popular_sovereignty_claims',\n            'anti_pluralist': 'anti_pluralist_exclusion',\n            'conspiracy': 'elite_conspiracy_systemic_corruption',\n            'authenticity': 'authenticity_vs_political_class',\n            'homogeneous': 'homogenous_people_construction',\n            'nationalist': 'nationalist_exclusion',\n            'economic': 'economic_populist_appeals'\n        }\n\n        raw_scores = {k: f\"{v}_raw\" for k, v in dim_cols.items()}\n        saliences = {k: f\"{v}_salience\" for k, v in dim_cols.items()}\n\n        # --- Tension Metrics ---\n        # Democratic-Authoritarian Tension\n        df['democratic_authoritarian_tension'] = np.minimum(df[raw_scores['sovereignty']], df[raw_scores['anti_pluralist']]) * \\\n                                                  np.abs(df[saliences['sovereignty']] - df[saliences['anti_pluralist']])\n\n        # Internal-External Focus Tension\n        df['internal_external_focus_tension'] = np.minimum(df[raw_scores['homogeneous']], df[raw_scores['nationalist']]) * \\\n                                                 np.abs(df[saliences['homogeneous']] - df[saliences['nationalist']])\n\n        # Crisis-Elite Attribution Tension\n        df['crisis_elite_attribution_tension'] = np.minimum(df[raw_scores['crisis']], df[raw_scores['conspiracy']]) * \\\n                                                  np.abs(df[saliences['crisis']] - df[saliences['conspiracy']])\n\n        # --- Populist Strategic Contradiction Index (PSCI) ---\n        # Calculated directly from base dimensions as per the spec for robustness\n        df['populist_strategic_contradiction_index'] = (df['democratic_authoritarian_tension'] +\n                                                       df['internal_external_focus_tension'] +\n                                                       df['crisis_elite_attribution_tension']) / 3\n\n        # --- Salience-Weighted Indices ---\n        epsilon = 0.001\n\n        # Core Populism\n        core_dims = ['manichaean', 'crisis', 'sovereignty', 'anti_pluralist']\n        core_numerator = sum(df[raw_scores[d]] * df[saliences[d]] for d in core_dims)\n        core_denominator = sum(df[saliences[d]] for d in core_dims) + epsilon\n        df['salience_weighted_core_populism_index'] = core_numerator / core_denominator\n\n        # Populism Mechanisms\n        mech_dims = ['conspiracy', 'authenticity', 'homogeneous']\n        mech_numerator = sum(df[raw_scores[d]] * df[saliences[d]] for d in mech_dims)\n        mech_denominator = sum(df[saliences[d]] for d in mech_dims) + epsilon\n        df['salience_weighted_populism_mechanisms_index'] = mech_numerator / mech_denominator\n\n        # Boundary Distinctions\n        bound_dims = ['nationalist', 'economic']\n        bound_numerator = sum(df[raw_scores[d]] * df[saliences[d]] for d in bound_dims)\n        bound_denominator = sum(df[saliences[d]] for d in bound_dims) + epsilon\n        df['salience_weighted_boundary_distinctions_index'] = bound_numerator / bound_denominator\n\n        # Overall Populism\n        all_dims = list(dim_cols.keys())\n        overall_numerator = sum(df[raw_scores[d]] * df[saliences[d]] for d in all_dims)\n        overall_denominator = sum(df[saliences[d]] for d in all_dims) + epsilon\n        df['salience_weighted_overall_populism_index'] = overall_numerator / overall_denominator\n        \n        # Clip values to be within [0, 1] range as a safeguard\n        for col in df.columns:\n            if 'index' in col or 'tension' in col:\n                df[col] = df[col].clip(0, 1)\n\n        return df\n\n    except KeyError as e:\n        # A required column was not found in the DataFrame\n        print(f\"Error: Missing required column for calculation: {e}\")\n        return None\n    except Exception as e:\n        print(f\"An unexpected error occurred in calculate_derived_metrics: {e}\")\n        return None\n\ndef get_descriptive_statistics(data, **kwargs):\n    \"\"\"\n    Provides descriptive statistics for all populist dimensions and derived metrics.\n\n    This function first calculates the derived metrics using the `calculate_derived_metrics`\n    function. It then computes and returns summary statistics (mean, std, min, 25%, 50%,\n    75%, max) for all raw score columns and all derived metric columns.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary containing two pandas DataFrames: 'raw_scores' and\n              'derived_metrics', with descriptive statistics for each. Returns\n              None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data.empty:\n            return None\n\n        # First, calculate derived metrics to ensure they are available\n        data_with_metrics = calculate_derived_metrics(data)\n        if data_with_metrics is None:\n            return None\n\n        # Select raw score columns\n        raw_score_cols = [col for col in data_with_metrics.columns if col.endswith('_raw')]\n        \n        # Select derived metric columns\n        derived_metric_cols = [\n            'democratic_authoritarian_tension',\n            'internal_external_focus_tension',\n            'crisis_elite_attribution_tension',\n            'populist_strategic_contradiction_index',\n            'salience_weighted_core_populism_index',\n            'salience_weighted_populism_mechanisms_index',\n            'salience_weighted_boundary_distinctions_index',\n            'salience_weighted_overall_populism_index'\n        ]\n\n        if not raw_score_cols or not all(c in data_with_metrics.columns for c in derived_metric_cols):\n            return None\n\n        # Calculate descriptive statistics\n        raw_stats = data_with_metrics[raw_score_cols].describe()\n        derived_stats = data_with_metrics[derived_metric_cols].describe()\n\n        return {\n            \"raw_scores_stats\": raw_stats.to_dict(),\n            \"derived_metrics_stats\": derived_stats.to_dict()\n        }\n\n    except Exception as e:\n        print(f\"An unexpected error occurred in get_descriptive_statistics: {e}\")\n        return None\n\ndef analyze_speaker_profiles(data, **kwargs):\n    \"\"\"\n    Analyzes and compares the populist discourse profiles of different speakers.\n\n    This function groups the data by speaker and calculates the average score for each\n    of the nine populist dimensions and all derived metrics. This allows for a direct\n    comparison of communication styles.\n\n    Methodology:\n    - Speaker Identification: As no corpus manifest is provided, speakers are identified\n      from the 'document_name' column using a helper function. This is a fallback\n      mechanism.\n    - Aggregation: The function calculates the mean of all raw scores and derived\n      metrics for each speaker.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary of speaker profiles, with speaker names as keys and their\n              average scores for all metrics as values. Returns None if data is\n              insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data.empty:\n            return None\n\n        # Calculate derived metrics and identify speakers\n        data_with_metrics = calculate_derived_metrics(data)\n        if data_with_metrics is None:\n            return None\n        \n        data_with_speakers = _add_speaker_column(data_with_metrics)\n\n        # Define columns to aggregate\n        raw_score_cols = [col for col in data_with_speakers.columns if col.endswith('_raw')]\n        derived_metric_cols = [\n            'democratic_authoritarian_tension',\n            'internal_external_focus_tension',\n            'crisis_elite_attribution_tension',\n            'populist_strategic_contradiction_index',\n            'salience_weighted_core_populism_index',\n            'salience_weighted_populism_mechanisms_index',\n            'salience_weighted_boundary_distinctions_index',\n            'salience_weighted_overall_populism_index'\n        ]\n        \n        all_metrics_cols = raw_score_cols + derived_metric_cols\n        \n        if not all(c in data_with_speakers.columns for c in all_metrics_cols):\n             return None\n\n        # Group by speaker and calculate the mean\n        speaker_profiles = data_with_speakers.groupby('speaker')[all_metrics_cols].mean()\n\n        return speaker_profiles.to_dict('index')\n\n    except Exception as e:\n        print(f\"An unexpected error occurred in analyze_speaker_profiles: {e}\")\n        return None\n\ndef analyze_strategic_tensions(data, **kwargs):\n    \"\"\"\n    Analyzes strategic tension patterns across the corpus and by speaker.\n\n    This function focuses on the three tension metrics and the overall Populist\n    Strategic Contradiction Index (PSCI). It provides both a corpus-wide average\n    and a breakdown by speaker to answer research questions about the strategic\n    coherence of populist messaging.\n\n    Methodology:\n    - Metrics: Uses the pre-calculated tension and PSCI metrics.\n    - Aggregation: Calculates the mean of these four metrics for the entire dataset\n      and for each individual speaker.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary with two keys: 'corpus_average_tensions' (a dict of overall\n              averages) and 'speaker_average_tensions' (a dict of speaker-specific\n              averages). Returns None if data is insufficient.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data.empty:\n            return None\n\n        data_with_metrics = calculate_derived_metrics(data)\n        if data_with_metrics is None:\n            return None\n        \n        data_with_speakers = _add_speaker_column(data_with_metrics)\n\n        tension_cols = [\n            'democratic_authoritarian_tension',\n            'internal_external_focus_tension',\n            'crisis_elite_attribution_tension',\n            'populist_strategic_contradiction_index'\n        ]\n        \n        if not all(c in data_with_speakers.columns for c in tension_cols):\n            return None\n\n        # Calculate corpus-wide average tensions\n        corpus_avg = data_with_speakers[tension_cols].mean().to_dict()\n\n        # Calculate speaker-specific average tensions\n        speaker_avg = data_with_speakers.groupby('speaker')[tension_cols].mean().to_dict('index')\n\n        return {\n            \"corpus_average_tensions\": corpus_avg,\n            \"speaker_average_tensions\": speaker_avg\n        }\n\n    except Exception as e:\n        print(f\"An unexpected error occurred in analyze_strategic_tensions: {e}\")\n        return None\n\ndef compare_ideological_styles(data, **kwargs):\n    \"\"\"\n    Compares populist conservative and progressive styles on key dimensions.\n\n    This function addresses the research question about how different ideological\n    styles use boundary construction and crisis narratives. It compares mean scores\n    for 'Nationalist Exclusion', 'Economic Populist Appeals', and 'Crisis-Restoration\n    Narrative'.\n\n    Methodology:\n    - Ideological Classification: Since no corpus manifest is provided, this function\n      uses a hardcoded mapping of speaker names (inferred from filenames) to\n      'progressive' or 'conservative' categories. This is an assumption based on\n      available data and should be adapted if a manifest becomes available.\n    - Comparison: It calculates the mean scores for the specified dimensions, grouped\n      by the assigned ideology.\n\n    Args:\n        data (pandas.DataFrame): DataFrame containing the raw analysis data.\n        **kwargs: Additional parameters (not used).\n\n    Returns:\n        dict: A dictionary of mean scores for the key dimensions, grouped by\n              ideology. Returns None if data is insufficient or no speakers\n              match the defined ideologies.\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    import json\n    import glob\n    from pathlib import Path\n\n    try:\n        if data.empty:\n            return None\n\n        # NOTE: This mapping is a necessary assumption due to the lack of a corpus manifest.\n        # It is based on common knowledge of the speakers in the sample data.\n        ideology_map = {\n            'bernie_sanders': 'progressive',\n            'alexandria_ocasio_cortez': 'progressive',\n            'steve_king': 'conservative',\n            'donald_trump': 'conservative', # Assuming this might appear\n            'john_mccain': 'conservative' # Included for contrast\n        }\n\n        data_with_speakers = _add_speaker_column(data)\n        data_with_speakers['ideology'] = data_with_speakers['speaker'].map(ideology_map)\n\n        # Filter for documents where an ideology could be assigned\n        ideological_data = data_with_speakers.dropna(subset=['ideology'])\n        if ideological_data.empty:\n            return {\"error\": \"No documents from speakers with a defined ideology found.\"}\n\n        comparison_dims = [\n            'nationalist_exclusion_raw',\n            'economic_populist_appeals_raw',\n            'crisis_restoration_narrative_raw'\n        ]\n        \n        if not all(c in ideological_data.columns for c in comparison_dims):\n            return None\n\n        # Group by ideology and calculate mean scores\n        comparison_results = ideological_data.groupby('ideology')[comparison_dims].mean()\n\n        return comparison_results.to_dict('index')\n\n    except Exception as e:\n        print(f\"An unexpected error occurred in compare_ideological_styles: {e}\")\n        return None\n\ndef run_complete_statistical_analysis(data: pd.DataFrame, alpha: float = 0.05) -> Dict[str, Any]:\n    \"\"\"\n    Run complete statistical analysis suite on the dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores\n        alpha: Significance level for hypothesis tests (default: 0.05)\n        \n    Returns:\n        Dictionary with all statistical analysis results\n    \"\"\"\n    results = {\n        'analysis_metadata': {\n            'timestamp': pd.Timestamp.now().isoformat(),\n            'sample_size': len(data),\n            'alpha_level': alpha,\n            'variables_analyzed': list(data.select_dtypes(include=[np.number]).columns)\n        }\n    }\n    \n    # Get all analysis functions from this module\n    import inspect\n    current_module = inspect.getmodule(inspect.currentframe())\n    \n    for name, obj in inspect.getmembers(current_module):\n        if (inspect.isfunction(obj) and \n            name.startswith(('calculate_', 'perform_', 'test_')) and \n            name != 'run_complete_statistical_analysis'):\n            try:\n                # Pass alpha parameter to functions that might need it\n                if 'alpha' in inspect.signature(obj).parameters:\n                    results[name] = obj(data, alpha=alpha)\n                else:\n                    results[name] = obj(data)\n            except Exception as e:\n                results[name] = {'error': f'Analysis failed: {str(e)}'}\n                \n    return results\n\n\ndef perform_statistical_analysis(data: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"\n    Template-compatible wrapper function for statistical analysis.\n    \n    This function is called by the universal notebook template and performs\n    comprehensive statistical analysis on the provided dataset.\n    \n    Args:\n        data: pandas DataFrame with dimension scores and derived metrics\n        \n    Returns:\n        Dictionary containing all statistical analysis results\n    \"\"\"\n    return run_complete_statistical_analysis(data)\n\n\ndef generate_statistical_summary_report(analysis_results: Dict[str, Any]) -> str:\n    \"\"\"\n    Generate a human-readable summary report from statistical analysis results.\n    \n    Args:\n        analysis_results: Results from run_complete_statistical_analysis()\n        \n    Returns:\n        String containing formatted statistical report\n    \"\"\"\n    report_lines = []\n    report_lines.append(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n    report_lines.append(\"=\" * 50)\n    \n    metadata = analysis_results.get('analysis_metadata', {})\n    report_lines.append(f\"Analysis Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n    report_lines.append(f\"Sample Size: {metadata.get('sample_size', 'Unknown')}\")\n    report_lines.append(f\"Alpha Level: {metadata.get('alpha_level', 'Unknown')}\")\n    report_lines.append(f\"Variables: {len(metadata.get('variables_analyzed', []))}\")\n    report_lines.append(\"\")\n    \n    # Summarize key findings\n    for analysis_name, result in analysis_results.items():\n        if analysis_name != 'analysis_metadata' and isinstance(result, dict):\n            if 'error' not in result:\n                report_lines.append(f\"{analysis_name.replace('_', ' ').title()}:\")\n                \n                # Extract key statistics based on analysis type\n                if 'p_value' in result:\n                    p_val = result['p_value']\n                    significance = \"significant\" if p_val < metadata.get('alpha_level', 0.05) else \"not significant\"\n                    report_lines.append(f\"  - p-value: {p_val:.4f} ({significance})\")\n                \n                if 'effect_size' in result:\n                    report_lines.append(f\"  - Effect size: {result['effect_size']:.4f}\")\n                \n                if 'correlation_matrix' in result:\n                    report_lines.append(f\"  - Correlation matrix generated with {len(result['correlation_matrix'])} variables\")\n                \n                if 'cronbach_alpha' in result:\n                    alpha_val = result['cronbach_alpha']\n                    reliability = \"excellent\" if alpha_val > 0.9 else \"good\" if alpha_val > 0.8 else \"acceptable\" if alpha_val > 0.7 else \"questionable\"\n                    report_lines.append(f\"  - Cronbach's \u03b1: {alpha_val:.3f} ({reliability})\")\n                \n                report_lines.append(\"\")\n            else:\n                report_lines.append(f\"{analysis_name}: ERROR - {result['error']}\")\n                report_lines.append(\"\")\n    \n    return \"\\n\".join(report_lines)\n",
  "cached_with_code": true
}